---
title: 'Adversarial Robustness of Deep Learning Models'
date: 2023-06-02
permalink: /posts/2023/06/intro/
tags:
  - AML
  - tutorial
---

The Good: The Age of Deep Neural Networks    
======

Deep Neural Networks (DNNs) have revolutionized various domains, showcasing remarkable achievements in computer vision [1], natural language processing [2], and speech processing [3]. These powerful models have surpassed human-level accuracy in tasks like image classification and language processing, propelling them into real-world applications. 
Nowadays, DNNs are ubiquitous, powering the technology we use every day, from voice assistants like Siri to self-driving Tesla cars.
Their impact is undeniable, transforming the way we interact with technology and reshaping industries across the globe.


The Bad: Current DNNs are not reliable and secure
====== 

While DNNs have achieved unprecedented success and widespread adoption, their reliability and security remain a concern. 
DNNs are known as black-box models, meaning that their internal workings are not transparent to users, and even their creators.
This lack of transparency makes it difficult to understand their behavior and trust their decisions.

For some low-stakes applications, such as fraud transactions detection or movie recommendation, it is not a big deal if the model makes a mistake. The consequences of incorrect predictions are not severe. However, for some high-stakes applications, such as autonomous driving, clinical diagnostics, auto-trading bots where the model's decisions can lead to life-threatening conditions or economic collapse, it is crucial to understand the model's behavior and trust its decisions. Just think about a situation, when you have a serious disease and a machine learning model predicts that you should take a specific medicine, would you trust the model's decision to take the medicine? 

| AI's application        | AI's risk                             | Consequence                 |
|-------------------------|---------------------------------------|-----------------------------|
|  Commercial Ads recommendation      | Matching "users-interest" incorrectly | Seeing non-interested Ads   |
| Auto-trading bot        | Triggering wrong signal               | Financial loss              |
| Autopilot in Tesla      | Mist-Classifying "Stop-Sign"          | Fatal crash                 |
| Autonomous drone swarms | Wrong targeting/attacking             | Fatal mistake - many deaths |

**Some examples of catastrophic failures of unreliable DNNs in real-life:**

- [Tesla behind eight-vehicle crash was in ‘full self-driving’ mode](https://www.theguardian.com/technology/2022/dec/22/tesla-crash-full-self-driving-mode-san-francisco)
- [IBM’s Watson supercomputer recommended ‘unsafe and incorrect’ cancer treatments](https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/)
- [A fake AI photo of a Pentagon blast wiped billions off Wall Street](https://www.smh.com.au/business/markets/how-a-fake-ai-photo-of-a-pentagon-blast-wiped-billions-off-wall-street-20230524-p5daqo.html)

**Conclusion: The more autonomous the AI system is, the more important it is to understand the model's behavior and trust its decisions.**

The Ugly: Adversarial examples on DNNs 
====== 

In addition to their lack of transparency, DNNs are also vulnerable to adversarial attacks including backdoor attacks, poisoning attacks, and adversarial examples. 
A notable work from Szegedy et al. (2014) [4] was the first work demonstrated that DNNs are susceptible to adversarial examples, subtle modifications to input data that can manipulate their behavior. 
And the worst part is that generating adversarial examples is easy and fast [5].

|![Adversarial Examples](/_posts/images/aml_intro/adversarial_1_10.png)|
|:--:|
| *Adversarial examples [(link to the demo)](https://github.com/tuananhbui89/demo_attack)* |

The above example illustrates an adversarial example generated from a pre-trained ResNet50 model. The image on the left is the original image of a koala, which is correctly classified as a koala with nearly 50% confidence. The image in the middle is the adversarial perturbation, which is imperceptible to the human eye. The image on the right is the adversarial example generated from the original image on the left. The adversarial example is misclassified as a ballon with nearly 100% confidence.

Efforts to Tackle Adversarial Examples
======

Since the discovery of adversarial examples [4], it has been an extensive with the number of papers on this topic increasing exponentially, as shown in the figure below. 

|![Number of adversarial examples papers published on arXiv since 2014](/_posts/images/aml_intro/papers_per_year_side_by_side.png)|
|:--:|
| *Number of adversarial examples papers published on arXiv from 2014 to May 2023. Data source from Carlini's post [(link)](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html)* |

On the one hand, various attack methods have been proposed to enhance effectiveness [5], computational efficiency [6], transferability among inputs [7] or among models [8]. 

On the other hand, there is also an extremely large number of defense methods proposed to mitigate adversarial attacks, from all aspects of the machine learning pipeline.  



References
======

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” CVPR, 2016.

[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need” NeurIPS, 2017.

[3] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” ICML, 2016.

[4] Szegedy, Christian, et al. "Intriguing properties of neural networks." ICLR, 2014. 

[5] Madry, Aleksander, et al. "Towards Deep Learning Models Resistant to Adversarial Attacks.", ICLR, 2017.

[6] Zhang, Yihua, et al. "Revisiting and advancing fast adversarial training through the lens of bi-level optimization." ICML, 2022. 

[7] Moosavi-Dezfooli, Seyed-Mohsen, et al. "Universal adversarial perturbations." CVPR, 2017.

[8] Papernot, Nicolas, Patrick McDaniel, and Ian Goodfellow. "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples." arXiv preprint arXiv:1605.07277 (2016).