<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tuananhbui89.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tuananhbui89.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-18T17:55:26+07:00</updated><id>https://tuananhbui89.github.io/feed.xml</id><title type="html">blank</title><subtitle>Researcher in Generative AI and Trustworthy AI
</subtitle><entry><title type="html">MIT 6.S184 - Lecture 2 - Constructing a Training Target</title><link href="https://tuananhbui89.github.io/blog/2025/mit-6s184-lec02/" rel="alternate" type="text/html" title="MIT 6.S184 - Lecture 2 - Constructing a Training Target" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/mit-6s184-lec02</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/mit-6s184-lec02/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/yFD-JSSG-D0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-introduction-and-problem-statement">Lecture introduction and problem statement</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-01-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-01-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-01-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-01-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture introduces <strong>flow</strong> and <strong>diffusion generative models</strong> and frames sampling as integrating either an <strong>ordinary differential equation (ODE)</strong> or a <strong>stochastic differential equation (SDE)</strong> from a simple initial distribution to the data distribution.<br /></p>

<ul>
  <li>Sampling is described as simulating an ODE/SDE from time 0 to 1 starting from a simple initial law (e.g., isotropic Gaussian).<br /></li>
  <li>The central training problem is to derive a suitable training target for the <strong>vector field (drift)</strong> so that samples at the terminal time follow the data distribution.<br /></li>
  <li>Untrained models output arbitrary samples, so the core technical question is: how do we construct a loss/target that, when minimized, yields a vector field whose induced endpoint distribution matches the data?<br /></li>
</ul>

<p>The lecture separates two distinct concerns:<br /></p>
<ol>
  <li><strong>Sampling mechanics</strong> — how to simulate an ODE/SDE from t=0 to t=1 to produce samples.<br /></li>
  <li><strong>Learning objectives</strong> — what target should the network predict (the vector field, the score, or related conditional quantities).<br /></li>
</ol>

<p>This distinction motivates the derivations that follow, which derive the conditional and marginal objects used as training targets.<br /></p>

<hr />

<h1 id="class-outline-and-the-six-central-objects-conditional-vs-marginal">Class outline and the six central objects (conditional vs marginal)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-04-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-04-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-04-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-04-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The plan is to derive the <strong>marginal vector field</strong> and the <strong>marginal score</strong>, and to emphasize six central mathematical objects:<br /></p>

<ul>
  <li>Three <strong>conditional</strong> objects (defined per data point z):
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td><strong>Conditional probability path</strong> — p_t(x</td>
              <td>z)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><strong>Conditional vector field</strong> — u_t(x; z)</li>
      <li><strong>Conditional score</strong> — s_t(x; z)<br />
<br /></li>
    </ul>
  </li>
  <li>Three <strong>marginal</strong> objects (obtained by averaging over the data distribution):
    <ul>
      <li><strong>Marginal probability path</strong> — p_t(x)</li>
      <li><strong>Marginal vector field</strong> — v_t(x)</li>
      <li><strong>Marginal score</strong> — s_t(x)<br />
<br /></li>
    </ul>
  </li>
</ul>

<p>Terminology: <strong>“conditional”</strong> refers to constructs for a fixed data sample z (per data point). <strong>“Marginal”</strong> denotes the corresponding quantity after marginalizing z under the data distribution p_data(z).<br /></p>

<p>Understanding the definitions and explicit formulas for these six objects is the primary technical requirement for later algorithmic development.<br /></p>

<hr />

<h1 id="probability-path-concept-and-conditional-probability-path">Probability path concept and conditional probability path</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-09-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-09-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-09-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-09-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A <strong>probability path</strong> is a time-indexed family of probability distributions that interpolates between a simple initial distribution at t=0 and a target concentrated at a single data point at t=1.<br /></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The <strong>conditional probability path</strong> for a fixed data point z is written **p_t(x</td>
          <td>z)** and satisfies:</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>**p_0(x</td>
              <td>z) = p_init(x)** (initial simple law)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><strong>p_1(x|z) = δ_z</strong> (point mass at z)<br />
<br /></li>
    </ul>
  </li>
  <li>Canonical example — <strong>Gaussian interpolation (the “Gaussian path”)</strong>:<br />
<strong>p_t(x|z) = N(x; α(t) z, β(t) I)</strong><br />
where α(0)=0, α(1)=1 and β(0)=1, β(1)=0, so means and variances move from noise to the fixed data point.<br /></li>
</ul>

<p>This conditional construction provides a skeleton of intermediate distributions used to derive per-data-point vector fields and scores.<br /></p>

<hr />

<h1 id="marginal-probability-path-and-its-density-formula">Marginal probability path and its density formula</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-17-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-17-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-17-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-17-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>marginal probability path</strong> is obtained by making the terminal data point z random under the data distribution and marginalizing it out:<br />
<strong>p_t(x) = ∫ p_t(x|z) p_data(z) dz</strong>.<br /></p>

<ul>
  <li>This describes how the entire dataset’s distribution evolves over time.<br /></li>
  <li>Boundary conditions are preserved:
    <ul>
      <li><strong>p_0(x) = p_init(x)</strong> (sampling at t=0 ignores z)</li>
      <li><strong>p_1(x) = p_data(x)</strong> (the path collapses to the data distribution at t=1)<br />
<br /></li>
    </ul>
  </li>
</ul>

<p>The marginal density formula above gives an explicit expression for the marginal likelihood at any time t and is essential for later derivations of scores and marginal vector fields.<br /></p>

<hr />

<h1 id="conditional-vector-field-and-ode-construction-per-data-point">Conditional vector field and ODE construction per data point</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-25-12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-25-12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-25-12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-25-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A <strong>conditional vector field</strong> <strong>u_t(x; z)</strong> is a time-dependent vector field for a fixed data point z that defines an ODE dx/dt = u_t(x; z) whose solution X_t, initialized from p_init, has marginals p_t(·|z) at all times.<br /></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Defining requirement: when trajectories are simulated under <strong>u_t(x; z)</strong> starting from x_0 ~ p_init, the law of X_t equals **p_t(x</td>
          <td>z)** for every t.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>In practice, explicit formulas for <strong>u_t</strong> exist for common conditional paths. For the Gaussian conditional path, <strong>u_t(x; z)</strong> reduces to a simple affine combination of z and x with coefficients determined by α, β and their time derivatives (involving α̇ and β̇).<br /></li>
</ul>

<p>Constructing these per-data-point ODEs provides the building blocks for obtaining marginal dynamics by later marginalization.<br /></p>

<hr />

<h1 id="marginal-vector-field-and-the-marginalization-trick">Marginal vector field and the marginalization trick</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-32-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-32-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-32-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-32-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>marginalization trick</strong> defines a <strong>marginal vector field</strong> <strong>v_t(x)</strong> so that simulating dx/dt = v_t(x) produces trajectories whose marginals are the marginal probability path <strong>p_t(x)</strong>.<br /></p>

<ul>
  <li>The marginal vector field is a posterior-weighted average of conditional vector fields:<br />
<strong>v_t(x) = ∫ u_t(x; z) p_t(z|x) dz</strong><br />
equivalently<br />
<strong>v_t(x) = (1 / p_t(x)) ∫ u_t(x; z) p_t(x|z) p_data(z) dz</strong>.<br /></li>
  <li>Intuition: given a location x at time t, <strong>v_t(x)</strong> is the conditional expectation of the per-data-point drift under the posterior over which data point z could have produced x.<br /></li>
</ul>

<p>This formula converts per-data-point solutions into a single deterministic vector field that maps the initial noise distribution to the full data distribution when integrated.<br /></p>

<hr />

<h1 id="continuity-equation-and-its-role-in-proving-marginalization">Continuity equation and its role in proving marginalization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-39-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-39-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-39-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-39-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>continuity equation</strong> (aka the transport equation) links a vector field and a time-varying density:<br />
<strong>∂_t p_t(x) = −∇·(p_t(x) v_t(x))</strong>, where ∇· denotes divergence.<br /></p>

<ul>
  <li>This equation is necessary and sufficient to ensure that the time evolution of the density under the deterministic flow induced by <strong>v_t</strong> matches the family {p_t}.<br /></li>
  <li>Divergence is the net outflow/inflow operator; verifying the continuity equation for the marginal <strong>v_t</strong> defined by the posterior-weighted average of <strong>u_t(x; z)</strong> proves that integrating the marginal vector field yields the marginal probability path.<br /></li>
</ul>

<p>The continuity equation is the analytic bridge converting local conditional dynamics into a global marginal dynamics statement.<br /></p>

<hr />

<h1 id="proof-sketch-deriving-the-marginal-continuity-equation-from-conditional-dynamics">Proof sketch: deriving the marginal continuity equation from conditional dynamics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-47-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-47-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-47-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-47-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Proof sketch to verify the continuity equation for the marginal field:<br /></p>

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Start from the marginal density **p_t(x) = ∫ p_t(x</td>
          <td>z) p_data(z) dz**.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Under regularity assumptions, interchange differentiation and integration (Leibniz rule): differentiate under the integral sign.<br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Substitute the conditional continuity equation **∂_t p_t(x</td>
          <td>z) = −∇·(p_t(x</td>
          <td>z) u_t(x; z))**.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Pull divergence outside the integral and algebraically multiply/divide by <strong>p_t(x)</strong> to identify the posterior-weighted average.<br /></li>
  <li>Conclude <strong>∂_t p_t(x) = −∇·(p_t(x) v_t(x))</strong>, i.e., the marginal continuity equation holds for <strong>v_t(x)</strong> defined as the posterior-weighted conditional drift.<br /></li>
</ol>

<p>This argument uses basic calculus identities (Leibniz rule, linearity of divergence) and the definition of <strong>v_t(x)</strong>; it yields the sufficient condition that evolving with <strong>v_t</strong> maps p_0 to p_1 through the marginal path.<br /></p>

<hr />

<h1 id="visualization-and-intuition-for-flows-mapping-noise-to-data">Visualization and intuition for flows mapping noise to data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-55-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-55-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/00-55-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/00-55-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Visual intuition and examples:<br /></p>

<ul>
  <li>Visualizations show conditional contour families and conditional vector fields that push mass from a noise distribution to a single data point z.<br /></li>
  <li>Marginalization across data points produces a family of trajectories that convert noise into the full data distribution.<br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Key intuition: design <strong>u_t(x; z)</strong> per data point so the conditional law is **p_t(·</td>
          <td>z)**, then average appropriately to obtain a global deterministic flow that maps p_init to p_data.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>These visual examples make concrete how local, z-parameterized flows align with global sampling behavior and illustrate that the analytic constructions can produce realistic samples (images, protein conformations, videos) when implemented at scale.<br /></p>

<hr />

<h1 id="score-functions-conditional-and-marginal-definitions-and-relation">Score functions: conditional and marginal definitions and relation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-04-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-04-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-04-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/01-04-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>score function</strong> is the gradient of the log-density with respect to x:<br /></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Conditional score:</strong> **s_t(x; z) = ∇_x log p_t(x</td>
          <td>z)**.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Marginal score:</strong> <strong>s_t(x) = ∇_x log p_t(x)</strong>.<br /></li>
</ul>

<table>
  <tbody>
    <tr>
      <td>The marginal score admits a posterior-weighted identity: **s_t(x) = ∫ s_t(x; z) p_t(z</td>
      <td>x) dz**, obtained by differentiating **p_t(x) = ∫ p_t(x</td>
      <td>z) p_data(z) dz** and applying the chain rule.<br /></td>
    </tr>
  </tbody>
</table>

<p>Concrete Gaussian example: for the Gaussian conditional path, <strong>s_t(x; z) = −(x − α(t) z) / β(t)</strong>, giving a simple closed-form conditional score. Score functions are central when converting deterministic flows to stochastic dynamics and when defining tractable training losses.<br /></p>

<hr />

<h1 id="sde-extension-score-correction-and-the-extension-trick">SDE extension (score correction) and the extension trick</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-12-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-12-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-12-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/01-12-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>extension trick</strong> generalizes deterministic marginal flows to SDEs by adding a diffusion term and a compensating drift correction involving the marginal score.<br /></p>

<ul>
  <li>For an arbitrary scalar diffusion coefficient σ(t), the SDE<br />
<strong>dX_t = [ v_t(X_t) + (1/2) σ(t)^2 s_t(X_t) ] dt + σ(t) dW_t</strong><br />
(with W_t a standard Wiener process) yields marginal laws equal to <strong>p_t(·)</strong> provided <strong>v_t</strong> is the marginal vector field and <strong>s_t</strong> is the marginal score.<br /></li>
  <li>The extra term accounts for the heat-like dispersion introduced by the injected noise.<br /></li>
  <li>Consequence: many stochastic samplers (different σ schedules) share the same marginal path when the drift includes the appropriate score-based correction, giving a family of sampling dynamics that all map the initial distribution to the data distribution.<br /></li>
</ul>

<hr />

<h1 id="practical-implications-training-target-and-course-wrap-up">Practical implications, training target and course wrap-up</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-19-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-19-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec02/01-19-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec02/01-19-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical goal and training perspective:<br /></p>

<ul>
  <li>The aim is to approximate one of the mathematical objects (typically the <strong>marginal vector field v_t</strong> or the <strong>marginal score s_t</strong>) with a neural network, then sample by integrating the corresponding ODE or SDE.<br /></li>
  <li>Training reduces to deriving a loss that supervises the network to match conditional targets and, via marginalization identities, the marginal targets (e.g., score-matching objectives).<br /></li>
  <li>The technically hardest step is deriving explicit formulas and identities for the six objects (conditional/marginal path, vector field, score). Once those are in hand, simple training losses allow efficient approximation without explicitly computing posterior integrals.<br /></li>
</ul>

<p>Implementation notes:<br /></p>
<ul>
  <li>The implementation/lab work focuses on the <strong>flow</strong> part (learning or approximating v_t and integrating the ODE).<br /></li>
  <li>The <strong>score-related</strong> terms are a present-day extension used to inject noise when desired (enabling SDE samplers and certain training objectives).<br /></li>
</ul>

<p>The lecture closes by listing the six canonical formulas and preparing to present practical training losses (e.g., score-matching) in the next class.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">MIT 6.S184 - Lecture 1 - Generative AI with SDEs</title><link href="https://tuananhbui89.github.io/blog/2025/mit-6s184-lec01/" rel="alternate" type="text/html" title="MIT 6.S184 - Lecture 1 - Generative AI with SDEs" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/mit-6s184-lec01</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/mit-6s184-lec01/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/GCoP2w-Cqtg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="course-introduction-and-learning-objectives">Course introduction and learning objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-02-12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-02-12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-02-12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-02-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The course presents an intensive introduction to <strong>flow and diffusion generative models</strong> with emphasis on both <strong>theoretical foundations</strong> (ordinary and stochastic differential equations) and <strong>practical implementation</strong> through labs.<br /></p>

<p>The material is framed around three primary goals:<br /></p>
<ul>
  <li><strong>Derive models from first principles</strong> — build models starting from fundamental continuous-time dynamics and probabilistic reasoning.<br /></li>
  <li><strong>Teach the minimal required mathematics</strong> — cover only the mathematical tools needed to understand and work with flows and SDEs (ODEs, SDEs, vector fields, numerical integrators).<br /></li>
  <li><strong>Provide hands-on implementation experience</strong> — labs that let students implement and experiment with image, video, and molecular generators so they can build end-to-end systems.<br /></li>
</ul>

<p>These goals are pursued in parallel: lectures for foundations, labs for implementation, and readings/examples to connect theory to practice.<br /></p>

<hr />

<h1 id="representation-of-generative-objects-and-formalizing-generation-via-probability">Representation of generative objects and formalizing generation via probability</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-06-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-06-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-06-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-06-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Digital objects</strong> to be generated (images, videos, molecular structures, etc.) are represented as vectors in <strong>R^d</strong>.<br /></p>

<p>Generation is formalized as sampling from an unknown <strong>data distribution</strong> <strong>P_data</strong> defined over that vector space.<br /></p>

<p>This formalization converts qualitative judgments of sample quality into a quantitative criterion: <strong>likelihood under P_data</strong>.<br /></p>

<p>Consequently the learning objective becomes: produce a model whose samples are (approximately) drawn from <strong>P_data</strong>, turning generation into a statistical matching problem.<br /></p>

<hr />

<h1 id="conditional-versus-unconditional-generation-and-conditional-data-distribution">Conditional versus unconditional generation and conditional data distribution</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-10-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-10-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-10-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-10-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Unconditional generation</strong> produces samples from a single target distribution (for example, the distribution of all dog images).<br /></p>

<table>
  <tbody>
    <tr>
      <td><strong>Conditional generation</strong> defines a <strong>conditional data distribution</strong> **P_data(x</td>
      <td>y)** that specifies the distribution of objects x given a conditioning variable y (prompts, labels, text, or structured specifications).<br /></td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Sampling conditionally means drawing x ~ **P_data(·</td>
      <td>y)<strong>, which enables **controllable generation</strong>.<br /></td>
    </tr>
  </tbody>
</table>

<p>Conditional generation is the primary focus for subsequent modeling and training because it supports targeted, controllable outputs given y.<br /></p>

<hr />

<h1 id="generative-model-concept-and-the-role-of-an-initial-distribution-p_init">Generative model concept and the role of an initial distribution P_init</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-11-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-11-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-11-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-11-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A <strong>generative model</strong> is an algorithm that transforms a simple, known initial distribution <strong>P_init</strong> (commonly an <strong>isotropic Gaussian</strong>) into the target data distribution <strong>P_data</strong> via a <strong>parameterized mapping</strong>.<br /></p>

<p>The modeling goal is therefore to design a transform such that samples drawn from <strong>P_init</strong> and pushed through the model produce outputs whose distribution matches <strong>P_data</strong>.<br /></p>

<p>In practice this means parameterizing a continuous-time or discrete mapping and optimizing its parameters so the terminal distribution aligns with the empirical data distribution.<br /></p>

<hr />

<h1 id="high-level-architecture-flow-component-versus-diffusion-component">High-level architecture: flow component versus diffusion component</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-13-05-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-13-05-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-13-05-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-13-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Flow and diffusion frameworks</strong> combine a <strong>deterministic flow component</strong> and a <strong>stochastic diffusion component</strong>.<br /></p>

<ul>
  <li>The <strong>flow component</strong> is a deterministic, continuous-time mapping defined by an <strong>ordinary differential equation (ODE)</strong>.<br /></li>
  <li>Understanding the flow component is foundational because it forms the conceptual basis for the stochastic extensions that define diffusion models.<br /></li>
</ul>

<p>Diffusion models extend this deterministic picture by adding controlled stochasticity to allow probabilistic exploration and density modeling.<br /></p>

<hr />

<h1 id="trajectory-vector-field-and-ordinary-differential-equation-ode-definitions">Trajectory, vector field, and ordinary differential equation (ODE) definitions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-15-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-15-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-15-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-15-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Trajectory</strong>: a time-indexed function <strong>x(t)</strong> mapping a time interval (commonly [0,1]) to <strong>R^d</strong> — it describes the state of a sample over time.<br /></p>

<p><strong>Vector field</strong>: <strong>u(t,x)</strong> maps each time and spatial location to a velocity vector; it prescribes instantaneous direction and speed.<br /></p>

<p><strong>Ordinary differential equation (ODE)</strong>: specifies that the time derivative satisfies <strong>dx/dt = u(t,x_t)</strong> with an initial condition <strong>x(0) = x0</strong> — thus trajectories are constrained to follow the instantaneous directions given by the vector field.<br /></p>

<hr />

<h1 id="flow-map-as-a-collection-of-ode-solutions-and-existenceuniqueness-conditions">Flow map as a collection of ODE solutions and existence/uniqueness conditions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-19-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-19-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-19-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-19-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A <strong>flow</strong> is the family of solutions <strong>s_t(x0)</strong> obtained by integrating the ODE for all initial conditions x0, producing a time-indexed mapping from initial to evolved states.<br /></p>

<p>Under standard regularity conditions on the vector field (continuous differentiability or <strong>Lipschitz continuity</strong> with bounded derivatives), existence and uniqueness of solution trajectories — and hence a well-defined <strong>flow map</strong> — are guaranteed.<br /></p>

<hr />

<h1 id="linear-vector-field-example-and-analytic-solution-numerical-simulation-by-euler-method">Linear vector field example and analytic solution; numerical simulation by Euler method</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-22-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-22-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-22-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-22-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>For the linear vector field <strong>u(x) = −θ x</strong> the ODE solution is <strong>s_t(x0) = exp(−θ t) x0</strong>, demonstrating <strong>exponential contraction</strong>.<br /></p>

<p>In general nonlinear cases where closed-form solutions are unavailable, numerical integrators approximate trajectories:<br /></p>
<ul>
  <li>The <strong>explicit Euler method</strong> iterates <strong>x_{t+h} = x_t + h u(t,x_t)</strong>.<br /></li>
  <li>The step size <strong>h</strong> controls numerical <strong>accuracy</strong> and <strong>convergence</strong>; smaller h gives better approximations at increased computational cost.<br /></li>
</ul>

<hr />

<h1 id="flow-models-in-machine-learning-parameterizing-the-vector-field-with-neural-networks">Flow models in machine learning: parameterizing the vector field with neural networks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-26-12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-26-12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-26-12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-26-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Flow-based generative models</strong> parameterize the vector field <strong>u_θ(t,x)</strong> using a neural network with parameters <strong>θ</strong>.<br /></p>

<p>Typical workflow:<br /></p>
<ol>
  <li>Sample an initial state <strong>x0 ~ P_init</strong> (e.g., Gaussian).<br /></li>
  <li>Integrate the ODE defined by <strong>u_θ</strong> to obtain the end state <strong>x1</strong> via a numerical solver.<br /></li>
  <li>Train (adjust <strong>θ</strong>) so that the terminal distribution of <strong>x1</strong> approximates <strong>P_data</strong>.<br /></li>
</ol>

<p>Neural network architecture is chosen per modality (images, video, proteins) to effectively represent the vector field and its time dependence.<br /></p>

<hr />

<h1 id="sampling-from-a-flow-model-and-the-practical-sampling-algorithm">Sampling from a flow model and the practical sampling algorithm</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-30-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-30-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-30-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-30-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Sampling from a trained flow model is straightforward:<br /></p>
<ol>
  <li>Draw <strong>x0</strong> from the known initial distribution <strong>P_init</strong>.<br /></li>
  <li>Numerically integrate the ODE using an ODE solver (discretize time, evaluate <strong>u_θ</strong> at each step).<br /></li>
  <li>Return the terminal state <strong>x1</strong> as the generated sample.<br /></li>
</ol>

<p>In practice this requires repeatedly evaluating the neural-network vector field along the discretized path and updating the state according to the chosen integrator.<br /></p>

<hr />

<h1 id="motivation-for-diffusion-models-and-introduction-to-stochastic-differential-equations-sdes">Motivation for diffusion models and introduction to stochastic differential equations (SDEs)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-34-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-34-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-34-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-34-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Diffusion models</strong> extend deterministic flows by injecting controlled stochasticity during integration.<br /></p>

<p>They model sample evolution as a <strong>stochastic process</strong> governed by a <strong>stochastic differential equation (SDE)</strong> that combines:<br /></p>
<ul>
  <li>a <strong>vector field (drift)</strong> term <strong>u(t,x)</strong>, and<br /></li>
  <li>a time-dependent <strong>diffusion coefficient</strong> <strong>σ(t)</strong> that scales random perturbations.<br /></li>
</ul>

<p>SDEs allow modeling distributions via both deterministic transport and stochastic exploration within a continuous-time probabilistic framework.<br /></p>

<hr />

<h1 id="brownian-motion-definition-gaussian-increments-and-independent-increments">Brownian motion: definition, Gaussian increments and independent increments</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-41-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-41-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-41-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-41-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Brownian motion</strong> <strong>W(t)</strong> is the canonical continuous-time stochastic process used for SDEs, characterized by:<br /></p>
<ul>
  <li><strong>Continuous sample paths</strong> and <strong>W(0)=0</strong>.<br /></li>
  <li><strong>Gaussian increments</strong>: <strong>W(t) − W(s) ~ N(0, (t−s) I)</strong> whose variance grows linearly with elapsed time.<br /></li>
  <li><strong>Independent increments</strong> across non-overlapping time intervals.<br /></li>
</ul>

<p>These properties make Brownian motion the natural continuous analogue of discrete random walks and the standard noise driver in diffusion modeling.<br /></p>

<hr />

<h1 id="symbolic-sde-notation-and-discretized-increment-interpretation">Symbolic SDE notation and discretized increment interpretation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-50-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-50-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/00-50-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/00-50-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The symbolic SDE notation <strong>dx_t = u(t,x_t) dt + σ(t) dW_t</strong> is interpreted via its discrete-time increment form:<br /></p>
<ul>
  <li>For small step <strong>h</strong>,<br />
<strong>x_{t+h} ≈ x_t + h u(t,x_t) + σ(t) (W_{t+h} − W_t) + r_t(h)</strong>,<br />
where <strong>W_{t+h} − W_t</strong> is Gaussian with covariance <strong>h I</strong> and <strong>r_t(h)</strong> is a remainder that vanishes as <strong>h → 0</strong>.<br /></li>
</ul>

<p>This increment representation replaces classical derivatives (which Brownian paths lack) and forms the basis for simulation and rigorous stochastic integration.<br /></p>

<hr />

<h1 id="existence-and-uniqueness-for-sdes-and-eulermaruyama-simulation-method">Existence and uniqueness for SDEs and Euler–Maruyama simulation method</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-00-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-00-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-00-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/01-00-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Under regularity conditions on the drift <strong>u(t,x)</strong> (e.g., <strong>Lipschitz continuity</strong> and bounded derivatives) an SDE admits a unique stochastic process solution.<br /></p>

<p>The standard numerical approximation for simulation is the <strong>Euler–Maruyama scheme</strong>:<br /></p>
<ul>
  <li>Iterate <strong>x_{t+h} = x_t + h u(t,x_t) + sqrt(h) σ(t) ξ</strong>, where <strong>ξ ~ N(0,I)</strong>.<br /></li>
  <li>The scheme samples Gaussian increments scaled by <strong>sqrt(h)</strong> to match Brownian variance and converges to the SDE solution as the step size decreases.<br /></li>
</ul>

<hr />

<h1 id="ornsteinuhlenbeck-linear-sde-example-and-transition-to-diffusion-model-definition">Ornstein–Uhlenbeck linear SDE example and transition to diffusion model definition</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-13-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-13-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-13-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/01-13-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>Ornstein–Uhlenbeck process</strong> illustrates a linear SDE with drift <strong>−θ x_t</strong> and constant diffusion <strong>σ</strong>, producing <strong>mean-reverting dynamics</strong> that converge to a Gaussian stationary distribution when <strong>σ &gt; 0</strong>.<br /></p>

<p>Diffusion generative models adopt the same SDE framework at scale by:<br /></p>
<ul>
  <li>parameterizing the <strong>drift</strong> with a neural network, and<br /></li>
  <li>specifying <strong>σ(t)</strong> (often fixed by design).<br /></li>
</ul>

<p>The generative objective is that samples initiated from <strong>P_init</strong> and evolved by the SDE match <strong>P_data</strong> at the terminal time.<br /></p>

<hr />

<h1 id="sampling-from-diffusion-models-and-practical-considerations">Sampling from diffusion models and practical considerations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-22-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-22-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-22-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/01-22-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Sampling from a diffusion generative model uses the <strong>Euler–Maruyama integrator</strong> with randomized initial states <strong>x0 ~ P_init</strong> and iterations:<br /></p>
<ul>
  <li><strong>x_{t+h} = x_t + h u_θ(t,x_t) + sqrt(h) σ(t) ξ_t</strong>, where <strong>ξ_t</strong> are independent standard normals.<br /></li>
</ul>

<p>Practical considerations:<br /></p>
<ul>
  <li>Choose <strong>σ(t)</strong> schedules to trade off exploration and numerical stability.<br /></li>
  <li>Implement conditioned generation by feeding conditioning variables into <strong>u_θ</strong>.<br /></li>
  <li>Focus on final-time samples <strong>x1</strong> as the generated outputs.<br /></li>
</ul>

<hr />

<h1 id="course-logistics-evaluation-and-resources">Course logistics, evaluation and resources</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-25-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-25-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec01/01-25-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec01/01-25-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Course logistics and assessment:<br /></p>
<ul>
  <li>Passing requires <strong>attendance or recorded lecture review</strong> plus <strong>lab completion</strong>.<br /></li>
  <li>Points are allocated across lectures and labs, and labs are submitted via provided <strong>GitHub notebooks</strong>.<br /></li>
  <li>Primary study resources are the <strong>course lecture notes</strong> (self-contained) and <strong>office hours</strong> for lab help.<br /></li>
  <li>The first lab implements basic sampling from flows and SDEs and is provided with specified deadlines and grading procedures.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 18 - Diffusion Models for Discrete Data</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec18/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 18 - Diffusion Models for Discrete Data" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec18</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec18/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/mCaRNnEnYwA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-introduction-and-topic-overview">Lecture introduction and topic overview</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-00-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-00-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-00-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-00-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture introduces <strong>diffusion models for discrete data</strong>, with an emphasis on applications to <strong>language</strong> and a guest lecture contributor who has done foundational research in <strong>discrete diffusion</strong> approaches.<br /></p>

<p>The objective is to adapt <strong>diffusion-based generative modeling</strong>—traditionally applied to continuous data like images—to <strong>discrete modalities</strong> such as text.<br /></p>

<p>The talk sets expectations to cover:</p>
<ul>
  <li><strong>Theoretical generalizations</strong> of diffusion to discrete spaces<br /></li>
  <li><strong>Algorithmic constructs</strong> for learning and sampling discrete diffusions<br /></li>
  <li><strong>Empirical comparisons</strong> to autoregressive language models<br /></li>
</ul>

<hr />

<h1 id="generative-modeling-framing-and-predominance-of-continuous-image-data">Generative modeling framing and predominance of continuous image data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-01-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-01-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-01-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-01-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Generative modeling</strong> is posed as learning a parameterized distribution <strong>P_θ</strong> to approximate an unknown data distribution <strong>P_data</strong> from i.i.d. samples, enabling synthesis of new samples when successful.<br /></p>

<p>The lecture highlights why many standard generative paradigms (GANs, VAEs, flows, diffusion) are illustrated with <strong>images</strong>:</p>
<ul>
  <li>Images live in a continuous R^d pixel space that supports <strong>interpolation</strong> and <strong>smooth transformations</strong>.<br /></li>
  <li>That continuous structure underlies algorithmic tools assuming <strong>differentiability</strong> and <strong>continuity</strong>, which explains why image-centric methods have dominated prior work.<br /></li>
</ul>

<hr />

<h1 id="motivation-for-discrete-generative-modeling">Motivation for discrete generative modeling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-04-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-04-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-04-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-04-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Discrete data domains such as <strong>natural language</strong>, <strong>DNA</strong>, and <strong>discretized image latents</strong> are ubiquitous and scientifically important, motivating probabilistic models that natively handle <strong>discrete tokens</strong>.<br /></p>

<p>Key points:</p>
<ul>
  <li><strong>Language modeling</strong> and large-scale pretraining explicitly model distributions over discrete token sequences, making discrete generative models directly applicable and often preferable.<br /></li>
  <li>Applications include:
    <ul>
      <li><strong>Language generation</strong><br /></li>
      <li><strong>Molecular design</strong> (sequence-based representations)<br /></li>
      <li><strong>VQ-style image latents</strong><br /></li>
    </ul>
  </li>
  <li>Discrete generative models are essential for sampling <strong>valid sequences</strong> and discovering <strong>novel structured objects</strong> in these domains.<br /></li>
</ul>

<hr />

<h1 id="why-continuous-generative-methods-do-not-trivially-extend-to-discrete-data">Why continuous generative methods do not trivially extend to discrete data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-07-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-07-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-07-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-07-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Many continuous generative methods rely on calculus and smooth <strong>change-of-variables</strong> operations that do not hold in discrete spaces, so naive adaptations fail.<br /></p>

<p>Concrete failure modes:</p>
<ul>
  <li>For <strong>normalizing flows</strong>, the change-of-variables formula requires <strong>bijective differentiable transforms</strong> and continuous Jacobians; discretizing breaks those properties and forces the base distribution to be as expressive as the data distribution.<br /></li>
  <li>For <strong>GANs</strong>, discrete outputs block gradient backpropagation through sampling because derivatives are undefined, so standard adversarial training does not apply without additional estimators or relaxations.<br /></li>
</ul>

<hr />

<h1 id="problems-with-embedding-discrete-tokens-to-continuous-spaces">Problems with embedding discrete tokens to continuous spaces</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-11-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-11-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-11-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-11-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Embedding discrete tokens into continuous spaces and then rounding generated vectors back to tokens creates problems:</p>
<ul>
  <li>It introduces large regions of <strong>empty space</strong> and <strong>unreliable discretization</strong> behavior.<br /></li>
  <li>While rounding can work for low-resolution image ranges (e.g., 0–255), <strong>token embedding spaces</strong> are high-dimensional and <strong>sparse</strong>, so sampling near but not exactly at an embedding often maps to semantically inappropriate tokens.<br /></li>
</ul>

<p>Consequences:</p>
<ul>
  <li>Continuous-token pipelines become <strong>brittle</strong>: small modeling errors lead to invalid discretizations and poor generative quality.<br /></li>
  <li>Methods that rely on near-perfect continuous generation are <strong>impractically sensitive</strong> to error accumulation.<br /></li>
</ul>

<hr />

<h1 id="autoregressive-models-as-the-standard-discrete-probabilistic-approach">Autoregressive models as the standard discrete probabilistic approach</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-14-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-14-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-14-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-14-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Autoregressive decomposition</strong> factorizes the joint probability of a token sequence into conditional token probabilities and estimates each conditional with a network (e.g., Transformer).<br /></p>

<p>Reasons this approach is dominant:</p>
<ul>
  <li>Scalability: each next-token distribution is computed over the vocabulary.<br /></li>
  <li>Expressivity: with sufficient capacity it can represent any sequence distribution in principle.<br /></li>
  <li>Alignment with generation: it matches the <strong>left-to-right</strong> generative process of natural language.<br /></li>
  <li>Training and evaluation: yields a straightforward <strong>maximum-likelihood</strong> objective and tractable exact sequence likelihoods.<br /></li>
</ul>

<hr />

<h1 id="limitations-of-autoregressive-modeling">Limitations of autoregressive modeling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-16-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-16-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-16-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-16-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Limitations of autoregressive sampling:</p>
<ul>
  <li><strong>Error accumulation</strong> during sequential sampling can cause generation drift.<br /></li>
  <li>Imposes a <strong>left-to-right inductive bias</strong> that may be unnecessary or suboptimal for many discrete domains.<br /></li>
  <li>Inherently <strong>sequential</strong> and therefore <strong>slow</strong> at sampling time.<br /></li>
  <li>Architectural constraints (e.g., causal attention masks) add modeling restrictions and complicate flexible conditioning or in-filling.<br /></li>
</ul>

<p>These limitations motivate exploring alternative discrete generative paradigms that can generate tokens <strong>in parallel</strong> and provide different inductive biases.<br /></p>

<hr />

<h1 id="score-matching-perspective-and-research-question">Score matching perspective and research question</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-18-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-18-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-18-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-18-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Score matching</strong> offers an alternative by modeling <strong>gradients of the log density</strong> rather than the density itself, which circumvents the need to normalize across an exponentially large discrete sample space.<br /></p>

<p>Central research question posed: can <strong>score-based techniques</strong> and <strong>diffusion-style generative frameworks</strong> be generalized to discrete spaces to yield practical discrete generative models?<br /></p>

<p>This motivates deriving discrete analogues of the <strong>score function</strong> and corresponding learning and sampling procedures.<br /></p>

<hr />

<h1 id="finite-difference-gradient-and-the-concrete-score">Finite-difference gradient and the concrete score</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-20-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-20-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-20-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-20-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The continuous score (gradient of log probability) generalizes to discrete finite differences:</p>
<ul>
  <li>Replace derivatives by <strong>differences</strong> f(y) − f(x) over neighboring states to obtain a discrete analogue of a gradient.<br /></li>
</ul>

<p>Applied to log-probabilities, this produces a collection of <strong>density ratios</strong> p(y)/p(x) indexed over neighbors y of x. That object is termed the <strong>concrete score</strong> and serves as the discrete counterpart to the continuous score function.<br /></p>

<p>The <strong>concrete score</strong> captures local relative probabilities and is the fundamental quantity to estimate for discrete score-based modeling.<br /></p>

<hr />

<h1 id="restricting-ratios-to-single-position-neighbors-to-control-complexity">Restricting ratios to single-position neighbors to control complexity</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-22-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-22-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-22-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-22-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modeling all p(y)/p(x) ratios for arbitrary y is computationally prohibitive because the number of pairs is exponential in sequence length.<br /></p>

<p>Practical restriction:</p>
<ul>
  <li>Focus on ratios between sequences that <strong>differ at a single token position</strong>, yielding a <strong>locality constraint</strong>.<br />
Benefits:</li>
  <li>Reduces computational complexity to O(N * D) for vocabulary size N and sequence length D.<br /></li>
  <li>Preserves expressivity for learning <strong>local transition behavior</strong> while keeping estimation and representation tractable for neural parameterization.<br /></li>
</ul>

<hr />

<h1 id="parameterizing-local-ratios-with-sequence-neural-networks">Parameterizing local ratios with sequence neural networks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-23-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-23-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-23-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-23-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Local density ratios can be parameterized by feeding the full token sequence into a sequence model that outputs per-position ratio vectors across the vocabulary.<br /></p>

<p>Design highlights:</p>
<ul>
  <li>Uses a <strong>parallel, non-autoregressive</strong> architecture: the network produces conditional scores for every position simultaneously rather than iterating left-to-right.<br /></li>
  <li>Implementationally resembles a <strong>non-causal Transformer</strong> which outputs D separate conditional distributions (ratio vectors) corresponding to flipping each token independently.<br /></li>
</ul>

<hr />

<h1 id="learning-the-concrete-score-via-a-score-entropy-loss">Learning the concrete score via a score-entropy loss</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-24-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-24-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-24-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-24-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A principled loss named <strong>score entropy</strong> generalizes continuous score-matching objectives to discrete ratios.<br /></p>

<p>Properties of score entropy:</p>
<ul>
  <li>Measures a divergence between the modeled ratio outputs and the true p(y)/p(x) ratios summed over neighbors and averaged under the data distribution.<br /></li>
  <li>Minimizing the score-entropy loss yields <strong>consistent recovery</strong> of the true discrete ratios under sufficient model capacity and data.<br /></li>
  <li>The loss is <strong>convex in the log-ratio parameterization</strong>, giving well-behaved optimization properties.<br /></li>
</ul>

<p>Score entropy serves as the principal objective for fitting the <strong>concrete score network</strong>.<br /></p>

<hr />

<h1 id="optimization-strategies-implicit-and-denoising-score-entropy">Optimization strategies: implicit and denoising score entropy</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-27-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-27-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-27-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-27-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Direct evaluation of the score-entropy loss requires unknown true ratios p(y)/p(x), so two practical strategies are proposed:</p>
<ul>
  <li><strong>Implicit score entropy</strong>: an implicit-estimation generalization (details omitted here), and<br /></li>
  <li><strong>Denoising score entropy</strong>: a denoising-based estimator that makes the objective computable.<br /></li>
</ul>

<p>Denoising score entropy assumes the data distribution is a convolution of the true data with a tractable perturbation kernel, allowing unknown terms to be rewritten as expectations over the corruption process—analogous to continuous denoising score matching.<br /></p>

<hr />

<h1 id="practical-training-procedure-using-a-perturbation-kernel">Practical training procedure using a perturbation kernel</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-29-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-29-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-29-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-29-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Under the denoising reduction, training proceeds as follows:</p>
<ol>
  <li>Sample a clean sequence x0 from the data.<br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Sample a corrupted sequence xt from a tractable perturbation kernel p(xt</td>
          <td>x0).<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Minimize the <strong>denoising score-entropy</strong> loss comparing the model output to transition ratios computable from the kernel.<br /></li>
</ol>

<table>
  <tbody>
    <tr>
      <td>Because the corruption kernel is tractable, the intractable p(y)/p(x) terms are replaced by computable ratios p_t(y</td>
      <td>x0) / p_t(x</td>
      <td>x0), making minibatch stochastic optimization feasible.<br /></td>
    </tr>
  </tbody>
</table>

<p>This mirrors <strong>continuous denoising score matching</strong> and provides a scalable training recipe.<br /></p>

<hr />

<h1 id="modeling-discrete-diffusion-as-a-continuous-time-markov-process">Modeling discrete diffusion as a continuous-time Markov process</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-31-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-31-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-31-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-31-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Discrete diffusion</strong> is expressed as the linear Kolmogorov forward equation on the probability vector p_t, governed by a <strong>rate matrix Q_t</strong> that controls jump rates between discrete states.<br /></p>

<p>Mathematical constraints on Q_t:</p>
<ul>
  <li>Off-diagonal entries must be <strong>nonnegative</strong> (rates).<br /></li>
  <li>Columns must sum to zero to preserve total probability.<br /></li>
</ul>

<p>Exponentiating Q_t Δt yields finite-time transition kernels. This <strong>continuous-time Markov chain</strong> formalism is the natural discrete analogue to continuous diffusion SDEs and provides a principled forward-noising process.<br /></p>

<hr />

<h1 id="computing-intermediate-densities-via-matrix-exponentiation">Computing intermediate densities via matrix exponentiation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-33-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-33-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-33-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-33-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Given a rate matrix Q and initial probability vector p0, intermediate densities p_t can be computed via matrix exponentiation p_t = exp(t Q) p0, which exactly solves the linear ODE.<br /></p>

<p>Practical considerations:</p>
<ul>
  <li>Use scalable approximations of the <strong>matrix exponential</strong> or exploit structure in Q to avoid explicit large-matrix operations.<br /></li>
  <li>The exponentiated kernel yields intuitive behavior: mass leaves states at rates given by diagonal entries and enters others via off-diagonals; as t → ∞ the chain approaches the designated <strong>base distribution</strong>.<br /></li>
</ul>

<hr />

<h1 id="uniform-and-absorbing-mask-transition-examples-and-implications">Uniform and absorbing (mask) transition examples and implications</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-35-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-35-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-35-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-35-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Two canonical Q choices illustrate distinct noising behaviors:</p>
<ul>
  <li><strong>Uniform transition Q</strong>: each token jumps to any other token uniformly, driving the distribution towards <strong>uniformity</strong> as t increases.<br /></li>
  <li><strong>Absorbing-mask Q</strong>: tokens transition to a special <strong>mask</strong> state, driving sequences to masked forms.<br /></li>
</ul>

<p>These options define different forward-noise geometries with distinct inductive biases:</p>
<ul>
  <li>Uniform noise <strong>randomizes broadly</strong>.<br /></li>
  <li>Masking <strong>preserves structural coherence</strong> by collapsing tokens to a neutral placeholder that is easier to denoise.<br /></li>
</ul>

<hr />

<h1 id="time-dependent-concrete-score-estimation-for-p_t-ratios">Time-dependent concrete score estimation for p_t ratios</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-37-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-37-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-37-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-37-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>concrete score</strong> must be estimated as a function of noise level or time t to produce ratios for intermediate distributions p_t.<br /></p>

<p>Practical extension:</p>
<ul>
  <li>Make the model <strong>time-conditioned</strong> so it predicts local density ratios conditional on the current corruption level, analogous to time-conditioned score networks in continuous diffusion.<br /></li>
  <li>Train with the denoising score-entropy loss applied at multiple t values sampled from a schedule, enabling the network to represent the family of intermediate concrete scores required for reverse sampling.<br /></li>
</ul>

<hr />

<h1 id="reverse-diffusion-and-relation-between-forward-and-reverse-generators">Reverse diffusion and relation between forward and reverse generators</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-39-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-39-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-39-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-39-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The reverse-time diffusion dynamics are defined by a rate matrix Q̄_t that depends on forward-time rates and the ratios p_t(j)/p_t(i):</p>
<ul>
  <li>For i ≠ j, Q̄_t(i → j) ∝ Q_t(j → i) * p_t(j) / p_t(i).<br /></li>
  <li>Diagonal entries are set to enforce column sums of zero.<br /></li>
</ul>

<p>This relationship shows the reverse generator can be written in terms of the forward generator and the <strong>concrete score</strong> ratios, making it possible to approximate reverse dynamics by substituting learned estimates of the concrete score.<br /></p>

<hr />

<h1 id="sampling-from-the-reverse-process-using-the-learned-concrete-score">Sampling from the reverse process using the learned concrete score</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-41-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-41-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-41-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-41-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Given the learned time-dependent concrete score network, sampling proceeds by simulating the <strong>reverse-time Markov chain</strong>:</p>
<ul>
  <li>Transition probabilities are computed from the estimated ratios and the known forward Q_t.<br /></li>
  <li>The parameterization yields <strong>parallel updates</strong> across token positions because the model provides per-position conditional ratios, enabling simultaneous proposed jumps to multiple states.<br /></li>
</ul>

<p>This produces a discrete reverse diffusion sampler analogous to ancestral sampling for continuous diffusion models but driven by estimated <strong>local density ratios</strong>.<br /></p>

<hr />

<h1 id="computational-bottleneck-of-single-token-jumping-and-simultaneous-step-acceleration">Computational bottleneck of single-token jumping and simultaneous-step acceleration</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-43-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-43-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-43-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-43-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Exact reverse simulation that flips single tokens per step can be computationally expensive because many small transitions are required to reach a clean sequence.<br /></p>

<p>To accelerate sampling:</p>
<ul>
  <li>Perform <strong>multiple token updates</strong> (simultaneous jumps) per reverse step, effectively leapfrogging several single-token transitions in one operation.<br /></li>
  <li>This controlled coarsening is a discretization strategy that trades per-step accuracy for sampling speed, akin to <strong>tau-leaping</strong> methods in stochastic chemical kinetics.<br /></li>
  <li>Empirical tuning balances <strong>speed</strong> and <strong>sample fidelity</strong>.<br /></li>
</ul>

<hr />

<h1 id="end-to-end-pipeline-and-illustrative-text-generation">End-to-end pipeline and illustrative text generation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-47-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-47-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-47-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-47-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The complete pipeline:</p>
<ol>
  <li>Sample data from P_data.<br /></li>
  <li>Define a forward discrete diffusion process (e.g., <strong>mask</strong> or <strong>uniform</strong>).<br /></li>
  <li>Train a time-conditioned <strong>concrete score network</strong> with denoising score-entropy.<br /></li>
  <li>Run the learned reverse chain with multi-step acceleration to generate sequences.<br /></li>
</ol>

<p>Notes:</p>
<ul>
  <li>Example generations using models of <strong>GPT-2 scale</strong> demonstrate plausible and coherent text outputs, indicating the method can produce high-quality sequences without autoregressive decoding.<br /></li>
  <li>The pipeline is <strong>modular</strong>: choice of Q, corruption schedule, and sampling discretization together control quality and speed.<br /></li>
</ul>

<hr />

<h1 id="quality-versus-compute-trade-off-and-comparison-to-autoregressive-models">Quality-versus-compute trade-off and comparison to autoregressive models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-49-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-49-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-49-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-49-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Discrete diffusion models (termed <strong>SDD</strong> in the lecture) exhibit a trade-off between the number of reverse sampling steps and sample quality:</p>
<ul>
  <li>Increasing steps yields monotonic improvements in generation quality, often in a <strong>log-linear</strong> manner.<br /></li>
  <li>At moderate numbers of steps (e.g., ~64) SDD can match autoregressive models like <strong>GPT-2</strong> in perceived coherence while offering faster <strong>parallel</strong> sampling.<br /></li>
  <li>With larger step budgets SDD quality improves further and can surpass small autoregressive baselines.<br /></li>
</ul>

<p>This demonstrates controllable compute–quality trade-offs that are absent in strictly sequential autoregressive decoders.<br /></p>

<hr />

<h1 id="controllable-generation-and-flexible-conditioning-prompting-and-infill">Controllable generation and flexible conditioning (prompting and infill)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-52-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-52-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-52-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-52-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The discrete diffusion framework supports conditioning on arbitrary token subsets, enabling principled <strong>in-filling</strong> and <strong>arbitrary-location prompting</strong>:</p>
<ul>
  <li>Clamp known tokens and sample the rest under the reverse dynamics.<br /></li>
  <li>Because reverse transitions operate over entire sequences rather than strictly left-to-right, conditioning can be applied at middle or multiple positions and the sampler will generate context-consistent completions around fixed tokens.<br /></li>
</ul>

<p>This flexibility yields control modalities that are difficult to realize with strictly causal autoregressive decoders.<br /></p>

<hr />

<h1 id="likelihood-evaluation-and-a-perplexity-upper-bound-via-denoising-score-entropy">Likelihood evaluation and a perplexity upper bound via denoising score entropy</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-54-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-54-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-54-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-54-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Perplexity, the standard metric for language-model likelihood evaluation, can be <strong>upper-bounded</strong> for discrete diffusion models by an expected denoising score-entropy functional plus a known constant under mild conditions on the base distribution and diffusion schedule.<br /></p>

<p>Concretely:</p>
<ul>
  <li>Negative log-likelihood is bounded by an integral of expectations that mirrors the denoising objective used in training.<br /></li>
  <li>This allows reporting <strong>perplexity bounds</strong> computed from the learned model and corruption kernel, providing a principled comparison to autoregressive likelihoods.<br /></li>
</ul>

<hr />

<h1 id="empirical-likelihood-and-perplexity-comparisons-to-autoregressive-baselines">Empirical likelihood and perplexity comparisons to autoregressive baselines</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-57-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-57-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-57-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-57-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Empirical evaluations show:</p>
<ul>
  <li>Autoregressive <strong>GPT-2</strong> models often achieve the best (lowest) perplexity.<br /></li>
  <li>Discrete diffusion (SDD) models with <strong>absorbing-mask</strong> transitions frequently achieve comparable perplexities within a modest bound (e.g., within ~10%) and sometimes outperform on certain datasets.<br /></li>
</ul>

<p>Caveat:</p>
<ul>
  <li>The reported numbers are <strong>upper bounds</strong> on negative log-likelihood derived from the denoising objective, so close performance indicates SDD models cover data modes sufficiently well and can be competitive with autoregressive baselines on standard language datasets.<br /></li>
</ul>

<hr />

<h1 id="summary-and-concluding-observations">Summary and concluding observations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec18/00-59-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec18/00-59-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec18/00-59-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec18/00-59-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>In summary:</p>
<ul>
  <li>Continuous generative methods do not directly transfer to sparse token spaces, making discrete probabilistic modeling challenging.<br /></li>
  <li><strong>Score-based techniques</strong> can be generalized by replacing continuous gradients with finite-difference <strong>density ratios</strong> (the <strong>concrete score</strong>).<br /></li>
  <li>Training via <strong>score-entropy</strong> and <strong>denoising score-entropy</strong> yields tractable objectives; diffusion forward mechanisms (Q matrices) provide principled corruption; and reverse-time sampling driven by learned concrete scores permits <strong>parallel</strong> and <strong>controllable</strong> generation.<br /></li>
</ul>

<p>Empirical results indicate discrete diffusion models can <strong>match or surpass</strong> autoregressive baselines in generation quality and approach comparable likelihoods, opening an alternative paradigm for discrete sequence modeling.</p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 17 - Discrete Latent Variable Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec17/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 17 - Discrete Latent Variable Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec17</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec17/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/vBv7Mf1zsg8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-and-plan">Lecture overview and plan</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-00-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-00-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-00-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-00-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment outlines the lecture’s objective to continue the exploration of <strong>diffusion models</strong> and to connect them with <strong>stochastic differential equations (SDEs)</strong> and <strong>ODE-based samplers</strong>.<br /></p>

<p>It frames <strong>score-based models</strong> and <strong>denoising diffusion probabilistic models (DDPMs)</strong> as two perspectives on the same <strong>noising/denoising family</strong> of generative models and motivates moving from <strong>discrete-time VAEs</strong> to <strong>continuous-time</strong> formulations.<br /></p>

<p>Practical interests emphasized in the lecture:<br /></p>
<ul>
  <li><strong>Efficient samplers</strong> (fewer model evaluations, accelerated integrators)</li>
  <li><strong>Exact and approximate likelihood evaluation</strong> (continuous-time change-of-variables)</li>
  <li><strong>Conditional generation techniques</strong> (text-to-image, classifier-guided sampling)</li>
</ul>

<p>The stage is set for formal descriptions of:<br /></p>
<ol>
  <li><strong>Forward noising processes</strong> (how data is corrupted over time)</li>
  <li><strong>Reverse-time dynamics</strong> (how to undo corruption using scores/denoisers)</li>
  <li><strong>Algorithmic implications</strong> for sampling and training (discretizations, solvers, and trade-offs)<br /></li>
</ol>

<hr />

<h1 id="score-based-models-and-ddpms-are-equivalent-perspectives-on-noisingdenoising-generative-modeling">Score-based models and DDPMs are equivalent perspectives on noising/denoising generative modeling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-01-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-01-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-01-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-01-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Score-based models</strong> and <strong>denoising diffusion probabilistic models (DDPMs)</strong> are two ways to define the same family of distributions obtained by progressively adding noise to data and learning a reverse process to remove that noise.<br /></p>

<p>Key structure:<br /></p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The <strong>forward (encoder) process</strong> is a sequence of Gaussian perturbations **q(x_t</td>
          <td>x_{t-1})** that gradually destroys data structure until pure noise is reached.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The <strong>learned reverse (decoder)</strong> defines **p_θ(x_{t-1}</td>
          <td>x_t)**, typically parameterized as Gaussians whose means are predicted by neural networks.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Optimization link:<br /></p>
<ul>
  <li>Optimizing the variational <strong>ELBO</strong> for DDPMs reduces, after algebra, to matching the reverse decoders to the forward noising transitions.</li>
  <li>This matching is equivalent to learning <strong>denoisers</strong> or <strong>score functions</strong> at each noise level.</li>
</ul>

<p>Consequence:<br /></p>
<ul>
  <li>Training either framework yields a sequence of denoisers: in DDPMs these are <strong>discrete-step decoders</strong>, and in score-based models they are <strong>noise-conditional score estimators</strong>.<br /></li>
</ul>

<hr />

<h1 id="elbo-training-reduces-to-learning-a-sequence-of-denoisers-and-score-functions">ELBO training reduces to learning a sequence of denoisers and score functions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-04-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-04-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-04-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-04-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>variational ELBO</strong> for diffusion models decomposes into terms that are KL divergences between the forward noising path and the modelled reverse path.<br /></p>

<p>Important simplifications:<br /></p>
<ul>
  <li>These KL terms simplify to <strong>denoising</strong> or <strong>score-matching</strong> objectives at individual noise levels.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Learning the optimal reverse conditional **p_θ(x_{t-1}</td>
          <td>x_t)** therefore amounts to estimating the <strong>score</strong> of the perturbed-data density or equivalently training <strong>denoisers</strong> that map noisy inputs to cleaner reconstructions.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Discrete vs continuous:<br /></p>
<ul>
  <li>In the <strong>discrete DDPM</strong> setup you train a finite set of denoisers (one per step).</li>
  <li>In the <strong>continuous</strong> formulation you train a continuum of denoisers (scores indexed by time).</li>
</ul>

<p>Why architectures and losses align:<br /></p>
<ul>
  <li>This formal equivalence explains why architectures and loss functions in <strong>score-based models</strong> and <strong>DDPMs</strong> are closely related.</li>
  <li>It also explains why sampling updates resemble <strong>Langevin-type gradient steps plus noise</strong>.<br /></li>
</ul>

<hr />

<h1 id="continuous-time-diffusion-the-diffusion-limit-generalizes-discrete-step-ddpms">Continuous-time diffusion (the diffusion limit) generalizes discrete-step DDPMs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-06-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-06-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-06-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-06-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Taking the <strong>diffusion limit</strong> replaces the discrete sequence of noise levels with a continuum indexed by <strong>t ∈ [0, T]</strong>, producing marginal densities <strong>p_t(x)</strong> that interpolate from data (t=0) to noise (t=T).<br /></p>

<p>How this arises:<br /></p>
<ul>
  <li>Viewing forward noising as an infinitesimal limit of Gaussian perturbations leads to an <strong>SDE</strong> that describes how <strong>x_t</strong> evolves under infinitesimal drift and diffusion.</li>
</ul>

<p>Practical benefits of the continuous view:<br /></p>
<ul>
  <li>Access to continuous-time tools: <strong>SDE/ODE solvers</strong>, <strong>Fokker–Planck</strong> analysis.</li>
  <li>Unified interpretation: <strong>DDPMs</strong> and <strong>score-based methods</strong> appear as discretizations of the same underlying process.</li>
  <li>Motivates more sophisticated numerical schemes and sampler designs because discrete DDPMs are time discretizations of the continuum.<br /></li>
</ul>

<hr />

<h1 id="the-forward-process-is-an-sde-with-drift-and-diffusion-describing-noise-addition">The forward process is an SDE with drift and diffusion describing noise addition</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-08-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-08-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-08-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-08-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Formally the forward noising process can be written as an SDE: <strong>dX_t = f(X_t,t) dt + g(t) dW_t</strong>, where:<br /></p>
<ul>
  <li><strong>f(X_t,t)</strong> is the <strong>drift</strong> field,</li>
  <li><strong>g(t)</strong> scales the noise, and</li>
  <li><strong>dW_t</strong> is the <strong>Wiener increment</strong> (Brownian motion).<br /></li>
</ul>

<p>Key points:<br /></p>
<ul>
  <li>In the simple isotropic noising case <strong>f = 0</strong>, and the dynamics reduce to <strong>pure diffusion</strong>.</li>
  <li>The differential notation <strong>dX_t</strong> captures infinitesimal changes obtained by taking finer discretizations of the discrete chain; integrating these infinitesimal updates yields the observed smoothed densities <strong>p_t(x)</strong>.</li>
  <li>Each marginal <strong>p_t(x)</strong> is a smoothed version of the data density because the forward SDE corresponds to incremental convolution with Gaussian kernels.<br /></li>
</ul>

<p>Why this matters:<br /></p>
<ul>
  <li>The SDE formalism provides the precise probabilistic object whose <strong>reverse-time dynamics</strong> will be used for generative sampling.<br /></li>
</ul>

<hr />

<h1 id="reverse-time-sde-introduces-a-score-dependent-drift-used-for-sampling">Reverse-time SDE introduces a score-dependent drift used for sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-11-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-11-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-11-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-11-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Reversing the forward SDE in time yields another SDE whose drift depends on the <strong>score</strong> (the gradient of log density) of the forward marginal <strong>p_t(x)</strong>.<br /></p>

<p>Intuition and sampling procedure:<br /></p>
<ol>
  <li>The <strong>score term</strong> provides the structured velocity field needed to move samples from noise toward regions of high data probability.</li>
  <li>To sample from the data distribution, initialize <strong>x_T</strong> from the simple prior and integrate the <strong>reverse SDE</strong> toward <strong>t = 0</strong>.</li>
  <li>Doing this requires access to the time-dependent score <strong>s(x,t) = ∇_x log p_t(x)</strong>.</li>
</ol>

<p>Notes on drift:<br /></p>
<ul>
  <li>When the forward SDE has <strong>zero drift</strong>, the reverse SDE’s drift is exactly the score term.</li>
  <li>If the forward SDE includes drift, that drift must be transformed appropriately in the reverse dynamics.</li>
</ul>

<p>Practical implication:<br /></p>
<ul>
  <li>The need to know <strong>s(x,t)</strong> motivates training neural networks to estimate it by <strong>score matching</strong> so the reverse SDE can be simulated approximately during inference.<br /></li>
</ul>

<hr />

<h1 id="score-estimation-and-discretization-yield-ddpm-like-denoising-updates">Score estimation and discretization yield DDPM-like denoising updates</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-15-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-15-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-15-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-15-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>score-based training objective</strong> estimates the continuum family of scores <strong>s(x,t)</strong> for all <strong>t</strong> using <strong>noise-conditional score matching</strong> losses, which regress model scores to true scores in an L2 sense under perturbed-data distributions.<br /></p>

<p>From training to sampling:<br /></p>
<ul>
  <li>Plugging estimated scores into the reverse SDE and discretizing time gives iterative update rules that are algebraically equivalent to the denoising and Langevin-style steps used in DDPMs: at each discrete time step, follow the estimated score and add a controlled noise term.</li>
</ul>

<p>Discretization trade-offs:<br /></p>
<ul>
  <li><strong>More steps</strong> → smaller numerical error, closer to continuous reverse flow.</li>
  <li><strong>Fewer, larger steps</strong> → faster, but introduce bias.</li>
</ul>

<p>Hybrid samplers:<br /></p>
<ul>
  <li>Combining a <strong>predictor</strong> (deterministic step) and a <strong>corrector</strong> (stochastic refinement) yields hybrid samplers that trade compute for sample accuracy.<br /></li>
</ul>

<hr />

<h1 id="an-equivalent-deterministic-ode-flow-exists-with-identical-marginals">An equivalent deterministic ODE (flow) exists with identical marginals</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-18-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-18-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-18-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-18-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Under mild conditions the stochastic reverse dynamics can be reparameterized into a <strong>deterministic ODE</strong> whose flow preserves the same family of marginal densities <strong>p_t(x)</strong> as the SDE.<br /></p>

<p>Consequences and structure:<br /></p>
<ul>
  <li>The <strong>ODE removes per-step stochasticity</strong> and concentrates randomness into the initial condition.</li>
  <li>The ODE defines an <strong>invertible mapping</strong> between latent noise <strong>x_T</strong> and data <strong>x_0</strong>, so solving it forward or backward yields a <strong>continuous normalizing flow</strong> parameterized by the score or related network.</li>
  <li>This enables interpreting diffusion models as <strong>continuous-time normalizing flows</strong> and leveraging <strong>neural ODE</strong> machinery: the model becomes a neural ODE whose vector field is given by score-derived functions.</li>
  <li>The deterministic ODE has unique, non-crossing trajectories, ensuring <strong>invertibility</strong> and enabling exact change-of-variable computations for likelihoods given an exact score.<br /></li>
</ul>

<hr />

<h1 id="likelihood-evaluation-via-ode-change-of-variables-uses-trace-jacobian-integration">Likelihood evaluation via ODE change-of-variables uses trace-Jacobian integration</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-22-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-22-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-22-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-22-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>For the deterministic ODE formulation the likelihood of a data point can be computed by mapping it to the latent noise space via solving the ODE backward and applying the continuous change-of-variables formula.<br /></p>

<p>Concretely:<br /></p>
<ul>
  <li>The log-likelihood obeys <strong>log p(x_0) = log p(x_T) + ∫_0^T -Tr(∂f/∂x)(x_t,t) dt</strong>, where <strong>f</strong> is the ODE vector field and <strong>Tr(∂f/∂x)</strong> is the trace of its Jacobian.</li>
  <li>The integral can be evaluated alongside ODE integration using black-box ODE solvers and <strong>stochastic trace estimators</strong> when dimensions are large.</li>
</ul>

<p>Practical notes:<br /></p>
<ul>
  <li>Numerical ODE solver technology gives access to <strong>adaptive step-size control</strong>, <strong>higher-order methods</strong>, and other techniques to improve sampling speed and accuracy.</li>
  <li>This perspective enables competitive likelihood estimation even when training used score-matching rather than maximum likelihood.</li>
  <li>Computing likelihoods requires integrating and potentially differentiating through the ODE solver for training-time likelihood optimization (numerically intensive), but evaluation alone is practical.<br /></li>
</ul>

<hr />

<h1 id="sampling-tradeoffs-odes-give-faster-deterministic-samples-while-sdes-provide-robustness">Sampling tradeoffs: ODEs give faster deterministic samples while SDEs provide robustness</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-25-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-25-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-25-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-25-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Trade-offs between ODE and SDE samplers:<br /></p>
<ul>
  <li><strong>ODE (deterministic)</strong> samplers: faster, noise-free generation, can use large adaptive steps, and enable exact likelihood computation if the score is exact.</li>
  <li><strong>SDE (stochastic)</strong> samplers and <strong>predictor-corrector</strong> hybrids: maintain stochasticity at each step, keeping inputs closer to the training distribution and often reducing compounding error—usually higher sample quality at greater compute cost.</li>
</ul>

<p>Discretization bias is the central tradeoff:<br /></p>
<ul>
  <li><strong>Coarse discretization</strong> (fewer, larger steps) → speed but greater numerical error.</li>
  <li><strong>Fine discretization</strong> (many small steps) → accuracy but higher compute.</li>
</ul>

<p>Hybrid approaches and MCMC-style refinements give a flexible accuracy-vs-cost spectrum for inference.<br /></p>

<hr />

<h1 id="accelerated-sampling-methods-ddim-and-coarse-discretization-reduce-step-count">Accelerated sampling methods (DDIM and coarse discretization) reduce step count</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-28-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-28-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-28-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-28-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Accelerated samplers</strong> exploit structure in diffusion dynamics to reduce required model evaluations from thousands to tens or hundreds while keeping acceptable sample quality.<br /></p>

<p>Examples and techniques:<br /></p>
<ul>
  <li><strong>DDIM</strong> (deterministic non-Markovian samplers): re-parameterizes steps to take larger jumps along a discretized approximation of the reverse dynamics.</li>
  <li><strong>Closed-form solutions</strong> for linear components of the SDE allow coarser time grids and larger steps, dramatically reducing network calls.</li>
</ul>

<p>Practical considerations:<br /></p>
<ul>
  <li>Coarse discretization yields large speed-ups but increases approximation error.</li>
  <li>Discretization granularity is a tunable knob traded against fidelity; advanced step selection and bespoke integrators further improve coarse-step samplers.<br /></li>
</ul>

<hr />

<h1 id="parallel-in-time-ode-solvers-trade-compute-for-wall-clock-speed-by-denoising-trajectory-segments-concurrently">Parallel-in-time ODE solvers trade compute for wall-clock speed by denoising trajectory segments concurrently</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-32-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-32-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-32-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-32-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Parallel-in-time</strong> techniques partition the ODE trajectory and use multiple accelerators to estimate trajectory segments concurrently instead of sequentially.<br /></p>

<p>How it works and trade-offs:<br /></p>
<ul>
  <li>Relies on good initial guesses for trajectory segments and iterative refinement.</li>
  <li>Many GPUs refine different temporal windows in parallel, effectively denoising multiple timesteps at once.</li>
  <li>This reduces wall-clock latency at the expense of greater aggregate compute and communication overhead.</li>
</ul>

<p>When it’s useful:<br /></p>
<ul>
  <li>Particularly effective for large models with stringent latency requirements and abundant parallel resources.</li>
  <li>Produces the same asymptotic solution as sequential integration but with much lower response time when engineered carefully.<br /></li>
</ul>

<hr />

<h1 id="progressive-distillation-and-consistency-models-compress-multi-step-samplers-into-very-few-steps">Progressive distillation and consistency models compress multi-step samplers into very few steps</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-35-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-35-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-35-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-35-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Progressive distillation</strong> compresses multi-step denoising into progressively fewer steps by training student networks to emulate a teacher’s multi-step behavior.<br /></p>

<p>Procedure (high-level):<br /></p>
<ol>
  <li>Train a teacher that performs k denoising steps.</li>
  <li>Train a student to map the teacher’s state after k steps to the teacher’s state after 2k steps.</li>
  <li>Recursively repeat, halving the step budget each distillation round until a small-step or one-step sampler is obtained.</li>
</ol>

<p>Outcomes and relatives:<br /></p>
<ul>
  <li>Produces exponential compression of the original step budget and dramatically reduces inference cost while preserving sample quality.</li>
  <li><strong>Consistency models</strong> pursue a related objective by directly learning mappings from noisy inputs to denoised outputs that are consistent across step sizes; they enable one-shot or very low-step generation with specialized losses.</li>
</ul>

<p>These distilled and consistency approaches are core to recent real-time text-to-image demos.<br /></p>

<hr />

<h1 id="latent-diffusion-uses-a-vae-style-encoder-to-reduce-dimensionality-and-accelerate-training-and-sampling">Latent diffusion uses a VAE-style encoder to reduce dimensionality and accelerate training and sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-39-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-39-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-39-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-39-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Latent diffusion models</strong> insert an encoder/decoder pair (VAE or autoencoder) upstream of the diffusion prior so the diffusion operates in a lower-dimensional continuous latent space rather than raw pixel space.<br /></p>

<p>Why this helps:<br /></p>
<ul>
  <li>Pretraining the encoder/decoder to prioritize reconstruction quality lets the diffusion prior act on <strong>compact latent codes</strong>, reducing memory and compute costs while retaining perceptual fidelity via the decoder.</li>
  <li>Enables scaling diffusion models to very large datasets and high resolutions.</li>
  <li>Supports modalities where raw inputs are discrete (e.g., text) by first mapping to continuous latents.</li>
</ul>

<p>Implementation notes:<br /></p>
<ul>
  <li>Often the encoder is pretrained and frozen for diffusion training, so the diffusion prior need not strictly match a Gaussian latent prior from the VAE; <strong>weak regularization</strong> suffices when the diffusion model acts as a powerful learned prior.</li>
  <li>This design underlies large-scale systems such as <strong>Stable Diffusion</strong>.<br /></li>
</ul>

<hr />

<h1 id="conditional-generation-is-implemented-by-supplying-side-information-to-the-scoredenoiser-network">Conditional generation is implemented by supplying side information to the score/denoiser network</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-44-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-44-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-44-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-44-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Conditional diffusion modeling</strong> of <strong>p(x | y)</strong> requires the score/denoiser to accept the conditioning variable <strong>y</strong> (class labels, captions, measurements) in addition to the noisy image <strong>x_t</strong> and time <strong>t</strong>.<br /></p>

<p>Architectural choices for injecting y:<br /></p>
<ul>
  <li><strong>Concatenation</strong> of embeddings with inputs,</li>
  <li><strong>FiLM layers</strong> (feature-wise linear modulation),</li>
  <li><strong>Cross-attention</strong> between image latents and text embeddings.</li>
</ul>

<p>Effect on sampling:<br /></p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The conditional score **s(x,t</td>
          <td>y)** steers reverse dynamics toward regions of the data manifold compatible with <strong>y</strong>, enabling conditional sampling by integrating the conditional SDE/ODE.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Proper handling of the conditioning signal at each noise level is essential because the denoiser must reason about both noisy inputs and the desired conditioning simultaneously.<br /></li>
</ul>

<hr />

<h1 id="bayesian-conditioning-via-classifier-guidance-posterior-scores-equal-prior-plus-likelihood-score">Bayesian conditioning via classifier guidance: posterior scores equal prior plus likelihood score</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-50-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-50-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-50-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-50-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>By Bayes’ rule <strong>p(x | y) ∝ p(x) p(y | x)</strong>, and taking gradients of log densities yields the posterior score as the sum of the prior score and the likelihood score:<br /></p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**∇_x log p(x</td>
          <td>y) = ∇_x log p(x) + ∇_x log p(y</td>
          <td>x)**.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Practical use:<br /></p>
<ul>
  <li>This identity lets you perform conditional sampling without retraining a conditional diffusion model by augmenting the prior score estimator with gradients from a <strong>classifier</strong> or a <strong>differentiable likelihood</strong> term.</li>
  <li>The intractable normalization constant disappears under gradient differentiation, making <strong>score-level Bayes conditioning</strong> tractable so long as gradients of the likelihood or classifier w.r.t. inputs are available.</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>This provides a principled method to steer unconditional generative models toward desired attributes defined by **p(y</td>
      <td>x)**.<br /></td>
    </tr>
  </tbody>
</table>

<hr />

<h1 id="classifier-guidance-and-classifier-free-guidance-provide-practical-conditioning-mechanisms">Classifier guidance and classifier-free guidance provide practical conditioning mechanisms</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-54-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-54-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-54-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-54-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Classifier guidance</strong> uses an external classifier to compute <strong>∇_x log p(y | x)</strong> and adds this term to the prior score during sampling, but the classifier must be evaluated on <strong>noisy inputs x_t</strong> or be adapted to those noise levels.<br /></p>

<p><strong>Classifier-free guidance</strong> sidesteps a separate classifier by training a single diffusion model on both conditional and unconditional data, then forming an implicit classifier by taking a scaled difference between conditional and unconditional score estimates during sampling.<br /></p>

<p>Comparison and practice:<br /></p>
<ul>
  <li>Both approaches approximate the posterior score and allow controllable sampling without computing normalization integrals.</li>
  <li><strong>Classifier-free guidance</strong> is often preferred for pipeline simplicity and empirical sample quality.</li>
  <li>These guidance techniques are used for editing, conditional synthesis, and inverse problems by defining the likelihood term appropriately.<br /></li>
</ul>

<hr />

<h1 id="applications-include-image-editing-inverse-problems-and-medical-imaging-where-likelihoods-encode-measurement-physics">Applications include image editing, inverse problems, and medical imaging where likelihoods encode measurement physics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/00-58-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/00-58-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/00-58-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/00-58-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The score-based Bayes conditioning framework naturally handles <strong>inverse problems</strong> where the likelihood <strong>p(y | x)</strong> describes a measurement process (e.g., MRI acquisition).<br /></p>

<p>How it applies:<br /></p>
<ul>
  <li>Adding the measurement gradient to the prior score yields principled posterior sampling for <strong>reconstruction</strong> and <strong>uncertainty quantification</strong>.</li>
  <li>For interactive editing, conditioning signals can be sparse (sketches, strokes), and the forward model plus denoiser produce coherent conditioned images by following the posterior score during reverse sampling.</li>
  <li><strong>Text-to-image</strong>: a caption embedding serves as <strong>y</strong> and conditions the denoiser, typically implemented via <strong>cross-attention</strong> between image latents and text embeddings.</li>
</ul>

<p>Deployment trade-offs:<br /></p>
<ul>
  <li>Balance <strong>conditioning strength</strong>, computational budget, and the fidelity of pretrained encoders/classifiers used in the guidance term.<br /></li>
</ul>

<hr />

<h1 id="classifier-free-guidance-and-other-practical-tricks-avoid-explicit-classifier-training">Classifier-free guidance and other practical tricks avoid explicit classifier training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/01-02-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/01-02-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/01-02-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/01-02-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Classifier-free guidance</strong> achieves the effect of classifier guidance by training a model that sometimes sees conditioning <strong>y</strong> and sometimes sees none, then computing a guidance vector as the difference between conditional and unconditional score outputs scaled by a guidance weight at sampling time.<br /></p>

<p>Practical alternatives and tricks:<br /></p>
<ul>
  <li>Estimating gradients w.r.t. noisy inputs, using <strong>learned guidance models</strong>, or approximating likelihood gradients through <strong>surrogate models</strong>.</li>
  <li>These variants aim for stability and scalability in large models.</li>
</ul>

<p>Empirical preference:<br /></p>
<ul>
  <li>Simple, robust conditioning procedures like <strong>classifier-free guidance</strong> are favored because they integrate easily into existing diffusion architectures and scale to large datasets.<br /></li>
</ul>

<hr />

<h1 id="recent-engineering-advances-yield-real-time-or-very-low-step-generation-by-combining-distillation-latent-models-and-advanced-solvers">Recent engineering advances yield real-time or very low-step generation by combining distillation, latent models, and advanced solvers</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/01-06-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/01-06-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/01-06-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/01-06-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>State-of-the-art systems achieve real-time or near-real-time text-to-image synthesis by composing several advances:<br /></p>
<ul>
  <li><strong>Latent diffusion</strong> (operate in compact latent spaces)</li>
  <li><strong>Large pretrained text encoders</strong> (strong conditioning representations)</li>
  <li><strong>Progressive distillation</strong> (compress many steps into a few)</li>
  <li><strong>Optimized ODE/SDE solvers</strong> including <strong>parallel-in-time</strong> algorithms</li>
</ul>

<p>Practical outcome:<br /></p>
<ul>
  <li>Groups like Stability AI combine these elements to trade aggregate compute for low wall-clock latency while preserving perceptual quality.</li>
  <li>The continuous-time perspective unifies these techniques and enables principled derivation of new solvers, consistency objectives, and distillation strategies that further reduce inference cost.</li>
</ul>

<p>Result: a rich toolbox for selecting trade-offs between <strong>sample quality</strong>, <strong>compute</strong>, and <strong>latency</strong> tailored to application constraints.<br /></p>

<hr />

<h1 id="summary-score-estimation-unlocks-multiple-inference-paradigms-and-practical-tradeoffs">Summary: score estimation unlocks multiple inference paradigms and practical tradeoffs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec17/01-10-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec17/01-10-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec17/01-10-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec17/01-10-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Estimating the time-dependent <strong>score function</strong> is the central learning problem that enables multiple inference paradigms:<br /></p>
<ul>
  <li><strong>Stochastic reverse SDE sampling</strong> (robust, high quality)</li>
  <li><strong>Deterministic ODE flow sampling</strong> (fast transforms, likelihoods)</li>
  <li><strong>Accelerated coarse-step samplers</strong> (fewer evaluations)</li>
  <li><strong>Conditional generation</strong> via score-level Bayes manipulation (guided sampling)</li>
</ul>

<p>Each paradigm presents a different accuracy-compute trade-off:<br /></p>
<ul>
  <li><strong>SDEs</strong>: robustness and high quality</li>
  <li><strong>ODEs</strong>: faster deterministic transforms and likelihood evaluation</li>
  <li><strong>Distillation/consistency models</strong>: compress multi-step dynamics into few- or single-step generation</li>
</ul>

<p>Latent diffusion models reduce dimensionality to scale, while <strong>classifier-free guidance</strong> and other conditioning tricks make controlled generation practical at scale.<br /></p>

<p>Together, these techniques form a unified framework for modern diffusion-based generative modeling, spanning theory to high-performance engineering.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 16 - Score Based Diffusion Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec16/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 16 - Score Based Diffusion Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec16</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec16/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/VsllsC2JMGY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-and-goals">Lecture overview and goals</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-00-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-00-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-00-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-00-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This lecture ties together several foundational ideas in modern generative modeling: <strong>score matching</strong>, <strong>diffusion models</strong>, <strong>variational autoencoders</strong>, <strong>SDE/ODE interpretations</strong>, and practical <strong>sampling and control</strong> techniques.<br /></p>

<ul>
  <li>It explains how <strong>score-based models</strong> can be viewed both as <strong>diffusion processes</strong> and as <strong>hierarchical variational autoencoders</strong>.<br /></li>
  <li>It shows that the <strong>reverse process</strong> of a diffusion yields flows with <strong>exact likelihoods</strong> when expressed as a probability-flow ODE.<br /></li>
  <li>It outlines how <strong>numerical solvers</strong> and <strong>conditioning</strong> enable efficient, controllable generation in practice.<br /></li>
  <li>The talk previews formal comparisons between <strong>denoising score matching</strong> objectives and the <strong>evidence lower bound (ELBO)</strong>, and highlights practical topics such as <strong>sampling acceleration</strong> and conditioning for tasks like <strong>text-to-image</strong> generation.<br /></li>
</ul>

<p>This high-level overview frames the technical developments and motivates the equivalences and algorithms developed later in the lecture.<br /></p>

<hr />

<h1 id="score-based-models-represent-densities-via-their-score-function-and-are-trained-by-denoising-score-matching-on-perturbed-data">Score-based models represent densities via their score function and are trained by denoising score matching on perturbed data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-05-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-05-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-05-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-05-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Score function</strong>: the gradient of the log density with respect to inputs — it defines a vector field that points toward higher-probability regions. <br /></p>

<ul>
  <li>Neural networks are used to approximate this vector field, but <strong>direct score matching</strong> is computationally challenging in high dimensions.<br /></li>
  <li><strong>Denoising score matching</strong> solves this by training a network to predict the score of a <strong>noise-perturbed</strong> version of the data — equivalently, to learn to remove Gaussian noise from noisy inputs.<br />
    <ul>
      <li>Training reduces to a regression-like objective where the network predicts either the <strong>added noise</strong> or the <strong>denoised signal</strong>.<br /></li>
      <li><strong>Amortization</strong> across multiple noise levels is achieved by conditioning the network on the noise intensity (e.g., a time or sigma input).<br /></li>
    </ul>
  </li>
  <li>Sampling from the learned score field uses stochastic-gradient dynamics such as <strong>Langevin dynamics</strong>.<br />
    <ol>
      <li>Initialize from noise.<br /></li>
      <li>Run iterative score-guided updates (and possibly injected noise) to move samples toward higher-density regions.<br /></li>
    </ol>
  </li>
  <li>Practical methods use a sequence of noise levels — <strong>annealed</strong> or <strong>multi-scale</strong> — to progressively refine samples from pure noise toward the data manifold.<br /></li>
</ul>

<p>Trade-off: the model learns scores of <strong>noisy distributions</strong> rather than the clean data distribution. This is more scalable, but it changes the exact target of estimation (the network models s(x; sigma) for noisy marginals rather than s(x; 0)).<br /></p>

<hr />

<h1 id="the-forward-diffusion-defines-a-fixed-markov-encoder-that-maps-data-to-a-sequence-of-increasingly-noisy-latent-variables-by-adding-gaussian-noise-incrementally">The forward diffusion defines a fixed Markov encoder that maps data to a sequence of increasingly noisy latent variables by adding Gaussian noise incrementally</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-13-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-13-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-13-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-13-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Forward / encoder process</strong>: a simple, fixed <strong>Markov chain Q</strong> that maps x0 → x1 → … → xT by repeatedly adding Gaussian noise (optionally with rescaling). <br /></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Each conditional Q(xt</td>
          <td>xt-1) is a <strong>Gaussian</strong> with known mean and variance, so the full joint factorizes as a product of simple conditionals.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Because repeated Gaussian kernels compose analytically, any marginal Q(xt</td>
          <td>x0) is itself a closed-form Gaussian — this makes simulation and marginal sampling efficient.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Important design points:<br />
    <ul>
      <li>The encoder Q is <strong>not learned</strong>; it is a deterministic design choice that defines multiple noisy views of each data point.<br /></li>
      <li>The latent representation is the entire trajectory x1:T, which is higher-dimensional than x0 and generally not invertible because noise is added at every step.<br /></li>
      <li>This structured latent family is well suited to <strong>variational approximations</strong>, and it underpins treating diffusion models as <strong>hierarchical variational autoencoders</strong> with a fixed encoder.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="repeated-gaussian-perturbations-produce-analytically-tractable-marginals-and-give-the-forward-process-a-diffusion-heat-interpretation">Repeated Gaussian perturbations produce analytically tractable marginals and give the forward process a diffusion (heat) interpretation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-19-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-19-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-19-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-19-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When Gaussian noise is applied incrementally, composing the stepwise kernels yields another Gaussian whose mean and variance can be computed in closed form by composing the step parameters. <br /></p>

<ul>
  <li>As a consequence, one can directly sample any marginal Q(xt) without simulating the entire chain from x0.<br /></li>
  <li>Interpreting the incremental noising as a <strong>diffusion process</strong> gives an intuition: probability mass spreads out like heat. This continuous viewpoint motivates choosing kernels that <strong>destroy structure gradually</strong>, so the terminal distribution becomes simple and easy to sample from.<br /></li>
  <li>Two practical requirements arise from this interpretation:<br />
    <ul>
      <li>The forward process must reliably <strong>erase structure</strong> so the end state is a known simple prior.<br /></li>
      <li>The intermediate marginals must be <strong>efficiently sampleable</strong>, because they supply training examples for denoising score matching.<br /></li>
    </ul>
  </li>
  <li>These considerations justify the widespread use of <strong>Gaussian transition kernels</strong> and carefully designed <strong>noise schedules</strong> in diffusion models.<br /></li>
</ul>

<hr />

<h1 id="the-reverse-generative-process-is-approximated-by-parameterizing-reverse-conditionals-as-gaussians-with-neural-network-predicted-parameters">The reverse generative process is approximated by parameterizing reverse conditionals as Gaussians with neural-network predicted parameters</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-27-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-27-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-27-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-27-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The true reverse kernel Q(xt-1 | xt) is generally unknown, so the learned decoder <strong>P_theta</strong> approximates it with Gaussian conditionals P_theta(xt-1 | xt) whose means (and sometimes variances) are produced by neural networks.<br /></p>

<p>Sampling from the generative model proceeds as follows:<br /></p>
<ol>
  <li>Sample xT from a simple prior (e.g., standard Gaussian that the forward process converges to).<br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>For t = T, …, 1: sample xt-1 from P_theta(xt-1</td>
          <td>xt).<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Output x0 as the generated sample.<br /></li>
</ol>

<ul>
  <li>This mirrors the <strong>variational decoder</strong> in a hierarchical VAE: the fixed forward chain Q is the encoder and P_theta is the learned decoder.<br /></li>
  <li>Parameters theta are chosen to make the approximate reverse conditionals close to the true reverse conditionals (typically by minimizing KL divergence terms in the ELBO).<br /></li>
  <li>Proper selection of stepwise noise parameters (<strong>betas/alphas</strong>) ensures the forward chain actually converges to the simple prior and makes the variational approximation tractable to learn.<br /></li>
</ul>

<hr />

<h1 id="maximizing-the-diffusion-elbo-with-a-particular-mean-parameterization-is-equivalent-to-denoising-score-matching-which-yields-the-ddpm-training-objective">Maximizing the diffusion ELBO with a particular mean parameterization is equivalent to denoising score matching, which yields the DDPM training objective</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-37-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-37-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-37-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-37-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Evidence lower bound (ELBO)</strong> for the diffusion VAE compares the fixed forward chain Q(x1:T | x0) to the learned reverse chain P_theta(x0:T) and simplifies to tractable terms because Q is Gaussian and fixed.<br /></p>

<ul>
  <li>If the decoder means are parameterized via an <strong>epsilon network</strong> that predicts the noise added to xt (so the mean is xt minus a scaled estimate of that noise), the ELBO terms algebraically simplify and become equivalent to a <strong>denoising score matching</strong> objective that trains the network to predict the added noise.<br />
    <ul>
      <li>This explains why the <strong>DDPM</strong> training procedure, which directly trains a noise-predicting network, matches a principled variational objective.<br /></li>
    </ul>
  </li>
  <li>Sampling options after training:<br />
    <ul>
      <li><strong>Ancestral sampling</strong> from the learned decoder Gaussians (the DDPM procedure).<br /></li>
      <li><strong>Langevin-style iterative refinement</strong> using the learned score model.<br /></li>
    </ul>
  </li>
  <li>Both approaches take the form of gradient-plus-noise steps, but they differ in <strong>discretization</strong> and <strong>scaling</strong>, which leads to different empirical behavior and trade-offs.<br /></li>
</ul>

<hr />

<h1 id="practical-model-design-hinges-on-choices-for-the-fixed-encoder-noise-schedule-loss-weighting-and-neural-architecture">Practical model design hinges on choices for the fixed encoder, noise schedule, loss weighting, and neural architecture</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-48-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-48-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-48-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-48-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Fixing the forward encoder to incremental Gaussian noising simplifies training and often yields better sample quality than attempting to jointly learn a flexible encoder. <br /></p>

<ul>
  <li>Trade-offs of learning the encoder vs keeping it fixed:<br />
    <ul>
      <li>Fixed encoder: simpler optimization, often better sample fidelity.<br /></li>
      <li>Learned encoder: can improve the ELBO but may reduce sample fidelity and increase optimization complexity.<br /></li>
    </ul>
  </li>
  <li><strong>Loss weighting factors (lambda_t)</strong> control emphasis on different noise levels in the ELBO.<br />
    <ul>
      <li>In practice, practitioners often use uniform or scaled weights rather than the exact ELBO-prescribed weights — this alters likelihood optimization behavior and can improve perceptual sample quality.<br /></li>
    </ul>
  </li>
  <li><strong>Beta schedule</strong> (and derived alphas) controls how quickly structure is destroyed. It can be tuned or learned to trade off training stability, sample quality, and compute cost; many empirical studies focus on finding schedules that give good generative performance.<br /></li>
  <li>Architectural choices:<br />
    <ul>
      <li><strong>U-Net–like image-to-image networks</strong> are the dominant practical choice for the epsilon or score network because they provide strong inductive biases for dense prediction.<br /></li>
      <li><strong>Transformers</strong> and other architectures can also be applied, but they typically require careful engineering to match U-Net performance in this domain.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="the-infinitesimal-limit-of-the-discrete-diffusion-yields-a-continuous-time-sde-whose-time-reversal-depends-on-the-score-function-and-is-solved-numerically-for-sampling">The infinitesimal limit of the discrete diffusion yields a continuous-time SDE whose time-reversal depends on the score function and is solved numerically for sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/00-59-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/00-59-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/00-59-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/00-59-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>As the discretization is refined, the forward Markov chain converges to a <strong>stochastic differential equation (SDE)</strong> that describes x_t with a deterministic drift and an infinitesimal diffusion term — this forward SDE corresponds to the encoder diffusion.<br /></p>

<ul>
  <li>The exact reverse-time process that maps noise back to data is another SDE whose drift depends explicitly on the <strong>time-dependent score function</strong> of the forward marginal at time t. Therefore, an accurate time-dependent score model s_theta(x, t) is sufficient to characterize the exact reverse dynamics.<br /></li>
  <li>In practice:<br />
    <ul>
      <li>The learned score network evaluates s_theta(x, t).<br /></li>
      <li>Numerical <strong>SDE solvers</strong> discretize the reverse SDE to generate samples.<br /></li>
      <li>Different solver families — <strong>predictor</strong>, <strong>corrector</strong>, or <strong>predictor–corrector</strong> — correspond to different sampling algorithms and discretization strategies.<br /></li>
    </ul>
  </li>
  <li>Score-based sampling methods often interleave deterministic score-driven updates with localized MCMC or Langevin corrections to control numerical error; <strong>DDPM</strong> corresponds to a specific <strong>Euler–Maruyama</strong> discretization, while score-based frameworks can use more sophisticated integrators for improved trade-offs.<br /></li>
</ul>

<hr />

<h1 id="the-diffusion-sde-admits-an-equivalent-deterministic-probability-flow-ode-with-identical-time-marginals-enabling-invertible-flow-formulations-and-exact-likelihood-computation">The diffusion SDE admits an equivalent deterministic probability-flow ODE with identical time-marginals, enabling invertible flow formulations and exact likelihood computation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec16/01-07-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec16/01-07-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec16/01-07-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec16/01-07-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>There exists a <strong>probability-flow ordinary differential equation (ODE)</strong> whose solution paths are deterministic and whose marginals at each time match those of the original SDE; the ODE drift can be expressed in terms of the score function and known SDE coefficients.<br /></p>

<ul>
  <li>Because the ODE mapping from xT to x0 is deterministic (and typically invertible), it defines a <strong>continuous normalizing flow</strong> and therefore permits <strong>exact likelihood evaluation</strong> via the change-of-variables formula by integrating the instantaneous Jacobian trace along the ODE trajectory.<br /></li>
  <li>Converting a diffusion model to this deterministic flow requires the same time-dependent score estimates used for the reverse SDE, so both <strong>likelihoods</strong> and <strong>efficient deterministic sampling</strong> depend critically on score accuracy.<br /></li>
  <li>The ODE perspective offers two principal advantages:<br />
    <ul>
      <li>Deterministic, potentially faster sampling using advanced ODE solvers.<br /></li>
      <li>Access to exact likelihoods for evaluation, likelihood-based training, or model comparison.<br /></li>
    </ul>
  </li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 15 - Evaluation of Generative Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec15/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 15 - Evaluation of Generative Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec15</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec15/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/MJt_ahtO-to" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-focus-on-evaluating-generative-models">Lecture overview: focus on evaluating generative models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-01-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-01-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-01-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-01-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces the lecture objective: to survey methods for evaluating <strong>generative models</strong>, and to emphasize that <strong>evaluation is a challenging open problem with no single consensus metric</strong>.<br /></p>

<p>Evaluation is framed as essential for:</p>
<ul>
  <li><strong>Comparing model families</strong> — e.g., <strong>autoregressive models</strong>, <strong>flows</strong>, <strong>latent-variable models</strong>, <strong>energy-based models (EBMs)</strong>, <strong>GANs</strong>, and <strong>score-based models</strong>.<br /></li>
  <li><strong>Guiding model and objective selection</strong> in both research and engineering contexts.<br /></li>
</ul>

<p>Motivations and connections:</p>
<ul>
  <li>Evaluation helps decide which model is appropriate for a particular <strong>dataset</strong> and <strong>downstream use</strong>.<br /></li>
  <li><strong>Quantitative assessment</strong> is critical for scientific <strong>progress</strong> and <strong>reproducibility</strong> in generative modeling.<br /></li>
</ul>

<p>Preview of evaluation goals:</p>
<ul>
  <li>Different goals require different metrics, for example:
    <ul>
      <li><strong>Density estimation</strong></li>
      <li><strong>Sample quality / perceptual realism</strong></li>
      <li><strong>Representation utility</strong> (for downstream tasks)<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="evaluative-comparison-requires-defining-what-better-means-for-a-task">Evaluative comparison requires defining what ‘‘better’’ means for a task</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-03-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-03-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-03-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-03-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines <strong>evaluation</strong> as the process of determining whether <strong>Model A is better than Model B</strong>, and it emphasizes that this comparison requires a clear notion of the <strong>task or objective</strong>.<br /></p>

<p>Key points:</p>
<ul>
  <li>Multiple <strong>model families</strong> and <strong>training objectives</strong> exist, so selection must be driven by the property that matters most for the intended use case:
    <ul>
      <li><strong>Density estimation</strong></li>
      <li><strong>Sampling quality</strong></li>
      <li><strong>Representation learning</strong><br /></li>
    </ul>
  </li>
  <li>Practical research motivation:
    <ul>
      <li>Open-source models enable iterative improvements, which in turn require <strong>objective evaluation criteria</strong> to establish real progress.<br /></li>
    </ul>
  </li>
</ul>

<p>Setup for the lecture:</p>
<ul>
  <li>Different <strong>evaluation metrics</strong> are appropriate depending on the <strong>end goal</strong>; the rest of the lecture explores which metrics align with which goals.<br /></li>
</ul>

<hr />

<h1 id="evaluating-generative-models-is-harder-than-discriminative-models-because-the-task-is-ill-defined">Evaluating generative models is harder than discriminative models because the task is ill-defined</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-06-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-06-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-06-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-06-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment contrasts evaluation for <strong>discriminative models</strong> with evaluation for <strong>generative models</strong>.<br /></p>

<p>For discriminative models (e.g., classifiers):</p>
<ul>
  <li>Tasks and metrics (like <strong>accuracy</strong>) are typically well defined.<br /></li>
  <li>There is usually a clear <strong>loss function</strong> and a well-specified <strong>test distribution</strong>, enabling straightforward comparison on held-out data.<br /></li>
</ul>

<p>For generative models:</p>
<ul>
  <li>The underlying task is often <strong>ambiguous</strong> because many objectives are possible:
    <ul>
      <li><strong>Likelihood</strong></li>
      <li><strong>Sampling fidelity / perceptual quality</strong></li>
      <li><strong>Compression</strong></li>
      <li><strong>Representation learning</strong></li>
      <li><strong>Downstream task performance</strong><br /></li>
    </ul>
  </li>
</ul>

<p>Takeaway:</p>
<ul>
  <li>Choosing an evaluation metric requires specifying the <strong>downstream or operational use</strong> of the generative model, since different metrics capture different aspects of performance.<br /></li>
</ul>

<hr />

<h1 id="likelihood-average-log-likelihood-is-the-canonical-metric-for-density-estimation-and-relates-to-compression">Likelihood (average log-likelihood) is the canonical metric for density estimation and relates to compression</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-11-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-11-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-11-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-11-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When the goal is <strong>accurate density estimation</strong>, <strong>average log-likelihood on held-out data</strong> (equivalently minimizing <strong>Kullback–Leibler divergence</strong>) is a principled metric.<br /></p>

<p>Practical evaluation procedure:</p>
<ol>
  <li><strong>Split the data</strong> into train / validation / test sets.<br /></li>
  <li><strong>Select hyperparameters</strong> using validation data to avoid overfitting.<br /></li>
  <li><strong>Report average log-likelihood</strong> on test data as the measure of model fit.<br /></li>
</ol>

<p>Connection to compression:</p>
<ul>
  <li><strong>Maximum likelihood</strong> is directly tied to <strong>lossless compression</strong> under <strong>Shannon information theory</strong>: better likelihoods imply shorter expected code lengths.<br /></li>
  <li>Practical schemes like <strong>arithmetic coding</strong> can approach the theoretical limits implied by likelihood estimates.<br /></li>
</ul>

<hr />

<h1 id="compression-objective-captures-structure-but-has-limitations-for-downstream-importance-of-bits">Compression objective captures structure but has limitations for downstream importance of bits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-16-26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-16-26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-16-26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-16-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment argues why <strong>likelihood-based compression</strong> is useful and also highlights its limitations.<br /></p>

<p>Why it helps:</p>
<ul>
  <li>Compression forces models to <strong>identify redundancy and structure</strong> in data, which often corresponds to learning salient relationships.<br /></li>
  <li>Historical motivations (e.g., the <strong>Hutter Prize</strong>) link <strong>compression</strong> to measures of intelligence and modeling skill.<br /></li>
  <li>Empirical comparisons (e.g., human next-character prediction vs. modern language models’ <strong>bits-per-character</strong>) illustrate how compression quantifies learned regularities.<br /></li>
</ul>

<p>Crucial limitations:</p>
<ul>
  <li><strong>Not all bits are equally important</strong> for downstream tasks — compression treats every bit the same.<br /></li>
  <li>Compression <strong>treats all prediction errors equally</strong>, regardless of semantic impact (life-critical vs. cosmetic features).<br /></li>
  <li>Therefore, <strong>likelihood alone</strong> may not prioritize task-relevant semantics, motivating alternative evaluation criteria when the goal is not pure density estimation.<br /></li>
</ul>

<hr />

<h1 id="models-without-tractable-likelihoods-gans-ebms-complicate-likelihood-based-evaluation">Models without tractable likelihoods (GANs, EBMs) complicate likelihood-based evaluation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-20-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-20-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-20-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-20-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains a practical problem: many popular generative model classes do <strong>not provide tractable exact likelihoods</strong>, which complicates direct comparison via average log-likelihood or compression measures.<br /></p>

<p>Relevant examples:</p>
<ul>
  <li><strong>GANs</strong></li>
  <li>Some <strong>EBMs</strong></li>
  <li>Certain <strong>VAEs</strong>, depending on tractability of the marginal likelihood<br /></li>
</ul>

<p>Practical notes:</p>
<ul>
  <li>For <strong>VAEs</strong>, the <strong>ELBO</strong> provides a lower bound on log-likelihood, but <strong>translating ELBO differences into comparable likelihoods across model families is problematic</strong>.<br /></li>
  <li>Practitioners therefore rely on <strong>alternative approximations</strong> or <strong>two-sample methods</strong> when exact likelihoods are unavailable.<br /></li>
  <li><strong>Kernel density estimation</strong> and other sample-based density approximations are pragmatic options, but they have significant limitations (discussed next).<br /></li>
</ul>

<hr />

<h1 id="kernel-density-estimation-kde-approximates-densities-from-samples-via-smoothing-kernels-but-fails-in-high-dimensions">Kernel density estimation (KDE) approximates densities from samples via smoothing kernels but fails in high dimensions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-26-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-26-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-26-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-26-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment describes <strong>histogram-based</strong> and <strong>kernel density estimators (KDEs)</strong> as sample-based approaches to approximate unknown model densities when only samples are available.<br /></p>

<p>KDE mechanism (stepwise):</p>
<ol>
  <li>Place a <strong>kernel</strong> (e.g., Gaussian) centered at each sample.<br /></li>
  <li><strong>Sum the kernels</strong> across samples and divide by the sample count to obtain the density estimate.<br /></li>
  <li>Control smoothness via the <strong>bandwidth parameter σ</strong> (larger σ = smoother estimate).<br /></li>
</ol>

<p>Practical aspects:</p>
<ul>
  <li><strong>Kernel choice</strong> (Gaussian, Epanechnikov, etc.) matters but bandwidth selection is often more important.<br /></li>
  <li><strong>Bandwidth selection</strong> is typically done by cross-validation to trade off bias and variance.<br /></li>
  <li>There is a tension between <strong>undersmoothing</strong> (noisy estimate) and <strong>oversmoothing</strong> (loss of detail).<br /></li>
</ul>

<p>Major limitation:</p>
<ul>
  <li><strong>Curse of dimensionality</strong> — KDE requires exponentially many samples as dimensionality grows (e.g., images), making KDE impractical for typical modern generative-modeling domains.<br /></li>
</ul>

<hr />

<h1 id="estimating-likelihoods-for-latent-variable-models-requires-importance-sampling-and-can-have-high-variance">Estimating likelihoods for latent-variable models requires importance sampling and can have high variance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-32-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-32-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-32-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-32-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment treats <strong>latent-variable models</strong> and the challenge of estimating the marginal likelihood p(x) = ∫ p(x|z) p(z) dz.<br /></p>

<p>Key points:</p>
<ul>
  <li><strong>Naive Monte Carlo</strong> estimation (sampling z from the prior) can have <strong>high variance</strong> when the prior and posterior differ substantially.<br /></li>
  <li><strong>Importance sampling</strong> can reduce variance by sampling from a proposal distribution closer to the posterior.<br /></li>
  <li><strong>Annealed / bridged strategies</strong> (e.g., <strong>sequential importance sampling</strong>, <strong>annealed importance sampling</strong>) interpolate between the prior and posterior to obtain more accurate estimates.<br /></li>
</ul>

<p>Practical implications:</p>
<ul>
  <li><strong>Naive sampling from the prior</strong> often yields poor likelihood estimates.<br /></li>
  <li><strong>Specialized estimators</strong> can substantially improve accuracy when likelihood evaluation is required for model comparison in latent-variable settings.<br /></li>
</ul>

<hr />

<h1 id="human-evaluation-remains-the-gold-standard-for-sample-quality-but-is-costly-and-has-caveats">Human evaluation remains the gold standard for sample quality but is costly and has caveats</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-37-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-37-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-37-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-37-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment advocates <strong>human perceptual studies</strong> as the most direct way to assess <strong>sample quality</strong> and <strong>visual realism</strong>.<br /></p>

<p>Common protocols:</p>
<ul>
  <li>Annotators <strong>compare real and generated samples</strong> or <strong>rate sample realism</strong> on a scale.<br /></li>
  <li><strong>Psychological-style evaluations</strong> measure how long it takes humans to distinguish real from fake images — faster distinction implies lower sample quality.<br /></li>
  <li>Measure <strong>deception rate</strong> when annotators have unlimited time (higher deception = higher perceptual realism).<br /></li>
</ul>

<p>Practical limitations:</p>
<ul>
  <li>Human evaluations are <strong>expensive</strong> and <strong>difficult to scale</strong> during model development.<br /></li>
  <li>Results are <strong>sensitive to task wording</strong> and experimental design, making reproducibility challenging.<br /></li>
  <li>Human studies can <strong>fail to reveal memorization</strong> (a model that memorizes training images may still fool annotators but lacks generalization).<br /></li>
</ul>

<hr />

<h1 id="inception-score-measures-sample-sharpness-and-label-diversity-using-a-pretrained-classifier">Inception Score measures sample sharpness and label diversity using a pretrained classifier</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-43-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-43-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-43-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-43-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines the <strong>Inception Score (IS)</strong> as an automated metric for labeled image domains that leverages a <strong>pretrained classifier</strong> to assess two aspects of generated samples: <strong>sharpness</strong> and <strong>diversity</strong>.<br /></p>

<p>How IS works (intuition):</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Sharpness</strong>: low conditional entropy **p(y</td>
          <td>x)** indicates the classifier is confident about the class of a generated image.<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Diversity</strong>: high entropy of the marginal <strong>p(y)</strong> across generated samples indicates coverage of many classes.<br /></li>
  <li>The IS summarizes these by computing an exponentiated <strong>KL divergence</strong> between the conditional and marginal label distributions.<br /></li>
</ul>

<p>Limitations:</p>
<ul>
  <li>IS inspects <strong>only generated samples</strong>, not how they compare to real data.<br /></li>
  <li>It <strong>depends on the chosen classifier and label set</strong>, so results vary with the network used.<br /></li>
  <li>IS can be <strong>gamed</strong> (e.g., produce images that maximize classifier confidence without capturing intra-class variability).<br /></li>
</ul>

<hr />

<h1 id="fréchet-inception-distance-fid-compares-pretrained-feature-distributions-between-real-and-generated-data">Fréchet Inception Distance (FID) compares pretrained feature distributions between real and generated data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-50-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-50-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-50-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-50-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment describes the <strong>Fréchet Inception Distance (FID)</strong>, a widely used metric that compares the distribution of <strong>pretrained-network features</strong> on real versus generated images.<br /></p>

<p>Procedure:</p>
<ol>
  <li>Compute activation vectors (e.g., <strong>Inception features</strong>) for many real and generated samples.<br /></li>
  <li>Fit <strong>multivariate Gaussians</strong> to each feature set — estimate mean and covariance.<br /></li>
  <li>Compute the <strong>Fréchet (Wasserstein-2) distance</strong> between these Gaussians; the distance has a closed-form expression in terms of means and covariances.<br /></li>
</ol>

<p>Intuition and trade-offs:</p>
<ul>
  <li><strong>Smaller FID</strong> indicates generated features more closely match real data features, emphasizing <strong>higher-level perceptual features</strong> rather than raw pixels.<br /></li>
  <li><strong>Kernel-based alternatives</strong> (e.g., <strong>MMD / KID</strong>) perform two-sample tests in feature space and can offer stronger statistical guarantees at higher computational cost.<br /></li>
</ul>

<hr />

<h1 id="kernel-based-two-sample-tests-mmdkid-operate-in-feature-space-and-offer-principled-comparisons">Kernel-based two-sample tests (MMD/KID) operate in feature space and offer principled comparisons</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-56-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-56-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-56-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-56-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains <strong>maximum mean discrepancy (MMD)</strong> and the <strong>kernel inception distance (KID)</strong> as kernel-based two-sample statistics.<br /></p>

<p>Computation (intuition):</p>
<ul>
  <li>Compare distributions by averaging pairwise <strong>kernel similarities</strong>:
    <ul>
      <li>Average kernel value among <strong>real–real</strong> pairs</li>
      <li>Average among <strong>fake–fake</strong> pairs</li>
      <li>Average among <strong>real–fake</strong> pairs</li>
    </ul>
  </li>
  <li>Combine these averages into a statistic that is <strong>zero iff the distributions match</strong> (for <strong>characteristic kernels</strong>).<br /></li>
</ul>

<p>Practical notes:</p>
<ul>
  <li>Using <strong>pretrained-network features</strong> (e.g., Inception activations) as the kernel input emphasizes <strong>perceptual similarity</strong> rather than raw-pixel proximity.<br /></li>
  <li>Trade-offs:
    <ul>
      <li><strong>MMD/KID are more principled</strong>, but are <strong>computationally heavier</strong> (quadratic in sample count).<br /></li>
      <li>They require <strong>careful kernel choice and scaling</strong> to be effective.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="holistic-evaluation-for-conditional-generative-tasks-text-to-image-requires-multiple-specialized-metrics">Holistic evaluation for conditional generative tasks (text-to-image) requires multiple specialized metrics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-02-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-02-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-02-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-02-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment observes that <strong>conditional generative tasks</strong> (e.g., text-to-image) require <strong>multifaceted evaluation</strong> beyond general sample quality because models must also respect the conditioning input and meet additional desiderata.<br /></p>

<p>Recommended combined metrics:</p>
<ul>
  <li><strong>Perceptual quality</strong> (e.g., FID / IS)<br /></li>
  <li><strong>Caption–image alignment</strong> (automated metrics and human alignment checks)<br /></li>
  <li><strong>Robustness to prompt variations</strong><br /></li>
  <li><strong>Originality</strong> and <strong>aesthetic measures</strong><br /></li>
  <li><strong>Bias / toxicity assessments</strong> and safety-related checks<br /></li>
</ul>

<p>Practice:</p>
<ul>
  <li>Holistic benchmark efforts aggregate many automated metrics and <strong>human studies</strong> to enable comprehensive comparisons.<br /></li>
  <li><strong>No single metric suffices</strong> for complex conditional tasks — evaluations must be multidimensional.<br /></li>
</ul>

<hr />

<h1 id="representation-quality-is-evaluated-via-downstream-tasks-such-as-clustering-reconstruction-and-supervised-transfer">Representation quality is evaluated via downstream tasks such as clustering, reconstruction, and supervised transfer</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-07-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment discusses evaluating <strong>learned representations</strong> from generative models by measuring <strong>utility on downstream tasks</strong>.<br /></p>

<p>Common evaluation paradigms:</p>
<ul>
  <li>Map data to <strong>latent features</strong>, then:
    <ul>
      <li>Apply simple <strong>classifiers</strong> to measure supervised performance (few-shot / linear-probe accuracy).<br /></li>
      <li>Apply <strong>clustering</strong> (e.g., <strong>k-means</strong>) and report cluster-quality measures (completeness, homogeneity, <strong>V-measure</strong>).<br /></li>
    </ul>
  </li>
  <li>Measure <strong>reconstruction fidelity</strong> for lossy-compression objectives using <strong>MSE / PSNR / SSIM</strong>.<br /></li>
  <li>Quantify <strong>compression ratios</strong> (bits or dimensionality reduction) versus reconstruction quality.<br /></li>
</ul>

<p>Key point:</p>
<ul>
  <li>The <strong>appropriate metric depends on the intended downstream use</strong> (classification, compression, clustering); representation comparisons are therefore necessarily <strong>task-dependent</strong>.<br /></li>
</ul>

<hr />

<h1 id="disentanglement-aims-for-interpretable-latent-factors-but-is-provably-unidentifiable-without-supervision">Disentanglement aims for interpretable latent factors but is provably unidentifiable without supervision</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-07-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines <strong>disentanglement</strong> and explains theoretical and practical limitations.<br /></p>

<p>Definition and measurement:</p>
<ul>
  <li><strong>Disentanglement</strong>: latent variables correspond to independent, interpretable generative factors (e.g., pose, lighting, object identity).<br /></li>
  <li>Practical metrics often use <strong>linear classifier accuracy</strong> to predict known factors from latent representations.<br /></li>
</ul>

<p>Theoretical limitation:</p>
<ul>
  <li><strong>Fully unsupervised disentanglement is generally unidentifiable</strong> — without inductive biases or supervision, the mapping from data to factors is not unique, so provable recovery of true factors from unlabeled data alone is impossible.<br /></li>
</ul>

<p>Practical implication:</p>
<ul>
  <li>Empirical methods sometimes produce disentangled representations, but success <strong>lacks general theoretical guarantees</strong>; reliable disentanglement typically requires <strong>supervision or structural constraints</strong>.<br /></li>
</ul>

<hr />

<h1 id="pretrained-language-models-can-be-adapted-to-downstream-tasks-via-prompting-or-fine-tuning">Pretrained language models can be adapted to downstream tasks via prompting or fine-tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-10-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-10-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-10-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-10-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains how <strong>autoregressive language models</strong> trained by maximum likelihood can be adapted for downstream tasks via <strong>prompting</strong> or <strong>fine-tuning</strong>.<br /></p>

<p>Two adaptation strategies:</p>
<ol>
  <li><strong>Prompting</strong>:
    <ul>
      <li>Craft a natural-language context that converts the task into <strong>next-token prediction</strong> (no parameter updates).<br /></li>
      <li>Example pattern for sentiment classification: <strong>instruction + few examples + target</strong> (i.e., few-shot prompt).<br /></li>
      <li>Advantages: requires no model updates and works with <strong>black-box API access</strong>.<br /></li>
    </ul>
  </li>
  <li><strong>Fine-tuning</strong>:
    <ul>
      <li>Modify model weights on labeled task examples to optimize task-specific performance.<br /></li>
      <li>Advantages: usually yields <strong>higher task accuracy</strong> but requires compute and maintenance.<br /></li>
    </ul>
  </li>
</ol>

<p>Operational trade-offs:</p>
<ul>
  <li>Strong pretrained models often show useful <strong>few-shot / zero-shot</strong> behavior via prompting.<br /></li>
  <li><strong>Fine-tuning</strong> typically improves accuracy at the cost of compute, storage, and operational complexity.<br /></li>
</ul>

<hr />

<h1 id="prompting-is-less-natural-for-non-sequential-modalities-but-models-can-be-adapted-using-preference-data-and-fine-tuning">Prompting is less natural for non-sequential modalities but models can be adapted using preference data and fine-tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-15-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-15-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-15-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-15-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment considers extending the <strong>prompting paradigm beyond text</strong> and highlights challenges when model APIs emit <strong>images</strong> rather than discrete tokens.<br /></p>

<p>Adaptation strategies for image or multimodal models:</p>
<ul>
  <li>Collect <strong>preference</strong> or <strong>preference-pair</strong> data (which image is preferred for a caption).<br /></li>
  <li>Fine-tune models to optimize for <strong>human preferences</strong> using preference learning or <strong>reinforcement learning from human feedback (RLHF)</strong> to improve aesthetics, reduce toxicity, or correct biases.<br /></li>
</ul>

<p>Research and trade-offs:</p>
<ul>
  <li>Ongoing work aims to transfer <strong>instruction-style adaptation</strong> to vision and multimodal models.<br /></li>
  <li>Practical trade-offs include the need for <strong>labeled preference data</strong> and the additional training effort required to implement preference-based fine-tuning.<br /></li>
</ul>

<hr />

<h1 id="prompting-versus-fine-tuning-trade-offs-in-cost-accessibility-and-performance">Prompting versus fine-tuning: trade-offs in cost, accessibility, and performance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-18-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-18-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-18-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-18-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment compares <strong>prompting</strong> and <strong>fine-tuning</strong> as adaptation strategies for large pretrained models.<br /></p>

<p>Summary of trade-offs:</p>
<ul>
  <li><strong>Prompting</strong>
    <ul>
      <li>Highly accessible: <strong>no retraining</strong>, works with <strong>black-box API</strong> access.<br /></li>
      <li>Inexpensive in development time and easy to iterate.<br /></li>
      <li>May provide strong few-shot/zero-shot capabilities, but typically limited compared to fine-tuning.<br /></li>
    </ul>
  </li>
  <li><strong>Fine-tuning</strong>
    <ul>
      <li>Typically yields <strong>superior performance</strong> for many tasks.<br /></li>
      <li>Requires <strong>compute resources</strong>, access to model weights or permissive APIs, and expertise in training.<br /></li>
      <li>Better suited for repeated or large-scale deployments where higher accuracy justifies the cost.<br /></li>
    </ul>
  </li>
</ul>

<p>Recommendation:</p>
<ul>
  <li>Select the strategy according to <strong>resource constraints</strong>, <strong>privacy requirements</strong>, desired <strong>performance</strong>, and deployment scale; both approaches are actively used depending on context.<br /></li>
</ul>

<hr />

<h1 id="evaluation-of-generative-models-remains-an-open-research-area-requiring-multiple-complementary-metrics">Evaluation of generative models remains an open research area requiring multiple complementary metrics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-20-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-20-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-20-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-20-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This closing segment summarizes the core takeaways about evaluation in generative modeling.<br /></p>

<p>Core messages:</p>
<ul>
  <li><strong>Evaluation is an open problem</strong> with many partial solutions; there is no one-size-fits-all metric.<br /></li>
  <li>Metrics must be chosen to reflect the <strong>downstream objective</strong>:
    <ul>
      <li><strong>Likelihood</strong> for density estimation<br /></li>
      <li><strong>Human / perceptual metrics</strong> for sample quality<br /></li>
      <li><strong>Task-driven metrics</strong> for representation utility<br /></li>
    </ul>
  </li>
  <li><strong>Large-scale benchmarks</strong> that aggregate diverse tasks and metrics are useful, but they do not resolve how to weight or prioritize different metrics.<br /></li>
</ul>

<p>Practical recommendations:</p>
<ul>
  <li>Use a <strong>mixture of human and automated evaluations</strong>.<br /></li>
  <li>Practice <strong>careful experimental design</strong> to ensure reproducibility.<br /></li>
  <li>Continue research into <strong>new evaluation methodologies</strong> tailored to specific generative modeling applications.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 14 - Score Based Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec14/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 14 - Score Based Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec14</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec14/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/E69Lp_T9nVg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="score-based-models-represent-probability-distributions-via-neural-network-parameterized-score-vector-fields">Score-based models represent probability distributions via neural network parameterized score vector fields</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-01-12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-01-12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-01-12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-01-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Score-based models parameterize the gradient of the log-density—the <strong>score</strong>—as a vector-valued neural network that maps each point in data space to the local gradient of log probability. <br /></p>

<ul>
  <li>The model output is a vector field <strong>s_theta(x)</strong> intended to approximate <strong>∇_x log p_data(x)</strong>. <br /></li>
  <li>Training seeks to make that vector field match the true score field. <br /></li>
  <li><strong>Score matching</strong> provides a principled loss for fitting this vector field by minimizing an expectation derived via integration by parts. <br /></li>
  <li>The direct objective, however, involves the <strong>trace of a Jacobian</strong>, which is computationally prohibitive in high dimensions. <br /></li>
</ul>

<p>Consequently, naive score matching is impractical for image-scale problems without scalable approximations or alternative formulations. <br /></p>

<hr />

<h1 id="denoising-score-matching-trains-the-score-model-on-noise-perturbed-data-rather-than-the-clean-data-distribution">Denoising score matching trains the score model on noise-perturbed data rather than the clean data distribution</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-03-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-03-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-03-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-03-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Denoising score matching (DSM)</strong> learns the score of a noise-perturbed data distribution q_sigma(x_t | x) obtained by adding noise (typically Gaussian) to a clean data point x. <br /></p>

<ul>
  <li>Instead of estimating <strong>∇ log p_data(x)</strong> directly, DSM trains the network to predict the score of the <strong>corrupted distribution</strong>, <strong>∇_x log q_sigma(x_t)</strong>. <br /></li>
  <li>This can be implemented as a simple regression objective (e.g., <strong>L2 loss</strong>) between the network output and the tractable score of the perturbation kernel. <br /></li>
  <li>For Gaussian perturbations the score has a <strong>closed-form expression</strong> proportional to the difference between the corrupted input and the clean mean, yielding an efficient per-sample training target and removing the need to compute Jacobian traces. <br /></li>
</ul>

<p>DSM is computationally scalable, compatible with common neural architectures, and admits a natural <strong>denoising interpretation</strong>: the network learns to recover the added noise or the clean signal as a function of noise level. <br /></p>

<hr />

<h1 id="slice-score-matching-uses-random-one-dimensional-projections-to-make-score-estimation-scalable-while-targeting-the-true-data-score">Slice score matching uses random one-dimensional projections to make score estimation scalable while targeting the true data score</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-06-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-06-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-06-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-06-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Slice score matching</strong> projects the vector-valued score at each point along randomly sampled directions v and matches scalar projections rather than full gradients. <br /></p>

<ul>
  <li>At each sample the method draws a random direction <strong>v</strong> and computes the projected scalar score ⟨v, ∇_x log p(x)⟩. <br /></li>
  <li>The model is trained to match that scalar via an objective that can be rewritten to depend only on the model using integration by parts. <br /></li>
  <li>This replaces full Jacobian computations with <strong>directional derivatives</strong>, which are far cheaper to compute. <br /></li>
</ul>

<p>Compared to DSM: slice score matching directly targets the score of the <strong>clean data distribution</strong> (no corruption), but it still requires derivative computations and is somewhat slower in practice. <br /></p>

<p>When the true-data score is well defined, slice score matching yields consistency with that score. <br /></p>

<hr />

<h1 id="langevin-dynamics-uses-the-score-field-for-sampling-but-fails-in-practice-on-high-dimensional-manifold-supported-data">Langevin dynamics uses the score field for sampling but fails in practice on high-dimensional manifold-supported data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-08-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-08-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-08-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-08-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Langevin dynamics</strong> integrates noisy gradient ascent on log-density using the learned score field: x_{t+1} = x_t + α s_theta(x_t) + √(2α) ξ. <br /></p>

<ul>
  <li>The scheme moves particles toward high-probability regions by combining score-driven updates with injected noise. <br /></li>
  <li>It assumes well-defined scores away from training samples, but natural-image data typically lies on or near a <strong>low-dimensional manifold</strong> in pixel space where the score can be ill-defined or explode off the manifold. <br /></li>
  <li>Learned scores are most accurate near high-density training regions and unreliable in low-density regions. <br /></li>
  <li>As a result, naive Langevin chains can get lost, mix poorly between modes, and fail to converge to realistic samples. <br /></li>
</ul>

<p>These limitations motivate strategies to make score estimation more robust off the data manifold and to improve mixing during sampling. <br /></p>

<hr />

<h1 id="adding-noise-to-data-remedies-manifold-and-low-density-problems-by-giving-the-perturbed-distribution-full-support">Adding noise to data remedies manifold and low-density problems by giving the perturbed distribution full support</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-11-05-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-11-05-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-11-05-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-11-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Convolving the data distribution with <strong>isotropic noise</strong> (e.g., Gaussian) produces a family of noise-perturbed densities that have <strong>full support</strong> in ambient space, eliminating singular manifold support and producing well-defined, bounded scores everywhere. <br /></p>

<ul>
  <li>Estimating scores for these perturbed densities is empirically easier and yields <strong>smoother loss landscapes</strong>: small Gaussian noise regularizes the score estimation problem and improves optimization stability. <br /></li>
  <li>The tradeoff is that the learned score corresponds to the <strong>perturbed (noisy) distribution</strong>, so following that score naively produces noisy samples rather than clean data. <br /></li>
</ul>

<p>This motivates either methods to <strong>denoise samples</strong> after sampling or learning scores across <strong>multiple noise scales</strong> so sampling can transition from very noisy to nearly clean distributions. <br /></p>

<hr />

<h1 id="the-data-manifold-concept-explains-why-noise-magnitude-matters-and-motivates-a-tradeoff-between-estimation-accuracy-and-target-mismatch">The data manifold concept explains why noise magnitude matters and motivates a tradeoff between estimation accuracy and target mismatch</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-16-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-16-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-16-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-16-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Real data typically occupy a <strong>low-dimensional embedded manifold</strong> in high-dimensional observation space. <br /></p>

<ul>
  <li>Many pixel combinations thus have essentially zero probability under <strong>p_data</strong>, and <strong>∇ log p_data</strong> can be ill-behaved off the manifold. <br /></li>
  <li>Adding a small amount of noise smooths the distribution and eases estimation near the manifold, but: <br />
    <ul>
      <li>Very small noise does not resolve low-density estimation far from data. <br /></li>
      <li>Very large noise destroys the signal needed to recover clean samples. <br /></li>
    </ul>
  </li>
</ul>

<p>There is an inherent tradeoff: increasing noise improves global score estimation and mixing but moves the learned target away from <strong>p_data</strong>; decreasing noise yields a score closer to the true data score but is harder to estimate and leads to poor mixing. <br /></p>

<p>This tension motivates methods that jointly consider <strong>multiple noise magnitudes</strong> rather than a single perturbation scale. <br /></p>

<hr />

<h1 id="diffusion--score-based-models-jointly-learn-scores-at-multiple-noise-levels-and-sample-by-annealing-from-high-to-low-noise">Diffusion / score-based models jointly learn scores at multiple noise levels and sample by annealing from high to low noise</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-22-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-22-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-22-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-22-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Diffusion / score-based models</strong> estimate score fields for a sequence of noise scales σ_L, …, σ_1 that interpolate between heavy corruption and near-clean data. <br /></p>

<ul>
  <li>Sampling starts from essentially pure noise and then sequentially applies a sampling procedure (e.g., <strong>annealed Langevin dynamics</strong>) that: <br />
    <ol>
      <li>Uses the score for a <strong>large-noise</strong> level to obtain reasonably mixed initial particles. <br /></li>
      <li>Progressively switches to scores for <strong>smaller noise</strong> levels to introduce finer structure. <br /></li>
    </ol>
  </li>
  <li>The multiscale strategy yields accurate directional information at all stages: <strong>coarse scales</strong> guide global structure and mixing, while <strong>fine scales</strong> refine details, enabling generation of nearly clean samples despite estimation challenges at any single noise level. <br /></li>
</ul>

<p>Practical implementations discretize a continuum of noise levels (often ~1000 steps) and amortize computation by <strong>conditioning one neural network</strong> on the noise level. <br /></p>

<hr />

<h1 id="a-single-noise-conditional-neural-network-amortizes-estimation-across-many-noise-levels-and-balances-model-capacity-and-efficiency">A single noise-conditional neural network amortizes estimation across many noise levels and balances model capacity and efficiency</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-28-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-28-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-28-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-28-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Training a separate score network for each noise scale is computationally costly, so practice uses a single <strong>noise-conditional network</strong> <strong>s_theta(x, σ)</strong> that takes the noise scale σ (or time index) as an additional input and jointly approximates scores for all desired corruptions. <br /></p>

<ul>
  <li>This network shares computation and parameters across scales, amortizing learning. <br /></li>
  <li>Implementation choices include <strong>embedding σ</strong> and concatenating or injecting it via adaptive layers. <br /></li>
  <li>The resulting vector fields are not required to be <strong>conservative</strong> (exact gradients of an energy), although conservative parameterizations are possible; empirically, free-form vector fields perform well. <br /></li>
</ul>

<p>Key hyperparameters: the number of discrete noise levels, maximum and minimum magnitudes, and the <strong>interpolation schedule</strong> (commonly geometric), which control overlap between successive noise shells and the success of annealed sampling. <br /></p>

<hr />

<h1 id="training-uses-a-weighted-mixture-of-denoising-objectives-across-noise-scales-and-often-parameterizes-outputs-as-noise-predictors">Training uses a weighted mixture of denoising objectives across noise scales and often parameterizes outputs as noise predictors</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-33-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-33-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-33-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-33-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The training objective sums or integrates <strong>denoising score-matching</strong> losses over the chosen noise scales and typically weights each term with a function <strong>λ(σ)</strong> that balances contributions by noise magnitude and numerical conditioning. <br /></p>

<ul>
  <li>For Gaussian perturbations a convenient parameterization is to predict the <strong>added noise ε</strong> (using <strong>ε_theta</strong>) from the corrupted input x_t; this <strong>noise-prediction</strong> parameterization is algebraically equivalent to predicting the scaled score and often improves numerical stability. <br /></li>
  <li>The loss is implemented by: <br />
    <ol>
      <li>Sampling a mini-batch of clean data. <br /></li>
      <li>Sampling σ (or an index) per example and generating noisy inputs x_t = x + σ ε. <br /></li>
      <li>Minimizing the weighted squared error between predicted and true noise (or between predicted and true score). <br /></li>
    </ol>
  </li>
  <li>Proper choice of <strong>λ(σ)</strong> or scaling factors ensures no single noise level dominates training and yields balanced performance across scales. <br /></li>
</ul>

<hr />

<h1 id="training-is-implemented-by-stochastic-gradient-descent-with-per-sample-noise-level-selection-and-amortized-denoising-tasks">Training is implemented by stochastic gradient descent with per-sample noise-level selection and amortized denoising tasks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-38-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-38-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-38-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-38-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Each training iteration proceeds as follows: <br /></p>

<ol>
  <li>Sample a batch of data points. <br /></li>
  <li>For each point, independently sample a noise scale σ (commonly uniformly or according to a prescribed schedule). <br /></li>
  <li>Draw Gaussian noise and form the corrupted input x_t = x + σ ε. <br /></li>
  <li>Evaluate the network <strong>s_theta(x_t, σ)</strong> or <strong>ε_theta(x_t, σ)</strong> and compute the per-sample denoising regression loss weighted by <strong>λ(σ)</strong>. <br /></li>
  <li>Backpropagate gradients and update parameters with standard optimizers (SGD/Adam). <br /></li>
</ol>

<p>This single-model, multi-task setup amortizes the cost of solving many denoising tasks and yields a model usable at inference across all noise scales. <br />
In practice, practitioners discretize σ levels (e.g., 1000) and may optionally ensemble or use multiple networks for incremental gains if compute permits. <br /></p>

<hr />

<h1 id="sampling-uses-annealed-langevin-dynamics-or-numerical-solvers-of-reverse-time-sdes-with-tradeoffs-between-steps-and-sample-quality">Sampling uses annealed Langevin dynamics or numerical solvers of reverse-time SDEs, with tradeoffs between steps and sample quality</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-48-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-48-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-48-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-48-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Sampling from the model begins from samples of a <strong>high-noise prior</strong> (essentially Gaussian) and iteratively reduces noise by running a stochastic procedure that uses <strong>s_theta(x, σ)</strong> to push particles toward higher-density regions while injecting appropriate noise at each step. <br /></p>

<ul>
  <li><strong>Annealed Langevin dynamics</strong> applies multiple Langevin updates at each discrete σ, using the corresponding conditional score and optionally decreasing step sizes. More steps yield higher quality but increase inference cost because each step requires a full model evaluation. <br /></li>
  <li>Viewing the noise sequence as a discretization of a continuous diffusion process leads to <strong>reverse-time SDEs</strong> whose numerical integration (predictor-corrector methods) provides principled solvers and can be combined with Langevin correctors for improved mixing. <br /></li>
</ul>

<p>The practical tradeoff is <strong>compute versus fidelity</strong>: state-of-the-art models often use thousands of network evaluations to generate very high-quality images, which is far more expensive at inference than alternative generator architectures but produces superior results and stable training behavior. <br /></p>

<hr />

<h1 id="the-continuous-time-diffusion-perspective-formulates-forward-corruptions-as-an-sde-and-sampling-as-solving-a-reverse-time-sde-conditioned-on-scores">The continuous-time diffusion perspective formulates forward corruptions as an SDE and sampling as solving a reverse-time SDE conditioned on scores</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/01-04-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/01-04-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/01-04-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/01-04-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A continuum of noise levels is naturally modeled as a stochastic process {x_t}_{t∈[0,T]} obtained by an <strong>SDE</strong> that gradually corrodes data into noise; the marginal at each time t corresponds to the data distribution convolved with the appropriate Gaussian. <br /></p>

<ul>
  <li>Reversing time yields a <strong>reverse-time SDE</strong> whose drift term depends on the score <strong>∇_x log p_t(x)</strong>, so accurate score estimates across t allow construction of an exact generative reverse-time dynamics. <br /></li>
  <li>Replacing the true score by <strong>s_theta(x, t)</strong> yields a tractable reverse SDE that can be numerically integrated with standard SDE solvers; discretization recovers annealed sampling procedures. <br /></li>
  <li>This SDE viewpoint clarifies connections to classical diffusion theory and enables use of advanced numerical techniques (higher-order solvers, predictor-corrector steps) to trade off step count and sample quality. <br /></li>
</ul>

<hr />

<h1 id="diffusion-models-connect-to-deterministic-odes-and-continuous-normalizing-flows-and-exhibit-strong-empirical-performance-with-memorization-caveats">Diffusion models connect to deterministic ODEs and continuous normalizing flows, and exhibit strong empirical performance with memorization caveats</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/01-18-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/01-18-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/01-18-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/01-18-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Under particular constructions the stochastic forward process admits an equivalent deterministic <strong>ODE</strong> that matches the forward marginals; integrating the ODE backwards yields a continuous-time <strong>normalizing flow</strong> that is invertible and maps noise to data with a tractable Jacobian flow. <br /></p>

<ul>
  <li>This connection means diffusion models can be interpreted as very deep invertible flows trained with score-based losses rather than maximum likelihood, which also enables computation of likelihoods in certain formulations. <br /></li>
  <li>Empirically, diffusion models achieve <strong>state-of-the-art image synthesis quality</strong>, are more stable to train than adversarial models, and scale well with compute at inference time—though they require many model evaluations to sample. <br /></li>
  <li>Practical concerns include rare <strong>memorization</strong> of training images (detectable with nearest-neighbor checks and loss monitoring) and frequent failure modes (e.g., hands/fingers) that improve with more data and model capacity. <br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 13 - Score Based Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec13/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 13 - Score Based Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec13</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec13/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/8G-OsDs1RLI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="course-overview-and-motivations-for-score-based-generative-models">Course overview and motivations for score-based generative models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-01-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-01-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-01-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-01-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines the landscape of modern generative modeling and motivates <strong>score-based approaches</strong>.<br /></p>

<ul>
  <li><strong>Likelihood-based models</strong> (e.g., <strong>autoregressive models</strong>, <strong>normalizing flows</strong>) require explicit <strong>normalized densities</strong> and enable principled likelihood training and monitoring.<br /></li>
  <li><strong>Implicit generator models</strong> define flexible <strong>sampling procedures</strong> but make likelihood evaluation intractable, so they rely on adversarial or other unstable training methods.<br /></li>
</ul>

<p>Trade-offs:<br /></p>
<ul>
  <li>Likelihood models: <strong>principled training</strong> and easy evaluation, but they impose <strong>architectural constraints</strong>.<br /></li>
  <li>Implicit models: <strong>high flexibility</strong>, but need <strong>unstable</strong> or complex training (e.g., GANs).<br /></li>
</ul>

<p>This motivates <strong>score-based (diffusion) models</strong>: instead of modeling the density itself, they model the <strong>gradient of the log density</strong> — the <strong>score</strong> — which avoids explicit normalization while allowing flexible parameterizations for continuous data modalities.<br /></p>

<hr />

<h1 id="definition-and-interpretation-of-the-score-function">Definition and interpretation of the score function</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-05-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-05-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-05-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-05-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Score function</strong>: the gradient of the log probability density with respect to the input, ∇_x log p(x).<br /></p>

<ul>
  <li>Geometric interpretation: the score is a <strong>vector field</strong> that points in the direction of greatest increase of log density at each input location — it encodes local directions toward <strong>higher-probability regions</strong>.<br /></li>
  <li>Working with the score removes the explicit normalization constraint because gradients eliminate additive constants in the log density.<br /></li>
  <li>This is computationally attractive when densities exist for continuous random variables.<br /></li>
</ul>

<p>Analogy: think of <strong>potentials</strong> (log density / energy) versus <strong>fields</strong> (score) — equivalent information up to constants, but the field representation avoids partition-function issues and can be more convenient for some tasks.<br /></p>

<hr />

<h1 id="energy-based-models-and-score-matching-motivation">Energy-based models and score matching motivation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-09-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-09-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-09-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-09-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment positions score-based modeling relative to <strong>energy-based models (EBMs)</strong> and introduces the <strong>Fisher divergence</strong> as a training objective.<br /></p>

<ul>
  <li><strong>EBMs</strong> parameterize an unnormalized <strong>energy</strong> and face partition-function estimation difficulties when trained by likelihood.<br /></li>
  <li>
    <p>Crucially, the <strong>score</strong> (the gradient of the energy) does <strong>not</strong> depend on the partition function, so matching scores avoids normalization issues.<br /></p>
  </li>
  <li><strong>Fisher divergence</strong>: the expected squared difference between data and model scores.
    <ul>
      <li>Via integration by parts, the Fisher objective can be rewritten so it does not require computing the partition function explicitly.<br /></li>
      <li>This motivates <strong>score matching</strong> as a tractable alternative for fitting flexible unnormalized models.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="modeling-scores-directly-as-vector-valued-neural-networks">Modeling scores directly as vector-valued neural networks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-12-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-12-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-12-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-12-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Core modeling choice in score-based methods:<br /></p>

<ul>
  <li>Parameterize the <strong>score directly</strong> as a vector-valued neural network <strong>s_theta(x)</strong> mapping R^d → R^d.<br /></li>
  <li>The model family becomes a set of <strong>vector fields</strong> obtained by varying network parameters; each output vector matches the input dimensionality because it represents a gradient.<br /></li>
  <li>Training objective: find parameters that make the model vector field close to the <strong>data score vector field</strong> in a suitable norm, using only samples from the data distribution.<br /></li>
</ul>

<p>This formulation generalizes EBMs by <strong>not requiring</strong> that the modeled vector field be the gradient of a scalar potential (i.e., the field need not be conservative).<br /></p>

<hr />

<h1 id="statistical-considerations-and-overfitting-when-learning-scores-from-samples">Statistical considerations and overfitting when learning scores from samples</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-16-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-16-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-16-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-16-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Learning-theoretic and practical issues with finite samples:<br /></p>

<ul>
  <li>Training from samples turns score matching into an <strong>empirical risk minimization</strong> problem subject to standard concerns: <strong>sample complexity</strong>, <strong>estimation error</strong>, and <strong>overfitting</strong>.<br /></li>
  <li>Matching scores everywhere is limited by finite data and model capacity; derivative information (scores) contains the same information as densities up to a normalization constant, so estimation remains challenging.<br /></li>
  <li>Conceptual difference: modeling an <strong>energy</strong> (a scalar potential) versus modeling a <strong>vector field</strong> directly.
    <ul>
      <li>Direct score models can represent <strong>non-conservative</strong> fields that an energy parameterization cannot.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="score-matching-objective-integration-by-parts-and-computational-bottleneck">Score-matching objective, integration by parts, and computational bottleneck</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-21-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-21-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-21-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-21-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment derives the Fisher-divergence training objective and explains the computational bottleneck.<br /></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The empirical objective is E_data[</td>
          <td> </td>
          <td>s_theta(x) − s_data(x)</td>
          <td> </td>
          <td>^2].<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Integration by parts rewrites this into a form depending only on model outputs and their <strong>derivatives with respect to inputs</strong>.<br /></li>
  <li>The problematic term is the <strong>trace of the Jacobian</strong> (sum of ∂(s_theta)_i / ∂x_i), which, if computed naively, requires <strong>O(d)</strong> backpropagations and becomes intractable in high-dimensional data.<br /></li>
</ul>

<hr />

<h1 id="denoising-score-matching-concept-and-noise-perturbed-densities">Denoising score matching concept and noise-perturbed densities</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-28-12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-28-12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-28-12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-28-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces <strong>denoising score matching</strong> as a scalable approximation that sidesteps computing Jacobian traces.<br /></p>

<ul>
  <li>Key idea: convolve the data distribution with a known noise kernel (commonly <strong>Gaussian</strong>) to obtain a <strong>smoothed density</strong> Q_sigma whose score is easier to estimate.<br /></li>
  <li>Training targets the score of this <strong>noise-perturbed</strong> distribution instead of the original data score.<br /></li>
  <li>The learning objective for the smoothed density can be rewritten into an <strong>efficient denoising loss</strong>, eliminating the intractable trace term.<br /></li>
  <li>If the added noise level is small, Q_sigma approximates the original data distribution, so learning the noisy score yields a useful approximation of the clean score.<br /></li>
</ul>

<hr />

<h1 id="algebraic-transformation-from-fisher-divergence-to-denoising-objective">Algebraic transformation from Fisher divergence to denoising objective</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-34-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-34-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-34-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-34-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment sketches the algebraic steps that convert the Fisher objective on the noisy distribution into a tractable denoising loss.<br /></p>

<ul>
  <li>Write the noisy marginal Q_sigma(x_tilde) as an integral over clean data x and <strong>exchange gradients with the integral</strong>.<br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The intractable dependence on the unknown data score disappears and is replaced by gradients of the <strong>conditional noise kernel</strong> p(x_tilde</td>
          <td>x).<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>For a <strong>Gaussian kernel</strong> these conditional-score terms have <strong>closed-form expressions</strong> (linear in x_tilde − x), so the troublesome cross-term simplifies to an expectation involving known quantities and the model output.<br /></li>
  <li>The end result is a computable <strong>squared-error denoising loss</strong> that depends only on samples (x, x_tilde) and the model’s output at x_tilde.<br /></li>
</ul>

<hr />

<h1 id="denoising-objective-interpretation-and-practical-algorithm">Denoising objective interpretation and practical algorithm</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-41-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-41-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-41-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-41-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Final denoising loss form and practical training algorithm:<br /></p>

<ul>
  <li>For Gaussian perturbations the loss reduces to an <strong>L2 regression objective</strong>: train s_theta(x_tilde) to predict the noise vector (x_tilde − x) scaled by the inverse variance (or equivalently predict the score of the noisy density).<br /></li>
</ul>

<p>Practical mini-batch training algorithm (per update):<br /></p>
<ol>
  <li>Sample clean data x from the dataset.<br /></li>
  <li>Sample noise eps ∼ N(0, σ^2 I) and form x_tilde = x + eps.<br /></li>
  <li>Compute the denoising target (e.g., eps / σ^2 or the equivalent score target) and evaluate the squared-error loss between model output s_theta(x_tilde) and the target.<br /></li>
  <li>Update θ by minimizing the empirical denoising loss (standard gradient-based optimizer).<br /></li>
</ol>

<ul>
  <li><strong>Sigma trade-off</strong>: smaller σ yields a closer approximation to the clean score but can cause numerical instability or poor conditioning; in practice one picks or <strong>schedules multiple σ values</strong> to balance accuracy and stability.<br /></li>
</ul>

<hr />

<h1 id="tweedie-formula-conditional-expectation-and-implications-for-denoising">Tweedie formula, conditional expectation, and implications for denoising</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-47-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-47-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-47-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-47-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Statistical rationale behind denoising score matching using <strong>Tweedie’s formula</strong> and conditional expectations:<br /></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>For additive Gaussian noise, the <strong>MMSE denoiser</strong> E[x</td>
          <td>x_tilde] is related to the gradient of the log-perturbed density by a closed-form relation (Tweedie’s formula).<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Minimizing the L2 denoising loss implicitly aligns the model with the <strong>noisy-score direction</strong> that points toward high-probability clean images.<br /></li>
  <li>Other noise kernels can be used provided their conditional scores are computable, but <strong>Gaussian noise</strong> yields particularly simple expressions and theoretical guarantees that are convenient in practice for sampling and score estimation.<br /></li>
</ul>

<hr />

<h1 id="sliced-random-projection-fisher-divergence-for-scalable-exact-score-matching">Sliced (random-projection) Fisher divergence for scalable exact score matching</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-53-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-53-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-53-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-53-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment presents an alternative scalable approach: <strong>sliced score matching</strong> (projecting onto random directions).<br /></p>

<ul>
  <li>Project the vector-valued score s(x) onto random directions v and match scalar projections v^T s(x) instead of the full vector field.<br /></li>
  <li>The <strong>sliced Fisher divergence</strong> compares these projections; integration by parts still applies and the problematic trace term reduces to <strong>Jacobian-vector products</strong> (directional derivatives).<br /></li>
  <li>Jacobian-vector products can be computed with a single backpropagation using standard automatic differentiation, making this approach computationally cheap.<br /></li>
  <li>Sampling random directions per data sample yields unbiased estimators of the full objective; variance is controllable by averaging multiple projections per sample.<br /></li>
</ul>

<hr />

<h1 id="sampling-with-estimated-scores-via-langevin-dynamics-and-mcmc">Sampling with estimated scores via Langevin dynamics and MCMC</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/00-59-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/00-59-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/00-59-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/00-59-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains how to generate samples when only the score is available, using <strong>Langevin dynamics</strong>.<br /></p>

<ul>
  <li>Langevin sampling alternates two steps repeatedly:<br />
    <ol>
      <li>take a small step in the direction of the estimated score (approximate gradient ascent on log density), and<br /></li>
      <li>add Gaussian noise to inject stochasticity.<br /></li>
    </ol>
  </li>
  <li>In the limit of vanishing step size and many iterations, Langevin dynamics recovers samples from the target distribution when the score estimator is accurate.<br /></li>
  <li>Important caution: pure gradient ascent without noise leads to <strong>mode-seeking</strong> and poor diversity; Langevin dynamics combines <strong>gradient guidance</strong> with <strong>stochasticity</strong> to approximate the correct stationary distribution.<br /></li>
</ul>

<hr />

<h1 id="failure-modes-data-manifolds-and-low-density-regions-causing-poor-sampling-and-mixing">Failure modes: data manifolds and low-density regions causing poor sampling and mixing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/01-07-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/01-07-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/01-07-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/01-07-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical failure modes of score-based sampling and their causes:<br /></p>

<ul>
  <li><strong>Data concentrated on low-dimensional manifolds</strong>: the score may blow up or be undefined off the manifold, causing unstable or meaningless gradients away from the data support.<br /></li>
  <li><strong>Poor score estimates in low-density regions</strong>: finite-data training yields accurate scores only near high-density regions, leaving large gaps with unreliable estimates.<br /></li>
  <li>Consequences for Langevin sampling:<br />
    <ul>
      <li>Failure to reach or mix between modes.<br /></li>
      <li>Misrepresentation of mode probabilities (e.g., mixtures with disjoint supports where the score does not encode mixture weights).<br /></li>
    </ul>
  </li>
  <li>These issues lead to degraded sample quality and limited exploration unless addressed by improved training or model design.<br /></li>
</ul>

<hr />

<h1 id="conclusion-limitations-of-naive-score-matching-and-path-to-diffusion-models">Conclusion: limitations of naive score matching and path to diffusion models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec13/01-16-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec13/01-16-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec13/01-16-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec13/01-16-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Summary and practical outlook:<br /></p>

<ul>
  <li>Direct score modeling and its scalable variants (denoising, sliced) resolve <strong>partition-function</strong> issues and reduce computational cost compared to naive EBM likelihood training.<br /></li>
  <li>However, naive application is <strong>insufficient for high-dimensional real data</strong> because of manifold effects and poor estimation in low-density regions.<br /></li>
  <li>Reliable score estimation across the input space requires improved training procedures and architectures (e.g., multi-scale noise schedules, better samplers).<br /></li>
  <li>In practice, <strong>diffusion-model techniques</strong> that combine careful noise scheduling and enhanced sampling procedures address these challenges and achieve state-of-the-art generative performance, positioning diffusion-based sampling plus robust score estimation as the practical solution to the issues identified.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 12 - Energy Based Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec12/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 12 - Energy Based Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec12</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec12/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Nci1Bepcy0g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="energy-based-models-ebms-define-probability-densities-via-an-unnormalized-energy-function-and-a-parameter-dependent-partition-function">Energy-based models (EBMs) define probability densities via an unnormalized energy function and a parameter-dependent partition function</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-00-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-00-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-00-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-00-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Energy-based models represent probability densities using an <strong>energy function f_theta(x)</strong> so that the model density is<br />
<strong>p_theta(x) = exp(f_theta(x)) / Z(theta)</strong>, where the <strong>partition function Z(theta)</strong> normalizes the unnormalized probability.<br /></p>

<ul>
  <li>The <strong>energy function</strong> can be any parameterized function (commonly a neural network), which makes EBMs a highly <strong>flexible family of distributions</strong>.<br /></li>
  <li>The partition function <strong>Z(theta)</strong> equals the integral or sum of exp(f_theta(x)) over the sample space and <strong>depends on the model parameters</strong>, so likelihood values are meaningful only relative to Z(theta).<br /></li>
  <li>Because <strong>Z(theta) couples all possible x values</strong>, evaluating exact normalized likelihoods typically becomes <strong>computationally intractable in high-dimensional settings</strong>.<br /></li>
</ul>

<hr />

<h1 id="partition-function-intractability-makes-direct-likelihood-evaluation-and-maximum-likelihood-training-difficult-but-relative-probability-comparisons-are-feasible">Partition function intractability makes direct likelihood evaluation and maximum likelihood training difficult, but relative probability comparisons are feasible</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-02-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-02-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-02-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-02-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The partition function <strong>Z(theta)</strong> is generally <strong>intractable</strong> to compute for multivariate or high-dimensional x because it requires summing or integrating the unnormalized probability across an exponentially large space.<br /></p>

<ul>
  <li>Direct probability comparisons are feasible via ratios because <strong>Z(theta)</strong> cancels: <strong>p_theta(x) / p_theta(x’)</strong>, enabling relative-likelihood comparisons useful for many sampling procedures.<br /></li>
  <li><strong>Maximum likelihood training</strong> is challenging because the log-likelihood gradient contains two theta-dependent terms:
    <ol>
      <li>The gradient of the <strong>energy at the data</strong> (depends on f_theta at data points).<br /></li>
      <li>The <strong>gradient of log Z(theta)</strong>, which requires knowing how parameter changes reweight the entire space.<br /></li>
    </ol>
  </li>
  <li>Consequently, <strong>practical training requires approximations or methods that avoid direct evaluation of Z(theta)</strong>.<br /></li>
</ul>

<hr />

<h1 id="contrastive-divergence-estimates-likelihood-gradients-by-contrasting-training-data-with-samples-from-the-model">Contrastive Divergence estimates likelihood gradients by contrasting training data with samples from the model</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-05-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-05-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-05-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-05-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Contrastive Divergence (CD)</strong> approximates the log-likelihood gradient by comparing energy gradients at data samples and at samples drawn from the current model, turning the intractable partition-function gradient into a <strong>sample-based estimate</strong>.<br /></p>

<ol>
  <li>Start with a minibatch of real data samples.</li>
  <li>Generate short-run samples from the current model (e.g., a few MCMC/Langevin steps) starting from the data or other initializations.</li>
  <li>Compute the gradient difference: increase unnormalized probability (decrease energy) for real data and decrease it for the synthetic samples.<br /></li>
</ol>

<ul>
  <li>The intuitive update is: <strong>raise mass on data, lower mass on generated samples</strong>, approximating the effect of the partition-function term.<br /></li>
  <li>This requires the ability to <strong>generate samples from the model</strong>; when those samples approximate p_theta well, the CD gradient approximates the true maximum-likelihood gradient.<br /></li>
  <li>Practically, CD shifts complexity from evaluating <strong>Z(theta)</strong> to <strong>generating representative model samples</strong>.<br /></li>
</ul>

<hr />

<h1 id="markov-chain-monte-carlo-mcmc-methods-enable-sampling-from-ebms-via-local-proposals-and-acceptance-rules-that-satisfy-detailed-balance">Markov chain Monte Carlo (MCMC) methods enable sampling from EBMs via local proposals and acceptance rules that satisfy detailed balance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-09-12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-09-12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-09-12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-09-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Sampling from an EBM is typically done by constructing a Markov chain whose transition operator satisfies <strong>detailed balance</strong> with respect to <strong>p_theta</strong>.<br /></p>

<ul>
  <li>A common recipe:
    <ol>
      <li>Propose a local perturbation x’ from the current state x.</li>
      <li>Accept or reject x’ based on the ratio of unnormalized probabilities (the Metropolis–Hastings acceptance rule).<br /></li>
    </ol>
  </li>
  <li>The acceptance rule enforces that transitions occur with probabilities making <strong>p_theta</strong> a fixed point of the Markov operator, so under mild conditions repeated application converges to <strong>p_theta</strong> regardless of initialization.<br /></li>
  <li>Intuitively, <strong>MCMC</strong> is stochastic local search (or stochastic hill-climbing): uphill moves are accepted deterministically, downhill moves accepted with probability proportional to the unnormalized-probability ratio, preserving the correct stationary distribution.<br /></li>
  <li>In high-dimensional spaces, however, <strong>mixing can be extremely slow</strong>, and many steps may be required to obtain high-quality independent samples.<br /></li>
</ul>

<hr />

<h1 id="langevin-dynamics-and-noisy-gradient-ascent-are-efficient-mcmc-proposals-for-continuous-ebms-using-score-information">Langevin dynamics and noisy gradient ascent are efficient MCMC proposals for continuous EBMs using score information</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-15-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-15-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-15-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-15-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Langevin dynamics</strong> perform MCMC in continuous state spaces by combining gradient ascent on the log unnormalized density (the <strong>score</strong>) with injected Gaussian noise.<br /></p>

<ul>
  <li>Each update has the form: <strong>x_{t+1} = x_t + (epsilon^2 / 2) * grad_x log p_theta(x_t) + epsilon * Normal(0, I)</strong>, so the step size <strong>epsilon</strong> controls the signal-to-noise trade-off and must be scaled relative to the gradient magnitude to ensure convergence.<br /></li>
  <li>Variants:
    <ul>
      <li><strong>MALA</strong> (Metropolis-adjusted Langevin algorithm) adds an MH accept/reject step to correct discretization bias.</li>
      <li><strong>Unadjusted Langevin</strong> always accepts; both converge to p_theta in the limit of small step size and many iterations under technical conditions.<br /></li>
    </ul>
  </li>
  <li>Using gradient information (the score) usually yields <strong>much faster practical mixing</strong> than naive local proposals, but each step is computationally costly because it requires evaluating <strong>grad_x f_theta(x)</strong> via backprop.<br /></li>
</ul>

<hr />

<h1 id="sampling-and-contrastive-training-costs-make-naive-likelihood-based-training-of-ebms-impractical-at-scale">Sampling and contrastive-training costs make naive likelihood-based training of EBMs impractical at scale</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-22-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-22-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-22-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-22-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Training EBMs with sampling-based inner loops (e.g., <strong>CD</strong>, <strong>MCMC</strong>, <strong>Langevin</strong>) requires generating fresh model samples repeatedly during optimization, which multiplies the cost of forward and backward passes through the energy network by the number of sampling steps.<br /></p>

<ul>
  <li>If thousands or tens of thousands of sampling steps are needed per sample to reach high-probability modes, including such sampling during each training update becomes <strong>computationally prohibitive</strong>.<br /></li>
  <li>Therefore, <strong>alternative training objectives that avoid model sampling during training</strong> are desirable for efficient, scalable EBM training.</li>
  <li>These alternatives typically exploit quantities that <strong>do not depend on the partition function</strong>.<br /></li>
</ul>

<hr />

<h1 id="the-score-function-is-the-gradient-of-the-log-density-with-respect-to-the-input-and-does-not-depend-on-the-partition-function">The score function is the gradient of the log-density with respect to the input and does not depend on the partition function</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-27-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-27-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-27-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-27-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>score function s_theta(x) = grad_x log p_theta(x)</strong> equals <strong>grad_x f_theta(x)</strong> because the partition function <strong>Z(theta)</strong> does not depend on x and drops out when differentiating with respect to the input.<br /></p>

<ul>
  <li>Consequences:
    <ul>
      <li>The <strong>score</strong> is directly computable from the energy network without evaluating <strong>Z(theta)</strong>.</li>
      <li>It provides a vector field that points in the direction of steepest increase of log-density at every x.</li>
      <li>For simple parametric families (e.g., Gaussians) the score has closed-form dependence on x and parameters; for neural-network energies it is available via automatic differentiation.<br /></li>
    </ul>
  </li>
  <li>Using the <strong>score</strong> rather than the log-density itself opens a pathway to construct training losses that <strong>avoid partition-function evaluation</strong>.<br /></li>
</ul>

<hr />

<h1 id="score-matching-and-the-fisher-divergence-compare-distributions-by-their-score-vector-fields-and-yield-a-partition-free-training-objective">Score matching and the Fisher divergence compare distributions by their score vector fields and yield a partition-free training objective</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-32-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-32-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-32-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-32-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>Fisher divergence</strong> measures discrepancy between two densities p and q by the expected squared L2 norm of the difference between their score functions:<br />
<strong>E_{x~p}[ || grad_x log p(x) - grad_x log q(x) ||^2 ]</strong>, which vanishes iff p = q under mild conditions.<br /></p>

<ul>
  <li>Because it depends only on <strong>score fields</strong>, the Fisher divergence does <strong>not require evaluating normalization constants</strong>, making it well suited to EBMs whose scores are available from the energy gradient.</li>
  <li>Minimizing the Fisher divergence between the data distribution p_data and a model p_theta corresponds to <strong>matching their score fields</strong> and provides an alternative to KL-based maximum likelihood that is free of explicit partition-function dependence.</li>
  <li>Conceptually, this reframes density matching as aligning a conserved vector field (the <strong>score</strong>) rather than matching scalar likelihood values.<br /></li>
</ul>

<hr />

<h1 id="integration-by-parts-and-its-multivariate-analogue-converts-the-fisher-divergence-into-a-computable-loss-involving-model-derivatives-only">Integration by parts (and its multivariate analogue) converts the Fisher divergence into a computable loss involving model derivatives only</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-43-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-43-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-43-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-43-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Although the Fisher divergence contains the unknown data score term, <strong>integration by parts</strong> (univariate) or the <strong>divergence theorem</strong> (multivariate) can move derivatives from the unknown data density onto the model score, eliminating explicit dependence on <strong>grad_x log p_data</strong> under mild boundary conditions.<br /></p>

<ul>
  <li>In one dimension the manipulation yields an objective composed of:
    <ul>
      <li>The squared model score, and</li>
      <li>The derivative of the model score — both computable for a parametric energy model.<br /></li>
    </ul>
  </li>
  <li>In multiple dimensions the transformed objective involves:
    <ul>
      <li>The squared norm of the model score, plus</li>
      <li>The trace of the model score’s Jacobian (equivalently, the <strong>Hessian of log p_theta</strong>).<br /></li>
    </ul>
  </li>
  <li>Up to a theta-independent constant, the result is an expectation over data samples of terms depending only on the model and its derivatives, enabling empirical estimation by sample averages and stochastic optimization using only training data and energy-network derivatives.<br /></li>
</ul>

<hr />

<h1 id="practical-computation-of-the-score-matching-loss-requires-approximations-for-second-derivatives-and-scalable-estimators-such-as-sliced-or-denoising-score-matching">Practical computation of the score-matching loss requires approximations for second derivatives and scalable estimators such as sliced or denoising score matching</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-52-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-52-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-52-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-52-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Direct evaluation of the multivariate objective requires computing the <strong>trace of the Hessian of log p_theta(x)</strong> (or equivalent second-order input derivatives), which is expensive in high dimensions because it naively requires many backward passes.<br /></p>

<ul>
  <li>Scalable alternatives:
    <ul>
      <li><strong>Hutchinson’s estimator</strong> (randomized trace estimation) projects Hessian action onto random vectors to approximate the trace.</li>
      <li><strong>Sliced score matching</strong> approximates the multivariate trace via random one-dimensional projections.</li>
      <li><strong>Denoising score matching</strong> recovers score information by training to denoise noisy inputs, avoiding explicit second-order computation.<br /></li>
    </ul>
  </li>
  <li>Interpreting the resulting loss shows it encourages:
    <ul>
      <li>Data points to be <strong>stationary points</strong> of the model log-density (small gradients), and</li>
      <li>Those points to be <strong>local maxima rather than minima</strong> (controlled by second-derivative terms), aligning model mass with data modes.<br /></li>
    </ul>
  </li>
  <li>These approximations retain the <strong>partition-free advantage</strong> while enabling practical optimization of score-based objectives at scale.<br /></li>
</ul>

<hr />

<h1 id="noise-contrastive-estimation-nce-trains-a-classifier-to-distinguish-data-from-a-known-noise-distribution-yielding-density-ratio-estimates">Noise-contrastive estimation (NCE) trains a classifier to distinguish data from a known noise distribution, yielding density-ratio estimates</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-59-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-59-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-59-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-59-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Noise-contrastive estimation (NCE)</strong> reframes density estimation as a supervised binary classification problem: a discriminator learns to distinguish real data from samples drawn from a chosen <strong>noise distribution p_n(x)</strong>.<br /></p>

<ul>
  <li>The optimal classifier recovers the density ratio <strong>p_data(x) / (p_data(x) + p_n(x))</strong>, so because <strong>p_n(x)</strong> is chosen to be tractable to sample from and to evaluate, the classifier’s outputs provide information about p_data relative to p_n.</li>
  <li>NCE turns unsupervised density estimation into <strong>likelihood-free discriminative training</strong> that does not require sampling from the parametric model being trained.</li>
  <li>The choice of <strong>noise distribution</strong> is critical: when p_n is similar to p_data the classifier must learn subtle structure and yields stronger learning signals.<br /></li>
</ul>

<hr />

<h1 id="an-energy-based-discriminator-with-a-learnable-partition-constant-turns-nce-into-a-trainable-ebm-without-inner-loop-sampling">An energy-based discriminator with a learnable partition constant turns NCE into a trainable EBM without inner-loop sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/01-07-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/01-07-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/01-07-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/01-07-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>NCE can be specialized to EBMs by parameterizing the classifier’s model likelihood term with <strong>p_theta(x) = exp(f_theta(x)) / Z</strong> and treating <strong>log Z</strong> as an additional free scalar parameter <strong>z</strong> that is optimized jointly with theta.<br /></p>

<ul>
  <li>Under the cross-entropy objective, optimizing (theta, z) to make the classifier distinguish data from noise pushes <strong>p_theta</strong> toward <strong>p_data</strong> in the infinite-data, perfect-optimization limit, and the learned <strong>z</strong> converges to the true log partition function in that limit.</li>
  <li>Training proceeds by:
    <ol>
      <li>Sampling minibatches of real data and noise samples from <strong>p_n</strong>.</li>
      <li>Evaluating the discriminator probability via the energy and the noise density.</li>
      <li>Applying stochastic gradient updates to (theta, z).<br /></li>
    </ol>
  </li>
  <li>This approach fits EBMs <strong>without generating samples from p_theta</strong>. In finite-data or imperfect-optimization regimes the learned <strong>z</strong> may not equal the true partition function, so the energy and normalization are approximate, but NCE remains a practical, likelihood-free fitting method.<br /></li>
</ul>

<hr />

<h1 id="adapting-the-noise-distribution-via-parametric-flows-yields-flow-contrastive-estimation-that-jointly-trains-a-tractable-generator-and-an-ebm-like-discriminator">Adapting the noise distribution via parametric flows yields flow-contrastive estimation that jointly trains a tractable generator and an EBM-like discriminator</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/01-17-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/01-17-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/01-17-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/01-17-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A refinement is to parameterize the noise distribution <strong>p_n(x; phi)</strong> with a tractable <strong>flow model</strong> that can both generate samples efficiently and evaluate likelihoods, and to update phi adversarially to make the discriminator’s task harder.<br /></p>

<ul>
  <li>In <strong>flow-contrastive estimation</strong>:
    <ul>
      <li>The discriminator (an energy plus normalization scalar) and the flow-based noise model are trained jointly in a <strong>minimax-style</strong> scheme.</li>
      <li>The flow model is optimized to approximate <strong>p_data</strong> and confuse the discriminator, while discriminator updates push the energy toward <strong>p_data</strong>.<br /></li>
    </ul>
  </li>
  <li>This yields two learned objects—a <strong>flow generator</strong> and an <strong>energy function</strong>—and in practice the flow can provide high-quality samples while the energy captures discriminative density structure.</li>
  <li>The approach bridges <strong>score-based</strong>, <strong>contrastive</strong>, and <strong>adversarial</strong> paradigms but requires careful tuning due to the adversarial component. Empirically, learning the noise distribution often improves sample quality compared to a fixed noise distribution, while theoretical guarantees revert to the infinite-data, perfect-optimization limits.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Stanford CS236 - Deep Generative Models I 2023 I Lecture 11 - Energy Based Models</title><link href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec11/" rel="alternate" type="text/html" title="Stanford CS236 - Deep Generative Models I 2023 I Lecture 11 - Energy Based Models" /><published>2025-12-18T00:00:00+07:00</published><updated>2025-12-18T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs236-2023-lec11</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec11/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/m61KiAMCJ5Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-and-generative-model-design-space">Lecture overview and generative model design space</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-00-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-00-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-00-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-00-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture introduces <strong>energy-based models (EBMs)</strong> as a family of generative models and situates them within the general design space for generative modeling: choose a <strong>model family</strong> and a <strong>loss function</strong> given IID data samples.<br /></p>

<p>Principled objectives like <strong>maximum likelihood</strong> and <strong>Kullback–Leibler (KL) divergence</strong> are appropriate when models provide tractable likelihoods. This motivates architectures that allow exact or approximate density evaluation, such as <strong>autoregressive models</strong> and <strong>normalizing flows</strong>.<br /></p>

<p>The central tradeoff is framed clearly:<br /></p>
<ul>
  <li>Models that permit likelihood evaluation impose <strong>architectural constraints</strong>.<br /></li>
  <li><strong>Likelihood-free</strong> or implicitly defined samplers (for example, <strong>GANs</strong>) relax those constraints but require alternative training objectives and often unstable minimax optimization.<br /></li>
</ul>

<p>This tension motivates exploring <strong>EBMs</strong>, which aim to combine high flexibility in parameterizing distributions with likelihood-informed or likelihood-based training strategies.<br /></p>

<hr />

<h1 id="likelihood-based-models-impose-structural-constraints-to-ensure-valid-densities">Likelihood-based models impose structural constraints to ensure valid densities</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-02-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-02-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-02-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-02-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Models that permit direct likelihood evaluation</strong> must satisfy two constraints: <strong>non-negativity</strong> and <strong>normalization</strong>. Enforcing these constraints forces particular architectural designs:<br /></p>

<ul>
  <li><strong>Autoregressive</strong> constructions: use the chain rule to produce normalized conditionals.<br /></li>
  <li><strong>Invertible networks / flows</strong>: produce a tractable change-of-variables Jacobian for exact density evaluation.<br /></li>
  <li><strong>Latent-variable / variational</strong> approaches: use analytic or approximated marginalization.<br /></li>
</ul>

<p>These constraints limit admissible neural architectures because arbitrary networks do not automatically yield valid probability densities.<br /></p>

<p>When likelihood evaluation is infeasible, alternative approaches like <strong>GANs</strong> define the model implicitly via a <strong>sampler</strong> and use two-sample tests or discriminator-based objectives to train. However, these <strong>minimax objectives</strong> introduce:<br /></p>
<ul>
  <li>training instability,<br /></li>
  <li>difficulty detecting convergence,<br /></li>
  <li>challenges for principled evaluation.<br /></li>
</ul>

<p>The lecture emphasizes these practical costs and motivates methods that recover architectural flexibility without abandoning principled training entirely.<br /></p>

<hr />

<h1 id="motivation-for-energy-based-models-as-a-flexible-probabilistic-parametrization">Motivation for energy based models as a flexible probabilistic parametrization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-04-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-04-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-04-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-04-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Energy-based models (EBMs)</strong> lift many architectural restrictions by defining distributions implicitly via an <strong>unnormalized energy or score function</strong>.<br /></p>

<ul>
  <li>EBMs allow essentially <strong>arbitrary neural network architectures</strong> to output a scalar <strong>energy</strong> for any input.<br /></li>
  <li><strong>Normalization</strong> is enforced by dividing the unnormalized density by a <strong>partition function</strong>, enabling very expressive model families.<br /></li>
</ul>

<p>The lecturer highlights connections to <strong>maximum likelihood</strong> and related losses, suggesting EBMs can yield more stable training than adversarial methods while retaining links to likelihood-informed objectives.<br /></p>

<p>EBMs are also closely related to <strong>diffusion models</strong>, which have achieved state-of-the-art sampling in continuous domains, and to compositional modeling: EBMs can be <strong>composed or combined</strong> with other model families to capture intersecting concepts.<br /></p>

<hr />

<h1 id="probabilistic-models-must-satisfy-non-negativity-and-normalization-constraints">Probabilistic models must satisfy non-negativity and normalization constraints</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-06-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-06-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-06-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-06-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A valid probability mass or density function must be <strong>non-negative everywhere</strong> and <strong>integrate (or sum) to one</strong>; these two constraints are conceptually distinct in enforcement difficulty.<br /></p>

<ul>
  <li><strong>Non-negativity</strong> is easy to enforce for arbitrary neural networks by applying elementwise transforms such as <strong>squaring</strong>, <strong>exponentiation</strong>, <strong>absolute value</strong>, or similar final-layer operations that guarantee non-negative outputs.<br /></li>
  <li><strong>Normalization</strong> is far more restrictive: it requires the integral or sum over the entire domain to equal a constant independent of parameters. This typically forces special architectures (autoregressive factorization, invertible transforms) or analytic functional choices.<br /></li>
</ul>

<p>The lecture uses an analogy of dividing a cake among outcomes to emphasize that <strong>enforcing a fixed total mass</strong> is the primary challenge motivating alternative formulations like EBMs.<br /></p>

<hr />

<h1 id="why-normalization-is-hard-and-how-to-form-normalized-densities-from-unnormalized-functions">Why normalization is hard and how to form normalized densities from unnormalized functions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-08-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-08-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-08-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-08-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Given an arbitrary parameterized non-negative function <strong>G_theta(x)</strong> produced by a neural network, the total integral (or sum) over x is generally a parameter-dependent scalar and will not equal one by default.<br /></p>

<p><strong>Energy-based modeling</strong> embraces this by defining a normalized density:</p>
<ul>
  <li>p_theta(x) = G_theta(x) / Z_theta,
where <strong>Z_theta</strong> is the <strong>partition function</strong> (the integral or sum of G_theta over the domain). This division produces a valid probability distribution for any non-negative G_theta.<br /></li>
</ul>

<p>However, computing <strong>Z_theta</strong> analytically is feasible only for restricted functional forms. EBMs therefore acknowledge that <strong>Z_theta is typically intractable</strong> and must be handled explicitly—either approximated or avoided by algorithmic design. This reparameterization opens the door to highly flexible G_theta choices while making clear the computational bottleneck: the <strong>partition function</strong>.<br /></p>

<hr />

<h1 id="partition-function-concept-normalization-by-division-and-consequences">Partition function concept, normalization by division, and consequences</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-13-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-13-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-13-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-13-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Dividing a non-negative unnormalized density by its <strong>partition function</strong> yields a mathematically valid normalized probability model, which is the defining construction of EBMs.<br /></p>

<ul>
  <li><strong>Z_theta</strong> is the integral (continuous case) or sum (discrete case) of the unnormalized function and depends on parameters, so it must be considered during evaluation and learning.<br /></li>
  <li>In simple families (e.g., <strong>Gaussian</strong>, <strong>exponential</strong>) this integral can be computed in closed form, yielding classical normalized distributions.<br /></li>
</ul>

<p>For general neural-network-parameterized unnormalized functions, <strong>Z_theta is intractable</strong> due to the curse of dimensionality and exponential growth in domain size. Therefore, EBMs trade expressivity for the need to either <strong>approximate Z_theta</strong> or develop training and sampling methods that avoid requiring its exact value.<br /></p>

<hr />

<h1 id="classical-examples-and-the-exponential-family-as-normalized-quotients">Classical examples and the exponential family as normalized quotients</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-18-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-18-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-18-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-18-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Many familiar distributions fit the <strong>unnormalized-plus-division</strong> pattern:<br /></p>
<ul>
  <li><strong>Gaussian</strong>: exp(−(x−μ)^2/(2σ^2)) divided by sqrt(2πσ^2).<br /></li>
  <li><strong>Exponential</strong>: exp(−λx) divided by 1/λ.<br /></li>
</ul>

<p>More generally, distributions in the <strong>exponential family</strong> take the form p(x) ∝ exp(θ·T(x)) with a <strong>log-partition function</strong> that normalizes the density; these families are analytically tractable under specific sufficient-statistic choices and capture a wide class of common distributions.<br /></p>

<p>The lecture explains that <strong>EBMs generalize this paradigm</strong> by allowing complex neural-network-based energies in the exponent, removing the requirement that normalization be analytically solvable. This extension increases modeling flexibility but transfers the computational burden to approximating or otherwise handling the partition function during learning and inference.<br /></p>

<hr />

<h1 id="relations-between-normalized-model-constructions-and-compositional-architectures">Relations between normalized-model constructions and compositional architectures</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-21-32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-21-32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-21-32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-21-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Autoregressive models, latent-variable models, flows, and mixtures</strong> can be interpreted as structured ways to build complex normalized densities from simpler normalized components:<br /></p>

<ul>
  <li><strong>Autoregressive</strong>: the joint is a product of normalized conditionals, so the full joint is normalized by design.<br /></li>
  <li><strong>Latent-variable</strong>: marginalizing over simple normalized conditionals yields normalized marginals.<br /></li>
  <li><strong>Flows</strong>: use invertible transforms with tractable Jacobians to convert between densities.<br /></li>
  <li><strong>Mixtures</strong>: convex combinations of normalized components remain normalized.<br /></li>
</ul>

<p>These constructive approaches guarantee normalization for all parameter settings, but they impose design constraints and may limit flexibility compared to arbitrary unnormalized energies. <strong>EBMs</strong> are contrasted as freeing architecture choices at the cost of making the normalization constant parameter-dependent and generally intractable.<br /></p>

<hr />

<h1 id="formal-definition-of-an-energy-based-model-and-choice-of-exponential-parametrization">Formal definition of an energy based model and choice of exponential parametrization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-26-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-26-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-26-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-26-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>An energy-based model parameterizes a probability density as:</p>
<ul>
  <li>
    <p>p_theta(x) = exp(f_theta(x)) / Z_theta,
where <strong>f_theta(x)</strong> is an arbitrary scalar-valued function (often a neural network) and <strong>Z_theta</strong> is the partition function.<br /></p>
  </li>
  <li>The <strong>exponential map</strong> guarantees non-negativity and conveniently models large dynamic ranges in relative probabilities: small changes in f_theta can yield large multiplicative changes in p_theta(x).<br /></li>
  <li>This form generalizes <strong>softmax-style normalization</strong> for finite discrete outputs and recovers classical exponential-family forms when f_theta is simple.<br /></li>
</ul>

<p>The primary practical costs are that evaluating normalized probabilities and drawing samples are difficult because <strong>Z_theta is generally intractable</strong> for high-dimensional x.<br /></p>

<hr />

<h1 id="expressivity-vs-computational-costs-sampling-and-likelihood-evaluation-challenges">Expressivity vs. computational costs: sampling and likelihood evaluation challenges</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-34-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-34-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-34-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-34-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>EBMs provide maximal flexibility</strong> in choosing f_theta, enabling arbitrary neural architectures to model data. But this flexibility incurs computational costs:<br /></p>

<ul>
  <li>Evaluating normalized likelihoods requires computing <strong>Z_theta</strong>.<br /></li>
  <li>Sampling from p_theta(x) is typically expensive or intractable by straightforward methods because <strong>Z_theta couples probabilities across the entire domain</strong>.<br /></li>
</ul>

<p>Numerical integration or brute-force summation scales exponentially with dimensionality and is infeasible for images, audio, and other high-dimensional modalities. The lecturer notes that <strong>diffusion models</strong> effectively exploit energy-based parametrizations and approximations to achieve strong sampling performance, illustrating that the flexibility payoff can be realized with careful algorithm design.<br /></p>

<p>Overall, EBMs trade architectural freedom for the need to employ sophisticated approximations for inference, sampling, and training.<br /></p>

<hr />

<h1 id="curse-of-dimensionality-and-implications-for-partition-function-estimation">Curse of dimensionality and implications for partition function estimation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-43-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-43-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-43-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-43-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The central computational barrier for EBMs is the <strong>curse of dimensionality</strong>: the number of domain configurations grows exponentially with the number of variables, so exact computation of <strong>Z_theta</strong> or naive numerical approximations are infeasible.<br /></p>

<ul>
  <li>This affects discrete domains (combinatorial explosion of assignments) and continuous domains (volume discretization required to approximate integrals).<br /></li>
  <li>Likelihood-based training and exact sampling become prohibitive for high-dimensional data.<br /></li>
</ul>

<p>Consequently, practical EBM algorithms focus on methods that either <strong>bypass the need to compute Z_theta exactly</strong> (e.g., contrastive objectives, score-based formulations) or <strong>approximate the partition function or its gradients</strong> with Monte Carlo and MCMC techniques. The lecture frames subsequent material as addressing how to learn and sample from EBMs despite this fundamental complexity.<br /></p>

<hr />

<h1 id="tasks-that-do-not-require-explicit-partition-function-evaluation">Tasks that do not require explicit partition function evaluation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-49-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-49-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-49-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-49-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Many practical tasks require only <strong>relative comparisons between model scores</strong> rather than absolute normalized probabilities, so the partition function cancels out in ratios and can be ignored for those tasks.<br /></p>

<p>Examples include:<br /></p>
<ul>
  <li><strong>Ranking</strong><br /></li>
  <li><strong>Anomaly detection</strong><br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Conditional MAP inference</strong> (finding argmax_y p(y</td>
          <td>x))<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Many discriminative tasks where only relative orderings of likelihoods matter<br /></li>
</ul>

<table>
  <tbody>
    <tr>
      <td>The lecture illustrates <strong>denoising</strong> as a conditional inference problem where the posterior mode argmax_y p(y</td>
      <td>x) can be found without knowing Z_theta because normalization over y given x is constant across candidate y values. This property makes EBMs useful for a range of applications despite the intractability of Z_theta and motivates sampling and optimization techniques that exploit score comparisons.<br /></td>
    </tr>
  </tbody>
</table>

<hr />

<h1 id="derivative-based-properties-and-model-composition-via-product-or-mixture-ensembles">Derivative-based properties and model composition via product or mixture ensembles</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-54-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-54-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-54-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-54-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>gradient of the log-probability</strong> with respect to parameters often eliminates dependence on the partition function in useful ways, enabling learning algorithms that do not require Z_theta explicitly because derivatives of log p_theta(x) remove additive constants.<br /></p>

<p>Energy-based representations also enable flexible ensembling operations:<br /></p>
<ul>
  <li><strong>Product-of-experts</strong>: multiplying normalized model densities yields an unnormalized product whose <strong>log-energy is the sum</strong> of individual log-densities. This behaves like an <strong>AND</strong> operator—any expert assigning near-zero probability drives the product low—enabling composition that captures intersections of concepts. However, the product requires renormalization with a global partition function, reintroducing computational costs for sampling and likelihood evaluation.<br /></li>
  <li><strong>Mixtures</strong>: convex combinations behave like a <strong>soft OR</strong> and remain normalized by construction, but they do not impose intersection constraints and are easier to work with when component partition functions are known.<br /></li>
</ul>

<hr />

<h1 id="product-of-experts-behavior-and-sampling-considerations">Product-of-experts behavior and sampling considerations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/00-58-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/00-58-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/00-58-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/00-58-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>product-of-experts</strong> approach multiplies individual densities to produce a combined unnormalized density whose energy is the sum of component energies. Key points:<br /></p>
<ul>
  <li>The combined model emphasizes <strong>intersections of support</strong> across experts (semantic compositionality).<br /></li>
  <li>The global <strong>partition function</strong> for the product must be computed or approximated to normalize the model.<br /></li>
  <li><strong>Sampling</strong> from the product is harder than sampling each component independently because the product couples variables and typically requires specialized MCMC or other approximate samplers.<br /></li>
</ul>

<p>Although sampling is expensive, it is not impossible: practical implementations use approximate inference strategies to make products of experts useful in applications.<br /></p>

<hr />

<h1 id="restricted-boltzmann-machine-rbm-as-a-discrete-latent-variable-energy-model">Restricted Boltzmann Machine (RBM) as a discrete latent-variable energy model</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/01-02-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/01-02-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/01-02-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/01-02-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>Restricted Boltzmann Machine (RBM)</strong> is a canonical discrete EBM with binary visible variables x and binary hidden variables z, defined by an energy that is a quadratic form comprising visible biases, hidden biases, and pairwise visible–hidden interactions weighted by a matrix W.<br /></p>

<ul>
  <li>The RBM has <strong>no intra-layer interactions</strong> (no visible–visible or hidden–hidden terms), which makes conditional sampling between layers tractable and enables efficient <strong>Gibbs</strong> updates for certain inference steps.<br /></li>
  <li>Historically, RBMs and stacked compositions (Deep Belief Networks) were among the first deep generative models to produce compelling samples and served as unsupervised pretraining for deep supervised networks.<br /></li>
</ul>

<p>Despite their historical importance, RBMs exemplify the <strong>partition function problem</strong>, since computing Z requires summing over exponentially many joint assignments of visible and hidden units.<br /></p>

<hr />

<h1 id="partition-function-in-rbms-and-why-exact-likelihood-training-is-infeasible">Partition function in RBMs and why exact likelihood training is infeasible</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/01-07-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/01-07-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/01-07-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/01-07-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>In an RBM the unnormalized probability is straightforward to evaluate for any configuration via the energy computation, but the partition function requires summing exp(−energy) over all 2^n × 2^m combinations of visible and hidden binary variables.<br /></p>

<ul>
  <li>This exponential summation makes exact computation of normalized probabilities impractical except for very small models, so directly applying maximum likelihood is infeasible at realistic scales.<br /></li>
  <li>Learning therefore relies on approximate methods that either estimate gradients via Monte Carlo sampling or perform specialized approximations that avoid exact evaluation of Z, acknowledging that parameter changes affect both the unnormalized numerator and the partition-function denominator.<br /></li>
</ul>

<p>The lecture frames <strong>contrastive methods</strong> as a practical solution to this challenge.<br /></p>

<hr />

<h1 id="gradient-of-the-log-likelihood-and-the-contrastive-divergence-monte-carlo-approximation">Gradient of the log-likelihood and the contrastive-divergence Monte Carlo approximation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/01-14-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/01-14-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/01-14-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/01-14-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The exact gradient of the log-likelihood for an EBM decomposes into two terms:<br /></p>
<ol>
  <li>The gradient of the energy evaluated at a <strong>data point</strong> (the positive term).<br /></li>
  <li>Minus the <strong>expected gradient of the energy under the model distribution</strong> (the negative term).<br /></li>
</ol>

<p>The second term is an expectation over the model distribution and therefore depends on <strong>Z_theta</strong> implicitly; computing it exactly is intractable but it can be approximated with Monte Carlo by drawing samples from the model.<br /></p>

<p><strong>Contrastive Divergence (CD)</strong> approximates this expected term with a small number of samples (often one), yielding a low-bias stochastic estimate of the gradient direction that increases the model’s relative probability of observed data compared to typical model samples. This operationalizes the intuitive objective of making training data more likely than typical negative samples drawn from the current model.<br /></p>

<hr />

<h1 id="sampling-from-ebms-using-local-proposals-and-markov-chain-monte-carlo">Sampling from EBMs using local proposals and Markov chain Monte Carlo</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec11/01-20-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec11/01-20-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec11/01-20-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec11/01-20-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Sampling from high-dimensional EBMs typically uses <strong>Markov chain Monte Carlo (MCMC)</strong> methods that perform local proposals followed by accept/reject decisions so samples asymptotically follow the target distribution.<br /></p>

<p>A generic MCMC sampling loop looks like this:<br /></p>
<ol>
  <li>Initialize x (e.g., random or previous state).<br /></li>
  <li>Propose a local perturbation (for example, add noise or make a small move).<br /></li>
  <li>Compare unnormalized probabilities (or energies) of proposed and current states. Uphill (higher-probability) proposals are accepted deterministically; downhill moves are accepted probabilistically according to a <strong>Metropolis–Hastings</strong> acceptance rule based on the ratio of unnormalized densities.<br /></li>
  <li>Repeat steps 2–3 many times to allow exploration and mixing.<br /></li>
</ol>

<p>Occasional acceptance of downhill proposals is essential to avoid trapping in local modes and to permit exploration of the state space. Running the chain for sufficiently many iterations yields samples from the true model in the limit.<br /></p>

<p>Practical MCMC for EBMs can be computationally expensive and requires careful proposal design and mixing diagnostics, but it provides a principled mechanism both for sampling and for generating the <strong>negative samples</strong> used in contrastive learning.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry></feed>