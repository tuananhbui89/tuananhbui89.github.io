<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tuananhbui89.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tuananhbui89.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-10T22:10:53+11:00</updated><id>https://tuananhbui89.github.io/feed.xml</id><title type="html">blank</title><subtitle>Researcher in Generative AI and Trustworthy AI
</subtitle><entry><title type="html">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</title><link href="https://tuananhbui89.github.io/blog/2024/erasing-concepts/" rel="alternate" type="text/html" title="Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection" /><published>2024-02-08T00:00:00+11:00</published><updated>2024-02-08T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2024/erasing-concepts</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2024/erasing-concepts/"><![CDATA[<!-- Pre-intro. Story heading. Summarising the story flow -->

<h2 id="introduction">Introduction</h2>

<!-- Taylor Swift incident and the raise of sexualized generated images -->

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_concepts/taylor-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_concepts/taylor-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_concepts/taylor-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_concepts/taylor.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Recently, X (Twitter) had been flooded with <strong>sexually explicit</strong> AI-generated images of Taylor Swift, shared by many X users. As reported by <a href="https://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending">The Verge</a>, <em>“One of the most prominent examples on X attracted more than 45 million views, 24,000 reposts, and hundreds of thousands of likes and bookmarks before the verified user who shared the images had their account suspended for violating platform policy. The post remained live on the platform for about 17 hours before its removal”</em>.
Soon after, X had to block the searches for Taylor Swift as the last resort to prevent the spread of these images (Ref to <a href="https://www.theverge.com/2024/1/27/24052841/taylor-swift-search-blocked-x-twitter-ai-images">The Verge</a>).</p>

<p>While this incident has certainly raised public awareness about the threat of AI-generated content, including the spread of misinformation, racism, and sexism, the general public might think such incidents only happen to famous figures like Taylor Swift and may not take it personally. However, that is not the case. With recent advancements in personalized AI-generated content, led by the Dreambooth project <d-cite key="ruiz2023dreambooth"></d-cite>, it has become very easy and efficient to customize or personalize generated content with just a few sample images of a person. Therefore, generating sexually explicit images of any individual, not just Taylor Swift, is alarmingly easy.
Therefore, it is very easy to generate sexual explicit images of any person, not just Taylor Swift. 
In fact, this is already occurring, as reported <a href="https://apnews.com/article/generative-ai-illegal-images-child-abuse-3081a81fa79e2a39b67c11201cfd085f">here</a> and <a href="https://lifehacker.com/evil-week-you-can-make-personalized-porn-images-with-a-1850978902">here</a> and a more thorough search will reveal plenty of similar reports.</p>

<p><strong>Naive approaches to prevent unwanted concepts</strong></p>

<p>There are several naive approaches aimed at preventing the generation of unwanted content, but none have proven fully effective, particularly with the release of generative models like Stable Diffusion, which come complete with source code and pre-trained models accessible to the public. For instance:</p>

<ul>
  <li>
    <p>Implementing a Not-Safe-For-Work (NSFW) detector to filter out harmful content. This detector, developed by the Stable Diffusion team, can be easily bypassed due to open access to the source code; by modifying this <a href="https://github.com/huggingface/diffusers/blob/7c8cab313e4c66a813d146bcf92023b0489a2369/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L551">line</a> in Huggingface library.</p>
  </li>
  <li>
    <p>Modifying the text encoder to transform text embeddings of harmful concepts into a zero vector or a random vector. This approach means that a prompt such as “naked Taylor Swift” would result in a random image, rather than something sexually explicit. However, the ease of accessing and replacing the pre-trained models makes this method unreliable.</p>
  </li>
  <li>
    <p>Excluding all training data containing harmful content and retraining the model from scratch. This method was employed by the Stable Diffusion team in their <a href="https://github.com/Stability-AI/stablediffusion">version 2.0</a>, which utilized 150,000 GPU-hours to process the 5-billion-image LAION dataset. Despite this effort, the quality of generated images declined, and the model wasn’t entirely sanitized, as highlighted in the ESD paper <d-cite key="gandikota2023erasing"></d-cite>. The reduction in image quality led to <a href="https://thealgorithmicbridge.substack.com/p/stable-diffusion-2-is-not-what-users">dissatisfaction within the AI community</a>, prompting a return to less restrictive NSFW training data in <a href="https://github.com/Stability-AI/stablediffusion">version 2.1</a> :joy:.</p>
  </li>
</ul>

<p>To date, the most effective strategy appears to be sanitizing the model (specifically, the UNet model) after training on raw, unfiltered data and before its public release which will be covered in the next section.</p>

<p><strong>The new adversarial game</strong></p>

<p>The adversarial game between attackers and defenders have been well-known in the field of AI,  tracing back to the pioneering work on adversarial examples by Szegedy et al. (2013) <d-cite key="szegedy2013intriguing"></d-cite> or even earlier studies by Biggio et al. (2008) <d-cite key="biggio2018wild"></d-cite>.
These confrontations are most prevalent in areas like image classification, object detection, and other discriminative model applications. In these scenarios, attackers aim to generate adversarial examples to alter the model’s predictions, while defenders strive to prevent the model from being fooled by these adversarial examples.</p>

<p>However, with the rise of generative models capable of producing high-quality outputs, and not only by researchers but also by being decentralized to the public, the scope of adversarial games has broadened. This expansion introduces a myriad of new challenges and scenarios within the realm of generative models. For instance, as discussed in a <a href="https://tuananhbui89.github.io/blog/2023/watermark-diffusion/">previous post</a>, I explored an adversarial game involving watermarking, pitting concept owners (e.g., artists seeking to safeguard their creations) against concept synthesizers (individuals utilizing generative models to replicate specific artworks).</p>

<p>In this post, I will delve into a new adversarial game that pits concept erasers (individuals aiming to eliminate harmful or unwanted content such as sexually explicit material, violence, racism, sexism, or personalized concepts like Taylor Swift) against concept injectors (those who wish to introduce new concepts or restore previously erased ones).</p>

<p>Specifically, I will introduce some notable works from the two parties include:</p>

<ul>
  <li><strong>Erasing harmful concepts</strong>: Erasing Concepts from Diffusion Models (ESD) <d-cite key="gandikota2023erasing"></d-cite>, Editing Implicit Assumptions in Text-to-Image Diffusion Models (TIME) <d-cite key="orgad2023editing"></d-cite>, Unified Concept Editing in Diffusion Models (UCE) <d-cite key="gandikota2024unified"></d-cite>.</li>
  <li><strong>Injecting new concepts</strong>: Circumventing Concept Erasure Methods For Text-to-Image Generative Models <d-cite key="gandikota2024unified"></d-cite>.</li>
  <li><strong>Anti personalization</strong>: Anti-Dreambooth <d-cite key="van2023anti"></d-cite> and Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis <d-cite key="ma2023generative"></d-cite> (introduced in the previous post <a href="https://tuananhbui89.github.io/blog/2023/anti-personalization/">here</a>).</li>
</ul>

<!-- TakeAway Conclusion -->

<p><strong>Takeaway conclusion</strong></p>

<ul>
  <li>The recent incident of AI-generated sexual explicit images of Taylor Swift has raised a lot of concerns about the potential of AI to generate unwanted concepts and the urgent need on research to prevent the generation of unwanted concepts.</li>
  <li>There is an initial research on the adversarial games between concept erasers and concept injectors. The concept erasers try to erase unwanted concepts while the concept injectors try to inject new concepts or recover the erased concepts.</li>
  <li>While the concept erasers have shown some initial success in erasing unwanted concepts, the concept injectors have also shown that it is easy to circumvent the concept erasers.</li>
</ul>

<h2 id="erasing-concepts-from-diffusion-models-">Erasing Concepts from Diffusion Models <d-cite key="gandikota2023erasing"></d-cite></h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/erasing_concepts/examples-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/erasing_concepts/examples-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/erasing_concepts/examples-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/erasing_concepts/examples.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Examples of erasing nudity, Van Gogh style or an objects from a Stable Diffusion model (Image source: <a href="https://erasing.baulab.info/">Gandikota et al. (2023)</a>).
</div>

<ul>
  <li>Project page: <a href="https://erasing.baulab.info/">https://erasing.baulab.info/</a></li>
</ul>

<h3 id="summary-esd">Summary ESD</h3>

<ul>
  <li><strong>Goal</strong>: ESD aims to erase harmful concepts such as nudity, violence, or specific artist styles like “Van Gogh” from the generative models such as Stable Diffusion while maintaining the quality of the generated images for other concepts.</li>
  <li><strong>Approach</strong>: The authors proposed to alter the guiding signal regarding the concept to be erased to the one regarding the “null” concept (i.e., a neural prompt like “A photo”, “A person”), i.e., \(\epsilon_\theta(z_t, c, t) \to \epsilon(z_t, c_{null}, t)\). The approach varies depending on whether the concept is directly expressible (such as “truck” or “dog”) or more abstract (like “nudity”). For direct concepts, modifications to the cross-attention layers of the diffusion model prove more effective, whereas abstract concepts necessitate adjustments to different layers for successful removal.</li>
</ul>

<h3 id="central-optimization-problem">Central Optimization Problem</h3>

<p>The central optimization problem is to reduce  the probability of generating an image \(x\) according to the likelihood that is described by the concept, scaled by a power factor \(\eta\).</p>

\[P_\theta(x) \propto \frac{P_{\theta^*}(x)}{P_{\theta^*}(c \mid x)^\eta}\]

<p>where \(P_{\theta^*}(x)\) is the distribution generated by the original model \(\theta^*\) and \(P_{\theta^*}(c \mid x)\) is the probability of the concept \(c\) given the image \(x\). The power factor \(\eta\) controls the strength of the concept erasure. A larger \(\eta\) means a stronger erasure. \(\theta\) is the parameters of the model after unlearning the concept \(c\).</p>

<p>It can be interpreted as: if the concept \(c\) is present in the image \(x\) in which \(P_{\theta^*} (c \mid x)\) is high, then the likelihood of the image \(x\) under the new model \(P_{\theta} (x)\) will be reduced.
While if the concept \(c\) is not present in the image \(x\) in which \(P_{\theta^*} (c \mid x)\) is low, then the likelihood of the image \(x\) under the new model \(P_{\theta} (x)\) will be increased.</p>

<p>Because of the Bayes’ rule, the likelihood of the concept \(c\) given the image \(x\) can be rewritten as follows:</p>

\[P_{\theta^*} (c \mid x) = \frac{P_{\theta^*} (x \mid c) P_{\theta^*} (c)}{P_{\theta^*} (x)}\]

<p>Therefore, the above equation can be rewritten when taking the derivative w.r.t. \(x\) as follows:</p>

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta \nabla_{x} \log P_{\theta^*} (c \mid x)\]

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta (\nabla_{x} \log P_{\theta^*} (x \mid c) + \nabla_{x} \log P_{\theta^*} (c) - \nabla_{x} \log P_{\theta^*} (x))\]

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta (\nabla_{x} \log P_{\theta^*} (x \mid c) - \nabla_{x} \log P_{\theta^*} (x))\]

<p>Because in the diffusion model, each step has been approximated to a Gaussian distribution, therefore, the gradient of the log-likelihood is computed as follows:</p>

\[\nabla_{x} \log P_{\theta^*} (x) = \frac{1}{\sigma^2} (x - \mu)\]

<p>where \(\mu\) is the mean of the diffusion model, \(\sigma\) is the standard deviation of the diffusion model, and \(c\) is the concept.
Based on the repameterization trick, the gradient of the log-likelihood is correlated with the noise \(\epsilon\) at each step as follows (linking between DDPM <d-cite key="ho2020denoising"></d-cite> and the score-based matching <d-cite key="song2020score"></d-cite>):</p>

\[\epsilon_{\theta}(x_t,t) \propto \epsilon_{\theta^*} (x_t,t) - \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t))\]

<p>where \(\epsilon_{\theta}(x_t,t)\) is the noise at step \(t\) of the diffusion model after unlearning the concept \(c\).
Finally, to fine-tune the diffusion model from pretrained model \(\theta^*\) to new cleaned model \(\theta\), the authors proposed to minimize the following loss function:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[ \sum_{t=0}^{T-1} \left\| \epsilon_{\theta}(x_t,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(x_0\) is the input image sampled from data distribution \(\mathcal{D}\), \(T\) is the number of steps of the diffusion model.</p>

<p>Instead of recursively sampling the noise \(\epsilon_{\theta}(x_t,t)\) at every step, we can sample the time step \(t \sim \mathcal{U}(0, T-1)\) and then sample the noise \(\epsilon_{\theta}(x_t,t)\) at that time step.
Therefore, the loss function can be rewritten as follows:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[ \left\| \epsilon_{\theta}(x_t,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(t \sim \mathcal{U}(0, T-1)\).</p>

<h3 id="final-objective-function">Final Objective Function</h3>

<p>However, in the paper, instead of using the above loss function, the author proposed to use the following loss function:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[  \left\| \epsilon_{\theta}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(t \sim \mathcal{U}(0, T-1)\).</p>

<p>The difference between the two loss functions is that the first loss function is computed based on the unconditional noise \(\epsilon_{\theta}(x_t,t)\) at the time step \(t\) while the second loss function is computed based on the noise \(\epsilon_{\theta}(x_t,c,t)\) at the time step \(t\) conditioned on the concept \(c\).</p>

<p><strong>Interpretation of the loss function</strong>: By minimizing the above loss function, we try to force the conditional noise \(\epsilon_{\theta}(x_t,c,t)\) to be close to the unconditional noise \(\epsilon_{\theta^*} (x_t,t)\) of the original model. Because the noise \(\epsilon_{\theta^*} (x_t,t)\) is the signal to guide the diffusion model to generate the image \(x_{t-1}\) (recall the denoising equation \(x_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta^*} (x_t,t)) + \sigma_t z\)), therefore, by forcing the conditional noise \(\epsilon_{\theta}(x_t,c,t)\) to be close to the unconditional noise \(\epsilon_{\theta^*} (x_t,t)\), we try to force the diffusion model to generate the image \(x_{t-1}\) close to the image generated without the concept \(c\).</p>

<p><strong>Note</strong>: In the above objective function, \(x_t\) is the image from the training set \(\mathcal{D}\) at time step \(t\). However, as mentioned in the paper “We exploit the model’s knowledge of the concept to synthesize training samples, thereby eliminating the need for data collection”. Therefore, in the implementation, \(x_t\) is the image generated by the fine-tuned model at time step \(t\).</p>

<h2 id="editing-implicit-assumptions-in-text-to-image-diffusion-models-time-">Editing Implicit Assumptions in Text-to-Image Diffusion Models (TIME) <d-cite key="orgad2023editing"></d-cite></h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_concepts/Time%20-%20fig1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_concepts/Time%20-%20fig1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_concepts/Time%20-%20fig1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_concepts/Time%20-%20fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Paper: <a href="https://arxiv.org/abs/2303.08084">https://arxiv.org/abs/2303.08084</a></p>

<p>Code: <a href="https://github.com/bahjat-kawar/time-diffusion">https://github.com/bahjat-kawar/time-diffusion</a></p>

<h3 id="summary-time">Summary TIME</h3>

<ul>
  <li><strong>Goal</strong>: receives an under-specified “source” prompt (e.g., “A pack of roses”), which is requested to be well-aligned with a “destination” prompt (e.g., “A pack of blue roses”) containing an attribute that the user wants to promote (e.g., “blue roses”). After the editing, the model should change its behavior on only related prompts (e.g., image generated by a prompt “A field of roses” will be changed to red roses) while not affecting the characteristics or perceptual quality in the generation of different concepts (e.g., image generated by a prompt “A poppy field” will not be changed) (Ref to the figure above).</li>
  <li><strong>Implications</strong>: The change is expected to manifest in generated images for related concepts, while not affecting the characteristics or perceptual quality in the generation of different ones. This would allow us to fix incorrect, biased, or outdated assumptions that text-to-image models may make. For example, gender bias with the concept “doctor” or “teacher”. This method can also be used to erase harmful concepts such as “nudity” or “gun” from the model by mapping them to “safe/neutrual” concept like “flower” or “cat” or “null”.</li>
  <li><strong>Important</strong> this approach edits the projection matrices in the <strong>cross-attention</strong> layers to map the source prompt close to the destination, without substantially deviating from the original weights. Because these matrices <strong>operate on textual data</strong> irrespective of the diffusion process or the image contents, they constitute a compelling location for editing a model based on textual prompts.</li>
</ul>

<h3 id="central-optimization-problem-1">Central Optimization Problem</h3>

<p>Given a pretrain layer \(W^{*}\), a set of concepts to be edited \(E\) (i.e., “roses”, “doctor”, “nudity”), the goal is to find a new layer \(W\) that is close to \(W^{*}\) but does not contain any concept in \(E\).</p>

<p>To do that, <d-cite key="orgad2023editing"></d-cite> proposed to use the following optimization problem:</p>

\[\underset{W}{\min} \sum_{c_i \in E} \| W c_i - v_i^* \|^2_2 + \lambda \| W - W^{*} \|^2_2\]

<p>where \(v_i^*\) is the targeted vector for concept \(c_i\).</p>

<p>As derived in <d-cite key="orgad2023editing"></d-cite>, the solution of this optimization problem is:</p>

\[W = \left( \sum_{c_i \in E} v_i^* c_i^T + \lambda W^{*}  \right) \left( \sum_{c_i \in E} c_i c_i^T + \lambda \mathbb{I} \right)^{-1}\]

<p>By defining \(v^*\) differently, the authors proposed 3 types of editing:</p>

<ul>
  <li><strong>Erasing/Moderation</strong>: Choose \(v^* = W c^*\), where \(c^*\) is the targeted concept different from the concepts to be earased \(c_i \in E\)$. For example, harmful concept like “nudity” or “gun” can be erased to “safe/neutrual” concept like “flower” or “cat”, or artistic concept like “Kelly Mckernan” or “Van Gogh” to “non-artistic” concept like “art” or “ “.</li>
  <li><strong>Debiasing</strong>: Choose \(v^* = W (c_i + \sum_{t=1}^p \alpha_t a_t)\) where \(c_i\) is “doctor” and \(a_t\) is attributes that we want to distribute across such as “white”, “asian”, “black”. By this way, the original concept “doctor” no longer only associated with “white” but also with “asian” and “black”.</li>
</ul>

<h3 id="pros-and-cons">Pros and Cons</h3>

<p><strong>Pros:</strong></p>

<ul>
  <li>It does not require training or finetuning, it can be applied in parallel for all cross-attention layers, and it modifies only a small portion of the diffusion model weights while leaving the language model unchanged. When applied on the publicly available Stable Diffusion, TIME edits a mere 2.2% of the diffusion model parameters, does not modify the text encoder, and applies the edit in a fraction of a second using a single consumergrade GPU.</li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
  <li>It risks interference with surrounding concepts when editing a particular concept. For example, editing doctors to be female might also affect teachers to be female. <d-cite key="gandikota2024unified"></d-cite>.</li>
  <li>TIME has a regularization term that prevents the edited matrix from changing too radically. However, it is a general term and thus affects all vector rep- resentations equally. The follow-up work of <d-cite key="orgad2023editing"></d-cite> proposes an alternative preservation term that allows targeted editing of the parameters of the pretrained generative model while maintaining its core capabilities.</li>
</ul>

<h2 id="unified-concept-editing-in-diffusion-models-">Unified Concept Editing in Diffusion Models <d-cite key="gandikota2024unified"></d-cite></h2>

<ul>
  <li>Accepted to WACV 2024. <a href="https://arxiv.org/pdf/2308.14761.pdf">https://arxiv.org/pdf/2308.14761.pdf</a></li>
  <li>Affiliation: Northeastern University, Technion and MIT. Same group with the ESD paper.</li>
  <li>Link to Github: <a href="https://github.com/rohitgandikota/unified-concept-editing">https://github.com/rohitgandikota/unified-concept-editing</a></li>
</ul>

<h3 id="summary-uce">Summary UCE</h3>

<ul>
  <li>It is a follow-up work of <d-cite key="orgad2023editing"></d-cite> that proposes an alternative preservation term that allows targeted editing of the parameters of the pretrained generative model while maintaining its core capabilities.</li>
</ul>

<h3 id="central-optimization-problem-2">Central Optimization Problem</h3>

<p>Given a pretrain layer \(W^{*}\), a set of concepts to be edited \(E\) and a set of concepts to be preserved \(P\), the goal is to find a new layer \(W\) that is close to \(W^{*}\) but does not contain any concept in \(E\) and preserve all concepts in \(P\).</p>

<p>To do that, <d-cite key="gandikota2024unified"></d-cite> proposed to use the following optimization problem:</p>

\[\underset{W}{\min} \sum_{c_i \in E} \| W c_i - v_i^* \|^2_2 + \sum_{c_j \in P} \| W c_j - W^* c_j \|^2_2\]

<p>where \(v_i^*\) is the targeted vector for concept \(c_i\).</p>

<p>As derived in <d-cite key="orgad2023editing"></d-cite>, the solution of this optimization problem is:</p>

\[W = \left( \sum_{c_i \in E} v_i^* c_i^T + \sum_{c_j \in P} W^* c_j cj^T \right) \left( \sum_{c_i \in E} c_i c_i^T + \sum_{c_j \in P} c_j c_j^T \right)^{-1}\]

<p>By defining \(v^*\) differently, the authors proposed 3 types of editing:</p>

<ul>
  <li><strong>Erasing/Moderation</strong>: Choose \(v^* = W c^*\), where \(c^*\) is the targeted concept different from the concepts to be earased \(c_i \in E\)$. For example, harmful concept like “nudity” or “gun” can be erased to “safe/neutrual” concept like “flower” or “cat”, or artistic concept like “Kelly Mckernan” or “Van Gogh” to “non-artistic” concept like “art” or “ “.</li>
  <li><strong>Debiasing</strong>: Choose \(v^* = W (c_i + \sum_{t=1}^p \alpha_t a_t)\) where \(c_i\) is “doctor” and \(a_t\) is attributes that we want to distribute across such as “white”, “asian”, “black”. By this way, the original concept “doctor” no longer only associated with “white” but also with “asian” and “black”.</li>
</ul>

<h3 id="pros-and-cons-1">Pros and Cons</h3>

<p><strong>Pros:</strong></p>

<ul>
  <li>Fast and efficient. It can finish the editing in less than 5 minutes on a single V100 GPU (Compared to 1 hour for the ESD method). The editing performance in some settings (e.g., erasing object-related concepts such as “trucks”, “tench”) is better than the ESD method.</li>
</ul>

<p><strong>Poor performance</strong>
The performance on erasing concepts is still limited. As I reproduced the experiment to erase artist concept call “Kelly Mckernan” and compare with the original model, the two generated images from two models are still very similar.</p>

<p><strong>Limited Expressiveness</strong>
The authors use textual prompt as the input to specify the concept to be erased, e.g., “Kelly Mckernan” or “Barack Obama” or “nudity”. However, a concept can be described in many different ways, for example, “Barack Obama” can be described as “the 44th president of the United States” or “the first African American president of the United States”. Therefore, it is not possible to erase all concepts related to “Barack Obama” by just erasing the keyword “Barack Obama”.</p>

<p><strong>Unaware of the time step</strong>
In this formulation, the authors just proposed to rewrite the projection matrices \(W_K\) and \(W_V\) of the attention layer \(W\) independently and ignore the query matrix \(W_Q\). However, the query ouput \(W_Q x\) has the information about the time step \(t\) of the diffusion model.</p>

<p><strong>Unknown preserved concepts</strong>
In term of methodology, while there is a closed-form solution for the optimization problem, it is not clear how to solve the optimization problem when the number of preserved concepts is large and even uncountable (i.e., how we can know how many concept that Stable Diffusion can generate?).
In fact, I have tried to run the experiment to erase 5 concepts from the ImageNette dataset while not specifying the preserved concepts. While the erasing rate can be 100\%, the preserving rate is low, especially for those concepts that are not specified to be preserved.</p>

<p><strong>Invertibility issue</strong>
If we just ignore the preserved concepts, the optimization problem is still problematic.</p>

\[W = \left( \sum_{c_i \in E} v_i^* c_i^T \right) \left( \sum_{c_i \in E} c_i c_i^T \right)^{-1}\]

<p>where \(v_i^*\) is the targeted vector for concept \(c_i\).</p>

<p>Let’s dig deeper into this OP. As mentioned in the paper, \(v_i^*=W^* c_{tar}\) where \(c_{tar}\) is the targeted concept different from the concepts to be earased \(c_i \in E\)$ such as “nudity” or “gun” can be erased to “safe/neutrual” concept like “flower” or “cat”, or artistic concept like “Kelly Mckernan” or “Van Gogh” to “non-artistic” concept like “art” or “ “.</p>

<p>In implementation, \(c_i\) and \(c_{tar}\) are input of the attention layer \(W\) which are ouput of the text encoder, therefore, they are unchanged during the optimization process.</p>

<p>Therefore, the optimization problem can be rewritten as follows:</p>

\[W = \left( \sum_{c_i \in E} W^* c_{tar} c_i^T \right) \left( \sum_{c_i \in E} c_i c_i^T \right)^{-1}\]

<p>where \(W^* c_i\) is the projected vector.</p>

<p>As mentioned in Appendix A of the paper, one condition to ensure that the optimization problem has a solution is that the matrix \(\sum_{c_i \in E} c_i c_i^T\) is invertible. To ensure this condition, the authors proposed to add \(d\) additional preservation terms along the canonical basis vectors (i.e., adding identity matrix) as follows:</p>

\[W = \left( \sum_{c_i \in E} W^* c_{tar} c_i^T \right) \left( \sum_{c_i \in E} c_i c_i^T + \lambda I \right)^{-1}\]

<p>where \(\lambda\) is a regularization factor and \(I\) is the identity matrix. While this trick can ensure the invertibility, it can be seen that these additional preservation terms can affect the projection of the concepts to be erased \(c_i \in E\) and thus affect the erasing process (i.e., too big \(\lambda\))</p>

<p>Recall some basic linear algebra:</p>

<blockquote>
  <p>\(c_i\) is a vector with \(d\) dimensions, therefore, \(c_i c_i^T\) is a matrix with \(d \times d\) dimensions.</p>
</blockquote>

<blockquote>
  <p>W is a projection matrix with \(d_o \times d\) dimensions, therefore, \(W c_i\) is a vector with \(d_o\) dimensions and \(W c_i c_i^T\) is a matrix with \(d_o \times d\) dimensions.</p>
</blockquote>

<blockquote>
  <p>If \(c_i\) is a non-zero vector, then \(c_i c_i^T\) has rank 1. Therefore, \(\sum_{c_i \in E} c_i c_i^T\) has rank at most \(\min(\mid E \mid, d)\).</p>
</blockquote>

<blockquote>
  <p><strong>what is the canonical basic vectors?</strong></p>

  <p>The canonical basis vectors are the vectors with all components equal to zero except for one component equal to one. For example, in \(\mathbb{R}^3\), the canonical basis vectors are \(e_1 = (1, 0, 0)\), \(e_2 = (0, 1, 0)\) and \(e_3 = (0, 0, 1)\).</p>
</blockquote>

<h2 id="circumventing-concept-erasure-methods-for-text-to-image-generative-models-">Circumventing Concept Erasure Methods For Text-to-Image Generative Models <d-cite key="pham2023circumventing"></d-cite></h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_concepts/circumvent-fig1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_concepts/circumvent-fig1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_concepts/circumvent-fig1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_concepts/circumvent-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paper: <a href="https://openreview.net/forum?id=ag3o2T51Ht">https://openreview.net/forum?id=ag3o2T51Ht</a></li>
  <li>Accepted to ICLR 2024</li>
</ul>

<h3 id="summary">Summary</h3>

<ul>
  <li>The paper proposes a method called Concept Inversion to circumvent 7 recent concept erasure methods for text-to-image generative models including Erased Stable Diffusion <d-cite key="gandikota2023erasing"></d-cite>, Selective Amnesia <d-cite key="heng2023selective"></d-cite>, Forget-me-not <d-cite key="zhang2023forget"></d-cite>, Ablating Concepts <d-cite key="kumari2023ablating"></d-cite>, Unified Concept Editing <d-cite key="gandikota2024unified"></d-cite>, Negative Prompt <d-cite key="negativeprompt1111, miyake2023negative"></d-cite>, and Safe Latent Diffusion <d-cite key="schramowski2023safe"></d-cite>. The authors show that with even with zero training or fine-tuning the pretrained erased model, it is possible to generate the erased concept with a suitably constructed prompt.</li>
  <li>The authors utilized the Textual Inversion <d-cite key="gal2022image"></d-cite> technique to find special word embeddings that can recover the erased concepts. The method is simply yet effective showing that existing concept erasure methods actually perform some form of concept hiding or textually obfuscating rather than concept erasure. For example, while the erased model may not generate images of “nudity” when prompted with a word “nudity”, it can still generate images of “nudity” when prompted with a special phrase “a person without clothes”. We can intuitively understand the approach is find a special embedding \(S^{*}\) that represents these special phrases by inversing some images with the erased concept and then use this special embedding to generate the erased concept like “A person \(S^{*}\)”.</li>
  <li><strong>Cons</strong> Because using the Textual Inversion technique, this method needs to replace the original Embedding Lookup table so that it can map the placeholder \(S^{*}\) to the special embedding \(v^{*}\).</li>
</ul>

<p><strong>Recall the Textual Inversion technique</strong> <d-cite key="gal2022image"></d-cite>:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/textual_inversion/examples-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/textual_inversion/examples-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/textual_inversion/examples-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/textual_inversion/examples.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/textual_inversion/method-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/textual_inversion/method-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/textual_inversion/method-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/textual_inversion/method.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Given a pretrained text-to-image generative model (Unet) \(\epsilon_\theta\), textual encoder \(c_\phi\) (denoted as \(c_\theta\) as the figure above, but it seems to be confused with the Unet \(\epsilon_\theta\)) and set of target images \(X\), and a specific text placeholder \(S^{*}\) that corresponds to a specific textual embedding vector \(v^{*}\), the goal is to find the special textual embedding vector \(v^{*}\) that can reconstruct the input image \(x \sim X\). The authors proposed to use the following optimization problem which is the same as the DDPM model but with the special placeholder/prompt \(S^{*}\):</p>

\[v^{*} = \underset{v}{\arg\min} \; \mathbb{E}_{z \sim \varepsilon(x), x \sim X, \epsilon \sim \mathcal{N}(0,I), t} [ \|\epsilon - \epsilon_\theta (z_t, c_\phi(v), t) \|_2^2 ]\]

<p>where \(v\) is the textual embedding vector \(v = \text{Lookup}(S^{*})\).</p>

<p><strong>Adapt to the concept erasure problem</strong></p>

<p>Given the background of the Textual Inversion technique, it is just straightforward to adapt this technique to circumvent the concept erasure problem. Most of the concept erasure methods are hacked by standard Textual Inversion. More details can be found in the paper. One important thing is that the authors need to have a set of target images \(X\) that contains the erased concept. The authors made an assume that the adversary can access a small number of
examples of the targeted concept from Google Images, specifically, 6 samples for art style concept (e.g., Van Gogh), 30 samples for object concept (e.g., cassette player), and 25 samples for ID concept (e.g., Angelina Jolie).</p>]]></content><author><name>Tuan-Anh Bui</name></author><category term="tml" /><category term="genai" /><category term="diffusion" /><category term="tutorial" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Tutorials on Diffusion Models and Adversarial Machine Learning</title><link href="https://tuananhbui89.github.io/blog/2023/showcases/" rel="alternate" type="text/html" title="Tutorials on Diffusion Models and Adversarial Machine Learning" /><published>2023-11-01T00:00:00+11:00</published><updated>2023-11-01T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2023/showcases</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/showcases/"><![CDATA[<h2 id="tutorials-on-diffusion-models">Tutorials on Diffusion Models</h2>

<ul>
  <li><a href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial/">Part 1: Denoising Diffusion Probabilistic Models (DDPM)</a></li>
  <li><a href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial-p2/">Part 2: DDIM, Diffusion Inversion and Accelerating Inference</a></li>
  <li><a href="https://github.com/tuananhbui89/diffusion_tf2">Implementation: DDPM with Tensorflow 2</a></li>
  <li><a href="https://github.com/tuananhbui89/diffusion_demo">Implementation: DDIM and Diffusion Inversion</a></li>
</ul>

<h2 id="tutorials-on-adversarial-machine-learning">Tutorials on Adversarial Machine Learning</h2>

<ul>
  <li><a href="https://tuananhbui89.github.io/blog/2023/aml-intro/">Part 1: The Good, The Bad, The Ugly</a></li>
  <li><a href="https://tuananhbui89.github.io/blog/2023/aml-overview/">Part 2: Adversarial Attacks</a></li>
  <li><a href="https://github.com/tuananhbui89/AML-Leaders">List of research groups and notable researchers in the field of Adversarial Machine Learning</a></li>
</ul>]]></content><author><name></name></author><category term="genai" /><category term="diffusion" /><category term="tutorial" /><category term="tml" /><category term="reading" /><summary type="html"><![CDATA[All-in-one place]]></summary></entry><entry><title type="html">Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust</title><link href="https://tuananhbui89.github.io/blog/2023/watermark-diffusion/" rel="alternate" type="text/html" title="Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust" /><published>2023-10-17T00:00:00+11:00</published><updated>2023-10-17T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2023/watermark-diffusion</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/watermark-diffusion/"><![CDATA[<h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Published at NeurIPS 2023</li>
  <li>Affiliation: University of Maryland. Tom Goldstein’s group</li>
  <li>Link to the paper: <a href="https://arxiv.org/pdf/2305.20030.pdf">https://arxiv.org/pdf/2305.20030.pdf</a></li>
  <li>Link to Github: <a href="https://github.com/YuxinWenRick/tree-ring-watermark">https://github.com/YuxinWenRick/tree-ring-watermark</a></li>
  <li>Link to Yannic Kilcher’s video: <a href="https://youtu.be/WncUlZYpdq4?si=thX3fiKHS59SQ1IG">https://youtu.be/WncUlZYpdq4?si=thX3fiKHS59SQ1IG</a></li>
</ul>

<p><strong>Side information</strong>: Tom is one of the most famous and active researchers in the field of Trustworthy Machine Learning, particulaly Adversarial Machine Learning.
His group has published several notable papers, such as <a href="https://arxiv.org/abs/1910.14667">Invisible Cloak</a><d-cite key="wu2020making"></d-cite>, <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/22722a343513ed45f14905eb07621686-Paper.pdf">clean-label data poisoning</a><d-cite key="shafahi2018poison"></d-cite>, <a href="https://arxiv.org/abs/1904.12843">adversarial training for free</a><d-cite key="shafahi2019adversarial"></d-cite>, <a href="https://arxiv.org/abs/1712.09913">visualizing the loss landscape of neural networks</a><d-cite key="li2018visualizing"></d-cite>. Recently, his group has (moved) explored TML aspects of modern generative models, such as Diffusion Models <d-cite key="wen2023tree"></d-cite>, LLMs <d-cite key="jain2023baseline, shu2023exploitability, kirchenbauer2023watermark"></d-cite>.</p>

<p><strong>Summary</strong>:</p>

<ul>
  <li><strong>Problem setting</strong>: How to insert a watermark into a generated image such that the watermark is robust to the attack and invisible to the human eye?</li>
  <li><strong>Approach</strong>: The authors proposed a simple yet effective watermarking framework for diffusion models which consits generation phase and detection phase. The method is based on the idea of <strong>diffusion inversion</strong> which allows us to invert the diffusion process. The key idea is to embed a watermark into the initial noise in frequency domain and then use the diffusion inversion to extract the watermark from the generated image in detection phase.</li>
  <li><strong>Pros</strong>: The approach doesn’t require to change the weight of the diffusion model but just need to modify the input noise. Therefore, every user can have their own secret watermarking without changing the model.</li>
  <li><strong>Cons</strong>: The method is evaluated under a quite weak black-box attack. This method is only applicable to DDIM (deterministic version of DDPM) and not applicable to other generative models such as VAEs or Flow-based models.</li>
</ul>

<p><strong>Follow-up ideas</strong>:</p>

<ul>
  <li>How to fine-tune the foundation model (i.e., Stable Diffusion which does not have the watermark) to a new model with watermarking capability naturally? In this case, every generated output will have secret watermarking and from that we now can know whether an image is real or fake!</li>
  <li>How about stochastic diffusion model like DDPM?</li>
</ul>

<p>After all, we still don’t know whether an image is real or fake :joy:.</p>

<h2 id="background">Background</h2>

<h3 id="watermarking">Watermarking</h3>

<p><strong>What is Watermarking?</strong> Watermarking is a technique to embed some information into a signal (image, audio, video, etc.) in a way that the signal is not changed much, but the information can be extracted later. The information can be used for many purposes, such as authentication, copyright.</p>

<p><strong>Watermarking: Attack and Defense Game</strong> The watermarking process can be seen as an adversarial game between two parties: <strong>attacker and defender</strong>. The attacker tries to remove the watermark from the signal, while the defender tries to make the watermark robust to the attacker’s removal process. The game is illustrated in the following figure.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/watermark-attack-defense-flow-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/watermark-attack-defense-flow-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/watermark-attack-defense-flow-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/watermark-attack-defense-flow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Watermarking: Attack and Defense Game (image source <a href="https://www.researchgate.net/publication/343385316_Digital_Watermarking_-_Comparison_of_DCT_and_DWT_methods">Jovanovic et al. 2009</a>)
</div>

<p>The defender has two main goals: (1) to make the watermark <strong>robust to the attacker</strong>’s removal process, and (2) to make the watermark <strong>invisible to the human eye</strong>. The first goal is measured by the robustness of the watermark, while the second goal is measured by the fidelity of the watermark. Similar as the trade-off between accuracy and robustness in the adversarial machine learning, the <strong>robustness and fidelity are usually conflicting</strong>, i.e., the more robust the watermark is, the more visible it is.</p>

<p>To extract the watermark, the defender needs a secret key and a secret decoder which are usually known only to the defender. There are two types of attack settings: <strong>white-box</strong> and <strong>black-box</strong> attacks depending on whether the attacker knows the secret key and decoder or not. Again, similar as the adversarial machine learning, the white-box attack is usually more powerful but less practical than the black-box attack.</p>

<p><strong>Adaptive Attack</strong> is a special type of attack where the attacker knows everything about the defender, i.e., the secret key, decoder, and the defense algorithm and can adaptively change the attack strategy based on the defender’s strategy. This type of attack is usually the most powerful and the most difficult to defend (and almost impossible to defend in the adversarial machine learning). In this paper, the authors evaluated their method under non-adaptive white-box attack.</p>

<p><strong>Why Watermarking in Generative Models?</strong> Originally, watermarking is to protect the ownership of the authors on their digital products. However, in the context of generative models, where a product is generated from a model with users’ input, the ownership is not clear. And when digging deeper, I found that copyright of AI art is complicated. Some important points that I got from this article <a href="https://www.yankodesign.com/2023/05/27/who-owns-ai-generated-content-understanding-ownership-copyrighting-and-how-the-law-interprets-ai-generated-art/#:~:text=As%20far%20as%20art%20goes,of%20it%20or%20copyright%20it.">WHO OWNS AI-GENERATED CONTENT? UNDERSTANDING OWNERSHIP, COPYRIGHTING, AND HOW THE LAW INTERPRETS AI-GENERATED ART</a></p>

<ul>
  <li>According to (US) copyright law, only humans can be granted copyrights. If it’s created by AI, nobody can claim ownership of it or copyright it.</li>
  <li>But, if a person uses AI as a tool and gives very distinct/creative inputs in the process to create something, then the person can (again, as my understanding) claim ownership of the final product. For example, as mentioned in the arcticle, <code class="language-plaintext highlighter-rouge">graphic-novel artist Kris Kashtanova was granted copyright for their AI-generated comic book “Zarya of the Dawn” for the simple reason that there was human input in creating the entire comic book and its underlying storyline. The entire comic book was “AI-assisted” and not “AI-generated”, which is why it was eligible for copyright.</code></li>
  <li>Specific to text-to-image models as Stable Diffusion, Dall-E, MidJourney, etc, the answer depends from case to case and if you care about the ownership, the first thing to do is read the <code class="language-plaintext highlighter-rouge">Terms and Conditons</code> carefully. In general, there are common points from these models:
    <ul>
      <li>User own all Assets the user create with the Services, <strong>to the extent possible under current law</strong>.</li>
      <li>However, user’s input (e.g., text prompt, input images) is granted to the company to use to improve and maintain their services.</li>
      <li>User is responsible for the content and ensuring that it does not violate any laws or intellectual property rights</li>
    </ul>
  </li>
</ul>

<p>Now, given the above points, back to the question: <strong>Why Watermarking in Generative Models?</strong>, I think the main purpose of the watermarking is to protect the ownership of the users on their generated products.</p>

<p>However, it is a much more interesting implication of watermarking in generative models than just authentication. If we can robustly and reliably detect a watermark in a generated image, we can know whether the image is real or fake, which is a very important problem in the field of Trustworthy Machine Learning.</p>

<p><strong>What is DFT and why watermarking loves DFT?</strong></p>

<p>Reference: <a href="https://vincmazet.github.io/bip/filtering/fourier.html">https://vincmazet.github.io/bip/filtering/fourier.html</a> and <a href="https://www.cs.unm.edu/~brayer/vision/fourier.html">https://www.cs.unm.edu/~brayer/vision/fourier.html</a></p>

<p>As studied in the classical watermarking literature, the watermarking process is usually done in the frequency domain. Some important points about the frequency domain are (to my understanding):</p>

<ul>
  <li>Modification in the frequency domain is more robust to image transformation such as rotation, translation, scaling, etc. than modification in the spatial domain (<a href="https://www.cs.unm.edu/~brayer/vision/fourier.html">ref</a>)</li>
  <li>Modification in the frequency domain is easier to make the watermark invisible to the human eye. DFT transformation converts an image to a phase and an amplitude. The amplitude represents the intensity of the different frequencies in the image while the phase represents the location of the frequencies. The human vision comprehends the shape of an object better than its intensity, therefore, the phase is more important than the amplitude. This is the reason why we can remove/add the watermark by modifying the amplitude while keeping the phase unchanged.</li>
</ul>

<!-- ### DDPM and DDIM

One very important note is that this framework has been based on the DDIM <d-cite key="song2020denoising"></d-cite>, which is a deterministic version of DDPM <d-cite key="ho2020denoising"></d-cite>. In the DDPM framework, the forward diffusion process has a nice property that:

$$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t$$

where $$x_0$$ is the initial image, $$\epsilon_t \sim \mathcal{N}(0, I)$$ is the noise at time $$t$$. This property allows us to `predict` noisy version of $$x_0$$ at any arbitrary time $$t$$. On the other hand, given $$\epsilon_t = \epsilon_\theta(x_t, t)$$ is the predicted noise at time $$t$$ by the denoising network $$\epsilon_\theta$$ and $$x_t$$, we can `predict` $$\tilde{x_0}$$ as follows:

$$\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}$$

Based on this observation, the authors <d-cite key="song2020denoising"></d-cite> proposed to sample $$x_{t-1}$$ from $$x_t$$ as follows:

$$x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \tilde{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta(x_t, t) + \sigma_t \epsilon_t$$

where $$\epsilon_t \sim \mathcal{N}(0, I)$$ is the noise at time $$t$$.

So $$x_{t-1} \sim q(x_{t-1} \mid x_t, x_0) = \mathcal{N} (x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \tilde{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta(x_t, t), \sigma_t^2 I)$$ -->

<h3 id="diffusion-inversion">Diffusion Inversion</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/gan-inversion-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/gan-inversion-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/gan-inversion-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/gan-inversion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Illustration of GAN inversion (Image source <d-cite key="xia2022gan"></d-cite>).
</div>

<p>Generative inversion is a technique that allows us to invert the generation process. In other words, given a pre-trained generative model \(g_\theta(z)\) and an image \(x\) which can be either real image or generated one, we can find the noise \(z\) such that \(g_\theta(z)\) is close to \(x\). This technique was first proposed for GANs in Zhu et al. (2016) <d-cite key="zhu2016generative"></d-cite>, Creswell et al. (2016) <d-cite key="creswell2018inverting"></d-cite> not long after the introduction of GAN in 2014. It can be seen that, obtaining the inverted latent code brings many useful implications such as capability to edit/manipulate generated images by editing the latent code, or adversarial perturbation removal <d-cite key="samangouei2018defense"></d-cite>.</p>

<p>Because requring the deterministic property: one noise \(z\) always generates the same image \(x\), this technique is not trivial to apply to other generative models such as VAEs or Flow-based models. For Diffusion Models, thanks to the deterministic property in DDIM, we can apply this technique to invert the diffusion process, i.e., given an image \(x_0\), we can find the noise \(x_T\) to reconstruct \(x_0\). And with the blooming of Diffusion Models in the last two years, we can see many cool applications of this technique such as Textual Inversion <d-cite key="gal2022image"></d-cite>, Image Editing <d-cite key="mokady2023null"></d-cite> or the work we are discussing - Watermarking <d-cite key="wen2023tree"></d-cite>).</p>

<p>In the DDPM framework, the forward diffusion process has a nice property that:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t\]

<p>where \(x_0\) is the initial image, \(\epsilon_t \sim \mathcal{N}(0, I)\) is the noise at time \(t\). This property allows us to <code class="language-plaintext highlighter-rouge">predict</code> noisy version of \(x_0\) at any arbitrary time \(t\). On the other hand, given \(\epsilon_t = \epsilon_\theta(x_t, t)\) is the predicted noise at time \(t\) by the denoising network \(\epsilon_\theta\) and \(x_t\), we can <code class="language-plaintext highlighter-rouge">predict</code> \(\tilde{x_0}\) as follows:</p>

\[\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\]

<p>Now we consider the next step in the forward diffusion process:</p>

\[x_{t+1} = \sqrt{\bar{\alpha}_{t+1}} x_0 + \sqrt{1 - \bar{\alpha}_{t+1}} \epsilon_{t+1}\]

<p>where \(\epsilon_{t+1} \sim \mathcal{N}(0, I)\) is the noise at time \(t+1\). If we replace the original \(x_0\) with the predicted \(\tilde{x}_0\) and assume that the diffusion process is large enough so that \(\epsilon_{t+1} \approx \epsilon_\theta(x_t, t)\), we can obtain the inverted diffusion process as follows:</p>

\[x_{t+1} = \sqrt{\bar{\alpha}_{t+1}} \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} + \sqrt{1 - \bar{\alpha}_{t+1}} \epsilon_\theta(x_t, t)\]

<p>which now depends only on \(x_t\) and \(\epsilon_\theta(x_t, t)\). Repeating this process from \(t=0\) to \(t=T\), we can obtain the inverted code \(x_T\) that reconstructs \(x_0\) (again it works for DDIM model only). This is the key technique used in this watermarking method.</p>

<h2 id="tree-ring-watermark">Tree-Ring Watermark</h2>

<h3 id="threat-model">Threat Model</h3>

<p>In adversarial machine learning, a threat model is a description of capabilities and objectives of all parties in the attack and defense game. From that, we can narrow down the defense space and scope to this specific threat model (It is because in the real world, we cannot know every possible attack and defend against it). In this paper, the authors considered the following threat model:</p>

<ul>
  <li>Model owner (generative phase): The generative model owner generates an image \(x\) with a secret watermark \(k\). The constraint is that the watermarking algorithm  should have a negligible effect on the generation process, so that quality is maintained and watermarking leaves no visible trace.</li>
  <li>Attacker or Forger (attack phase): The attacker tries to remove the watermark from the generated image \(x\) to get \(x'\) (and then can claim his ownership on \(x'\), etc.). The attacker uses data augmentations only and knows nothing about the watermarking algorithm and the generative model (a <strong>quite weak black-box attack</strong>).</li>
  <li>Model owner (detection phase): The model owner tries to detect the watermark in the image \(x'\) to know whether it was modified from the original image \(x\) or not. The model owner knows nothing about the attack including its techniques and hyper-parameters.</li>
</ul>

<h3 id="watermarking-process">Watermarking Process</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/fig1-pipeline-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/fig1-pipeline-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/fig1-pipeline-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/fig1-pipeline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>The watermarking process is illustrated in the above figure. The watermarking process consists of two main steps: <strong>watermark embedding</strong> and <strong>watermark detection</strong>.</p>

<p>In the watermark embedding step, an initial Gaussian noise \(x_T\) is first converted to the frequency domain using DFT \(\mathcal{F}\). Then, a pre-defined watermark \(k\) is embedded into the frequency domain of \(x_T\) by a simple binary masking operation. The watermarked noise \(x_T^k\) is then converted back to the time domain using inverse DFT \(\mathcal{IF}\). Finally, the watermarked noise \(x_T^k\) is used to generate the watermarked image \(x_0^k\) (now call \(x\)) using the standard <strong>DDIM model</strong> (again not stochastic one like DDPM).</p>

<p>In the watermark detection, the transformed image \(x'=\mathcal{A}(x_0^k)\) is first inverted to obtain the <strong>approximated</strong> noise \(x'_T\) using the <strong>DDIM inversion</strong>. Then, the watermark \(k'\) is extracted from the frequency domain of \(x'_T\) using the same binary masking operation as in the watermark embedding step. Finally, the extracted watermark \(k'\) is compared with the original watermark \(k\) to determine whether the image \(x'\) is from the original image \(x\) or not.</p>

<p>The simple process can be describe as follows:</p>

<blockquote class="block-quote">
  <p><strong>Watermark Embedding</strong></p>

  <p>Input: initial noise \(x_T\), secret key \(k\), mask \(M\), DDIM model \(\mathcal{D}\) <br />
\(x_T^f = \mathcal{F}(x_T)\) <br />
\(x_T^k = Masking(x_T^f, k, M)\) <br />
\(x_0^k = \mathcal{D}(\mathcal{IF}(x_T^k))\) <br />
Output: watermarked image \(x_0^k\) or \(x\)</p>
</blockquote>

<blockquote class="block-quote">
  <p><strong>Watermark Detection</strong></p>

  <p>Input: transformed image \(x'\), secret key \(k\), mask \(M\), DDIM inversion \(\mathcal{D}^I\) <br />
\(x'_T = \mathcal{D}^I (x')\) <br />
\({x'}_T^f = \mathcal{F}(x'_T)\) <br />
\(k' = UnMasking({x'}_T^f, k, M)\) <br />
calculate distance \(d(k, k')\) between \(k'\) and \(k\) <br />
Output: \(d(k, k')\)</p>
</blockquote>

<p>As described in the paper, the masking opearation will produce output:</p>

\[x_{i,T}^k \sim \left\{
  \begin{array}{ c l }
    k_i &amp; \quad \textrm{if } i \in M \\
    \mathcal{N} (0,1)                 &amp; \quad \textrm{otherwise}
  \end{array}
\right.\]

<p>where \(k_i\) is the \(i\)-th element of the key \(k\), \(M\) is the mask. Note that the Fourier transform of a Gaussian noise array is also distributed as
Gaussian noise. The distance function is the L1 distance \(d(k, k') = \frac{1}{\mid M \mid} \sum_{i \in M} \mid k_i - k'_i \mid\).</p>

<h3 id="constructing-the-key">Constructing the key</h3>

<p>As mentioned in the paper, choosing the key pattern \(k\)  (as similar the binary mask \(M\)) strongly effects the robustness and visibility of the watermark. The authors proposed to use a <strong>tree-ring pattern</strong> which is a circular mask with radius \(r\) centered on the low frequency modes as the key pattern. This pattern brings several benefits such as invariant to rotation, translation, and dilation (which was studied in classical watermarking literature). The authors proposed three variants of the tree-ring pattern:</p>

<ul>
  <li>Tree-ring Zero: all elements in the tree-ring pattern are zero.</li>
  <li>Tree-ring Rands: all elements in the tree-ring pattern are randomly sampled from \(\mathcal{N}(0,1)\).</li>
  <li>Tree-ring Rings: multiple rings with different radiuses.</li>
</ul>

<p><strong>Why ring pattern?</strong></p>

<h3 id="how-to-detect-the-watermark">How to detect the watermark?</h3>

<p>Given an image \(x'\) and from the watermark detection process, we can obtain \(k'\) which is the extracted pattern from \(x'\). Now, how we can decide whether \(k'\) is the same as the original pattern \(k\) or not?</p>

<p>To do that, the authors defined a null hypothesis \(H_0\) and find the P-value of the null hypothesis. The null hypothesis is defined as follows:</p>

\[H_0: k' \sim \mathcal{N}(0, \sigma^2 I)\]

<p>Here, the variance \(\sigma^2\) is unknown and be estimated from each image as \(\sigma^2 = \frac{1}{\mid M \mid} \sum_{i \in M} \mid k'_i \mid^2\).</p>

<blockquote class="block-quote">
  <p><strong>What is Null Hypothesis?</strong></p>

  <p>Null hypothesis is the claim that no relationship exists between two sets of data or variables being analyzed. For example, in the context of watermarking, the null hypothesis is a statement that the extracted pattern \(k'\) is just a random noise and not related to the original pattern \(k\). On the other hand, the alternative hypothesis is a statement that the extracted pattern \(k'\) is related to the original pattern \(k\).</p>
</blockquote>

<blockquote class="block-quote">
  <p><strong>What is P-value?</strong></p>

  <p>The P-value is the probability of obtaining results as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. The smaller the P-value, the stronger the evidence against the null hypothesis. The P-value is calculated from the null hypothesis and the observed data using a statistical test. For example, in the context of watermarking, the P-value is the probability of obtaining the extracted pattern \(k'\) from the null hypothesis \(H_0\).</p>
</blockquote>

<p>The P-value is calculated as follows:</p>

\[p = Pr \left( \chi^2_{\mid M \mid, \lambda} \leq \eta \mid H_0 \right) = \Phi_{\chi^2} (z)\]

<p>where \(\chi^2_{\mid M \mid, \lambda}\) is the chi-squared distribution with \(\mid M \mid\) degrees of freedom and non-centrality parameter \(\lambda\), \(\eta = \frac{1}{\sigma^2} \sum_{i \in M} \mid k_i - k'_i \mid^2\), \(z = \frac{\eta - \lambda}{\sqrt{2 \lambda}}\), and \(\Phi_{\chi^2}\) is the cumulative distribution function of the chi-squared distribution.</p>

<p>From that, an image is considered as a forgery if \(p &lt; \alpha\) where \(\alpha\) is a pre-defined threshold (too small \(\alpha\) will lead to many false positives, while too large \(\alpha\) will lead to many false negatives).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/fig3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/fig3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/fig3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/fig3-attack-watermark-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/fig3-attack-watermark-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/fig3-attack-watermark-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/fig3-attack-watermark.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Measuring P-value in different settings (w/o watermark, w/ watermark, w/ watermark and attack). The extreme low P-value in the last setting indicates that the watermark is robust to the attack.
</div>

<p>The figure above shows the P-value of the null hypothesis in three different settings including (1) without watermark, (2) with watermark, and (3) with watermark and attack. As we can see, the P-value of image with watermark is much lower than that of image without watermark, and the P-value in the last setting is extremely low, which indicates that the watermark is robust to the attack. The authors also provided a more quantitative analysis as Table 1 in the paper.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/tab1-main-results-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/tab1-main-results-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/tab1-main-results-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/tab1-main-results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>The metric was used to measure the performance is <strong>AUC/TPR@1%FPR</strong> which is the area under the ROC curve (AUC) or the true positive rate (TPR) at 1% false positive rate (FPR). The authors also used FID score to measure the quality of the generated images and CLIP score to measure the semantic similarity between the generated images and the prompts.</p>

<p><strong>That’s all for the paper!</strong> There are still many experiments and analysis in the paper, but the post is already too long. Further details can be found in the paper.</p>

<h2 id="summary">Summary</h2>

<p><strong>Summary</strong>:</p>

<ul>
  <li><strong>Problem setting</strong>: How to insert a watermark into a generated image such that the watermark is robust to the attack and invisible to the human eye?</li>
  <li><strong>Approach</strong>: The authors proposed a simple yet effective watermarking framework for diffusion models which consits generation phase and detection phase. The method is based on the idea of <strong>diffusion inversion</strong> which allows us to invert the diffusion process. The key idea is to embed a watermark into the initial noise in frequency domain and then use the diffusion inversion to extract the watermark from the generated image in detection phase.</li>
  <li><strong>Pros</strong>: The approach doesn’t require to change the weight of the diffusion model but just need to modify the input noise. Therefore, every user can have their own secret watermarking without changing the model.</li>
  <li><strong>Cons</strong>: The method is evaluated under a quite weak black-box attack. This method is only applicable to DDIM (deterministic version of DDPM) and not applicable to other generative models such as VAEs or Flow-based models.</li>
</ul>

<p><strong>Follow-up ideas</strong>:</p>

<ul>
  <li>How to fine-tune the foundation model (i.e., Stable Diffusion which does not have the watermark) to a new model with watermarking capability naturally? In this case, every generated output will have secret watermarking and from that we now can know whether an image is real or fake!</li>
  <li>How about stochastic diffusion model like DDPM?</li>
</ul>]]></content><author><name>Tuan-Anh Bui</name></author><category term="reading" /><category term="genai" /><category term="tml" /><category term="diffusion" /><summary type="html"><![CDATA[How to know whether an image is real or fake?]]></summary></entry><entry><title type="html">A Tutorial on Diffusion Models (Part 1)</title><link href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial/" rel="alternate" type="text/html" title="A Tutorial on Diffusion Models (Part 1)" /><published>2023-10-03T00:00:00+11:00</published><updated>2023-10-03T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2023/diffusion-tutorial</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial/"><![CDATA[<!-- I have been asked by Dinh to develop a short tutorial/lecture on diffusion models for the course "Deep Learning" at Monash University (FIT3181). And, here it is. -->

<h2 id="resources">Resources</h2>

<ul>
  <li>The Jupyter notebooks associated with this tutorial can be found <a href="https://github.com/tuananhbui89/diffusion_tf2">here</a></li>
  <li>The slide can be found here: <a href="https://www.dropbox.com/scl/fi/x7ucu2reluvv0v7rahw75/A-short-tutorial-on-Diffusion_v2.pdf?rlkey=yplk7jib1fx1wqg39fdibwlh3&amp;dl=0">(Dinh’s revision)</a>.</li>
  <li>The lecture about Generative Models which includes VAE, GAN and Diffusion Models that I taught at VietAI is available <a href="https://docs.google.com/presentation/d/1WT0OeAuTrRpCWq0agIbfaSh5VCuscUND85i3WT-ggZs/edit?usp=sharing">here</a>.</li>
</ul>

<h2 id="ddpm">DDPM</h2>

<p>When talking about diffusion models, we usually refer to three notable works including Sohl-Dickstein et al., 2015 <d-cite key="sohl2015deep"></d-cite>, Yang &amp; Ermo, 2019 <d-cite key="song2019generative"></d-cite>, and maybe the most popular one DDPM by Ho et al., 2020 <d-cite key="ho2020denoising"></d-cite>.</p>

<p>The general diffusion framework includes two processes: the <strong>forward diffusion process</strong> in which a random noise is added to an input image to destruct the image, and the <strong>backward (reverse) diffusion process</strong> in which the image is denoised by removing the noise added in the forward diffusion process to reconstruct the original image.</p>

<h3 id="forward-diffusion-process">Forward Diffusion Process</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/forward-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/forward-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/forward-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/forward.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Forward Diffusion Process (image source <a href="https://cvpr2023-tutorial-diffusion-models.github.io/ ">https://cvpr2023-tutorial-diffusion-models.github.io/ </a>)
</div>

<p>In the DDPM model, the forward diffusion process is formulated as Markov chain with T steps such that at each time step \(t\), the image \(x_t\) is distributed according to a Gaussian distribution \(x_t \sim q(x_t \mid x_{t-1}) = \mathcal{N}(x_t \mid \mu_t, \sigma_t^2)\) with mean \(\mu_t=\sqrt{1-\beta_t} x_{t-1}\) and variance \(\sigma_t^2 = \beta_t I\), where \(0&lt; \beta_t &lt; 1\) is the diffusion coefficient at time step \(t\).</p>

<p>Intuitively, in the input space, data points are usually not uniformly distributed in the entire space, but they are usually concentrated in some regions with high density (as illustrated in the following figure). Therefore, the diffusion process can be seen as a process to <strong>spread out the data points</strong> in the input space, which is similar as the process of <strong>diffusion</strong> in physics. Analogy speaking, we can imagine that the entire possible input space is a room filled with air, and the data points are some heat sources in the room. The air molecules will <strong>move randomly (Brownian motion) in all directions</strong> and collide with neighboring molecules. As a result, the heat will be <strong>spread out from the high density regions to the low density regions</strong>. This process will continue until the temperature is uniform in the room (reaching the equilibrium state). Back to the forward diffusion process in DDPM, at the time step \(t\), the image \(x_{t-1}\) is added with some noise so that \(x_t \sim \mathcal{N}(x_t \mid \mu_t, \sigma_t^2)\) with <strong>the variance is a little bit larger</strong> than previous step (lower density). This process will continue until the image \(x_T\) is completely destroyed (reaching the equilibrium state).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/forward_distribution_shift-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/forward_distribution_shift-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/forward_distribution_shift-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/forward_distribution_shift.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Forward Diffusion Process as Distribution Shift (image source <a href="https://handbook.monash.edu/2022/units/FIT3181">FIT 3181, Deep Learning, Monash University</a>)
</div>

<p><strong>How to sample \(x_t\).</strong> While it is possible to form a Gaussian distribution \(q(x_t \mid x_{t-1})\) and sample \(x_t\) from this distribution, it is computationally expensive. Scaling to the Markov chain with \(T\) steps, this is obviously not a nice solution. Instead, by using <strong>reparameterization trick</strong>, the authors proposed a more efficient way to sample \(x_t\) as follows:</p>

\[x_t = \sqrt{1-\beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon_t\]

<p>where \(\epsilon_t \sim \mathcal{N}(0, I)\) is a standard Gaussian noise.
This simple trick not only allows us to sample \(x_t\) efficiently from \(q(x_t \mid x_{t-1})\), but magically also allows us to jump to any arbitrary time step \(t\) and sample \(x_t\) from that. Details of this trick can be found in the original paper. Basically, \(x_t \sim q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)\) where \(\bar{\alpha}_t = \prod_{i=1}^{t} (1-\beta_i)\).</p>

<p>Let’s define \(\alpha_t = 1 - \beta_t\), then \(\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i\). With \(\epsilon_t \sim \mathcal{N}(0, I) \; \forall t \in [1, 2, ..., T]\), we have:</p>

\[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_t\]

\[x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1-\alpha_{t-1}} \epsilon_{t-1}\]

\[...\]

\[x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1-\alpha_1} \epsilon_1\]

<p>Replacing \(x_{t-1}\) in the first equation with the second equation, we have:</p>

\[x_t = \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1-\alpha_{t-1}} \epsilon_{t-1}) + \sqrt{1-\alpha_t} \epsilon_t\]

\[x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t (1-\alpha_{t-1})} \epsilon_{t-1} + \sqrt{1-\alpha_t} \epsilon_t\]

<!-- We know that sum of two Gaussian distributions $$\mathcal{N}(\mu_1, \sigma_1^2)$$ and $$\mathcal{N}(\mu_2, \sigma_2^2)$$ is also a Gaussian distribution $$\mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$$, therefore, in our case, $$\mathcal{N}(\sqrt{\alpha_t} x_{t-1}, 1-\alpha_t)$$ shifted by factor $$\sqrt{\alpha_{t-1}}/2$$ and $$\mathcal{N}(\sqrt{\alpha_{t-1}} x_{t-2}, 1-\alpha_{t-1})$$ shifted by factor $$\sqrt{\alpha_t}/2$$ is also a Gaussian distribution with mean $$\sqrt{\alpha_t \alpha_{t-1}} x_{t-2}$$ and variance $$1 - \alpha_t \alpha_{t-1}$$.

$$x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \epsilon$$ -->

<p>As mentioned very briefly in the paper <d-cite key="ho2020denoising"></d-cite> that the forward process has a nice property that the distribution of \(x_t\) is a Gaussian distribution with mean \(\sqrt{\alpha_t \alpha_{t-1}} x_{t-2}\) and variance \(1 - \alpha_t \alpha_{t-1}\) (I couldn’t prove it :joy:). Applying the same reparemeterization trick, we can have:</p>

\[x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \epsilon\]

<p>where \(\epsilon \sim \mathcal{N}(0, I)\).</p>

<p>Finally, we have:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon\]

<p>where \(\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i\). This allows us to jump to any arbitrary time step \(t\) and sample \(x_t\) from that.</p>

<p><strong>Properties of the forward diffusion process</strong>:</p>

<ul>
  <li>At any arbitrary time step \(t\), we have \(q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)\).</li>
  <li>\(\sqrt{\bar{\alpha}_t}\) is a bit smaller than \(\sqrt{\bar{\alpha}_{t-1}}\), while \((1-\bar{\alpha}_t)\) is a bit larger than \((1-\bar{\alpha}_{t-1})\). Which is similar as the process of diffusion in physics when the temperature is spread out from the high density regions \(\mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0, (1-\bar{\alpha}_{t-1})I)\) to the low density regions \(\mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)\).</li>
  <li>If T is large enough, \(\bar{\alpha}_t \approx 0\), therefore, \(q(x_T \mid x_0) \approx \mathcal{N}(x_t; 0, I)\) which is the equilibrium state of the diffusion process.</li>
</ul>

<h3 id="backward-diffusion-process">Backward Diffusion Process</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/backward-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/backward-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/backward-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/backward.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Backward Diffusion Process (image source <a href="https://cvpr2023-tutorial-diffusion-models.github.io/ ">https://cvpr2023-tutorial-diffusion-models.github.io/ </a>)
</div>

<p>In the backward process, the goal is to remove the noise added in the forward process to reconstruct the original image. The backward process is also formulated as Markov chain with T steps as illustrated in the above figure. At each time step \(t\), the denoised image \(x_{t-1}\) is distributed according a distribution \(q(x_{t-1} \mid x_t)\), which is approximated by \(p_\theta(x_{t-1} \mid x_t)\) parameterized by a neural network \(f_\theta\).</p>

<p>To learn the reverse process, we can minimize the variational bound on negative log likelihood as follows:</p>

\[L = \mathbb{E}_q \left[ - \log \frac{p_\theta (x_{0:T})}{q(x_{1:T} \mid x_0)} \right]\]

\[L = \mathbb{E}_q \left[ - \log p(x_T) - \sum_{t \geq 1} \log \frac{p_\theta (x_{t-1} \mid x_t)}{q (x_t \mid x_{t-1})} \right]\]

<p>As derived in Appendix A of the paper <d-cite key="ho2020denoising"></d-cite>, the objective function can be rewritten as follows:</p>

\[L = \mathbb{E}_q \left[ D_{KL} \left( q(x_T \mid x_0) \| p(x_T) \right) + \sum_{t \geq 1} D_{KL} \left( q(x_{t-1} \mid x_t, x_0) \| p_\theta (x_{t-1} \mid x_t) \right) - \log p_\theta (x_0 \mid x_1) \right]\]

<p>The three terms in the above equation can be interpreted as follows:</p>

<ul>
  <li>The \(L_{T} = \mathbb{E}_q \left[ D_{KL} \left( q(x_T \mid x_0) \| p(x_T) \right) \right]\) is to make the distribution in the equilibrium state \(q(x_T \mid x_0)\) close to the prior distribution \(p(x_T)\). In learning process, this term is usually ignored because it is a constant regarding to the model parameters \(\theta\).</li>
  <li>The \(L_{1:T-1} =  \mathbb{E}_q \left[ \sum_{t \geq 1} D_{KL} \left( q(x_{t-1} \mid x_t, x_0) \| p_\theta (x_{t-1} \mid x_t) \right) \right]\) is to make the distribution in the backward process \(q(x_{t-1} \mid x_t, x_0)\) close to its approximation \(p_\theta (x_{t-1} \mid x_t)\).</li>
  <li>The \(L_{0} = \mathbb{E}_q \left[ - \log p_\theta (x_0 \mid x_1) \right]\) can be understood as the reconstruction loss to reconstruct the original image \(x_0\) from the denoised image \(x_1\). In learning model parameters \(\theta\), this term is also ignored.</li>
</ul>

<h3 id="magic-simplification">Magic Simplification</h3>

<p>So after all, the only term that we need to care about is the \(L_{1:T-1}\). However, simplifying the \(L_{1:T-1}\) is not easy (please refer the <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lil’s tutorial</a> for more details). High-level speaking, the authors showed that \(q(x_{t-1} \mid x_t, x_0)\) can be refactored as a Gaussian distribution \(\mathcal{N} (x_{t-1}; \tilde{\mu} (x_t, x_0), \tilde{\beta}_t I)\) with \(\tilde{\mu} (x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_t \right)\) and \(\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}\).</p>

<p><span style="color:blue">It is a nice property that \(\tilde{\mu} (x_t, x_0)\) does not depend on \(x_0\), therefore, we can generate new images without knowing the original image \(x_0\) (obvious but important)</span>.</p>

<p>Because \(p_\theta (x_{t-1} \mid x_t)\) is also a Gaussian distribution \(\mathcal{N} (x_{t-1}; \mu_\theta(x_t, t), \beta_\theta(x_t, t) I)\). Therefore, by simplifying that the two Gaussian distribution have the same variance, the KL-divergence between \(q(x_{t-1} \mid x_t, x_0)\) and \(p_\theta (x_{t-1} \mid x_t)\) can be simplified as matching the two means \(\tilde{\mu} (x_t, x_0)\) and \(\mu_\theta(x_t, t)\) as follows:</p>

\[L_{1:T-1} = \mathbb{E}_q \left[ \sum_{t \geq 1} \frac{1}{2 \sigma_t^2} \| \tilde{\mu} (x_t, x_0) - \mu_\theta(x_t, t) \|^2 \right]\]

<p>Since \(\tilde{\mu} (x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_t \right)\), we can introduce a denoising network \(\epsilon_\theta (x_t, t)\) so that \(\mu_\theta(x_t, t)\) has the same form as \(\tilde{\mu} (x_t, x_0)\) as \(\mu_\theta (x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta (x_t, t) \right)\)</p>

<p>Finally, after all the magic, the objective function becomes a simple form of matching the predicted noise \(\epsilon_\theta (x_t, t)\) and the true noise \(\epsilon_t\) as follows:</p>

\[L_{1:T-1} = \mathbb{E}_q \left[ \sum_{t \geq 1} \frac{ (1-\alpha_t)^2 }{2 \sigma_t^2 \alpha_t (1 - \bar{\alpha}_t) } \| \epsilon_\theta (x_t, t) - \epsilon_t \|^2 \right]\]

<p>To further simplifying and avoid expensive computation, we can uniformly sample time step \(t\) from \([1, 2, ..., T]\) and ignore scaling factor, we come to the final objective function:</p>

\[L_{1:T-1} = \mathbb{E}_{x_0 \sim q(x_0), t \sim U \{1, T\}, \epsilon \sim \mathcal{N}(0, I)} \left[ \| \epsilon_\theta (x_t, t) - \epsilon \|^2 \right]\]

<p>where \(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon\) as in the forward diffusion process.</p>

<h3 id="generating-new-images">Generating New Images</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/algorithm_sampling-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/algorithm_sampling-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/algorithm_sampling-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/algorithm_sampling.png" class="img-fluid rounded z-depth-1" width="100" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Sampling algorithm (image source <d-cite key="ho2020denoising"></d-cite>)
</div>

<p>It is worth to remind that \(x_{t-1}\) is sampled from \(\mathcal{N} (x_{t-1}; \tilde{\mu} (x_t, x_0), \tilde{\beta}_t I)\) with \(\tilde{\mu} (x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_t \right)\) and \(\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}\). By using the reparameterization trick again, we can sample \(x_{t-1}\) as follows:</p>

\[x_{t-1} = \tilde{\mu} (x_t, x_0) + \sqrt{\tilde{\beta}_t} z\]

<p>where \(z \sim \mathcal{N}(0, I)\). From now, we will call \(\sigma_t = \sqrt{\tilde{\beta}_t} = \sqrt{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}}\)</p>

<p>After training and obtaining the denoising network \(\epsilon_\theta (x_t, t)\), we can approximate \(\tilde{\mu} (x_t, x_0) \approx \mu_\theta(x_t, t) =  \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta (x_t, t) \right)\).</p>

<p>So, the final equation to sample \(x_{t-1}\) for us :joy: is:</p>

\[x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta (x_t, t) \right) + \sigma_t z\]

<p>where \(z \sim \mathcal{N}(0, I)\).</p>

<h2 id="implementation">Implementation</h2>

<p>Here is the embedded Jupyter notebook.</p>




    <div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/assets/jupyter/diffusion_models_tf2_fixed.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>]]></content><author><name>Tuan-Anh Bui</name></author><category term="reading" /><category term="genai" /><category term="tutorial" /><category term="diffusion" /><summary type="html"><![CDATA[DDPM with Tensorflow2 implementation]]></summary></entry><entry><title type="html">A Tutorial on Diffusion Models (Part 2)</title><link href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial-p2/" rel="alternate" type="text/html" title="A Tutorial on Diffusion Models (Part 2)" /><published>2023-10-03T00:00:00+11:00</published><updated>2023-10-03T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2023/diffusion-tutorial-p2</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial-p2/"><![CDATA[<!-- I have been asked by Dinh to develop a short tutorial/lecture on diffusion models for the course "Deep Learning" at Monash University (FIT3181). And, here it is. -->

<h2 id="resources">Resources</h2>

<ul>
  <li>The Jupyter notebooks associated with this tutorial can be found <a href="https://github.com/tuananhbui89/diffusion_demo">here</a>.</li>
  <li>The first part of this tutorial can be found <a href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial/">here</a>.</li>
  <li>FastAI tutorial about DDIM can be found <a href="https://course.fast.ai/Lessons/lesson21.html">here</a>.</li>
</ul>

<h2 id="ddim">DDIM</h2>

<p>One of the main drawbacks of DDPM is that training process requires a large \(T\) to reach the equilibrium state. In inference, to obtain a sample \(x_0\), we need to run through \(T\) reverse steps, sequentially, which is very slow. To address this issue, Song et al. <d-cite key="song2020denoising"></d-cite> proposed a new diffusion model called DDIM (Denoising Diffusion Implicit Model) which allows us to accelerate the inference process while using the same training process as DDPM (it means that you can use the pre-trained DDPM model to inference with DDIM method).</p>

<h3 id="change-of-notation">Change of Notation</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/notation-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/notation-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/notation-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/notation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Meme stealing from <a href="https://www.tanishq.ai/">Tanishq</a> at <a href="https://course.fast.ai/Lessons/lesson21.html">https://course.fast.ai/Lessons/lesson21.html</a>
</div>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>DDPM</th>
      <th>DDIM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>atomic</strong> param :joy:</td>
      <td>\(0 &lt; \alpha_t &lt; 1, \beta_t = 1 - \alpha_t\)</td>
      <td>\(0 &lt; \bar{\alpha}_t &lt; 1, \beta_t = 1 - \bar{\alpha}_t\)</td>
    </tr>
    <tr>
      <td><strong>cummulative</strong> param</td>
      <td>\(\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i\)</td>
      <td>\(\alpha_t = \prod_{i=1}^{t} \bar{\alpha}_i\)</td>
    </tr>
    <tr>
      <td>\(q(x_t \mid x_{t-1})\)</td>
      <td>\(\mathcal{N} (x_t; \sqrt{\alpha_t} x_t, (1 - \alpha_t) I)\)</td>
      <td>\(\mathcal{N} (x_t; \sqrt{\frac{\alpha_t}{\alpha_{t-1}}}x_{t-1}, (1 - \frac{\alpha_t}{\alpha_{t-1}})I)\)  \(^{\star}\)</td>
    </tr>
    <tr>
      <td>\(q(x_t \mid x_0)\)</td>
      <td>\(\mathcal{N} (x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\)</td>
      <td>\(\mathcal{N} (x_t; \sqrt{\alpha_t} x_0, (1 - \alpha_t) I)\)</td>
    </tr>
    <tr>
      <td>(forward) sampling \(x_t\)</td>
      <td>\(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon\)</td>
      <td>\(x_t = \sqrt{\alpha_t} x_0 + \sqrt{1-\alpha_t} \epsilon\)</td>
    </tr>
    <tr>
      <td>(reverse) sampling \(x_{t-1}\)</td>
      <td>\(\frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta (x_t, t) \right) + \sigma_t z\)</td>
      <td>\(\frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \frac{1 - \bar{\alpha}_t}{\sqrt{1 - \alpha_t}} \epsilon_\theta (x_t, t) \right) + \sigma_t z\)</td>
    </tr>
    <tr>
      <td>\(\sigma_t\)</td>
      <td>\(\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}\)</td>
      <td>\(\frac{1 - \alpha_{t-1}}{1 - \alpha_t}\)</td>
    </tr>
  </tbody>
</table>

<p>\(^{\star}\) In the DDIM paper, the authors made a note that <code class="language-plaintext highlighter-rouge">covariance matrix is ensured to have positive terms on its diagonal</code>. The \(\alpha_{1:T} \in  (0, 1]^T\) is a decreasing sequence, i.e., \(\alpha_{t+1} \leq \alpha_t\), \(\alpha_1 = 1\) and \(\alpha_T \approx 0\) where \(T \rightarrow \infty\).</p>

<p>With this in mind, I believe that the variable naming in the <code class="language-plaintext highlighter-rouge">LDM</code> implementation (which can be found here: <a href="https://github.com/Stability-AI/stablediffusion/blob/main/ldm/models/diffusion/ddpm.py#">https://github.com/Stability-AI/stablediffusion/blob/main/ldm/models/diffusion/ddpm.py</a> cannot confuse you anymore :smile:.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">to_torch</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">betas</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">betas</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">alphas_cumprod</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">alphas_cumprod_prev</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">alphas_cumprod_prev</span><span class="p">))</span>

    <span class="c1"># calculations for diffusion q(x_t | x_{t-1}) and others
</span>    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">sqrt_alphas_cumprod</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">)))</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">sqrt_one_minus_alphas_cumprod</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)))</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">log_one_minus_alphas_cumprod</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)))</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">sqrt_recip_alphas_cumprod</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">)))</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">sqrt_recipm1_alphas_cumprod</span><span class="sh">'</span><span class="p">,</span> <span class="nf">to_torch</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">alphas_cumprod</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
</code></pre></div></div>

<p>Fortunately, in the implementation of <code class="language-plaintext highlighter-rouge">DDIM</code>, the authors keep the same notation of the <code class="language-plaintext highlighter-rouge">DDPM</code> implementation and introduce some new variables just for the <code class="language-plaintext highlighter-rouge">DDIM</code> model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># ddim sampling parameters
</span>    <span class="n">ddim_sigmas</span><span class="p">,</span> <span class="n">ddim_alphas</span><span class="p">,</span> <span class="n">ddim_alphas_prev</span> <span class="o">=</span> <span class="nf">make_ddim_sampling_parameters</span><span class="p">(</span><span class="n">alphacums</span><span class="o">=</span><span class="n">alphas_cumprod</span><span class="p">.</span><span class="nf">cpu</span><span class="p">(),</span>
                                                                                <span class="n">ddim_timesteps</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">ddim_timesteps</span><span class="p">,</span>
                                                                                <span class="n">eta</span><span class="o">=</span><span class="n">ddim_eta</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">ddim_sigmas</span><span class="sh">'</span><span class="p">,</span> <span class="n">ddim_sigmas</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">ddim_alphas</span><span class="sh">'</span><span class="p">,</span> <span class="n">ddim_alphas</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">ddim_alphas_prev</span><span class="sh">'</span><span class="p">,</span> <span class="n">ddim_alphas_prev</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">ddim_sqrt_one_minus_alphas</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">ddim_alphas</span><span class="p">))</span>
    <span class="n">sigmas_for_original_sampling_steps</span> <span class="o">=</span> <span class="n">ddim_eta</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod_prev</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod_prev</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="motivation">Motivation</h3>

<h3 id="ddim-sampling">DDIM Sampling</h3>

<p>(To be continued)</p>

<h2 id="diffusion-inversion">Diffusion Inversion</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/gan-inversion-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/gan-inversion-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/gan-inversion-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/gan-inversion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Illustration of GAN inversion (Image source <d-cite key="xia2022gan"></d-cite>).
</div>

<p>Generative inversion is a technique that allows us to invert the generation process. In other words, given a pre-trained generative model \(g_\theta(z)\) and an image \(x\) which can be either real image or generated one, we can find the noise \(z\) such that \(g_\theta(z)\) is close to \(x\). This technique was first proposed for GANs in Zhu et al. (2016) <d-cite key="zhu2016generative"></d-cite>, Creswell et al. (2016) <d-cite key="creswell2018inverting"></d-cite> not long after the introduction of GAN in 2014. It can be seen that, obtaining the inverted latent code brings many useful implications such as capability to edit/manipulate generated images by editing the latent code, or adversarial perturbation removal <d-cite key="samangouei2018defense"></d-cite>.</p>

<p>Because requring the deterministic property: one noise \(z\) always generates the same image \(x\), this technique is not trivial to apply to other generative models such as VAEs or Flow-based models. For Diffusion Models, thanks to the deterministic property in DDIM, we can apply this technique to invert the diffusion process, i.e., given an image \(x_0\), we can find the noise \(x_T\) to reconstruct \(x_0\). And with the blooming of Diffusion Models in the last two years, we can see many cool applications of this technique such as Textual Inversion <d-cite key="gal2022image"></d-cite>, Image Editing <d-cite key="mokady2023null"></d-cite> or the work we are discussing - Watermarking <d-cite key="wen2023tree"></d-cite>).</p>

<p>In the DDPM framework, the forward diffusion process has a nice property that:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t\]

<p>where \(x_0\) is the initial image, \(\epsilon_t \sim \mathcal{N}(0, I)\) is the noise at time \(t\). This property allows us to <code class="language-plaintext highlighter-rouge">predict</code> noisy version \(x_t\) of \(x_0\) at any arbitrary time \(t\). On the other hand, given \(\epsilon_t = \epsilon_\theta(x_t, t)\) is the predicted noise at time \(t\) by the denoising network \(\epsilon_\theta\) and \(x_t\), we can <code class="language-plaintext highlighter-rouge">predict</code> \(\tilde{x_0}\) as follows:</p>

\[\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\]

<p>Now we consider the next step in the forward diffusion process:</p>

\[x_{t+1} = \sqrt{\bar{\alpha}_{t+1}} x_0 + \sqrt{1 - \bar{\alpha}_{t+1}} \epsilon_{t+1}\]

<p>where \(\epsilon_{t+1} \sim \mathcal{N}(0, I)\) is the noise at time \(t+1\). If we replace the original \(x_0\) with the predicted \(\tilde{x}_0\) and assume that the diffusion process is large enough so that \(\epsilon_{t+1} \approx \epsilon_\theta(x_t, t)\), we can obtain the inverted diffusion process as follows:</p>

\[x_{t+1} = \sqrt{\bar{\alpha}_{t+1}} \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} + \sqrt{1 - \bar{\alpha}_{t+1}} \epsilon_\theta(x_t, t)\]

<p>which now depends only on \(x_t\) and \(\epsilon_\theta(x_t, t)\). Repeating this process from \(t=0\) to \(t=T\), we can obtain the inverted code \(x_T\) that reconstructs \(x_0\) (again it works for DDIM model only).</p>

<h2 id="implementation">Implementation</h2>

<h3 id="jumping-prediction">Jumping Prediction</h3>

<p>In this demo, I will show you one of the applications of diffusion inversion - jumping prediction. The goal is to predict the initial image \(x_0\) from the image \(x_t\) at any arbitrary time \(t\) in the diffusion process. It can be done by using the following equation:</p>

\[\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\]

<p>where \(\tilde{x}_0\) is the predicted image at time \(t=0\) given the image \(x_t\) at time \(t\) and the noise \(\epsilon_\theta(x_t, t)\).</p>

<p><strong>Why care about this?</strong></p>

<p><strong>Standard Diffusion Model</strong> In the first part, I use the <a href="https://github.com/openai/guided-diffusion">Guided-Diffusion</a> by OpenAI as the codebase to demonstrate this technique (i.e., predicting \(x_0\) from \(x_t\)). The codebase is for the <a href="http://arxiv.org/abs/2105.05233">Diffusion Models Beat GANS on Image Synthesis</a> paper. I have blogged about this paper and some important components of the codebase <a href="https://tuananhbui89.github.io/blog/2023/conditional-diffusion/">here</a>.</p>

<p>The main function to predict \(x_0\) from \(x_t\) as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">pred_eps_and_x0_from_xstart_uncond</span><span class="p">(</span><span class="n">model_fn</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        the (uncondition/standard) $$\epsilon_</span><span class="se">\t</span><span class="s">heta(x_t,t)$$
        the (uncondition/standard) $$</span><span class="se">\t</span><span class="s">ilde{x}_0 = </span><span class="se">\f</span><span class="s">rac{x_t - \sqrt{1 - </span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t} \epsilon_</span><span class="se">\t</span><span class="s">heta(x_t,t)}{\sqrt{</span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t}}$$
        note 1: the _predict_xstart_from_eps() function does not have parameter, therefore, using auxiliary_diffusion or diffusion does not matter
        </span><span class="sh">"""</span>
        <span class="k">assert</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># only this step has trainable parameter
</span>        <span class="k">assert</span> <span class="n">eps</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="o">*</span><span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="n">eps</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x_0</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">_predict_xstart_from_eps</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_0</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">pred_eps_and_x0_from_xstart_cond</span><span class="p">(</span><span class="n">model_fn</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">,</span> <span class="n">cond_fn</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        the condition $$\hat{\epsilon}_{</span><span class="se">\t</span><span class="s">heta}(x_t,t,y,\phi) = \epsilon_</span><span class="se">\t</span><span class="s">heta(x_t,t) - \sqrt{1 - </span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t} </span><span class="se">\n</span><span class="s">abla_{x_t} \log p_\phi (y \mid x_t)$$ as in classifier-guidance model
        the condition $$</span><span class="se">\t</span><span class="s">ilde{x}_0 = </span><span class="se">\f</span><span class="s">rac{x_t - \sqrt{1 - </span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t} \hat{\epsilon}_{</span><span class="se">\t</span><span class="s">heta}(x_t,t,y,\phi)}{\sqrt{</span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t}}$$
        note 1: the _predict_xstart_from_eps() function does not have parameter, therefore, using auxiliary_diffusion or diffusion does not matter
        note 2: the classifier should be the ORIGINAL classifier, not the auxiliary classifier
        </span><span class="sh">"""</span>
        <span class="k">assert</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>
        <span class="n">alpha_bar</span> <span class="o">=</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">diffusion</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># only this step has trainable parameter
</span>        <span class="k">assert</span> <span class="n">eps</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="o">*</span><span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="n">eps</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="nf">cond_fn</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar</span><span class="p">).</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">x_0</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">_predict_xstart_from_eps</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_0</span>
</code></pre></div></div>

<p><strong>Latent Diffusion Model</strong> 
<!-- In this part, I will use the codebase from the paper [Erasing Concepts from Diffusion Models](https://erasing.baulab.info/) which is based on the [Stable Diffusion Model](https://github.com/CompVis/stable-diffusion) by [CompVis lab](https://github.com/CompVis) as the codebase to demonstrate this technique.  -->
Unlike the previous codebase, the latent diffusion model has three main components: encoder \(\mathcal{E}\) and decoder \(\mathcal{D}\), U-Net \(\epsilon_\theta\), and the conditioning mechanism \(\tau\), in which the diffusion process is on the latent space instead of the image space.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/latent-diffusion-architecture-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/latent-diffusion-architecture-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/latent-diffusion-architecture-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/latent-diffusion-architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Latent Diffusion Model architecture. Image credit to <d-cite key="rombach2022high"></d-cite>
</div>

<p>Therefore, to make a prediction of \(x_0\) from \(x_t\) we need the following steps:</p>

<p><strong>Step 1</strong>: Getting \(z_t\). There are two ways to obtain \(z_t\):</p>

<ul>
  <li>Using forward process given \(x_0\) as \(z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t\) where \(z_0 = \mathcal{E}(x_0)\) is the latent code of the input image \(x_0\), and \(\epsilon_t \sim \mathcal{N}(0, I)\) is the noise at time \(t\).</li>
  <li>Using the reverse process given \(z_T\) and a prompt \(c\) as \(z_{t-1} = \frac{1}{\sqrt{\alpha_t}} (z_{t+1} - \frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta (z_{t+1}, t, \tau(c))) + \sigma_t \epsilon\) where \(\epsilon\) is the noise. It is important to note that we consider the conditional diffusion process, i.e., \(\epsilon_\theta (z_{t+1}, t, \tau(c))\) where \(\tau(c)\) is the embedding vector of the prompt \(c\).</li>
</ul>

<p><strong>Step 2</strong>: Predict the latent code \(z_0\) from \(z_t\) using a similar equation as in the standard diffusion model. However, again, we need to consider the conditional diffusion process.</p>

\[\tilde{z}_0 = \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(z_t, t, \tau(c))}{\sqrt{\bar{\alpha}_t}}\]

<p><strong>Step 3</strong>: Using the decoder to obtain the image \(x_0\) from \(z_0\).</p>

\[\tilde{x}_0 = \mathcal{D}(\tilde{z}_0)\]

<!-- Detailed implementation as follows:

```python
quick_sample_till_t = lambda c, s, start_code, t: sample_model(model, sampler,
                                                                 c, image_size, image_size, ddim_steps, s, ddim_eta,
                                                                 start_code=start_code, till_T=t, verbose=False)

# Step 1: get z_t
z_t = quick_sample_till_t(emb_c, start_guidance, start_code, t_enc)

# Step 2a: predict epsilon_t
e_t = model.apply_model(z_t, t_enc_ddpm, emb_c)

# Step 2b: predict z_0
z_0_tilde = (z_t - (1 - alpha_bar).sqrt() * e_t) / alpha_bar.sqrt()

# Step 3: predict x_0
``` -->

<p>It is a worth noting that, in the inference process of the LDM (with <code class="language-plaintext highlighter-rouge">diffusers</code>) (which can be found in the <a href="https://github.com/rohitgandikota/erasing/blob/a2189e9ae677aca22a00c361bde25d3d320d8a61/eval-scripts/generate-images.py#L132"><code class="language-plaintext highlighter-rouge">generate-images.py</code></a>) in each inference step, the U-Net outputs the unconditional noise \(\epsilon_u\) and the conditional noise \(\epsilon_c\). And the final noise \(\epsilon = \epsilon_u + \text{guidance_scale} (\epsilon_c - \epsilon_u)\) is used to sample the next latent code \(z_{t+1}\).</p>

<h3 id="notebooks">Notebooks</h3>

<p>Here is the embedded Jupyter notebook. The result is really interesting. In this example, I use a prompt <code class="language-plaintext highlighter-rouge">Image of cassette player</code> to generate images with Stable Diffusion version 1.4 with DDIM and 100 steps. Each row shows the <code class="language-plaintext highlighter-rouge">Predicted image</code> \(\tilde{x}_0 = \mathcal{D}(\tilde{z}_0)\), <code class="language-plaintext highlighter-rouge">Generated image using the current latent code</code> \(\tilde{x}_t = \mathcal{D}(\tilde{z}_t)\), and <code class="language-plaintext highlighter-rouge">Scaled Difference</code> between two images \(\delta = \frac{\tilde{x}_0 - \tilde{x}_t}{\max (\tilde{x}_0 - \tilde{x}_t)}\), respectively.</p>

<p>We can see that, at early steps (i.e., \(t &gt; 80\)) the two images look very noisy and do not show any sign of desired object in the prompt. However, if looking to the difference image (i.e., <code class="language-plaintext highlighter-rouge">delta</code>), we can still see some patterns of the object. It shows that the prediction technique works even at very early steps of the diffusion process. And, through the diffusion process, the difference becomes smaller and smaller and the two images nearly identical at the end of the process.</p>




    <div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/assets/jupyter/demo_ddim_jump.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>



<h3 id="how-to-generate-an-image-with-a-gaussian-noise">How to generate an image with a Gaussian noise?</h3>]]></content><author><name>Tuan-Anh Bui</name></author><category term="reading" /><category term="genai" /><category term="tutorial" /><category term="diffusion" /><summary type="html"><![CDATA[DDIM, Diffusion Inversion and Accelerating Inference]]></summary></entry><entry><title type="html">Flow Matching for Generative Modeling</title><link href="https://tuananhbui89.github.io/blog/2023/flowmatching/" rel="alternate" type="text/html" title="Flow Matching for Generative Modeling" /><published>2023-09-22T00:00:00+10:00</published><updated>2023-09-22T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2023/flowmatching</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/flowmatching/"><![CDATA[<h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Published at ICLR 2023 (splotlight, top 5%)</li>
  <li>Affiliations: Meta AI, Weizmann Institute of Science</li>
  <li>Link to the paper: <a href="https://openreview.net/pdf?id=PqvMRDCJT9t">https://openreview.net/pdf?id=PqvMRDCJT9t</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Continuous Normalizing Flows (CNF) is a class of generative models that can be trained by maximum likelihood. The main idea is to transform a simple distribution (e.g., Gaussian) to a complex distribution (e.g., ImageNet dataset) by a series of invertible transformations. The main challenge is to design a transformation that is invertible and can be computed efficiently.</p>

<p>The flow \(\phi_t(x)\) presents a time-dependent diffeomorphic map that transforms the input \(x\) to the output \(y\) at time \(t\). The flow is defined as follows:</p>

\[\frac{d}{dt} \phi_t(x) = v_t(\phi_t(x))\]

<p>where \(v_t\) is a time-dependent vector field. \(\phi_0(x) = x\) means that the flow at time \(t=0\) is the identity map.</p>

<p>Given \(p_0\) is the simple distribution (e.g., Gaussian), the flow \(\phi_t\) transforms \(p_0\) to \(p_t\) as follows:</p>

\[p_t = [ \phi_t ] * p_0\]

<p>where \([ \phi_t ] * p_0\) is the push-forward measure of \(p_0\) under the map \(\phi_t\). 
The push-forward measure is defined as follows:</p>

\[[ \phi_t ] * p_0(A) = p_0(\phi_t^{-1}(A))\]

<p>where \(A\) is a subset of the output space. The push-forward measure can be interpreted as the probability of the output \(y\) falls into the subset \(A\).</p>

\[p_t(x) = p_0(\phi_t^{-1}(x)) \left| \det \frac{d \phi_t^{-1}(x)}{dx} \right|\]

<p>The function \(v_t\) can be intepreted as the velocity of the flow at time \(t\), i.e., how fast the flow moves at time \(t\). In comparison with diffusion process, the velocity \(v_t\) is similar as the denoising function that is used to denoise the image \(x\) at time \(t\), where \(\phi_t(x)\) is the distribution of the denoised images at time \(t\).</p>

<!-- The flow is invertible because it is a diffeomorphic map. The inverse flow is defined as follows:

$$ \frac{d}{dt} \phi_t^{-1}(y) = -v_t(\phi_t^{-1}(y)) $$
 -->

<p><strong>Flow matching objective</strong>: Given a target probability density path \(p_t(x)\) and a corresponding vector field \(u_t(x)\) which generates \(p_t(x)\), the flow matching objective is to find a flow \(\phi_t(x)\) and a corresponding vector field \(v_t(x)\) that generates \(p_t(x)\).</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{t, p_t(x)} \| v_t(x) - u_t(x) \|\]

<p>It is a bit confusing in notation here, so \(u_t(x)\) can be understand as the target vector field that generates the target probability density path \(p_t(x)\), while \(v_t(x)\) is the vector field to be learned to approximate \(u_t(x)\).</p>

<p>It can be seen that the Flow Matching objective is a simple and attractive objective but intractable to use because we don’t know the target vector field \(u_t(x)\).
The main contribution of the paper is to propose the way to simplify the above objective function. And their approach is quite similar as in DDPM where the solution relies on conditioning to a previous point in the sequence.</p>

<p>The marginal probability path</p>

\[p_t(x) = \int p_t(x \mid x_1) q(x_1) dx_1\]

<p>where $x_1$ is a particular data sample, and \(p_t(x \mid x_1)\) is the conditional probability path such that \(p_t(x \mid x_1) = p_t(x)\) at time \(t=0\). 
The important point is that they design the \(p_1(x \mid x_1)\) at time \(t=1\) to be a normal distribution around \(x_1\) with a small variance, i.e., \(p_1 (x \mid x_1) = \mathcal{N}(x_1, \sigma^2 I)\). In the above equation, \(q(x_1)\) is the prior distribution of \(x_1\).</p>

<p>Where in particular at time \(t=1\), the marginal probability path \(p_1\) will approximate the data distribution \(q\),</p>

\[p_1(x) = \int p_1(x \mid x_1) q(x_1) dx_1 \approx q(x)\]

<p>And the vector field \(u_t(x)\) can be defined as follows:</p>

\[u_t(x) = \int u_t(x \mid x_1) \frac{p_t (x \mid x_1) q(x_1)}{p_t(x)} dx_1\]

<p>Theorem 1: Given vector fields \(u_t(x \mid x_t)\) that generate conditional probability paths \(p_t(x \mid x_t)\) for any distribution \(q(x_1)\), the marginal vector field \(u_t(x)\) in the above equation generates the marginal probability path \(p_t(x)\).</p>

<p>So it means that if we can learn \(u_t (x \mid x_t)\) we can obtain \(u_t(x)\) and then we can use \(u_t(x)\) to generate \(p_t(x)\).</p>

<p>Now we can rewrite the Flow Matching objective to Conditional Flow Matching objective as follows:</p>

\[\mathcal{L}_{CFM} (\theta) = \mathbb{E}_{t, q(x_1), p_t(x \mid x_1)} \| v_t(x) - u_t(x \mid x_1) \|\]

<p>where \(v_t(x)\) is the vector field to be learned to approximate \(u_t(x \mid x_1)\). 
Now the question is how can we obtain \(u_t(x \mid x_1)\)?</p>

<p>In the work, they consider conditional probability paths</p>

\[p_t(x \mid x_1) = \mathcal{N} (x \mid \mu_t (x_1), \sigma_t (x_1)^2 I)\]

<p>where \(\mu_t (x_1)\) and \(\sigma_t (x_1)\) are the mean and variance of the conditional probability path \(p_t(x \mid x_1)\), and they are time-dependent. Later, they will show that we can choose \(\mu_t (x_1)\) and \(\sigma_t (x_1)\) very flexiblely, as long as they can satisfy some conditions, for example, \(\mu_0 (x_1) = 0\) and \(\sigma_0 (x_1) = 1\), and \(\mu_1 (x_1) = x_1\) and \(\sigma_1 (x_1) = \sigma_{min}\), which is set sufficiently small so that \(p_1 (x \mid x_1)\) is a concentrated distribution around \(x_1\).</p>

<p>The canonical transformation for Gaussian distributions is defined as follows:</p>

\[\psi_t (x) = \mu_t (x_1) + \sigma_t (x_1) \odot x\]

<p>where \(\psi_t (x)\) is the canonical transformation of \(p_t(x \mid x_1)\), and \(\odot\) is the element-wise multiplication.</p>

<p>to be continued…</p>]]></content><author><name></name></author><category term="reading" /><category term="genai" /><summary type="html"><![CDATA[A cool paper about continuous normalizing flows]]></summary></entry><entry><title type="html">Diffusion Models Beat GANs on Image Synthesis</title><link href="https://tuananhbui89.github.io/blog/2023/conditional-diffusion/" rel="alternate" type="text/html" title="Diffusion Models Beat GANs on Image Synthesis" /><published>2023-09-14T00:00:00+10:00</published><updated>2023-09-14T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2023/conditional-diffusion</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/conditional-diffusion/"><![CDATA[<h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Published at NeurIPS 2021</li>
  <li>Affiliations: OpenAI</li>
  <li>One of the very first works on diffusion model. Showing that diffusion model can be used for image synthesis and outperform GANs on FID score. One important contribution of the paper is proposing conditional diffusion process by using gradient from an auxiliary classifier, which is used to sample images from a specific class</li>
  <li>Link to the paper: <a href="https://arxiv.org/pdf/2105.05233.pdf">https://arxiv.org/pdf/2105.05233.pdf</a></li>
  <li>Link to the code: <a href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a></li>
</ul>

<h2 id="understanding-conditional-diffusion-process">Understanding Conditional Diffusion Process</h2>

<p>In this section, we will go through the Conditional Diffusion Process introduced in Appendix H of the paper.</p>

<p>We start by defining a conditional Markovian noising process \(\hat{q}\) similar to \(q\), and assume that \(\hat{q}(y \mid x_0)\) is a known and readily available label distribution for each sample.</p>

<ul>
  <li>\(\hat{q}(x_0) = q(x_0)\): the initial distribution of the process is the same as the unconditional process.</li>
  <li>\(\hat{q}(y \mid x_0)\) is the label distribution for each sample \(x_0\) which is known and readily available.</li>
  <li>\(\hat{q}(x_{t+1} \mid x_t, y) = q(x_{t+1} \mid x_t)\): <strong>This is the key point that will later enable us to derive the conditional diffusion process</strong>. This explains that the transition distribution is the same as the unconditional process, i.e., the noise adding in the forward diffusiion process is independent to label \(y\). However, this might not neccessary be the case. If using SDE (Stochastic Differential Equation) to model the diffusion process, then the forward diffusion process can be conditioned on \(y\). <strong>This can be a future work to explore.</strong></li>
  <li>
\[\hat{q}(x_{1:T} \mid x_0, y) = \prod_{t=1}^T \hat{q}(x_t \mid x_{t-1}, y)\]
  </li>
</ul>

<p>From the above definition, we can derive the following properties:</p>

<ul>
  <li>\(\hat{q}(x_{t+1} \mid x_t, y) = \hat{q}(x_{t+1} \mid x_t) = q(x_{t+1} \mid x_t)\) the forward process conditioned on \(y\) is the same as the unconditional forward process.</li>
  <li>\(\hat{q}(y \mid x_t, x_{t+1}) = \hat{q}(y \mid x_t)\): the label distribution is independent of the next sample \(x_{t+1}\).</li>
  <li>\(\hat{q}(y \mid x_t, x_{t+1}) \neq \hat{q}(y \mid x_{t+1})\): Need confirmation on this. But if this is true, then it means that \(\hat{q}(y \mid x_t) \neq \hat{q}(y \mid x_{t+1})\) or the label distribution has changed after adding noise at each step. Then we cannot use the same classifier to approximate the label distribution at each step. <strong>However, in the paper, the authors still use the same classifier!!!</strong>. One possible idea is that we can consider a classifier that is conditioned to time step \(t\).</li>
</ul>

<p>Based on the above properties, we now can derive the conditional reverse process as follows:</p>

\[\hat{q}(x_{t} \mid x_{t+1}, y) = \frac{\hat{q}(x_{t} \mid x_{t+1}) \hat{q}(y \mid x_{t}, x_{t+1})}{\hat{q}(y \mid x_{t+1})}\]

\[\hat{q}(x_{t} \mid x_{t+1}, y) = \frac{q(x_{t} \mid x_{t+1}) \hat{q}(y \mid x_{t})}{\hat{q}(y \mid x_{t+1})}\]

<p>The term \(\hat{q}(y \mid x_{t+1})\) is considered as constant w.r.t. \(x_t\). So \(x_t\) can be sampled from the above distribution, where \(\hat{q}(y \mid x_{t})\) is approximated by an auxiliary classifier, which is trained to predict the label \(y\) from the sample \(x_t\). And \(q(x_{t} \mid x_{t+1})\) is the reverse process of the unconditional diffusion process which has been trained.</p>

<h2 id="classifier-guidance">Classifier Guidance</h2>

<p>After understanding the conditional diffusion process, we now go through the classifier guidance to see how to use the classifier to guide the sampling process.
In the paper, the authors proposed two sampling approaches:</p>

<ul>
  <li><strong>Conditional Reverse Noising Process</strong>: which factorizes the conditional transition \(p_{\theta, \phi}(x_t \mid x_{t+1}, y) = Z p_\theta(x_t \mid x_{t+1} p_\phi (y \mid x_t))\). This can be approximated by a Gaussian similar to the unconditional reverse process, but with its mean shifted by \(\Sigma g\), where \(g\) is the gradient of the classifier w.r.t. the input.</li>
  <li><strong>Conditional Sampling for DDIM</strong>: which can be applied for deterministic sampling methods like DDIM. This can be done by using the conditioning trick adapted from Song et al. (2021).</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/classifier_guidance/two_sampling_methods-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/classifier_guidance/two_sampling_methods-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/classifier_guidance/two_sampling_methods-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/classifier_guidance/two_sampling_methods.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Two sampling methods
</div>

<h2 id="how-to-implement">How to implement</h2>

<p>Link to the original implementation: <a href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a></p>

<p>Minimal code to implement the classifier guidance diffusion as follows, which is based on the code from file <a href="https://github.com/openai/guided-diffusion/blob/main/scripts/classifier_sample.py"><code class="language-plaintext highlighter-rouge">classifier_sample.py</code></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># create unet and scheduler of the diffusion model
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">diffusion</span> <span class="o">=</span> <span class="nf">create_model_and_diffusion</span><span class="p">()</span>

    <span class="c1"># load the pretrained unet 
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="c1"># create classifier which is Unet's encoder with linear layer on top
</span>    <span class="n">classifier</span> <span class="o">=</span> <span class="nf">create_classifier</span><span class="p">()</span>

    <span class="c1"># load the pretrained classifier
</span>    <span class="n">classifier</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">()</span>
    <span class="n">classifier</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="c1"># define the gradient of the classifier w.r.t. the input as guidance for sampling 
</span>    <span class="k">def</span> <span class="nf">cond_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">enable_grad</span><span class="p">():</span>
            <span class="n">x_in</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="nf">classifier</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">selected</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)),</span> <span class="n">y</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">selected</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">x_in</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">args</span><span class="p">.</span><span class="n">classifier_scale</span>   
    

    <span class="k">def</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">return</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span> <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">class_cond</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span>

    <span class="c1"># main loop 
</span>    <span class="k">while</span> <span class="n">gothrough_all_images</span><span class="p">:</span>
        <span class="c1"># random target classes
</span>        <span class="n">classes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">()</span>

        <span class="c1"># calling sample function with the classifier guidance 
</span>        <span class="n">sample_fn</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">diffusion</span><span class="p">.</span><span class="n">p_sample_loop</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="p">.</span><span class="n">use_ddim</span> <span class="k">else</span> <span class="n">diffusion</span><span class="p">.</span><span class="n">ddim_sample_loop</span>
        <span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="nf">sample_fn</span><span class="p">(</span>
            <span class="n">model_fn</span><span class="p">,</span>
            <span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">image_size</span><span class="p">),</span>
            <span class="n">clip_denoised</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">clip_denoised</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="n">cond_fn</span><span class="o">=</span><span class="n">cond_fn</span><span class="p">,</span> <span class="c1"># classifier guidance
</span>            <span class="n">device</span><span class="o">=</span><span class="n">dist_util</span><span class="p">.</span><span class="nf">dev</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="c1"># save the output 
</span></code></pre></div></div>

<p>The crucial part of the above code is the <code class="language-plaintext highlighter-rouge">cond_fn</code> function which defines the gradient of the classifier w.r.t. the input as guidance for sampling. Another important part is the <code class="language-plaintext highlighter-rouge">diffusion.p_sample_loop</code> or <code class="language-plaintext highlighter-rouge">diffusion.ddim_sample_loop</code> which will use the classifier guidance to sample images from the diffusion model.</p>

<p>The diffusion model with these above sampling methods can be found in the file <a href="https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/script_util.py#L74"><code class="language-plaintext highlighter-rouge">script_util.py</code></a> and the sampling methods are defined in the file <a href="https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/gaussian_diffusion.py"><code class="language-plaintext highlighter-rouge">gaussian_diffusion.py</code></a></p>

<p>The Algorithm 1 (Conditional Reverse Noising Process, i.e., <code class="language-plaintext highlighter-rouge">p_sample_loop</code>) can be implemented as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># one step of the sampling process
</span><span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">clip_denoised</span><span class="p">,</span> <span class="n">denoised_fn</span><span class="p">,</span> <span class="n">cond_fn</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">):</span>

    <span class="c1"># sample from the unconditional reverse process
</span>    <span class="c1"># the output includes "mean" and "log_variance" of the Gaussian distribution
</span>    <span class="c1"># the output also includes "pred_xstart"
</span>    <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">p_mean_variance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">clip_denoised</span><span class="p">,</span> <span class="n">denoised_fn</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

    <span class="c1"># Shift the mean by the gradient of the classifier w.r.t. the input
</span>    <span class="c1"># equation: new_mean = mean + sigma * g 
</span>    <span class="c1"># where sigma is the standard deviation of the Gaussian distribution, i.e., out["varaince"]
</span>    <span class="k">if</span> <span class="n">cond_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">condition_mean</span><span class="p">(</span><span class="n">cond_fn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>
    
    <span class="c1"># create nonzero mask
</span>    <span class="n">nonzero_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">t</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="p">)</span>  <span class="c1"># no noise when t == 0
</span>
    <span class="c1"># sample from the shifted Gaussian distribution
</span>    <span class="n">sample</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">nonzero_mask</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">log_variance</span><span class="sh">"</span><span class="p">])</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">sample</span><span class="sh">"</span><span class="p">:</span> <span class="n">sample</span><span class="p">,</span> <span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">:</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">]}</span>

<span class="c1"># the progressive sampling loop from T to 0, where the $$x_t$$ will be used to sample $$x_{t-1}$
</span>
<span class="k">def</span> <span class="nf">p_sample_loop_progressive</span><span class="p">():</span>

    <span class="bp">...</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">th</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">p_sample</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">img</span><span class="p">,</span>
                <span class="n">t</span><span class="p">,</span>
                <span class="n">clip_denoised</span><span class="o">=</span><span class="n">clip_denoised</span><span class="p">,</span>
                <span class="n">denoised_fn</span><span class="o">=</span><span class="n">denoised_fn</span><span class="p">,</span>
                <span class="n">cond_fn</span><span class="o">=</span><span class="n">cond_fn</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">yield</span> <span class="n">out</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">sample</span><span class="sh">"</span><span class="p">]</span>

</code></pre></div></div>

<p>The Algorithm 2 (Conditional Sampling for DDIM, i.e., <code class="language-plaintext highlighter-rouge">ddim_sample_loop</code>) can be implemented as below. As described in the paper, the stochastic process can be controlled by the parameter <code class="language-plaintext highlighter-rouge">eta</code>. When <code class="language-plaintext highlighter-rouge">eta=0</code>, the sampling process is truly deterministic, while <code class="language-plaintext highlighter-rouge">eta &gt; 0</code>, the sampling process is stochastic.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># one step of the sampling process
</span><span class="k">def</span> <span class="nf">ddim_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">clip_denoised</span><span class="p">,</span> <span class="n">denoised_fn</span><span class="p">,</span> <span class="n">cond_fn</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>

    <span class="c1"># sample from the unconditional reverse process
</span>    <span class="c1"># the output includes "mean" and "log_variance" of the Gaussian distribution
</span>    <span class="c1"># the output also includes "pred_xstart"
</span>    <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">p_mean_variance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">clip_denoised</span><span class="p">,</span> <span class="n">denoised_fn</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

    <span class="c1"># calculate score 
</span>    <span class="k">if</span> <span class="n">cond_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">condition_score</span><span class="p">(</span><span class="n">cond_fn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">)</span>    
    
    <span class="c1"># calculate epsilon_t 
</span>    <span class="n">eps</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_predict_eps_from_xstart</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">])</span>

    <span class="c1"># calculate alpha_bar_t and alpha_bar_prev and sigma 
</span>    <span class="n">sigma</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">eta</span>
        <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar_prev</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar</span><span class="p">))</span>
        <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar</span> <span class="o">/</span> <span class="n">alpha_bar_prev</span><span class="p">)</span>
    <span class="p">)</span>    

    <span class="c1"># calculate x_{t-1} as in Algorithm 2
</span>    <span class="n">mean_pred</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">th</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha_bar_prev</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar_prev</span> <span class="o">-</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="p">)</span>

    <span class="c1"># Still random sample from a Gaussian distribution 
</span>    <span class="c1"># but the mean is calculated as above
</span>    <span class="n">sample</span> <span class="o">=</span> <span class="n">mean_pred</span> <span class="o">+</span> <span class="n">nonzero_mask</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">mean_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">sample</span><span class="sh">"</span><span class="p">:</span> <span class="n">sample</span><span class="p">,</span> <span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">:</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">]}</span>    
</code></pre></div></div>

<p>The main component of the above code is the unconditional reverse process <code class="language-plaintext highlighter-rouge">p_mean_variance</code> which is defined as follows in the file <code class="language-plaintext highlighter-rouge">gaussian_diffusion.py</code>. It is worth noting that this function not only returns the \(x_{t-1}\) but also the prediction of the initial \(x_0\), i.e., <code class="language-plaintext highlighter-rouge">pred_xstart</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">p_mean_variance</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">clip_denoised</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">denoised_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="bp">None</span>
    <span class="p">):</span>

    <span class="sh">"""</span><span class="s">
    Apply the model to get p(x_{t-1} | x_t), as well as a prediction of
    the initial x, x_0.

    :param model: the model, which takes a signal and a batch of timesteps
                    as input.
    :param x: the [N x C x ...] tensor at time t.
    :param t: a 1-D Tensor of timesteps.
    :param clip_denoised: if True, clip the denoised signal into [-1, 1].
    :param denoised_fn: if not None, a function which applies to the
        x_start prediction before it is used to sample. Applies before
        clip_denoised.
    :param model_kwargs: if not None, a dict of extra keyword arguments to
        pass to the model. This can be used for conditioning.
    :return: a dict with the following keys:
                - </span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s">: the model mean output.
                - </span><span class="sh">'</span><span class="s">variance</span><span class="sh">'</span><span class="s">: the model variance output.
                - </span><span class="sh">'</span><span class="s">log_variance</span><span class="sh">'</span><span class="s">: the log of </span><span class="sh">'</span><span class="s">variance</span><span class="sh">'</span><span class="s">.
                - </span><span class="sh">'</span><span class="s">pred_xstart</span><span class="sh">'</span><span class="s">: the prediction for x_0.
    </span><span class="sh">"""</span>

    <span class="n">model_output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

    <span class="c1"># really long process 
</span>    <span class="p">...</span> 


    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_mean</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">variance</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_variance</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">log_variance</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_log_variance</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">:</span> <span class="n">pred_xstart</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></div>

<h3 id="conditional-sampling-for-ddpm-and-ddim">Conditional Sampling for DDPM and DDIM</h3>

<p>Another important component of the above code is the <code class="language-plaintext highlighter-rouge">condition_mean()</code> and <code class="language-plaintext highlighter-rouge">condition_score()</code> which are two conditioning functions that are used to condition the diffusion model on the gradient of the classifier w.r.t. the input as described in Algorithm 1 and 2, respectively. They are the main force to shift the output from uncondition to condition.
The two functions are defined as follows in the file <a href="https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/gaussian_diffusion.p"><code class="language-plaintext highlighter-rouge">gaussian_diffusion.py</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">condition_mean</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cond_fn</span><span class="p">,</span> <span class="n">p_mean_var</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compute the mean for the previous step, given a function cond_fn that
    computes the gradient of a conditional log probability with respect to
    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to
    condition on y.

    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).
    </span><span class="sh">"""</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="nf">cond_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
    <span class="n">new_mean</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">p_mean_var</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">].</span><span class="nf">float</span><span class="p">()</span> <span class="o">+</span> <span class="n">p_mean_var</span><span class="p">[</span><span class="sh">"</span><span class="s">variance</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_mean</span>

<span class="k">def</span> <span class="nf">condition_score</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cond_fn</span><span class="p">,</span> <span class="n">p_mean_var</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compute what the p_mean_variance output would have been, should the
    model</span><span class="sh">'</span><span class="s">s score function be conditioned by cond_fn.

    See condition_mean() for details on cond_fn.

    Unlike condition_mean(), this instead uses the conditioning strategy
    from Song et al (2020).
    </span><span class="sh">"""</span>
    <span class="n">alpha_bar</span> <span class="o">=</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">eps</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_predict_eps_from_xstart</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">p_mean_var</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">])</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar</span><span class="p">).</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="nf">cond_fn</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span>
    <span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">p_mean_var</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_predict_xstart_from_eps</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">],</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_posterior_mean_variance</span><span class="p">(</span>
        <span class="n">x_start</span><span class="o">=</span><span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">pred_xstart</span><span class="sh">"</span><span class="p">],</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<p>While the <code class="language-plaintext highlighter-rouge">condition_mean</code> is quite simple \(\hat{\mu} = \mu + \Sigma g\), the <code class="language-plaintext highlighter-rouge">condition_score</code> is more complicated. It is based on the DDIM model from <a href="https://openreview.net/forum?id=St1giarCHLP">Song et al. (2020)</a> which I have introduced in another <a href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial/#diffusion-inversion">blog post</a>. The following code is based on a nice property of the forward process such that:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t\]

<p>where \(x_0\) is the initial image, \(\epsilon_t \sim \mathcal{N}(0, I)\) is the noise at time \(t\). This property allows us to <code class="language-plaintext highlighter-rouge">predict</code> noisy version \(x_t\) of \(x_0\) at any arbitrary time \(t\). On the other hand, given \(\epsilon_t = \epsilon_\theta(x_t, t)\) is the predicted noise at time \(t\) by the denoising network \(\epsilon_\theta\) and \(x_t\), we can <code class="language-plaintext highlighter-rouge">predict</code> \(\tilde{x_0}\) as follows:</p>

\[\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\]

<p>Therefore, we can see the following functions in the code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_predict_eps_from_xstart</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">pred_xstart</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span>
        <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recip_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_t</span>
        <span class="o">-</span> <span class="n">pred_xstart</span>
    <span class="p">)</span> <span class="o">/</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recipm1_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">_predict_eps_from_xstart</code> function is equivalent to the following equation which allows us to <code class="language-plaintext highlighter-rouge">predict</code> the noise at time \(t\) from the predicted initial image \(\tilde{x}_0\) and the image \(x_t\):</p>

\[\hat{\epsilon}_t = \frac{ 1/\sqrt{\bar{\alpha}_t} x_t - \tilde{x}_0}{\sqrt{1/\bar{\alpha}_t - 1}} = \frac{x_t - \sqrt{\bar{\alpha}_t} \tilde{x}_0}{\sqrt{1 - \bar{\alpha}_t}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_predict_xstart_from_eps</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">eps</span><span class="p">.</span><span class="n">shape</span>
    <span class="nf">return </span><span class="p">(</span>
        <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recip_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_t</span>
        <span class="o">-</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recipm1_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">_predict_xstart_from_eps</code> function is equivalent to the following equation which allows us to <code class="language-plaintext highlighter-rouge">predict</code> the initial image \(\tilde{x}_0\) from the image \(x_t\) at time \(t\) and the noise \(\epsilon_t\):</p>

\[\tilde{x}_0 = \frac{x_t}{\sqrt{\bar{\alpha}_t}} - \sqrt{1/\bar{\alpha}_t - 1} \epsilon_t = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_t}{\sqrt{\bar{\alpha}_t}}\]

<p>With that, now we can understand how the <code class="language-plaintext highlighter-rouge">condition_score</code> reflect Algorithm 2.</p>

<h3 id="how-to-train-the-diffusion-model">How to train the diffusion model</h3>

<p>Training the diffusion model in this project is similar as in the DDPM or DDIM papers. Because even using auxiliary classifier, they are trained independently. The minimal code to train the diffusion model is as follows, which is based on the code from file <a href="https://github.com/openai/guided-diffusion/blob/main/scripts/image_train.py"><code class="language-plaintext highlighter-rouge">image_train.py</code></a> and <a href="https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/train_util.py#L22"><code class="language-plaintext highlighter-rouge">train_util.py</code></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># run loop
</span><span class="k">def</span> <span class="nf">run_loop</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">some_conditions</span><span class="p">:</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">cond</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># run one step
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">forward_backward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">cond</span><span class="p">)</span>
        <span class="n">took_step</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mp_trainer</span><span class="p">.</span><span class="nf">optimize</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">opt</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">took_step</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">_update_ema</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_anneal_lr</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">log_step</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">None</span>

<span class="c1"># forward and backward
</span><span class="k">def</span> <span class="nf">forward_backward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">cond</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">mp_trainer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">):</span>
        <span class="n">micro</span><span class="p">,</span> <span class="n">micro_cond</span> <span class="o">=</span> <span class="p">...</span> 

        <span class="c1"># sampling time step t and the weights from a schedule sampler (e.g, uniform))
</span>        <span class="n">t</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">schedule_sampler</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">micro</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dist_util</span><span class="p">.</span><span class="nf">dev</span><span class="p">())</span>

        <span class="n">compute_losses</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span>
                <span class="n">self</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">training_losses</span><span class="p">,</span>
                <span class="n">self</span><span class="p">.</span><span class="n">ddp_model</span><span class="p">,</span>
                <span class="n">micro</span><span class="p">,</span>
                <span class="n">t</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="o">=</span><span class="n">micro_cond</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="nf">compute_losses</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">weights</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">mp_trainer</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="bp">None</span>

<span class="c1"># where the diffusion.training_losses is defined as follows in the file gaussian_diffusion.py
</span>
<span class="k">def</span> <span class="nf">training_losses</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

    <span class="c1"># sample x_t from the unconditional forward process
</span>    <span class="n">x_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>

    <span class="c1"># consider the MSE loss only 
</span>    <span class="n">model_output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

    <span class="c1"># get target from the reverse process
</span>    <span class="n">target</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">ModelMeanType</span><span class="p">.</span><span class="n">PREVIOUS_X</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_posterior_mean_variance</span><span class="p">(</span>
                    <span class="n">x_start</span><span class="o">=</span><span class="n">x_start</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">ModelMeanType</span><span class="p">.</span><span class="n">START_X</span><span class="p">:</span> <span class="n">x_start</span><span class="p">,</span>
                <span class="n">ModelMeanType</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">:</span> <span class="n">noise</span><span class="p">,</span>
            <span class="p">}[</span><span class="n">self</span><span class="p">.</span><span class="n">model_mean_type</span><span class="p">]</span>
    

    <span class="n">terms</span><span class="p">[</span><span class="sh">"</span><span class="s">mse</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">mean_flat</span><span class="p">((</span><span class="n">target</span> <span class="o">-</span> <span class="n">model_output</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">terms</span>

</code></pre></div></div>

<h3 id="how-to-train-the-classifier">How to train the classifier</h3>

<p>In the following code snippet, we will go through the minimal code to train the classifier. The code is based on the file <code class="language-plaintext highlighter-rouge">classifier_train.py</code>. It is worth noting that the classifier can be trained on either training set or generated images from the diffusion model, controlled by the parameter <code class="language-plaintext highlighter-rouge">args.noised</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># create unet and scheduler of the diffusion model
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">diffusion</span> <span class="o">=</span> <span class="nf">create_model_and_diffusion</span><span class="p">()</span>

    <span class="c1"># init schedule sampler
</span>    <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">noised</span><span class="p">:</span>
        <span class="n">schedule_sampler</span> <span class="o">=</span> <span class="nf">create_named_schedule_sampler</span><span class="p">(</span>
            <span class="n">args</span><span class="p">.</span><span class="n">schedule_sampler</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">)</span>

    <span class="c1"># create optimizer
</span>    <span class="n">mp_trainer</span> <span class="o">=</span> <span class="nc">MixedPrecisionTrainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">use_fp16</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">classifier_use_fp16</span><span class="p">,</span> <span class="n">initial_lg_loss_scale</span><span class="o">=</span><span class="mf">16.0</span><span class="p">)</span>

    <span class="c1"># create unet model? repeat from previous step
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">...)</span>

    <span class="c1"># create data loader 
</span>    <span class="n">data</span> <span class="o">=</span> <span class="nf">load_data</span><span class="p">(...)</span>

    <span class="c1"># create optimizer
</span>    <span class="n">opt</span> <span class="o">=</span> <span class="nc">AdamW</span><span class="p">(</span><span class="n">mp_trainer</span><span class="p">.</span><span class="n">master_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">)</span>


</code></pre></div></div>

<h2 id="references">References</h2>

<p>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ICLR 2021.</p>

<p>Song, Jiaming, Chenlin Meng, and Stefano Ermon. “Denoising Diffusion Implicit Models.” ICLR. 2020.</p>]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="diffusion" /><summary type="html"><![CDATA[Try to understand classifier guidance and how to implement it]]></summary></entry><entry><title type="html">Comprehensive Algorithm Portfolio Evaluation using Item Response Theory</title><link href="https://tuananhbui89.github.io/blog/2023/fairness-irt/" rel="alternate" type="text/html" title="Comprehensive Algorithm Portfolio Evaluation using Item Response Theory" /><published>2023-09-01T00:00:00+10:00</published><updated>2023-09-01T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2023/fairness-irt</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/fairness-irt/"><![CDATA[<h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Link to the paper: <a href="https://arxiv.org/abs/2307.15850">https://arxiv.org/abs/2307.15850</a></li>
  <li>Authors: Sevvandi Kandanaarachchi, Kate Smith-Miles, CSIRO, Uni Melb</li>
  <li><a href="https://www.youtube.com/watch?v=gA-Ds1PEP_o&amp;ab_channel=OPTIMAARC">Talk at OPTIMA ARC</a> and <a href="https://www.slideshare.net/SevvandiKandanaarach/algorithm-evaluation-using-item-response-theorypptx?from_action=save">its slide</a></li>
</ul>

<h2 id="summary">Summary</h2>

<p>The paper proposed a framework to evaluate a portfolio of algorithms using Item Response Theory (IRT). Instead of using the standard IRT mapping, the authors proposed to invert the mapping by considering the datasets as agents and the algorithms as items. By using this mapping, the IRT model now can give more insights about the characteristics of algorithms including the algorithm anomalousness, consistency, and dataset difficulty. In addition, the framework also provides analysis of strengths and weaknesses of algorithms in the problem space which can be used to select the best algorithm for a given dataset.</p>

<h2 id="item-response-theory">Item Response Theory</h2>

<p>Item Response Theory (IRT) is a psychometric theory that models the relationship between the latent trait (such as verbal or mathematical ability, that cannot be directly measured) of a person and the probability of a person to answer a question correctly. Using the participant responses to the test items, an IRT model is fitted to estimate the discrimination and difficulty of test items and the ability of participants.</p>

<h3 id="family-of-irt-models">Family of IRT models</h3>

<h4 id="dichotomous-irt-model">Dichotomous IRT model</h4>

<p>The simplest IRT model is the dichotomous IRT model, which assumes that the probability of a person to answer a question correctly is a logistic function of the difference between the person’s ability and the item’s difficulty. The model is formulated as follows:</p>

\[P(X_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j) = \frac{1}{1 + e^{- \alpha_j (\theta_i - \beta_j)}}\]

<p>where \(X_{ij}\) is the response of person \(i\) to item \(j\), \(\theta_i\) is the ability of person \(i\), \(\beta_j\) is the difficulty of item \(j\), and \(\alpha_j\) is the discrimination parameter.</p>

<p>3PL: introducing one additional guesing parameter \(\gamma_j\) to the model:</p>

\[P(X_{ij} = 1 \mid \theta_i, \alpha_j, \beta_j, \gamma_j) = \gamma_j + (1 - \gamma_j) \frac{1}{1 + e^{- \alpha_j (\theta_i - \beta_j)}}\]

<p>Figure below shows the probability of a person to answer a question correctly as a function of the person’s ability \(\theta_i\) given the item’s parameters \(\alpha_j, \beta_j, \gamma_j\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/fairness-irt/3PL-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/fairness-irt/3PL-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/fairness-irt/3PL-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/fairness-irt/3PL.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    3PL
</div>

<h4 id="polytomous-irt-model">Polytomous IRT model</h4>

<p>The polytomous IRT model is an extension of the dichotomous IRT model that allows for more than two response categories (e.g., an answer is marked not just correct/incorrect but with score from 1 to K). The most common polytomous IRT model is the graded response model (GRM), in which the cummulative probabilities of a person to get a higher or equal to score \(k\) is formulated as follows:</p>

\[P(X_{ij} \geq k \mid \theta_i, \alpha_j, \beta_j^k) = \frac{1}{1 + e^{- \alpha_j (\theta_i - \beta_j^k)}}\]

<p>where \(X_{ij}\) is the response of person \(i\) to item \(j\), \(\theta_i\) is the ability of person \(i\), \(\beta_j^k\) is the difficulty of item \(j\) for response category \(k\), and \(\alpha_j\) is the discrimination parameter. Each item \(j\) has a set of difficulties \(\beta_j = \{ \beta_j^k \}_{k=1}^K\) which is making sense because the difficulty of an item is different for different response categories.</p>

<p>The probability of a person to get a score \(k\) is formulated as follows:</p>

\[P(X_{ij} = k \mid \theta_i, \alpha_j, \beta_j) = P(X_{ij} \geq k \mid \theta_i, \alpha_j, \beta_j^k) - P(X_{ij} \geq k+1 \mid \theta_i, \alpha_j, \beta_j^{k+1})\]

<p>Given parameters \(\alpha_j, \beta_j\) are known and fixed, the probability of a person to get a score \(k\) is a curve that is a function of the person’s ability \(\theta_i\). And given the person’s ability \(\theta_i\) and the item’s parameters \(\alpha_j, \beta_j\), we can estimate the most likely score \(k\) of the person which is the score that maximizes the probability \(P(X_{ij} = k \mid \theta_i, \alpha_j, \beta_j)\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/fairness-irt/poly-IRT-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/fairness-irt/poly-IRT-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/fairness-irt/poly-IRT-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/fairness-irt/poly-IRT.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Polytomous IRT
</div>

<h4 id="continuous-irt-model">Continuous IRT model</h4>

<p>The continuous IRT model is an extension of the dichotomous IRT model that allows for continuous responses (e.g., the response is a real number between 0 and 1). The density function of the continuous IRT model is formulated as follows:</p>

\[f(z_{ij} \mid \theta_i) = \frac{\alpha_j \gamma_j}{2 \pi} \exp(- \frac{\alpha_j^2}{2}(\theta_i - \beta_j - \gamma_j z_j))\]

<p>where \(\theta_i\) is the ability of person \(i\), \(\beta_j\) is the difficulty of item \(j\), \(\alpha_j\) is the discrimination parameter, and \(\gamma_j\) is the scaling factor. \(z_{ij}\) is the normalized response of person \(i\) to item \(j\) which is formulated as follows:</p>

\[z_{j} = \ln \frac{x_{j}}{ k_j -  x_j}\]

<p>where \(x_{j}\) is a continuous score between 0 and \(k_j\) and \(k_j\) is the maximum score of item \(j\). \(z_j\) has a range between \(-\infty\) and \(\infty\).</p>

<p>In this model, these are total 3 parameters for each item \(j\) (i.e., \(\beta_j, \alpha_j, \gamma_j\)$) and 1 parameter for each person \(i\) (i.e., \(\theta_i\)$). Unlike the polytomous IRT model, the difficulty \(\beta_j\) of an item \(j\) is the same for all response categories.</p>

<h2 id="mapping-algorithm-evaluation-to-irt">Mapping algorithm evaluation to IRT</h2>

<!-- Given a group of students (i.e., algorithms) and a set of items/questions (i.e., datasets), the goal of IRT is to estimate the ability of each student and the difficulty of each question. The ability of a student is the probability of the student to answer a question correctly. -->

<p>To understand the mapping better, let’s consider the following components of IRT:</p>

<ul>
  <li>The agents: a group of students or algorithms in which each agent is associated with a set of characteristics (e.g., ability of a student, parameters of an algorithm)</li>
  <li>The items: a set of questions or datasets in which each item is associated with a set of characteristics (e.g., difficulty, discrimination, bias)</li>
  <li>The responses: the responses of agents to items (e.g., the responses of students to questions, the performance of algorithms on datasets)</li>
</ul>

<p>IRT models the relationship between the characteristics of agents and items and the responses of agents to items. The goal of IRT is to estimate the characteristics of agents and items given the responses of agents to items, with the primary goal of estimating the characteristics of items (e.g., the difficulty of questions which is broader interest than the ability of each individual student).</p>

<p>It can be seen that, in the dichotomous IRT model, there are two parameters of an item (i.e., difficulty and discrimination) and one parameter of an agent (i.e., ability). In the polytomous IRT model, for each item, there are \(K\) parameters of difficulty (i.e., \(\{\beta_j^k\}_{k=1}^K\)$) and one parameter \(\alpha_j\) for discrimination, while there is only one parameter of an agent (i.e., ability \(\theta_i\)$).</p>

<p><strong>Mapping algorithm evaluation to IRT</strong>:</p>

<p>In the context of algorithm evaluation, the agents are algorithms and the items are datasets. The responses are the performance of algorithms on datasets. Let \(f_{\theta_i}\) is an agent (i.e., an algorithm) parameterized by \(\theta_i\). \(x_j\) is an item (i.e., a dataset) belonging to set of items \(X\), each dataset \(x_j\) is associated with a set of characteristics \(c_j\) (e.g., difficulty, discrimination, bias).</p>

<p>Within the context, the IRT model now estimates the probability of an algorithm \(f_{\theta_i}\) to solve a dataset \(x_j\) given the characteristics \(c_j\) of the dataset and the ability \(\theta_i\) of the algorithm.</p>

<p>However, as the authors mentioned in the paper, with this standard mapping, the IRT model is focusing on evaluating the characteristics of datasets (i.e., items) rather than the characteristics of algorithms (i.e., agents). Therefore, the authors proposed to invert the mapping by considering the datasets as agents and the algorithms as items.</p>

<p>By using the inverted mapping, the IRT model now can give more insights about the characteristics of algorithms rather than the characteristics of datasets, thanks to the fact that there are more parameters to be estimated for each algorithm (i.e., 3 parameters for each algorithm in the continuous IRT model) than for each dataset (i.e., 1 parameter for each dataset in the continuous IRT model).</p>

<p>More specifically, if we consider the continuous IRT model and the inverted mapping, the following are the parameters of the model:</p>

<ul>
  <li>\(\beta_j\) is the difficulty of item \(j\), in this case, the (reversed) difficulty limit of algorithm \(j\).</li>
  <li>\(\alpha_j\) is the discrimination parameter of item \(j\), in this case, the (reversed) algorithm anomalousness and consistency.</li>
  <li>\(\gamma_j\) is the scaling factor of item \(j\), in this case, the algorithm bias (I am not sure about this because it was not mentioned in the paper).</li>
  <li>\(\theta_i\) is the ability of agent \(i\), in this case, the (reversed) dataset difficulty.</li>
</ul>

<h2 id="characteristics-of-algorithms-estimated-by-irt-model">Characteristics of algorithms estimated by IRT model</h2>

<ul>
  <li>
    <p><strong>Dataset difficulty</strong>: It is estimated by \(\delta_i = -\theta_i\) which is the ability of agent \(i\), in this case the dataset difficulty. Given a fixed algorithm’s characteristics, the probability of a dataset to be solved by the algorithm will increase as \(\theta_i\) increases. Therefore, an dataset \(i\) is considered to be easy if \(\theta_i\) is large or vice versa.</p>
  </li>
  <li>
    <p><strong>Algorithm anomalousness</strong>: It is estimated by \(sign(\alpha_j)\) (i.e., TRUE if \(\alpha_j &lt; 0\) and FALSE if \(\alpha_j &gt; 0\)$) show whether the algorithm is anomalous or not. It is because in the logistic function, if \(\alpha_j &lt; 0\), then the probability of the agent (i.e., a dataset) act on the item (i.e., an algorithm) is decreasing as the ability of the agent increases (i.e., \(\theta_i\) or easiness of a dataset). In other words, the algorithm is more likely to fail on a easy dataset than on a hard dataset which is an anomalous behavior.</p>
  </li>
  <li>
    <p><strong>Algorithm consistency</strong>: It is estimated by \(1/\|\alpha_j\|\) (i.e., inverse of the absolute value of \(\alpha_j\)$), which shows the consistency of the algorithm. It is because in the logistic function, the \(\alpha_j\) is the slope of the curve, therefore, the larger the \(\alpha_j\), the steeper the curve, which means that the algorithm is changing its behavior more rapidly as the ability of the agent changes (i.e., \(\theta_i\) or easiness of a dataset). In other words, large \(\alpha_j\) or small \(1/\|\alpha_j\|\) means that the algorithm is less consistent/stable/robust against the change of the difficulty of a dataset.</p>
  </li>
  <li>
    <p><strong>Difficulty limit of algorithm</strong>: It is estimated by \(-\beta_j\) which is the difficulty of item \(j\). In this case, the difficulty limit of algorithm \(j\). It is because in the logistic function, the \(\beta_j\) is the point at which the probability of the agent (i.e., a dataset) act on the item (i.e., an algorithm) is 0.5. In other words, the difficulty limit of algorithm \(j\) is the difficulty of a dataset \(i\) at which the algorithm \(j\) has 50% chance to solve the dataset \(i\). The higher difficulty limit of algorithm \(j\), the more difficult dataset that the algorithm \(j\) can solve.</p>
  </li>
  <li>
    <p><strong>Algorithm bias</strong>: (This was not mentioned in the paper but just my analysis) It is estimated by \(\gamma_j\) which is the scaling factor of item \(j\). In this case, the algorithm bias. It can be seen that in the continuous IRT model, the \(\gamma_j\) has to be same sign as \(\alpha_j\) (i.e., \(\alpha_j \gamma_j &gt; 0\)$). Just consider the case when \(\gamma_j &gt; 0\), then the higher the \(\gamma_j\), the higher the probability that the dataset is solved by the algorithm. More interestingly, the \(\gamma_j\) is the scaling factor of the response \(z_j = \ln \frac{x_j}{k_j - x_j}\) which will be very large if \(x_j\) is close to \(k_j\) (i.e., the maximum score of item \(j\)$). Therefore, the density function \(f(z_{ij} \mid \theta_i)\) will be very large if \(x_j\) is close to \(k_j\) which means that the probability of the dataset to be solved by the algorithm is very high if \(x_j\) is close to \(k_j\) which is makes sense because the dataset is easy. In contrast, the density function \(f(z_{ij} \mid \theta_i)\) will be very small if \(x_j\) is close to 0. Therefore, the continuous IRT model is strongly biased towards the extreme values of the response \(x_j\) (i.e., \(x_j = 0\) or \(x_j = k_j\)$) which is not good. However, if the scaling factor \(\gamma_j\) is small, it reduces this bias issue, vice versa. Therefore, the \(\gamma_j\) can be used to measure the bias of the algorithm.</p>
  </li>
  <li>
    <p><strong>Performance-Dataset Difficulty Curve</strong>: after getting all the above parameteres of \(n\) algorithms on \(N\) datasets/problems, we can estimate the performance-dataset difficulty curve \(h_j(\delta)\) of each algorithm \(j\) by using function estimation method (i.e., smoothing spline as proposed in the paper).</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/fairness-irt/smoothing-spline-2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/fairness-irt/smoothing-spline-2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/fairness-irt/smoothing-spline-2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/fairness-irt/smoothing-spline-2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Smoothing spline
</div>

<ul>
  <li><strong>Strengths and weaknesses of algorithm</strong>: based on the performance-dataset difficulty curve \(h_j(\delta)\), we can identify the strengths and weaknesses of each algorithm \(j\) by comparing between curves/algorithms. For example, if the curve of algorithm \(j\) is above the curve of algorithm \(k\) for all \(\delta\), then algorithm \(j\) is better than algorithm \(k\) for all \(\delta\). Given a dataset difficulty \(\delta\), we can find the best algorithm which has the highest value of \(h_j(\delta)\). We also can define a region where an algorithm can be considered as algorithm’s strengths or weaknesses.</li>
</ul>

<h2 id="framework">Framework</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/fairness-irt/algorithm-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/fairness-irt/algorithm-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/fairness-irt/algorithm-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/fairness-irt/algorithm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    AIRT framework
</div>

<p>The AIRT framework can be found in page 28 of the paper, which consists of 3 main stages:</p>

<ul>
  <li><strong>Stage 1 - Fitting the IRT model with inverted mapping</strong> Given an input matrix \(Y_{N \times n}\) containing accuracy measures of \(n\) algorithms for \(N\) datasets, the IRT model is fitted to estimate the parameters of the model (i.e., \(\beta_j, \alpha_j, \gamma_j, \theta_i\)). The authors proposed to use the continuous IRT model with the inverted mapping. The parameters of the model are estimated using the Expectation-Maximization (EM) algorithm.</li>
  <li><strong>Stage 2 - Calculation of algorithm and dataset metrics</strong> For each algorithm \(j\) compute the anomalous indicator, algorithm consistency score and difficulty limit. For each dataset \(i\) compute the dataset difficulty \(\delta_i = - \theta_i\).</li>
  <li><strong>Stage 3 - Computing strengths and weaknesses and construct airt portfolio</strong></li>
</ul>

<h2 id="questions">Questions</h2>

<ul>
  <li>In IRT model, an agent (e.g., an algorithm) and an item (e.g., a dataset) are assumed to be independent. However, in the context of machine learning where an algorithm is trained on a training set and tested on a test set - which is the item in IRT model, the assumption of independence is not true.
For example, a good algorithm but was trained on a biased training set will perform worse on a test set than a bad algorithm but was trained on an unbiased training set. Therefore, the performance of an algorithm on a test set is not only determined by the algorithm itself but also the training set it was trained on. So how to deal with this issue in IRT model?</li>
  <li>Extend the above question, how to deal with the case when many algorithms were trained on the same training set?</li>
</ul>

<h2 id="future-work-irt-based-disentanglement-learning">Future work: IRT-based Disentanglement Learning</h2>

<h3 id="introduction">Introduction</h3>

<p>This project aims to disentangle ML-bias into algorithmic and data bias focussing on intersectional subgroups.</p>

<p>So what are algorithmic bias and data bias in the context of machine learning?</p>

<ul>
  <li><strong>Data bias</strong> is the bias in the training data that is used to train the ML model. It occurs when the data used for training is not representative of the real-world population or when it contains inherent biases. For example, a dataset of images of people that is used to train a facial recognition system may contain more images of white people than people of color. This can lead to the facial recognition system being less accurate when identifying people of color.</li>
  <li><strong>Algorithmic bias</strong> is the bias in the algorithm itself. It occurs when the algorithm is not designed to be fair or when it is not trained to be fair. For example, a facial recognition system that is trained with purpose to identify people in a specific demographic group (e.g., white people) will be less accurate when identifying people in other demographic groups (e.g., people of color).</li>
</ul>

<p>Recognizing data bias is a challenging task because the data bias is not always obvious and the dataset is usually large and complex.
Equally complex is the task of recognizing algorithmic bias because the training process of a ML model is also complex where the bias can be introduced at any stage such as data collection, feature selection, or the choice of objective function.</p>

<p>To adapt the IRT model to the context of machine learning, we need to consider the following:</p>

<ul>
  <li><strong>The algorithm</strong> is the pretrained model which is trained on a training set (which can be biased or unbiased but we don’t know)</li>
  <li><strong>The dataset</strong> is to mention the test set which is used to evaluate the algorithm. The dataset can be sampled from the same distribution as the training set or from a different distribution.</li>
  <li>The algorithms are assumed to be independent even are trained on the same training set. The algorithms also have the same task on the test set (e.g., the pretrained ResNet50, VGG19 models to predict an image into 1 of 10 classes).</li>
  <li>The dataset are assumed to be disjoint. The datasets also are served for the same task (e.g., to test performance of classification models)</li>
  <li><strong>The data bias</strong> problem in this context is the bias of the test item which is used to evaluated the algorithm (not the bias of the training set as in ML literature). For example, let’s consider education system where a test item is a set of questions and an algorithm is a student. We want to evaluate diverse knowledge of students on several subjects, i.e., math, physics, chemistry, literature, etc. The data bias problem in this context is that the test item is only able to evaluate on a specific subject (e.g., math) but not on all subjects. <strong>The algorithmic bias</strong> problem in this specific example is that the student is only good at math but not good at other subjects.</li>
</ul>

<p>Problem setting: Given \(n\) algorithms and \(N\) datasets, the goal is to identify which algorithm/dataset has bias problem. On the other words, how to distinguish a good algorithm performing poorly on a biased dataset from an equally performed bad algorithm?</p>

<p><strong>Note:</strong></p>

<ul>
  <li>The term “disentangle learning” is commonly referred to the approach that learns disentangled representations of data in machine learning. However, in this project, the term “disentangle learning” is used to refer to the approach that disentangle ML-bias into algorithmic and data bias.</li>
</ul>

<h3 id="proposed-framework">Proposed framework</h3>

<!-- Let $$F_b, F_u$$ be the set of biased and unbiased algorithms, and $$D_b, D_u$$ be the set of biased and unbiased datasets, respectively. $$\|F_b\| + \|F_u\| = n$$ and $$\|D_b\| + \|D_u\| = N$$. -->
<p>We need to make some assumptions as follows:</p>

<ul>
  <li>an biased algorithm might perform poorly on an unbiased dataset.</li>
  <li>an unbiased algorithm might perform poorly on a biased dataset.</li>
  <li>an biased algorithm might not necessary perform well on an biased dataset because it might be trained on different biased training set.</li>
  <li>in this project, a poor generalization algorithm which performs poorly on both biased and unbiased datasets might be different from a biased algorithm which might perform well on biased datasets but poorly on unbiased datasets.</li>
</ul>

<p>We consider two IRT models simultaneously, the standard IRT model and the IRT model with inverted mapping. The standard IRT model is used to estimate the characteristics of datasets (i.e., difficulty, discrimination) while the IRT model with inverted mapping is used to estimate the characteristics of algorithms (i.e., difficulty limit, anomalousness, consistency).</p>

<p>For the IRT model with inverted mapping, we consider the performance-dataset difficulty curve \(h^d_{a_j}(\delta_d)\) of each algorithm \(a_j\), while for the standard IRT model, we consider the performance-algorithm difficulty curve \(h^a_{d_i}(\delta_a)\) of each dataset \(d_i\). With the two curves, we can identify a biased dataset as follows:</p>

<ul>
  <li>A dataset \(d_i\) is considered to be biased if it is solved well by a biased algorithm \(a_j \in F_b\) but poorly by an unbiased algorithm \(a_j \in F_u\).</li>
</ul>

<p>\(h^d_{a_j} (\delta_d) \leq \epsilon_{low} \; \forall \; a_j \in F_u\)
\(h^d_{a_j} (\delta_d) \geq \epsilon_{up} \; \forall \; a_j  \in F_b\)</p>

<ul>
  <li>An algorithm \(a_j\) is considered to be biased if it performs well on a biased dataset \(d_i \in D_b\) but poorly on an unbiased dataset \(d_i \in D_u\).</li>
</ul>

<p>\(h^a_{d_i} (\delta_a) \leq \epsilon_{low} \; \forall \; d_i \in D_u\)
\(h^a_{d_i} (\delta_a) \geq \epsilon_{up} \; \forall \; d_i \in D_b\)</p>

<p>where \(\epsilon_{low}\) and \(\epsilon_{up}\) are the lower and upper thresholds, respectively.</p>

<p>We can also a 3D map where the z-axis is the performance of an algorithm on a dataset, the x-axis is the difficulty of the dataset, and the y-axis is the difficulty limit of the algorithm.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/fairness-irt/couple-IRT-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/fairness-irt/couple-IRT-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/fairness-irt/couple-IRT-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/fairness-irt/couple-IRT.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    3D map
</div>

<!-- Central question is how to select an unbiased and sufficiently diverse set of datasets to evaluate an algorithm portfolio. -->

<h2 id="references">References</h2>

<p>Fernando Martínez-Plumed, Ricardo BC Prudêncio, Adolfo Martínez-Usó, and José HernándezOrallo. Item Response Theory in AI: Analysing machine learning classifiers at the instance level. Artificial Intelligence, 271:18–42, 2019.</p>

<p>Yu Chen, Ricardo BC Prudêncio, Tom Diethe, Peter Flach, et al. β3-IRT: A New Item Response Model and its Applications. arXiv preprint, arXiv:1903.04016, 2019.</p>]]></content><author><name></name></author><category term="reading" /><category term="tml" /><summary type="html"><![CDATA[How to evaluate how good an algorithm is?]]></summary></entry><entry><title type="html">Fairness in Machine Learning</title><link href="https://tuananhbui89.github.io/blog/2023/fairness-101/" rel="alternate" type="text/html" title="Fairness in Machine Learning" /><published>2023-09-01T00:00:00+10:00</published><updated>2023-09-01T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2023/fairness-101</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/fairness-101/"><![CDATA[<p>(Work in progress)</p>

<h2 id="varieties-of-fairness">Varieties of Fairness</h2>

<p>One of the hardest problems in fairness is that there is no consensus on the definition of fairness or what does it mean to be fair. Depending on the context or culture, the definition of fairness can be different <d-cite key="du2020fairness"></d-cite>.</p>

<p>Researchers and designers at Google’s PAIR (People and AI Research) , <d-cite key="googlefairness"></d-cite> initiative created the What-If visualization tool as a pragmatic resource for developers of machine learning systems. The tool provides a set of fairness metrics that can be used to evaluate the fairness of a model. The metrics are grouped into five categories:</p>

<ul>
  <li>Group unaware: “group unaware” fairness is an approach that advocates for fairness by disregarding demographic characteristics like gender and making  decisions solely based on individual qualifications.</li>
  <li>Group threshold: “group threshold” is a fairness mechanism that recognizes that not all groups are the same, and historical disparities or biases may warrant different decision thresholds for different groups to promote equitable outcomes. It’s a technique used to fine-tune the behavior of AI models to ensure that they do not disproportionately disadvantage certain demographic groups while still maintaining some level of predictive accuracy.</li>
  <li>Demographic parity (or group fairness, statistical parity): is an approach to ensure that the composition of the selected or approved individuals or outcomes reflects the demographic composition of the overall population.</li>
  <li>Equal opportunity: aims to promote fairness by ensuring that individuals from different demographic groups are treated equally when they have the same qualifications or attributes relevant to a decision, and their chances of success are not influenced by factors like race, gender, or age.</li>
  <li>Equal accuracy: ensuring that the predictive accuracy of a model is similar across different demographic groups.</li>
</ul>

<p>It can be seen that, these proposed metrics are already complex and hard to understand. For example, in my opinion, the “group unware” and “equal opportunity” are quite similar to each other where both of them aim to ensure that the model does not discriminate based on “protected characteristics” like gender, age, race, etc. Overall, these metrics can be grouped into two categories: group fairness and individual fairness which are also the two main categories of fairness in machine learning.</p>

<h2 id="learning-fair-representations">Learning Fair Representations</h2>

<p>One of the milestone work in fairness is the paper “Learning Fair Representations” by Zemel et al. (2013) <d-cite key="zemel2013learning"></d-cite>. The authors proposed a method to learn fair representations by learning a latent representation that encodes the data well but obfuscates information about protected attributes. The method is based on the intuition that if the learned representation does not contain any information about the protected attribute, then any classifier based on these representation cannot use the protected attribute to make predictions.</p>

<p>The authors formulated this using the notion of statistical parity, which requires that the probability that a random element from \(X^+\) maps to a particular prototype is equal to the probability that a random element from
\(X^-\) maps to the same prototype</p>

\[P(Z = k \mid x^+ \in X^+) = P(Z = k \mid x^- \in X^-) \; \forall k\]

<p>Where \(X^+\) and \(X^-\) are the sets of protected and unprotected examples, respectively, and \(Z\) is the latent representation with \(K\) prototypes.</p>

<!-- The authors proposed to use an adversarial network to learn the latent representation. The adversarial network is trained to predict the protected attribute from the latent representation while the main network is trained to predict the label from the latent representation. The authors also proposed to use a regularization term to ensure that the latent representation is close to the original representation. The authors evaluated their method on the Adult dataset and showed that their method can achieve a better fairness score than the baseline method. -->

<h2 id="fairness-in-deep-learning">Fairness in Deep Learning?</h2>

<h2 id="fairness-in-generative-models">Fairness in Generative Models</h2>

<p>Fairness Machine Learning is mostly considered in the context of decision making models such as classification models. However, fairness is also an important issue in generative models, which is not well studied yet. Recently, the central problem of fairness in generative models is how to ensure diversity in the generated outputs. For example, a response to a question about famous musicians should not only include names or images of people of the same gender identity or skin tone <d-cite key="googlefairness"></d-cite>, <d-cite key="playingfairness"></d-cite>.</p>

<p>Some of the following attributes will be highly considered when talking about fairness in generative models:</p>

<ul>
  <li>Gender identity</li>
  <li>Cultural background and demographic</li>
  <li>Physical appearance attributes</li>
  <li>Political related attributes</li>
</ul>

<p>When evaluating the fairness of generative models, the authors of <d-cite key="googlefairness"></d-cite> suggest to consider the following metrics:</p>

<ul>
  <li>Diversity of the output: Given a set of prompts, the diversity along dimensions of identity attributes represented in the generated outputs. For example, given a set of prompts asking about “famous musicians”, the diversity of gender/culture/nationality  in the outputs will be measured. However, when asking about “famous men musicians”, the diversity of culture/nationality will be considered because the gender has been specified in the prompts.</li>
  <li>Ability on maintaining fairness: Given a set of prompts that contain counterfactuals of a sensitive attribute, ability to provide the same quality of service. For example, an user revealed his personal demographic information to the system (e.g., an Asian guy), then when the user asks about “famous musicians”, the system should not only provide names of Asian musicians. A fair system should provide answers as the same quality as the case when the user does not reveal his personal demographic information or when the user is a white guy.</li>
</ul>

<p>In summary, we can think about two scenarios when evaluating the fairness of generative models: same input - diverse output and diverse input - same output.</p>]]></content><author><name>Tuan-Anh Bui</name></author><category term="tml" /><category term="tutorial" /><summary type="html"><![CDATA[Just some notes]]></summary></entry><entry><title type="html">Anti-Personalization in Generative Models</title><link href="https://tuananhbui89.github.io/blog/2023/anti-personalization/" rel="alternate" type="text/html" title="Anti-Personalization in Generative Models" /><published>2023-08-23T00:00:00+10:00</published><updated>2023-08-23T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2023/anti-personalization</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2023/anti-personalization/"><![CDATA[<h2 id="anti-dreambooth">Anti-Dreambooth</h2>

<p>Link to other post: <a href="https://tuananhbui89.github.io/blog/2023/anti-dreambooth/">https://tuananhbui89.github.io/blog/2023/anti-dreambooth/</a></p>

<h2 id="generative-watermarking-against-unauthorized-subject-driven-image-synthesis">Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis</h2>

<h3 id="about-the-paper">About the paper</h3>

<ul>
  <li>Paper link: <a href="https://arxiv.org/abs/2306.07754">https://arxiv.org/abs/2306.07754</a></li>
</ul>

<h3 id="summary">Summary</h3>

<ul>
  <li>Problem setting: protection without sacrificing the utility of protected images for general (good) synthesis tasks. Unlike Anti-Dreambooth, where the goal is to completely prevent personalization, here the goal is to prevent unauthorized personalization with specific tasks (subject-driven synthesis).</li>
  <li>Real-world scenarios of misusing personalization generative models:
    <ul>
      <li><strong>Copyright of Hollie Mengert</strong>: a Reddit user published a DreamBooth model that is fine-tuned based on the artwork from an American artist Hollie Mengert. <a href="https://www.reddit.com/r/StableDiffusion/comments/yaquby/2d_illustration_styles_are_scarce_on_stable/">link</a></li>
      <li><strong>Elon Musk is dating GM CEO Mary Barra</strong>: <a href="https://twitter.com/blovereviews/status/1639988583863042050">link</a></li>
    </ul>
  </li>
  <li>Threat Model:
    <ul>
      <li>What is a threat model in TML? Threat model is a description of the capabilities and goals of an adversary. It also can describe the environment in which the machine learning model and its adversary operate.</li>
      <li>In this project, there are two parties involved: the subject owners and the subject synthesizers (benign or adversaries). The system includes a watermarking generator and a detector. There is also a public generative model (i.e., Stable Diffusion) that is used by the subject synthesizers.</li>
      <li>The subject owners are the ones who want to protect their images from unauthorized personalization. In this project, the subject owners use the generative watermarking to generate watermarked images. Then they can track the potential unauthorized use by detecting if the watermark appears in synthesized images (then the watermark to be added and the watermark to be detected are different?). The subject owners have full access to the generator and the detector and can also further improve them by fine-tuning.</li>
      <li>The subject synthesizers (benign or adversaries) are the ones who want to use the generative models to synthesize the target subject. The benign synthesizers are the ones who obtains authorization under the consent of the subject owners. The adversaries are the ones who want to synthesize the target subject without authorization. In this project, both benign and adversarial synthesizers have access to a public generative model (i.e., Stable Diffusion) and the protected/watermarked images.</li>
    </ul>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/AML/2306_07754_01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/AML/2306_07754_01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/AML/2306_07754_01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/AML/2306_07754_01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The Framework
</div>

<p>Approach:</p>

<p>Phase 1: Pre-training the watermarking generator and detector. Similar as GAN, there is an adversarial game between generator \(G\) and detector \(D\). The watermarked images denote as \(x_w=x+w\). The goal of the generator is to generate watermarked images that are indistinguishable from the real images. Its objective loss:
\(L_G = \text{max}(LPIPS(x,x_w) - p, 0)\)
where <em>LPIPS</em> is the perceptual similarity metric [1], \(p\) is a hyperparameter that controls the invisibility level, a smaller \(p\) means a more invisible watermark.</p>

<p>The detector \(D\) is trained to distinguish the watermarked images from the real images. Its objective loss:
\(L_D = -(1-y)\log(1-\hat{y}) - y \log(\hat{y})\)
where \(y\) is the ground truth label, \(\hat{y}\) is the predicted label.</p>

<p>Phase 2: Fine-tuning the detector with synthesized images. Using two set of images, \(X\) and \(X_w\), where \(X_w\) is the watermarked images of \(X\) to train corresponding personalized models \(M\) and \(M_w\) (i.e., with Textual Inversion or Dreambooth). Then using these models to generate synthesized images \(S\) and \(S_w\) with list of prompts. And use these images to fine-tune the detector \(D\), where \(S\) has label as real and \(S_w\) has label as watermarked.</p>

<p>[1] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 586–595. IEEE, 2018.</p>]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><category term="tml" /><summary type="html"><![CDATA[Anti-Dreambooth]]></summary></entry></feed>