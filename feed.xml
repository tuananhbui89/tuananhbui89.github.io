<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tuananhbui89.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tuananhbui89.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-10-02T22:08:51+10:00</updated><id>https://tuananhbui89.github.io/feed.xml</id><title type="html">blank</title><subtitle>Researcher in Generative AI and Trustworthy AI
</subtitle><entry><title type="html">GPT-5 Series - Safe Completion Training</title><link href="https://tuananhbui89.github.io/blog/2025/safe-completion-training/" rel="alternate" type="text/html" title="GPT-5 Series - Safe Completion Training" /><published>2025-08-08T00:00:00+10:00</published><updated>2025-08-08T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2025/safe-completion-training</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/safe-completion-training/"><![CDATA[<p>OpenAI just recently <a href="https://openai.com/gpt-5/">released their newest and most powerful model GPT-5</a>. In the post today, I want to talk about one of the most important aspects of LLMs: <strong>How to make them safe against malicious use</strong>.
In this version, OpenAI introduces a new paradigm called <strong>Safe Completion Training</strong> (which is built on top of  Deliberative Alignment [4])</p>

<iframe width="600" height="338" src="https://www.youtube.com/embed/0Uu_VJeVVfo" title="Introducing GPT-5" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>A significant paradigm shift in term of safety training has been proposed, moving away from the traditional “Refusal Training” towards a more nuanced approach known as “Safe-Completion Training”. This evolution directly addresses a long-standing headache for model developers: the delicate and often conflicting balance between helpfulness and safety.</p>

<h2 id="the-core-dilemma-helpfulness-vs-safety">The Core Dilemma: Helpfulness vs Safety</h2>

<p>The central challenge in aligning LLMs is managing the inherent trade-off between being a useful tool and preventing misuse.</p>

<ul>
  <li><strong>Prioritizing Helpfulness</strong>: If a model is optimized solely to be helpful, it can inadvertently become a tool for malicious actors. For example, a model that can explain how to combat a computer virus could, with the same knowledge, provide instructions on how to create one.</li>
  <li><strong>Prioritizing Safety</strong>: Conversely, if a model is made overly cautious, its utility plummets. This phenomenon, known as “over-refusal,” occurs when models reject perfectly benign requests because they contain keywords that trigger safety filters (e.g., refusing a programming query about how to “kill” a process). This not only frustrates users but also creates a competitive disadvantage, as less restrictive models may seem more capable.</li>
</ul>

<h2 id="refusal-training-and-its-limitations">Refusal Training and Its Limitations</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of the Refusal Training from <a href="https://openai.com/index/gpt-5-safe-completions/">OpenAI</a>.
</div>

<p>The standard method for tackling this has been Refusal Training. This involves teaching a model to recognize and reject harmful prompts, typically through methods like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). The model learns to classify user input as either “safe” (comply) or “unsafe” (refuse).</p>

<p>However, this paradigm has proven to be fundamentally brittle and easily bypassed. Its weaknesses are not just about being “jailbroken,” but are multifaceted:</p>

<ul>
  <li>
    <p><strong>Jailbreak</strong>: It has been shown that Refusal Training is not robust to jailbreak attacks, for example, by converting a harmful query into past-tense [2] or translating it into a different language [3] or requiring output format like JSON, code or ASCII art.</p>
  </li>
  <li>
    <p><strong>Semantic Brittleness</strong>: The models often don’t learn the abstract concept of harm but instead overfit to superficial patterns in the training data. A striking example is the “past-tense attack,” where models that refuse a prompt like “How do I make a Molotov cocktail?” will readily answer “How did people make a Molotov cocktail?”, treating it as a harmless historical query. This simple linguistic shift can cause jailbreak success rates on some models to jump from 1% to 88% [2].</p>
  </li>
  <li>
    <p><strong>Structural Flaws</strong>: Safety training often creates a refusal position bias, where models learn to issue a refusal only at the very beginning of a response. This is a critical flaw because the model is forced to make a refuse-or-comply decision based only on the initial prompt, which may lack context. If an attacker bypasses this initial check, the model has no mechanism to self-correct and refuse later in the generation process. A recent work [5] shows that the fixed structure of the refusal training (as always start with “I’m sorry, I can’t help with that”, etc.) leads to short-cut learning problem and can be easily bypassed by querying the model multiple times and averaging the responses to get the unlearned output.</p>
  </li>
  <li>
    <p><strong>Superficial Alignment</strong>: The alignment often acts as a shallow veneer. Models learn to mimic safety patterns rather than internalizing the principles. This is why attacks like “prefilling,” where a response is forced to start with “Sure, here is the answer,” are so effective. The model continues the harmful request because refusing would contradict the conversational context it has already started, revealing a conflict between its safety training and its core pre-training objective of predicting the next word.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of jailbreaking a unlearned LLM by querying it multiple times and averaging the responses to get the unlearned output from [5].
</div>

<h2 id="the-new-paradigm-safe-completion-training">The New Paradigm: Safe-Completion Training</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of the Safe-Completion Training from <a href="https://openai.com/index/gpt-5-safe-completions/">OpenAI</a>.
</div>

<p>In response to these deep-seated issues, Safe-Completion Training redefines the objective [1]. Instead of asking “Is this prompt safe?”, it asks, “What is the most helpful response I can generate that remains fully compliant with the safety policy?”.</p>

<p>The core innovation is shifting the safety evaluation from the user’s input to the model’s own output. This is especially powerful for handling <strong>“dual-use”</strong> queries—prompts, where a benign user request can be completed at a high level,
but might be dangerous if completed in a full detail, as example below:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Example of dual-user queries from <a href="https://openai.com/index/gpt-5-safe-completions/">OpenAI</a>.
</div>

<p>With Safe-Completion, a model can provide a helpful, high-level answer while omitting dangerous, operational details. For instance, it can explain the principles of virology without providing a step-by-step guide to creating a bioweapon.</p>

<p>This is achieved through a two-stage process:</p>

<ul>
  <li>
    <p><strong>Nuanced Fine-Tuning (SFT)</strong>: The model is trained to choose between three response types: a direct answer for harmless queries, a refusal with helpful redirection for malicious queries, and a safe completion for dual-use or borderline cases.</p>
  </li>
  <li>
    <p><strong>Constrained Reinforcement Learning (RL)</strong>: The model is optimized using a multiplicative reward function: <code class="language-plaintext highlighter-rouge">Reward = Helpfulness Score * Safety Score</code>. The multiplication is key; if a response is unsafe (<code class="language-plaintext highlighter-rouge">Safety Score = 0</code>), the total reward is zero, no matter how helpful it might seem. This transforms the problem from a trade-off into a constrained optimization: the model is incentivized to be maximally helpful only on the condition that it remains perfectly safe.</p>
  </li>
</ul>

<h2 id="supervised-fine-tuning-sft-in-safe-completion-training">Supervised Fine-Tuning (SFT) in Safe-Completion Training</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Overall structure of the safe-completion training stack from [1].
</div>

<p>The Supervised Fine-Tuning (SFT) stage is the first phase of Safe-Completion Training, designed to teach the model the initial, correct behaviors before they are refined by reinforcement learning. It moves beyond a simple comply/refuse decision and trains the model to adopt a more nuanced set of responses.</p>

<p>Firstly, we need to understand the data used for SFT including: (<code class="language-plaintext highlighter-rouge">prompt</code>, <code class="language-plaintext highlighter-rouge">CoT</code>, <code class="language-plaintext highlighter-rouge">answer</code>)</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt</code>: Safety-related input prompt.</li>
  <li><code class="language-plaintext highlighter-rouge">spec</code>: Content policy specification that defines the safety policy.</li>
  <li><code class="language-plaintext highlighter-rouge">CoT</code> and <code class="language-plaintext highlighter-rouge">answer</code>: <strong>ideal</strong> Chain of Thought (CoT) and the corresponding response for the model to generate.</li>
</ul>

<p>The input <code class="language-plaintext highlighter-rouge">prompt</code> has been augmented with the <code class="language-plaintext highlighter-rouge">spec</code> and an <code class="language-plaintext highlighter-rouge">instruction</code> to guide the model <strong>consult</strong> the <code class="language-plaintext highlighter-rouge">spec</code> before answering the <code class="language-plaintext highlighter-rouge">prompt</code>.
Interestingly, the <code class="language-plaintext highlighter-rouge">CoT</code> and <code class="language-plaintext highlighter-rouge">answer</code> are obtained not by human labeling but by an <strong>surrogate</strong> reasoning model (e.g., OpenAI o3) with the augmented <code class="language-plaintext highlighter-rouge">prompt</code>.</p>

<p>The final training data for SFT is then constructed from <strong>original, non-augmented</strong> <code class="language-plaintext highlighter-rouge">prompt</code> and the pair (<code class="language-plaintext highlighter-rouge">CoT</code>, <code class="language-plaintext highlighter-rouge">answer</code>) from the reasoning model. This training procedure is borrowed from the <strong>Deliberative Alignment</strong> [4], 
with the difference that rather two decisions <strong>comply</strong> or <strong>refuse</strong> as in DA [4], here we have three decisions: <strong>direct answer</strong> (a.k.a. <strong>comply</strong>), <strong>safe-completion</strong> and <strong>refusal</strong>. <strong>Safe-completion</strong> mode provides high-level, non-operational, and within-safety-constraint guidance
when the content is restricted but not outright disallowed. It can be done by instructing Reasoning Models to <strong>judge</strong> with three above options.</p>

<h2 id="constrained-reinforcement-learning-rl-in-safe-completion-training">Constrained Reinforcement Learning (RL) in Safe-Completion Training</h2>

<p>In the RL stage, the model is optimized its helpfulness as long as it remains within the safety policy.
To do so, for each safety-related prompt and sampled response, we query two reward models (RMs), each of which outputs <strong>helpfulness</strong> and <strong>safety</strong> scores normalized to [0,1].</p>

<ul>
  <li><strong>Safety score</strong>: ∈ [0, 1]: the degree to which the output adheres to the content policy spec, <code class="language-plaintext highlighter-rouge">safety-score = 0</code> if severe or definitive violations of the policy, <code class="language-plaintext highlighter-rouge">safety-score = 1</code> if the output is fully compliant with the policy.</li>
  <li><strong>Helpfulness score</strong>: ∈ [0, 1]: the degree to which the output is helpful to the user. It is worth noting here, there are two types of answers for a good helping response <strong>direct answer</strong> and <strong>indirect answer</strong> (e.g., a safe-completion). In other words, <code class="language-plaintext highlighter-rouge">it is still considered helpful to provide a safe-completion</code>, more <strong>helpful</strong> than naive <strong>refusal</strong> as previous LLMs.</li>
</ul>

<p>The final reward is computed as the product of the two scores: <code class="language-plaintext highlighter-rouge">Reward = Helpfulness Score * Safety Score</code>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Reward function in the RL stage from [1].
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In my opinion, this is a significant paradigm shift from the traditional Refusal Training (input-centric) to the Safe-Completion Training (output-centric).</p>

<p>At this moment, I am not sure how adversarial attacks will evolve in this new paradigm, but it sure will be interesting. Some ideas can be:</p>

<ul>
  <li>
    <p><strong>Breakdown big hamful output into multiple small harmless outputs</strong>. Because the model is now trained to detect and redirect harmful outputs, making it harder to get a whole harmful output like with previous LLMs. However, it might be weaker in handling each small piece of the whole harmful output.</p>
  </li>
  <li>
    <p><strong>The collapse of intelligence</strong>. Because the training procedure is now become self-referential where training data for the next version is generated by the previous reasoning models. While this addresses the problem of lacking human-labeled data, it might lead to a chain-reaction when a mistake of the previous version is propagated to the next version.</p>
  </li>
</ul>

<h2 id="references">References</h2>

<p>[1] Yuan Yuan et al. “From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.” OpenAI 2025.</p>

<p>[2] Andriushchenko, Maksym, and Nicolas Flammarion. “Does refusal training in llms generalize to the past tense?.” ICLR 2025.</p>

<p>[3] Deng, Yue, et al. “Multilingual jailbreak challenges in large language models.” arXiv preprint arXiv:2310.06474 (2023)</p>

<p>[4] Guan, Melody Y., et al. “Deliberative alignment: Reasoning enables safer language models.” arXiv preprint arXiv:2412.16339 (2024).</p>

<p>[5] Scholten, Yan, Stephan Günnemann, and Leo Schwinn. “A probabilistic perspective on unlearning and alignment for large language models.” ICLR 2025.</p>

<!-- mkdir -p assets/img/2025-safe-completion-training/ -->
<!-- mv _posts/2025-08-08-*.png assets/img/2025-safe-completion-training/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="llm" /><summary type="html"><![CDATA[OpenAI just recently released their newest and most powerful model GPT-5. In the post today, I want to talk about one of the most important aspects of LLMs: How to make them safe against malicious use. In this version, OpenAI introduces a new paradigm called Safe Completion Training (which is built on top of Deliberative Alignment [4])]]></summary></entry><entry><title type="html">Personalized LLMs</title><link href="https://tuananhbui89.github.io/blog/2025/personalized-llms/" rel="alternate" type="text/html" title="Personalized LLMs" /><published>2025-07-23T00:00:00+10:00</published><updated>2025-07-23T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2025/personalized-llms</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/personalized-llms/"><![CDATA[<h2 id="why-personalize-llms">Why Personalize LLMs?</h2>

<p>Large-scale generative AI models are trained on diverse datasets to acquire broad capabilities. However, they are typically not tailored to the needs, preferences, or knowledge of a specific user. For example, a general-purpose LLM can generate grammatically correct and factually relevant text, but it may fail to align with a user’s preferred tone or context-specific requirements.</p>

<p><strong>Personalized LLMs</strong> aim to bridge this gap by adapting to individual users in the following key aspects:</p>

<ul>
  <li><strong>Style and Tone Adaptation</strong>: Adjusting the writing style, tone, or formality of the model to align with a user’s preferences—useful in applications like education, mental health support, or customer service.</li>
  <li><strong>Personal Knowledge Integration</strong>: Utilizing user-specific data (e.g., calendar events, documents, preferences) to act as a digital assistant or agent.</li>
  <li><strong>Domain-Specific Customization</strong>: Incorporating specialized knowledge for a particular task or profession, such as a medical LLM for diagnosis or a legal LLM for contract review.</li>
</ul>

<hr />

<h2 id="two-modes-of-personalization">Two Modes of Personalization</h2>

<p>Personalized LLMs can be applied in two broad scenarios:</p>

<ul>
  <li>
    <p><strong>Category A: Assistant-Oriented Personalization</strong><br />
The LLM acts on behalf of the user by leveraging personal knowledge, effectively functioning as a digital twin. For instance, it can help write reports, schedule meetings, or generate personalized content using user-specific context.</p>
  </li>
  <li>
    <p><strong>Category B: Preference-Oriented Personalization</strong><br />
The LLM adapts its outputs—tone, recommendations, search results, etc.—based on the user’s preferences, interests, or behavioral history.</p>
  </li>
</ul>

<hr />

<h2 id="use-cases">Use Cases</h2>

<p>Based on the above taxonomy, here are some common use cases (adapted from <a href="https://arxiv.org/pdf/2406.01171">Tseng et al., 2024</a>):</p>

<ul>
  <li><strong>Personalized Recommendation</strong>: Suggesting content or products that align with a user’s interests (Category B).</li>
  <li><strong>Personalized Search</strong>: Enhancing search relevance by understanding historical queries and user intent (Category B).</li>
  <li><strong>Personalized Healthcare</strong>:
    <ul>
      <li><em>Category A</em>: Assisting with scheduling, medication reminders, or emergency support.</li>
      <li><em>Category B</em>: Providing medical insights tailored to a user’s health profile.</li>
    </ul>
  </li>
  <li><strong>Software Development Support</strong>: Assisting developers by understanding their coding style, project context, and documentation (Category A).</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-13-27-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-13-27-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-13-27-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-personalized-llms/2025-07-23-13-27-16.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Use cases of personalized LLMs, adapted from <a href="https://arxiv.org/pdf/2406.01171">Tseng et al., 2024</a>.
</div>

<hr />

<h2 id="two-tales-of-persona-in-llms-role-playing-vs-personalization">Two Tales of Persona in LLMs: Role-Playing vs. Personalization</h2>

<p>There are two main directions in how persona is handled in LLMs:</p>

<ul>
  <li>
    <p><strong>Role-Playing LLMs</strong>: The model is assigned a fixed persona or character (e.g., doctor, lawyer, coach) to interact with users in a consistent manner. The focus is on <strong>task fidelity</strong> and <strong>role simulation</strong>, often used in training or simulation settings.</p>
  </li>
  <li>
    <p><strong>Personalized LLMs</strong>: The model adapts its behavior, knowledge, or language based on the <strong>user’s identity, preferences, and needs</strong>. The emphasis is on <strong>personal relevance</strong> and <strong>user satisfaction</strong>.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-11-56-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-11-56-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-11-56-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-personalized-llms/2025-07-23-11-56-24.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Taxonomy of role-playing vs. personalized LLMs, from <a href="https://arxiv.org/pdf/2406.01171">Tseng et al., 2024</a>.
</div>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Role-Playing LLM</th>
      <th>Personalized LLM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Persona Source</td>
      <td>Assigned persona (e.g., fictional or occupational)</td>
      <td>Adapted from user’s identity, behavior, or context</td>
    </tr>
    <tr>
      <td>Primary Objective</td>
      <td>Simulate a role with high fidelity</td>
      <td>Optimize for individual user satisfaction</td>
    </tr>
    <tr>
      <td>Core Focus</td>
      <td>Role consistency and task performance</td>
      <td>Contextual relevance and adaptability</td>
    </tr>
    <tr>
      <td>Example</td>
      <td>Doctor persona for training simulations</td>
      <td>Shopping assistant based on browsing history</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="key-challenges">Key Challenges</h2>

<h3 id="injecting-personal-knowledge-into-llms">Injecting Personal Knowledge into LLMs</h3>

<p>Several methods have been proposed to personalize LLMs by incorporating user-specific knowledge:</p>

<ul>
  <li>
    <p><strong>Prompting and In-Context Learning</strong>: Injecting user information directly into the input prompt. While flexible, this approach faces context length limitations and higher inference costs <a href="#5">[5]</a>.</p>
  </li>
  <li>
    <p><strong>Fine-Tuning / Parameter-Efficient Tuning (PEFT)</strong>: Adapting the model weights using personal data, such as through LoRA. This requires enough personal data and may risk overfitting or degradation on general tasks.</p>
  </li>
  <li>
    <p><strong>Retrieval-Augmented Generation (RAG)</strong>: Storing user data externally and retrieving it during inference. This allows scalable personalization, but can suffer from noisy retrievals or incomplete information <a href="#6">[6]</a>.</p>
  </li>
</ul>

<hr />

<h3 id="updating-personal-knowledge">Updating Personal Knowledge</h3>

<hr />

<h3 id="evaluating-personalized-llms">Evaluating Personalized LLMs</h3>

<p>Assessing the quality of personalized outputs—especially for preference-based tasks like tone, emotion, or stylistic match—is an open challenge. Existing evaluation methods struggle to <strong>quantify subjective user satisfaction</strong> or <strong>match to personal styles</strong> <a href="#4">[4]</a>.</p>

<hr />

<h3 id="safety-and-privacy-concerns">Safety and Privacy Concerns</h3>

<p>Fine-tuning or storing user data poses serious privacy and safety issues:</p>

<ul>
  <li><strong>Alignment Drift</strong>: Personal fine-tuning may bypass original safety constraints, unintentionally enabling jailbreaks <a href="#1">[1]</a>.</li>
  <li><strong>Bias and Overfitting</strong>: Training on unrepresentative user data can produce biased or brittle behaviors.</li>
  <li><strong>Data Leakage</strong>: Training data could be extracted by adversaries through membership inference or extraction attacks <a href="#2">[2]</a>, <a href="#3">[3]</a>.</li>
</ul>

<hr />

<h2 id="further-reading">Further Reading</h2>

<h3 id="personalized-language-modeling-from-personalized-human-feedback">Personalized Language Modeling from Personalized Human Feedback</h3>

<h3 id="beyond-dialogue-a-profile-dialogue-alignment-framework-toward-general-role-playing-llms">Beyond Dialogue: A Profile-Dialogue Alignment Framework Toward General Role-Playing LLMs</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-20-25-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-20-25-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-20-25-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-personalized-llms/2025-07-23-20-25-15.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of the Beyond Dialogue framework from <a href="https://arxiv.org/pdf/2408.10903">Yu et al., 2024</a>.
</div>

<p>Problem setting:</p>

<ul>
  <li><strong>Bias between the Role Profile and Scene-Specific Dialogues</strong>:</li>
</ul>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Tseng, Yu-Min, et al. “Two tales of persona in LLMs: A survey of role-playing and personalization.” <em>arXiv preprint arXiv:2406.01171</em> (2024).</li>
  <li>Wang, Jeffrey G., et al. “Pandora’s White-Box: Precise Training Data Detection and Extraction in Large Language Models.” <em>arXiv:2402.17012</em> (2024).</li>
  <li>Lukas, Nils, et al. “Analyzing leakage of personally identifiable information in language models.” <em>IEEE S&amp;P</em> (2023).</li>
  <li>Samuel, Vinay, et al. “Personagym: Evaluating persona agents and LLMs.” <em>arXiv:2407.18416</em> (2024).</li>
  <li>Richardson, Chris, et al. “Integrating summarization and retrieval for enhanced personalization via large language models.” <em>arXiv:2310.20081</em> (2023).</li>
  <li>Tan, Zhaoxuan, et al. “Democratizing large language models via personalized parameter-efficient fine-tuning.” <em>arXiv:2402.04401</em> (2024).</li>
  <li>Chen, Nuo, et al. “The Oscars of AI Theater: A Survey on Role-Playing with Language Models.” <em>arXiv:2407.11484</em> (2024).</li>
  <li>Yu, Yeyong, et al. “Beyond dialogue: A profile-dialogue alignment framework towards general role-playing language model.” <em>arXiv:2408.10903</em> (2024).</li>
</ol>

<hr />

<h2 id="additional-resources">Additional Resources</h2>

<ul>
  <li><a href="https://github.com/HqWu-HITCS/Awesome-Personalized-LLM">Awesome-Personalized-LLM GitHub</a>: A curated list of papers, datasets, and benchmarks on personalized LLMs.</li>
</ul>

<!-- mkdir -p assets/img/2025-personalized-llms/ -->
<!-- mv _posts/2025-07-23-*.png assets/img/2025-personalized-llms/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="llm" /><summary type="html"><![CDATA[Why Personalize LLMs?]]></summary></entry><entry><title type="html">MS-Diffusion - Multi-subject Zero-shot Image Personalization with Layout Guidance (ICLR 2025)</title><link href="https://tuananhbui89.github.io/blog/2025/ms-diffusion/" rel="alternate" type="text/html" title="MS-Diffusion - Multi-subject Zero-shot Image Personalization with Layout Guidance (ICLR 2025)" /><published>2025-03-15T00:00:00+11:00</published><updated>2025-03-15T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/ms-diffusion</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/ms-diffusion/"><![CDATA[<p>Link to the paper: <a href="https://openreview.net/forum?id=PJqP0wyQek">MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance</a></p>

<p>Link to github: <a href="https://github.com/MS-Diffusion/MS-Diffusion">MS-Diffusion</a></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h2 id="key-contributions">Key contributions</h2>

<p><strong>Challenges</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Personalizing with multiple subjects is challenging, specifically:</p>

<ul>
  <li><strong>Subject neglect</strong>: one or more subjects are not properly represented in the generated image.</li>
  <li><strong>Subject overcontrol</strong>: The output does not match with the input prompt (e.g., “a dog and a cat on the beach” - but the output is not on the beach). The appearance or placement of a subject is unduly influenced by its reference image, potentially overriding the textual prompt or other subject conditions.</li>
  <li><strong>Subject-subject conflict</strong>:  where the interaction between multiple subjects in the generated image is unrealistic or undesirable, i.e., two similar dog and cat.</li>
</ul>

<p>The reasons for these challenges are:</p>

<ul>
  <li><strong>Difficulty in feature representation</strong>: compare to text embedding, image embedding are generally <strong>sparser and contain more information</strong>, making their projection into the condition space more difficult. The pooled output from the image encoder can omit many details (discussed in Section 3.4 of the paper). This can lead to a loss of granular details of individual subjects.</li>
  <li><strong>Complexity of Multi-subject interaction and control</strong>: Ensuring that the generated image aligns with the textual prompt while maintaining the correct placement and appearance of each subject is challenging.</li>
</ul>

<p><strong>Contributions</strong></p>

<ul>
  <li><strong>Grounding Resampler</strong>: A modified cross-attention layer that uses concatenated image and text embeddings as the condition embedding.</li>
  <li><strong>Data Construction</strong>: A data construction pipeline to collect a large amount of data with multiple subjects in the same image.</li>
  <li><strong>Multi-subject Cross-attention with Masks</strong>: A modified cross-attention layer that uses subject-specific masks to guide the model to pay attention to the subject.</li>
</ul>

<h2 id="background-stable-diffusion-with-image-prompt">Background: Stable Diffusion with Image Prompt</h2>

<p>Beyond controlling the generation process using text prompt, there is a hot topic in the community to control using image information/layout/prompt - which has a huge potential in applications, e.g., image inpainting, image-to-image generation, etc. In the standard Stable Diffusion, the condition embedding \(c_t\) is just a text embedding \(c_t = E_t(y)\) where \(y\) is the text prompt and \(E_t\) is a pre-trained text encoder such as CLIP.
IP-Adapter [1] proposes to use an additional image encoder to extract the image embedding from a reference image \(c_i = E_i(x)\) and then project it into the original condition space.
The objective function for IP-Adapter is:</p>

\[\mathcal{L}_{IP} = \mathbb{E}_{z, c, \epsilon, t} \left[ \mid \mid \epsilon - \epsilon_\theta(z_t \mid c_i, c_t, t) \mid \mid_2^2 \right]\]

<p>The cross-attention layer is also modified from the one in Stable Diffusion to include the image embedding \(c_i\) as a condition.</p>

\[\text{Attention}(Q, K_i, K_t, V_i, V_t) = \lambda \text{softmax}\left(\frac{QK_i^T}{\sqrt{d}} + c_i\right)V_i + \text{softmax}\left(\frac{QK_t^T}{\sqrt{d}}\right)V_t\]

<p>where \(Q=z W_Q\), \(K_i = c_i W_K^i\), \(K_t = c_t W_K^t\), \(V_i = c_i W_V^i\), \(V_t = c_t W_V^t\), and \(W_Q\), \(W_K^i\), \(W_K^t\), \(W_V^i\), \(W_V^t\) are the weights of the linear layers.
The model becomes the original Stable Diffusion when \(\lambda = 0\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-03-20-07-02-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h2 id="proposed-method">Proposed Method</h2>

<h3 id="grounding-resampler">Grounding Resampler</h3>

\[\text{RSAttn} = \text{softmax}\left( \frac{Q(f_q) K^T([c_i, c_q])}{\sqrt{d}} \right) V([c_i, c_q])\]

<p>where \(c_q\) is the learnable query feature, \(c_i\) is the image embedding.</p>

<p>My interpretation:</p>

<ul>
  <li>The grounding resampler is a modified cross-attention layer that uses the image embedding and the learnable query feature to generate the attention score.</li>
  <li>Compared to the previous IP-Adapter which uses the image and text embeddings <strong>separately</strong>, the \(\text{RSAttn}\) uses the concatenation of the two embeddings as the unified condition embedding.</li>
</ul>

<h3 id="data-construction">Data Construction</h3>

<p>The authors propose a data construction pipeline to <strong>collect</strong> a large amount of <strong>data with multiple subjects</strong> in the same image.
\(c_q\) was initialized by the <strong>text embedding</strong> of the entities (e.g., “dog”, “cat”, “beach”) and then optimized during training.
\(c_i\) is the <strong>image embedding</strong> of corresponding subject - detected by an <strong>additional object detector</strong>.</p>

<p>To prevent the model becoming overfitted/dependent on the grounding tokens (e.g., “dog”, “cat”) during inference <strong>(?)</strong>, the authors proposed to randomly replace these tokens with the original learnable queries in the training <strong>(?)</strong>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h3 id="multi-subject-cross-attention">Multi-subject Cross-attention</h3>

<p>The idea is to <strong>incorporate attention masks within cross-attention layers</strong> to focus on the relevant subjects and exclude other irrelevant in the text prompt and visual prompt.</p>

\[\mathbf{M}_j(x, y) = \begin{cases}
0 &amp; \text{if } [x, y] \in B_j \\
-\infty &amp; \text{if } [x, y] \notin B_j
\end{cases}\]

<p>Here, \(B_j\) denotes the coordinate set of <strong>bounding boxes related to the \(j\)th subject</strong>. By this means, the conditional image latent \(\hat{\mathbf{z}}_{img}\) is derived through:</p>

\[\hat{\mathbf{z}}_{img} = \text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}_i^\top}{\sqrt{d}} + \mathbf{M}\right)\mathbf{V}_i\]

<p>Herein, \(\mathbf{M}\) represents the concatenation of all subject-specific masks, \(\text{Concat}(\mathbf{M}_0,\ldots,\mathbf{M}_n)\). In this way, the model ensures each subject to be represented in a certain area, thus resolving the issues of subject neglect and conflict.</p>

<p><strong>Mask for Background</strong></p>

\[\mathbf{M}_{bg}(x, y) = \begin{cases}
1 &amp; \text{if } [x, y] \in B_{bg} \\
0 &amp; \text{if } [x, y] \notin B_{bg} \text{ a.k.a. subjects}
\end{cases}\]

<p>The mask for background is a matrix of the same size as the image, with the value of \(1\) for the background and \(0\) for the subjects.</p>

\[\mathbf{z}_{img} = (1 - \mathbf{M}_{bg}) \odot \mathbf{z}_{img}\]

<p>Where \(\odot\) is the element-wise multiplication. This operation ensures that the background is removed from the image latent.
As the authors mentioned, this approach is to ensure that text conditions predominate over areas lacking of any guided information (a.k.a. background).</p>

<p><strong>My interpretation:</strong></p>

<ul>
  <li>The mask \(\mathbf{M}\) is a matrix of the same size as the image, with the value of \(-\infty\) for the background and \(0\) for the subject.</li>
  <li>The mask is added to the attention score matrix \(\mathbf{Q}\mathbf{K}_i^\top\) to <strong>guide the model to pay attention to the subject</strong>.</li>
</ul>

<p><strong>Question</strong>: Where the \(\mathbf{z}_{img}\) is used? The authors did not use a consistent notations throughout the paper.</p>

<!-- mkdir -p assets/img/2025-ms-diffusion/ -->
<!-- mv _posts/2025-03-20-*.png assets/img/2025-ms-diffusion/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="diffusion" /><summary type="html"><![CDATA[Link to the paper: MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance]]></summary></entry><entry><title type="html">Unlearning LLMs</title><link href="https://tuananhbui89.github.io/blog/2025/unlearn-llms/" rel="alternate" type="text/html" title="Unlearning LLMs" /><published>2025-03-14T00:00:00+11:00</published><updated>2025-03-14T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/unlearn-llms</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/unlearn-llms/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Machine unlearning is becoming a critical topic in the era of foundation models. As LLMs grow more powerful, they also become harder to control — and removing harmful, private, or outdated knowledge post‑training is increasingly important. In this post, we’ll take a deep dive into unlearning for LLMs: why it matters, how it’s measured, and the main approaches used in recent research.</p>

<p>We will cover:</p>

<ul>
  <li><strong>Motivation</strong>: Why unlearning is necessary and what risks it mitigates.</li>
  <li><strong>Benchmarks &amp; Metrics</strong>: Datasets and metrics that evaluate how well unlearning works.</li>
  <li><strong>Methods</strong>: Key algorithmic approaches including gradient-based, optimization-based, and DPO‑based methods.</li>
  <li><strong>Practical Challenges</strong>: Open problems, trade-offs, and future directions.</li>
</ul>

<hr />

<h2 id="motivation-why-do-we-need-unlearning">Motivation: Why Do We Need Unlearning?</h2>

<p>Modern LLMs ingest massive datasets during pretraining, and as a result, they learn both desirable and undesirable information. There are several reasons to <em>remove</em> knowledge from an already‑trained model:</p>

<ul>
  <li><strong>Safety</strong>: Prevent the model from generating harmful or dangerous instructions (e.g., bomb-making guides).</li>
  <li><strong>Privacy</strong>: Ensure memorized personal information (like phone numbers or medical records) can be erased upon request (GDPR “right to be forgotten”).</li>
  <li><strong>Content moderation</strong>: Filter out toxic or biased outputs without fully retraining the model.</li>
  <li><strong>Compliance</strong>: Adjust knowledge to match regulations or cultural norms across regions.</li>
  <li><strong>Countering adversarial attacks</strong>: Current safeguards like refusal training [2] can be bypassed through adversarial attacks, and hazardous information can be reintroduced through finetuning. Similar to the case of enhancing inherent safety, unlearning especially when applied before model serving, can act as a countermeasure by removing the knowledge that these attacks or finetuning might exploit or reveal.</li>
</ul>

<p>A naive approach would be to retrain from scratch without the sensitive data — but this is computationally infeasible for today’s LLMs. Instead, we seek <em>unlearning</em>: efficiently modifying the model so that it behaves <strong>as if</strong> it never saw the data we want to remove.</p>

<blockquote class="block-tip">
  <p><strong>Refusal training</strong>:</p>

  <p>Refusal training is a method that trains a model to decline or refuse to generate responses to prompts that are associated with malicious use cases.</p>

  <p>This is typically achieved through (1) Supervised fine-tuning (SFT) with a dataset of pair harmful prompts with appropriate refusal responses, (2) RLHF with human preference data, (3) Adversarial training aiming to be more robust against adversarial prompts designed to bypass safety mechanisms.
However, this method is still be circumvented by aversaries, such as posing harmful prompts in the <strong><a href="https://arxiv.org/pdf/2407.11969">past tense</a></strong>.</p>
</blockquote>

<h2 id="the-technical-setting">The Technical Setting</h2>

<p>Before diving into methods, let’s define the core components used in a typical unlearning task. The process revolves around three key datasets:</p>

<ul>
  <li>Forget set: \(\mathcal{D}_f\) (used in fine-tuning) This is a dataset of examples representing the knowledge that the unlearning process aims to remove from the language model, e.g., the WMDP benchmark [1] which contains hazardous knowledge in biosecurity and cybersecurity.</li>
  <li>Retain set: \(\mathcal{D}_r\) (used in fine-tuning) This is a dataset of examples representing general, benign knowledge that the unlearning process should aim to preserve. Recent works [3] try to remove the need of a retain set by using a “flat” loss adjustment approach which adjusts the loss function using only the forget data.</li>
  <li>Testing set: \(\mathcal{D}_t\) (used in evaluation) to evaluate two aspects: (1) unlearning performance - Question-Answering (QA) accuracy on WMDP benchmark, and (2) retaining performance - other benchmarks like MMLU and MT-Bench.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-14-10-30-41.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Figure from [1]
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-10-51-49.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h2 id="benchmarks-how-to-evaluate-unlearning">Benchmarks: How to Evaluate Unlearning?</h2>

<p>To develop effective unlearning methods, we first need robust ways to measure unlearning success. There are three most common benchmarks: TOFU, MUSE and WMDP.</p>

<h3 id="tofu---a-task-of-fictitious-unlearning-for-llms-colm-2024">TOFU - A Task of Fictitious Unlearning for LLMs (COLM 2024)</h3>

<h4 id="motivation">Motivation</h4>

<p>The TOFU benchmark introduces an interesting premise: unlearning fictitious, synthetic author profiles instead of real authors.</p>

<p><strong>Why Fake/Fictitious Authors/Characters?</strong></p>

<ul>
  <li><strong>How to know model after unlearning is equivalent to the model trained without that concept?</strong> \(\mathcal{U}(\mathcal{L}(\theta, \mathcal{D})) \stackrel{?}{=} \mathcal{L}(\theta, \mathcal{D} \setminus \mathcal{D}_F)\), where \(\mathcal{U}\) is the unlearning operator, \(\mathcal{L}\) is the learning operator that trains the model \(\theta\) on the dataset \(\mathcal{D}\).</li>
  <li>When unlearning real-world concepts, this question is impossible to answer because we can’t afford to retrain foundation models. With synthetic data, we can train a smaller model from scratch and compare.</li>
</ul>

<p>This raises a key question: why should we care about this strict equality? This perspective comes from classical machine unlearning (often linked to differential privacy), where the goal is to provably forget a single data point.
<strong>However, in the case of generative AI, I don’t think we should care the same</strong>. The goal is often functional (e.g., the model no longer outputs harmful information) rather than a perfect statistical match to a retrained model.</p>

<h4 id="the-dataset">The dataset</h4>

<p><strong>Main features:</strong></p>

<ul>
  <li>200 diverse synthetic author profiles, each consisting of 20 question-answer pairs. Subset of these profiles are used as the target forget set.</li>
  <li><strong>Forget set</strong>: 2, 10 or 20 profiles depending on the setting.</li>
  <li><strong>Retain set</strong>: Remaining fake profiles + <strong>Real authors</strong> (such as Shakespeare) + <strong>World Facts</strong> (such as the capital of France).</li>
</ul>

<p><strong>Examples:</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-09-18-20-35-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-09-18-20-35-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-09-18-20-35-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-09-18-20-35-25.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Data Creation</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-09-18-22-11-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-09-18-22-11-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-09-18-22-11-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-09-18-22-11-37.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    System prompt for dataset generation.
</div>

<h4 id="evaluation-metrics">Evaluation Metrics</h4>

<ul>
  <li><strong>Retain quality</strong>
    <ul>
      <li><strong>Probability Score</strong> Treat each question \(q\) as a multiple choice question with \(n\) options \(\{ a_1, a_2, ..., a_n \}\), the probability of the correct answer is \(p(a_i \mid q; \theta_u)\) should be the same as of the initial model \(p(a_i \mid q; \theta_o)\)</li>
      <li><strong>ROUGE</strong> The model should be able to generate the same answer (with greedy sampling) as the initial model using the ROUGE-L recall score.</li>
      <li><strong>Truth Ratio</strong> Can the model still provide the correct paraphrased answer? (e.g. “Paris is the capital of France” -&gt; “France is the country with Paris as its capital”). <strong>What is the reason of having this metric?</strong> I guess it is because the most frequent/reasonable way to forget the target concept is not to provide a totally unrelated answer, but to phrasing the answer incorrectly.</li>
    </ul>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-09-18-21-52-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-09-18-21-52-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-09-18-21-52-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-09-18-21-52-22.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Truth Ratio. $$\hat{a}$$ is perturbed - incorrect answer (e.g. "France is the capital of Paris") where $$tilde{s}$$ is pharaphrased correct answer. The higher the Log Truth Ratio, the more the model forget the target concept (give incorrect answer).
</div>

<ul>
  <li><strong>Forget quality</strong>
    <ul>
      <li><strong>Challenge</strong> How to guarantee the model totally forget the target concept? Can “providing wrong answers about the target concept” be considered as good enough?</li>
      <li>In the paper, they use the Truth Ratio to measure the forget quality.</li>
    </ul>
  </li>
</ul>

<h3 id="wmdp---measuring-and-reducing-malicious-use-with-unlearning">WMDP - Measuring and Reducing Malicious Use With Unlearning</h3>

<p>Motivation:</p>

<ul>
  <li>The Weapon Weapons of Mass Destruction Proxy Benchmark (WMDP),</li>
</ul>

<h3 id="muse---machine-unlearning-six-way-evaluation-for-language-models-iclr-2025">MUSE - Machine Unlearning Six-Way Evaluation for Language Models (ICLR 2025)</h3>

<p>While TOFU provides a solid foundation, its metrics don’t cover the full spectrum of real-world concerns like privacy and scalability. To address these limitations, the MUSE benchmark proposed a more comprehensive six-way evaluation.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-10-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Six-way evaluation of machine unlearning
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-18-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison with a previous benchmark TOFU.
</div>

<p>This paper present a new benchmark for evaluating the quality of machine unlearning, which considers six aspects:</p>

<ul>
  <li>No verbatim memorization: The model should not exactly replicate any details from the forget set.</li>
  <li>No knowledge memorization: The model should be incapable of responding to questions about the forget set.</li>
  <li>No privacy leakage: It should be impossible to detect that the model was ever trained on the forget set.</li>
  <li>Utility preservation: The model should maintain high performance on the tasks it was trained for except for the forget set.</li>
  <li>Scalability: The method should be able to handle large forget set sizes.</li>
  <li>Substantiality: The method should be able to handle a large number of forget queries - continuous unlearning setting.</li>
</ul>

<p>This benchmark highlights critical trade-offs: maintaining model utility, forgetting performance, and scalability.</p>

<h4 id="metrics-for-each-aspect">Metrics for each aspect</h4>

<p><strong>Verbatim Memorization</strong>
To measure verbatim memorization, the paper uses the <a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE</a> score between the model’s continuation and the actual continuation in the forget set:</p>

\[\text{VerbMem}(f, \mathcal{D}) := \frac{1}{|\mathcal{D}_{\text{forget}}|} \sum_{x \in \mathcal{D}_{\text{forget}}} \text{ROUGE}(f(x_{[:l]}), x_{[l+1:]})\]

<p>ROUGE measures the overlap between the generated text (candidate) and one or more reference texts. It focuses on n-gram, sequence, and word-level similarity and is particularly recall-oriented, meaning it emphasizes how much of the reference text is captured by the generated text.</p>

\[\text{ROUGE-N} = \frac{\sum_{r \in R} \sum_{\text{gram}_n \in r} \min \big( \text{Count}_{C}(\text{gram}_n), \text{Count}_{r}(\text{gram}_n) \big)}{\sum_{r \in R} \sum_{\text{gram}_n \in r} \text{Count}_{r}(\text{gram}_n)}\]

<ul>
  <li><strong>Numerator:</strong> number of overlapping (n)-grams between candidate and references</li>
  <li><strong>Denominator:</strong> total number of (n)-grams in references</li>
</ul>

<p>Special cases:</p>

<ul>
  <li><strong>ROUGE-1</strong> → unigram overlap</li>
  <li><strong>ROUGE-2</strong> → bigram overlap</li>
</ul>

<p><strong>Knowledge Memorization</strong></p>

<p>To measure knowledge memorization, the paper uses the <a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE</a> score between the model’s continuation and the actual continuation in the forget set:</p>

\[\text{KnowMem}(f, \mathcal{D}_{\text{forget}}) := \frac{1}{|\mathcal{D}_{\text{forget}}|} \sum_{(q,a)\in\mathcal{D}_{\text{forget}}} \text{ROUGE}(f(q), a)\]

<p><strong>Privacy Leakage</strong></p>

<p>To measure privacy leakage, the paper uses (Min-K Prob) [6] a start-of-the-art MIA method to compute the standard AUC-ROC score of discriminating members \(\mathcal{D}_{\text{forget}}\) and non-members \(\mathcal{D}_{\text{holdout}}\) by using the logits of the last layer of the model.</p>

\[\text{PrivLeak} := \frac{\text{AUC}(f_{\text{unlearn}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}}) - \text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})}{\text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})}\]

<p>The good unlearning method should have \(\text{AUC}(f_{\text{unlearn}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}}) \approx \text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})\) which means the unlearned model is indistinguishable from the original model. In contrast, the bad unlearning method will get a large positive or negative difference.</p>

<p><strong>Min-K Prob [6]</strong></p>

<p>The Min-K Prob is based on the hypothesis that a non-member example is more likely to include a few outlier words with low probability, while a member example is less likely to include such words.</p>

<p>Consider a sequence of tokens in a sentence, denoted as \(x = x_1, x_2, ..., x_N\), the log-likelihood of a token, \(x_i\), given its preceding tokens is calculated as \(\log p(x_i \mid x_1, ..., x_{i-1})\). We then select the \(k\%\) of tokens from \(x\) with the minimum token probability to form a set, Min-K%(x), and compute the average log-likelihood of the tokens in this set:</p>

\[\text{MIN-K\% PROB}(x) = \frac{1}{E} \sum_{x_i \in \text{Min-K\%}(x)} \log p(x_i \mid x_1, ..., x_{i-1}).\]

<p>where \(E\) is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining data simply by thresholding this MIN-K% PROB result.</p>

<p><strong>Why this new benchmark is important?</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-29-24.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    It seems that unlearning methods are not good at scalability - when increasing the forget size and substantiality - when number of forget queries increase.
</div>

<p>This benchmark provides another perspective - from model developer - who want to keep the model utility after unlearning. The current benchmark is more focused on the data owner’s expectation - forgetting the data - but not the model utility. With this benchmark, we can see that current unlearning methods are not good at these metrics:</p>

<ul>
  <li>“Unlearning significantly degrades model utility”</li>
  <li>“Unlearning methods scale poorly with forget set sizes”</li>
  <li>“Unlearning methods cannot sustainably accommodate sequential unlearning requests”</li>
</ul>

<hr />

<h2 id="unlearning-methods">Unlearning Methods</h2>

<p>Now that we’ve seen how unlearning is measured, let’s explore some of the algorithmic approaches designed to achieve it.</p>

<p>A common formulation for unlearning is the following optimization problem:</p>

\[\mathcal{L} = \min_{\theta} \underbrace{\mathbb{E}_{x^f \in \mathcal{D}_f} \ell(y^f|h_{\theta}^{(l)}(x^f))}_{\text{forget loss}} + \alpha \underbrace{\mathbb{E}_{x^r \in \mathcal{D}_r} \ell(y^r|h_{\theta}^{(l)}(x^r))}_{\text{retain loss}}\]

<p>where \(\theta\) is the model parameters of an autoregressive LLM \(f_{\theta}\), \(\ell\) is the loss function, \(y^f\) and \(y^r\) are the target representations (e.g., representations of next token) for the forget and retain sets, and \(\alpha\) is a hyperparameter.</p>

<p>It is worth noting that \(h_{\theta}^{(l)}\) is the <strong>average</strong> output hidden state of all tokens (with the size of sequence length) in forget-sample \(x^f \in \mathcal{D}_f\). Similarly, \(h_{\theta}^{(l)}\) is the <strong>average</strong> output hidden state of all tokens in retain-sample \(x^r \in \mathcal{D}_r\). Using this average representation to represent the entire forget-sample is one of current limitations that requires further investigation.</p>

<p><strong>Intepretation</strong>: The goal is to maximize the loss on the forget set (encouraging the model to “unlearn” it, often by pushing its representations towards something random or nonsensical) while minimizing the loss on the retain set to keep its general knowledge intact.</p>

<h2 id="representation-misdirection-for-unlearning-rmu">Representation Misdirection for Unlearning (RMU)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a method to unlearn LLMs by steering the representation of forget samples towards a target random representation while keeping the representation of retain samples unchanged.</p>

<p><strong>Approach</strong></p>

<p>RMU [1] aims to steer model representation of forget samples (e.g., malicious use cases - that sampled from the forget set) in the intermediate layer towards a target random representation while keeping the representation of retain samples (e.g., benign use cases - that sampled from the retain set) unchanged.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-14-10-34-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of RMU from [1]
</div>

<p>More specifically, given a forget set \(\mathcal{D}_f\) and a retain set \(\mathcal{D}_r\), and a frozen model \(h_{\theta^{\text{frozen}}}\) with parameters \(\theta^{\text{frozen}}\), RMU steers the latent representation of forget-tokens to a predetermined random representation \(y^f = cu\), where \(u\) is a random unit vector each element is sampled from Uniform distribution \(U(0,1)\), \(c \in \mathbb{R}^+\) is a coefficient, and regularizes the latent representation of retain-tokens back to the frozen model’s representation. The loss of RMU is</p>

\[\mathcal{L} = \mathbb{E}_{x^f \in \mathcal{D}_f} \|h_{\theta^{\text{rm}}}^{(l)}(x^f) - cu\|^2 + \alpha\mathbb{E}_{x^r \in \mathcal{D}_r} \|h_{\theta^{\text{rm}}}^{(l)}(x^r) - h_{\theta^{\text{frozen}}}^{(l)}(x^r)\|^2,\]

<p>where \(\theta^{\text{rm}}\) is the parameters of the model to be optimized, and \(\alpha\) is a hyperparameter.</p>

<h2 id="adaptive-rmu-on-effects-of-steering-latent-representation-for-large-language-model-unlearning---aaai-2025">Adaptive RMU (On Effects of Steering Latent Representation for Large Language Model Unlearning - AAAI 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a <strong>layer-adaptive</strong> RMU method, which uses a different \(c\) for each layer.</p>

<p><strong>Key Observation</strong></p>

<p>This paper points out an interesing phenomenon that the performance of RMU is sensitive to the choice of coefficient \(c\) in the above loss function.</p>

<p>More specifically, the random unit vector \(u\) and the representation of forget samples \(\hat{h}_{\theta^{\text{rm}}}^{(l)}(x^f)\) are more aligned as \(c\) increases, suggesting the better unlearning performance.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-16-21-49.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The distribution of the representation of forget samples with different $c$ from [4]. The blue histogram becomes more Gaussian as $c$ increases.
</div>

<p>However, using a fixed \(c\) across all layers is not ideal, since the performance of RMU is observed to be <strong>layer-dependent</strong>.
More specifically, as discussed in Section 4.3 in [4], within early layers, the \(l^2\) norm of the representation of forget samples is relatively smaller than the coefficient \(c\) and during the unlearning process, that norm exponentially grows and appproaches \(c\), thereby facilitating the convergence of the unlearning process. However, in later layer, the \(l^2\) norm of the representation of forget samples is initially larger than \(c\) and remains unchanged during unlearning, making the unlearning process less effective.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-16-34-02.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The norm of the representation of forget samples in different layers from [4].
</div>

<p>Inspired by the above observation, this paper proposes a <strong>layer-adaptive</strong> RMU method, which uses a different \(c\) for each layer.</p>

\[\mathcal{L}^{\text{adaptive}} = \underbrace{\mathbb{E}_{x_F \in \mathcal{D}_{\text{forget}}} \|h_{\theta^{\text{unlearn}}}^{(l)}(x_F) - \beta\|h_{\theta^{\text{frozen}}}^{(l)}(x_F)\|u\|_2^2}_{\text{adaptive forget loss}} + \underbrace{\alpha \mathbb{E}_{x_R \in \mathcal{D}_{\text{retain}}} \|h_{\theta^{\text{unlearn}}}^{(l)}(x_R) - h_{\theta^{\text{frozen}}}^{(l)}(x_R)\|_2^2}_{\text{retain loss}}\]

<p>where \(\beta \|h_{\theta^{\text{frozen}}}^{(l)}(x_F)\|\) is the <strong>adaptive scaling coefficient</strong> for the forget loss which is computed by the norm of the representation of forget samples in the corresponding layer of the frozen model.</p>

<p>However, it is worth noting that intuitively, the higher \(c\) leads to a more alignment between forget representation and the random unit vector \(u\), which suggests the better unlearning performance - more randomness of the output with the forget prompt. However, it also leads to a worse retaining performance.</p>

<h2 id="llm-unlearning-via-loss-adjustment-with-only-forget-data-iclr-2025">LLM Unlearning via Loss Adjustment with Only Forget Data (ICLR 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a method to unlearn LLMs by adjusting the loss function using only the forget data.</p>

<h3 id="motivation---eliminating-the-need-for-a-retain-set">Motivation - Eliminating the need for a retain set</h3>

<p>Previous unlearning methods typically require a retain set or a reference model to maintain the performance of the unlearned model on the retain set.
The limitation of this requirement (as stated in [3]) is that it is may lead to a trade-off between model utility and forget performance (why?).
Furthermore, fine-tuning using both retain data and forget data would require a careful design of a data mixing strategy to avoid information leakage from the retain set to the forget set.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-17-17-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison between unlearning methods from [3]
</div>

<h3 id="loss-adjustments-via-f-divergence-maximization">Loss-Adjustments via f-divergence Maximization</h3>

<p>For each learning batch, we assume that we only have access to a set of forget samples \((x_f, y_f) \in D_f\). Instead of directly adopting gradient ascent over these forget samples, we propose to maximize the divergence between exemplary and bad generations of forget data. Key steps are summarized as below.</p>

<p><strong>Step 1</strong>: Equip example/template responses \(y_e\) for each forget sample \(x_f\). Together we denote the paired samples as \(D_e = \{(x_f^j, y_e^j)\}_{j\in[N]}\).</p>

<ul>
  <li>
    <p>This could be done by leveraging open-source LLMs such as Llama 3.1 [25] or self-defining the responses according to our wish, etc. The designated unlearning response could be a reject-based answer such as “I don’t know” (denoted as “IDK”) or an irrelevant answer devoid of the unlearning target-related information.</p>
  </li>
  <li>
    <p><strong>Motivation</strong>: Step 1 generates example responses for LLM fine-tuning and provides better instructions on what LLM should respond given the forget data. Besides, certain existing methods make LLM generate hallucinated responses after unlearning, which further illustrates the importance of example responses for LLM unlearning.</p>
  </li>
</ul>

<p><strong>Step 2</strong>: Loss adjustments w.r.t. the sample pairs \((x_f, y_e, y_f)\) through:</p>

\[L(x_f, y_e, y_f; \theta) = \lambda_e \cdot L_e(x_f, y_e; \theta) - \lambda_f \cdot L_f(x_f, y_f; \theta),\]

<p>where \(L_e, L_f\) are losses designed for the data sample \((x_f, y_e)\) and \((x_f, y_f)\), respectively.</p>

<ul>
  <li><strong>Motivation</strong>: Step 2 encourages the LLM to forget the forget data with bad responses, meanwhile, learn to generate good responses on relevant forget data [such as template answers].</li>
</ul>

<p><strong>Step 3</strong>: How to decide on the values of \(\lambda_e\) and \(\lambda_f\)?</p>

<p>We leverage f-divergence to illustrate the appropriate balancing between \(L_e(x_f, y_e; \theta)\) and \(L_f(x_f, y_f; \theta)\). Assume \(x_f, y_e\) is generated by the random variable \(X_f, Y_e\) jointly following the distribution \(\mathcal{D}_e\). Similarly, \(x_f, y_f\) is given by \(X_f, Y_f\) and \((X_f, Y_f) \sim \mathcal{D}_f\). Step 2 shares similar insights as if we are maximizing the divergence between \(\mathcal{D}_e\) and \(\mathcal{D}_f\). Our theoretical purpose is to obtain the model that maximizes the f-divergence between \(\mathcal{D}_e\) and \(\mathcal{D}_f\), defined as \(f_{div}(\mathcal{D}_e\|\mathcal{D}_f)\).</p>

<p><strong>The variational form f-divergence</strong>: Instead of optimizing the \(f_{div}\) term directly, we resolve to the variational form of it. Due to the Fenchel duality, we would have:</p>

\[f_{div}(\mathcal{D}_e\|\mathcal{D}_f) = \sup_g [\mathbb{E}_{z_e\sim\mathcal{D}_e} [g(z_e)] - \mathbb{E}_{z_f\sim\mathcal{D}_f} [f^*(g(z_f))]] := \sup_g \text{VA}(\theta, g),\]

<p>we define \(f^*\) as the conjugate function of the f-divergence function. For simplicity, we define \(\text{VA}(\theta, g^*) := \sup_g \text{VA}(\theta, g)\), where \(g^*\) is the optimal variational function.</p>

<h3 id="connection-with-dpo">Connection with DPO</h3>

<p><a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#ppo-and-dpo">Direct Preference Optimization</a> (DPO) is a method to align LLMs with human preferences, however, unlike PPO which uses a reward model, DPO simplifies PPO by directly optimizing the policy model to adhere to human preferences without the need for a reward model. In the context of unlearning, DPO can be used to unlearn the LLM by directly optimizing the original model to align forget prompt with the template response.</p>

<p>Given a dataset \(D = \{(x_f^j, y_e^j, y_f^j)\}_{j\in[N]}\), where \(y_e\) and \(y_f\) are preferred template and original forget responses to the forget prompt \(x_f\), DPO fine-tunes original model \(\theta_o\) using \(D\) to better align it with good answer preferences, which minimizes:</p>

\[L_{\text{DPO},\beta}(\theta) = -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta\log \frac{\pi_\theta(y_e \mid x_f)}{\pi_{\text{ref}}(y_e \mid x_f)} - \beta\log \frac{\pi_\theta(y_f \mid x_f)}{\pi_{\text{ref}}(y_f \mid x_f)}\right)\right]\]

\[= -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i}) - \log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i}))-M_{\text{ref}}\right)\right],\]

<p>where, \(\sigma(t) = \frac{1}{1+e^{-t}}\) is the sigmoid function, \(\beta &gt; 0\) is the inverse temperature, \(\pi_\theta := \prod_{i=1}^{\mid y \mid} h_\theta(x, y_{&lt;i})\) is the predicted probability of the response \(y\) to prompt \(x\) given by LLM \(\theta\), \(\pi_{\text{ref}}\) is the predicted probability given by reference model, and \(M_{\text{ref}} := \beta(\log \prod_{i=1}^{\mid y_e \mid}h_{\theta_o}(x_f, y_{e,i}) - \log \prod_{i=1}^{\mid y_f \mid}h_{\theta_o}(x_f, y_{f,i}))\).</p>

<p><strong>Intepretation</strong>: minimizing \(L_{\text{DPO},\beta}(\theta)\) is equivalent to maximizing \(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i})\) - which is the log probability of the template response - while minimizing \(\log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i})\) - that of the forget response.
the \(M_{\text{ref}}\) term is a constant w.r.t. \(\theta\) which is the gap between two log probabilities of the template and forget responses given by the reference model - so that the optimal should maintain the same gap as the reference model (IMO: should add max-margin loss here).</p>

<p><strong>FLAT</strong></p>

<p>As for FLAT, we calculate the average probability of all correctly generated tokens and employ a novel re-weighting mechanism that assigns different importance to each term using distinct activate functions for both the example and forget loss terms, which minimizes:</p>

\[L_{\text{FLAT}}(\theta) = -\mathbb{E}_D\left[g^*\left(\frac{1}{\mid y_e \mid} \sum_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_{e,&lt;i})\right) - f^*(g^*\left(\frac{1}{\mid y_f \mid} \sum_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_{f,&lt;i})\right))\right].\]

<p>Here, \(f^*(\cdot), g^*(f^*(\cdot))\) are the activate functions that assign appropriate weights to each loss term. The detailed derivation is in Appendix B.2 in the paper. Specifically, DPO relies on a reference model to guide the unlearning process, whereas FLAT only uses a sample pair dataset containing both exemplar and forget responses. Besides, FLAT differs from DPO in three critical aspects: the re-weighting activation function, whether to sum or average the token losses, and whether to apply the logarithm to the output probability.</p>

<p>An example (from Appendix B.2 in the paper) is as follows:</p>

<p>Here, \(v\) is the vocabulary size, \(y_{e,i,k}\) is the \(k\)-th element of vector representing the \(i\)-th token in the good response \(y_e\), \(y_{f,i,k}\) is the \(k\)-th element of vector representing the \(i\)-th token in the forget response \(y_f\). Additionally, \(h_\theta(x_f, y_{e,&lt;i})_k\) and \(h_\theta(x_f, y_{f,&lt;i})_k\) denote the \(k\)-th entry of the probability distribution for the correctly generated token.</p>

<p>For <strong>KL f-divergence</strong>, \(f^*(u) = e^{u-1}, g^*(v) = v\), hence, \(g^*(\mathbb{P}(x_f, y_e; \theta)) - f^*(g^*(\mathbb{P}(x_f, y_f; \theta))) = \mathbb{P}(x_f, y_e; \theta) - e^{\mathbb{P}(x_f, y_f; \theta)-1}\). We have:</p>

\[L_{\text{FLAT}}(\theta) = -\mathbb{E}_D\left[\frac{\sum_{i=1}^{ \mid y_e \mid} h_\theta(x_f, y_{e,&lt;i})}{\mid y_e \mid} - e^{\frac{\sum_{i=1}^{ \mid y_f \mid} h_\theta(x_f, y_{f,&lt;i})}{\mid y_f \mid}-1}\right].\]

<h2 id="a-closer-look-at-machine-unlearning-for-large-language-models-iclr-2025">A Closer Look at Machine Unlearning for Large Language Models (ICLR 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper investigates the effectiveness of unlearning LLMs for targeted and untargeted use cases.
It proposes a Maximizing Entropy (ME) objective for untargeted unlearning and Answer-Preservation (AP) objective for targeted unlearning.</p>

<h3 id="untargeted-vs-targeted-unlearning">Untargeted vs Targeted Unlearning</h3>

<p>Targeted unlearning hopes to make a specified template response to the questions in the forget set, while untargeted unlearning only requires not leaking the contents of the forget set.
Mathematically, the loss function for untargeted unlearning is (borrowing the notation of DPOfrom [3])</p>

\[= -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i}) - \log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i}))-M_{\text{ref}}\right)\right],\]

<p>Where \(y_e\) is the template response and \(y_f\) is the forget response. Targeted unlearning aims to make the model response to the forget set to be close to the template response, hence, maximizing the probability of the template response. On the other hand, untargeted unlearning only requires the model response to the forget set to be far from the forget response, hence, minimizing the probability of the forget response.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-16-15-58-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of untargeted and targeted unlearning from [5]
</div>

<h3 id="maximizing-entropy-for-untargeted-unlearning">Maximizing Entropy for Untargeted Unlearning</h3>

<p>The paper states that “the core of most untargeted unlearning methods is to adapt a gradient ascent direction that maximizes the prediction loss over the forget set - may have several challenges”.</p>

<ul>
  <li><strong>The behavior of the ideal retain model is unpredictable</strong>: The cost to retrain the model from scratch is extremely expensive in the case of LLMs. More importantly, gradient ascent on the forget set when retraining may lead to unpredicatable behavior of the model.</li>
  <li><strong>Potential hallucinations in the surrogate retain model</strong>: An alternative approach for the expensive retraining is to use a surrogate model - which is a base model such as Llama 2, fine-tuned on a small fictitious dataset \(\mathcal{D}^f = \{ \mathcal{D}^f_F,  \mathcal{D}^f_R \}\), where \(\mathcal{D}^f_F\) and \(\mathcal{D}^f_R\) are the forget and retain sets, respectively. However, this approach may lead to hallucinations, where the model generates responses that are not present in the retain set.</li>
</ul>

<p>Idea: <em>Align the prediction behavior of the unlearned model on the forget set with that of a <strong>randomly initialized</strong> model</em>.</p>

<ul>
  <li>The randomly initialized model is data-independent and does not contain any knowledge about the forget set, avoids the leakage of relevant information.</li>
  <li>The behavior of the randomly initialized model is random guessing - maximizing the entropy of the output distribution.</li>
</ul>

\[\mathcal{L}_{\text{ME}}(\mathcal{D}_F; \theta) = \mathbb{E}_{(x,y)\sim\mathcal{D}_F}\left[\frac{1}{T}\sum_{t=1}^T \text{KL}(P_t\|\mathcal{U}_{[K]})\right],\]

<p>where \(P_t = p(x'_t \mid x'_{&lt;t}; \theta)\) is the predicted probability for the \(t\)-th token in \(x' = x \circ y\) and \(\mathcal{U}_{[K]}\) is a uniform distribution over the vocabulary of size \(K\), where each value is \(1/K\).</p>

<p>Minimizing above loss is equivalent to Maximizing Entropy (ME) of predicted distribution for each next token. <strong>The greater the entropy, the higher the uncertainty of the prediction, indicating that the model behaves closer to a randomly initialized model for random guessing</strong>. This objective also avoids catastrophic collapse caused by the unbounded forget loss (Zhang et al., 2024a; Ji et al., 2024).</p>

<h3 id="mitigate-excessive-ignorance-of-targeted-unlearning">Mitigate Excessive Ignorance of Targeted Unlearning</h3>

<p><strong>Over Ignorance Issue</strong>: Refuse to answer most questions in the retain set - False Positive. It is due to (based on their argument) that \((\mathcal{X}_F, \mathcal{Y}_F) \approxeq (\mathcal{X}_R, \mathcal{Y}_R)\), therefore, increasing \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_F)\) will also increase \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_R)\).</p>

<p>Their experiment to support the above argument is as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-17-12-50-45.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The correlation between performances of unlearned models on the forget set and the retain set from [5].
</div>

<p><strong>Answer-Preservation (AP)</strong>:</p>

<p>Intuitively, given a question in the retain set, the regularization loss for targeted unlearning should satisfy two objectives:</p>

<ul>
  <li>Reduce the probability of the rejection template.</li>
  <li>Maintain the probability of the original answer.</li>
</ul>

<p>Thus, the authors propose the Answer Preservation (AP) loss as follows:</p>

\[\mathcal{L}_{\text{AP}}(\mathcal{D}_R, \mathcal{D}_{\text{IDK}}; \theta) = -\frac{1}{\beta}\mathbb{E}_{(x,y)\sim\mathcal{D}_R,y'\sim\mathcal{D}_{\text{IDK}}}\left[\log \sigma\left(-\beta\log \frac{p(y' \mid x; \theta)}{p(y \mid x; \theta)}\right)\right],\]

<p>where \(\sigma(\cdot)\) is the sigmoid function, \(\beta\) is a hyper-parameter.</p>

<p>The gradient of AP loss w.r.t. the model parameters is:</p>

\[\nabla_\theta\mathcal{L}_{\text{AP}}(\theta) = \mathbb{E}_{\mathcal{D}_R,\mathcal{D}_{\text{IDK}}}[W_\theta(x, y, y')\nabla_\theta (\log p(y' \mid x; \theta) - \log p(y \mid x; \theta))].\]

<p>The \(W_\theta(x, y, y') = 1/(1 + (\frac{p(y \mid x; \theta)}{p(y' \mid x; \theta)})^\beta)\) can be regarded as an adaptive gradient weight.</p>

<p>Given a question \(x\) in \(\mathcal{D}_R\), in the early stage of unlearning process, where \(p(y \mid x; \theta) \gg p(y' \mid x; \theta)\), we have \(W_\theta(x, y, y') \ll 1\).</p>

<p>As the unlearning proceeds, either a decrease in \(p(y \mid x; \theta)\) or an increase in \(p(y' \mid x; \theta)\) will result in a larger \(W_\theta(x, y, y')\), thereby providing stronger regularization. The gradient of AP loss consists of two terms in addition to the adaptive weight. The first term is equivalent to GA on the rejection template, which satisfies the first objective. The second term is equivalent to GD on the original answer, which satisfies the second objective.</p>

<p>My interpretation of the above equation is as follows:</p>

<ul>
  <li>Reminder that in gradient descent, we follow the negative gradient direction to update the model parameters, i.e., \(\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}\)</li>
  <li>In the first term, we follow the negative direction of \(\nabla_\theta (\log p(y' \mid x; \theta)\) - which aims to reduce the probability of \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_R)\)</li>
  <li>In the second term, we follow the direction of \(\nabla_\theta (\log p(y \mid x; \theta)\) - which aims to increase the probability of original answer \(P(y \mid \mathcal{X}_R)\)</li>
</ul>

<h2 id="references">References</h2>

<p>[1] <a href="https://arxiv.org/abs/2403.03218">Li, Nathaniel, et al. “The wmdp benchmark: Measuring and reducing malicious use with unlearning.” arXiv preprint arXiv:2403.03218 (2024).</a></p>

<p>[2] <a href="https://arxiv.org/pdf/2407.11969">Andriushchenko, Maksym, and Nicolas Flammarion. “Does Refusal Training in LLMs Generalize to the Past Tense?.” arXiv preprint arXiv:2407.11969 (2024).</a></p>

<p>[3] <a href="https://arxiv.org/pdf/2410.11143">Wang, Yaxuan, et al. “LLM Unlearning via Loss Adjustment with Only Forget Data.” arXiv preprint arXiv:2410.11143 (2024).</a></p>

<p>[4] <a href="https://arxiv.org/pdf/2408.06223">Huu-Tien, Dang, et al. “On effects of steering latent representation for large language model unlearning.” arXiv preprint arXiv:2408.06223 (2024).</a></p>

<p>[5] <a href="https://arxiv.org/pdf/2410.08109">Yuan, Xiaojian, et al. “A Closer Look at Machine Unlearning for Large Language Models.” arXiv preprint arXiv:2410.08109 (2024).</a></p>

<p>[6] <a href="https://arxiv.org/pdf/2310.16789">Weijia Shi, et al. “Detecting Pretraining Data from Large Language Models.” arXiv preprint arXiv:2310.16789 (2023).</a></p>

<!-- mkdir -p assets/img/unlearnllms/ -->
<!-- mv _posts/2025-03-17-*.png assets/img/unlearnllms/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Foundation of Diffusion Models</title><link href="https://tuananhbui89.github.io/blog/2025/diffusion-foundation/" rel="alternate" type="text/html" title="Foundation of Diffusion Models" /><published>2025-03-08T00:00:00+11:00</published><updated>2025-03-08T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/diffusion-foundation</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/diffusion-foundation/"><![CDATA[<p>(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)</p>

<h2 id="what-are-diffusion-models">What are Diffusion Models?</h2>

<p>Diffusion models are a class of generative models that generate data by progressively denoising a sample from pure noise. They are inspired by <a href="https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics"><strong>non-equilibrium thermodynamics</strong></a> and are based on a forward and reverse diffusion process:</p>

<ol>
  <li>Forward Process (Diffusion Process): A data sample (e.g., an image) is gradually corrupted by adding Gaussian noise over multiple timesteps until it becomes nearly pure noise.</li>
  <li>Reverse Process (Denoising Process): A neural network learns to reverse this corruption by gradually removing noise step by step, reconstructing the original data distribution.</li>
</ol>

<figure style="text-align: center;">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/JnIkGtkO-Js?si=faOgaMvGtqTLcG1T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>Diffusion - How molecules actually move</figcaption>
</figure>

<p><strong>Analogy: Ink Dissolving in Water</strong>
Imagine dropping a blob of ink into a glass of water:</p>

<ul>
  <li>Forward process (Diffusion Process): Initially, the ink is concentrated in one place (structured data). Over time, it spreads out randomly, blending with the water (adding noise). Eventually, the entire glass becomes a uniformly colored mixture, losing its original structure (complete noise).</li>
  <li>Reverse process (Denoising Process): If we had a way to perfectly reverse time, we could watch the ink particles retrace their paths, reassembling into the original drop (generating the original data from noise). Diffusion models learn to perform this “reverse process” step by step using machine learning.</li>
</ul>

<blockquote>
  <p><strong>Non-Equilibrium Thermodynamics</strong></p>

  <p>Thermodynamics studies <strong>how energy moves and changes</strong> in a system. In equilibrium thermodynamics, systems are in balance—nothing is changing. Non-equilibrium thermodynamics, on the other hand, deals with <strong>systems that are constantly evolving, moving between states of disorder and order</strong>.</p>

  <p>In diffusion models, the forward process (adding noise to data) and the reverse process (removing noise) resemble a non-equilibrium thermodynamic system because they describe an evolving state that moves from order (structured data) to disorder (pure noise) and back to order (reconstructed data).</p>
</blockquote>

<blockquote>
  <p><strong>Brownian Motion</strong></p>

  <p>Brownian motion <strong>describes the random movement</strong> of tiny particles (like pollen grains in water) due to <strong>collisions with molecules</strong>. This randomness is similar to how noise is added in diffusion models.</p>
</blockquote>

<h3 id="advantages-of-diffusion-models">Advantages of Diffusion Models</h3>

<p>Diffusion models offer several key advantages over traditional generative models like GANs and VAEs:</p>

<ol>
  <li>
    <p><strong>High-Fidelity Samples</strong>: Unlike VAEs and GANs which generate samples in one step, diffusion models create samples gradually by denoising. This step-by-step process allows the model to first establish coarse image structure before refining fine details, resulting in higher quality outputs.</p>
  </li>
  <li>
    <p><strong>Training Stability</strong>: Diffusion models are easier to train compared to GANs as they use a single tractable likelihood loss. They don’t suffer from training instabilities like mode collapse that often plague GANs.</p>
  </li>
  <li>
    <p><strong>Sample Diversity</strong>: Similar to VAEs, diffusion models maximize likelihood which ensures coverage of all modes in the training dataset. This leads to more diverse outputs compared to GANs which can suffer from mode collapse.</p>
  </li>
  <li>
    <p><strong>Flexible Architecture</strong>: The multi-step denoising process enables additional functionalities like inpainting or image-to-image generation by manipulating the input noise, without requiring architectural changes.</p>
  </li>
  <li>
    <p><strong>Consistent Quality</strong>: The gradual denoising process is more robust and consistent compared to GANs where quality can vary significantly between samples.</p>
  </li>
</ol>

<p>The main trade-off is generation speed - diffusion models require multiple neural network passes to generate samples, making them slower than single-pass models like GANs and VAEs. However, various sampling optimization techniques have been developed to significantly reduce this computational overhead.</p>

<h3 id="disadvantages-of-diffusion-models">Disadvantages of Diffusion Models</h3>

<p>While diffusion models have significant advantages, they also come with some trade-offs:</p>

<ul>
  <li>Slow Sampling: The reverse process requires multiple denoising steps, making inference slower compared to GANs.</li>
  <li>Compute Intensive: Training requires large amounts of data and computational power.</li>
  <li>Memory Usage: They require storing multiple intermediate noise distributions, making them more memory-intensive.</li>
  <li>Complex Implementation: The multi-step nature of diffusion models makes them more complex to implement compared to single-step models.</li>
</ul>

<h2 id="mathematical-foundation">Mathematical Foundation</h2>

<p>Diffusion models are built on a deep interplay between <strong>differential equations</strong>, <strong>probability theory</strong>, and <strong>variational inference</strong>. To understand why the model works, we need to trace how these ideas connect: from describing how systems evolve over time, to modeling probability densities, to designing trainable objectives.</p>

<h3 id="differential-equations-odes-and-sdes">Differential Equations: ODEs and SDEs</h3>

<p>We start with <strong>ordinary differential equations (ODEs)</strong>, which describe how a system changes deterministically over time based on its current state.</p>

\[\frac{dx}{dt} = f(x, t)\]

<p>where \(x(t)\) is the state of the system - the function we want to solve -and \(t\) is time. \(f(x, t)\) defines how \(x\) changes over time.</p>

<p>This is a useful starting point, but in real-world data generation, we must account for <strong>randomness</strong>. That brings us to <strong>stochastic differential equations (SDEs)</strong>, which incorporate random fluctuations into the system.</p>

\[dx = f(x, t) dt + g(x, t) dW_t\]

<p>where the <em>drift term</em> \(f(x, t) dt\) captures the deterministic trends, while the <em>diffusion term</em> \(g(x, t) dW_t\) captures the random fluctuations via a Wiener process \(W_t\).</p>

<p>👉 Motivation: Diffusion models inject noise step by step, so SDEs provide the natural language to describe this stochastic corruption process. More specifically, the drift term \(f(x, t)\) is the shift of the mean of the distribution, and the diffusion term \(g(x, t)\) is the spread of the distribution - injecting Gaussian noise.</p>

<h3 id="forward-and-reverse-diffusion-processes">Forward and Reverse Diffusion Processes</h3>

<p><strong>Forward Process (Adding Noise)</strong></p>

<p>The forward diffusion process transform a data sample \(x_0\) into pure noise \(x_T\) over time:</p>

\[dx = f(x, t)dt + g(t) dW_t\]

<p>Intuitively, the drift term \(f(x, t) dt\) shifts the mean of the distribution by a deterministic amount \(f(x,t)\) (i.e., is a function of \(x\) and current time \(t\)) to a zero mean distribution. The diffusion term \(g(t) dW_t\) spreads the distribution by injecting Gaussian noise, increasing the variance of the distribution.</p>

<p>Note that \(g(t)\) is a function of current time \(t\) only, to guarantee that each noised distribution remains Gaussian with a know mean and variance. This make the forward process to be simple and tractable, that we can have exact sampling formula for each specific time step \(t\), i.e., \(q (x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)\).</p>

<p><strong>Reverse Process (Removing Noise)</strong>
In order to generate data from pure noise \(x_T\), we need to reverse the diffusion process by Reverse-Time SDE (Anderson 1982).</p>

\[dx = \left[ f(x,t) - g^2(t) \nabla_x \log p_t(x) \right] dt + g(t) d\tilde{W}_t\]

<p>where \(\nabla_x \log p_t(x)\) is the <strong>score function</strong>, which estimates the structure of data at time \(t\) - how likely different data points are at each step. \(d\tilde{W}_t\) is another Wiener process but in the reverse direction.</p>

<p>👉 Motivation: Since \(f(x,t)\) and \(g(t))\) are known, to reverse noise, we must know the score function. So we train a neural network to approximate the score function \(\nabla_x \log p_t(x)\). This is the core of the diffusion model.</p>

<h3 id="fokker-planck-equation-from-trajectories-to-distributions">Fokker-Planck Equation: From Trajectories to Distributions</h3>

<p>SDEs describe how the distribution of a system changes over time, but what about the <strong>distribution of data</strong> as noise accumulates? The <strong>Fokker-Planck equation</strong> bridges the gap between trajectories and distributions, explaining how noise pushes data distributions \(p_t(x)\) toward isotropic Gaussian distributions.</p>

\[\frac{\partial p_t(x)}{\partial t} = -\nabla_x \cdot (f(x,t) p_t(x)) + \frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\]

<p>where \(p_t(x)\) is the distribution of the data at time \(t\).</p>

<p>The first term \(-\nabla_x \cdot (f(x,t) p_t(x))\) describes the change of the probability density \(p_t(x)\) with the drift term \(f(x,t)\) (as the velocity of that mass). The divergence operator \(\nabla_x \cdot\) measures how much the mass is spreading out (positive divergence) or converging/concentrating (negative divergence) at any given point \(x\). The whole term \(- \nabla_x \cdot (f(x,t) p_t(x))\) describes a <strong>rate of change</strong> of the probability density \(p_t(x)\) at any given point \(x\), where the positive value means the mass is flowing away from \(x\), causing \(p_t(x)\) to decrease (hence the negative sign), and vice versa.</p>

<p>The second term \(\frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\) presents the spreading and smoothing effect of the probability density \(p_t(x)\) over time due to the influence of the random fluctuations. More specifically, \(g(t)\) controls the magnitude of the random noise. The \(\nabla_x p_t(x)\) term describes the <strong>steepness</strong> or slope of \(p_t(x)\) at any given point \(x\). The whole term \(\frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\) describes a <strong>rate of change</strong> of the probability density \(p_t(x)\), i.e., the larger the gradient, the more the density rising sharply around \(x\). Similar to the first term, \(\nabla_x \cdot\) measures how much the mass is spreading out (positive divergence) or converging/concentrating (negative divergence) at any given point \(x\) with two differences:</p>

<ul>
  <li>It contains the random fluctuations term \(g(t)^2\) instead of the drift term \(f(x,t)\), introducing randomness into the system.</li>
  <li>It proportional to the gradient \(\nabla_x p_t(x)\), meaning that the <strong>slope/sharp region is more affected/spread out than the flat region</strong> (which has smaller gradient \(\nabla_x p_t(x)\)).</li>
</ul>

<h3 id="score-matching-and-denoising">Score Matching and Denoising</h3>

<p>Since the reverse SDE depends on the score function \(\nabla_x \log p_t(x)\) (which is intractable), we need to design a training objective to approximate this function. Vincent et al. proposed <strong>denoising score matching</strong> <a href="https://ieeexplore.ieee.org/abstract/document/6795935/">(Vincent 2011)</a> to approximate by training a neural network \(s_{\theta}(x_t, t)\) to approximate the conditional score function \(\nabla_x \log q(x_t \mid x_0, \epsilon)\) (where \(q\) is a tractable forward process, \(dx = f(x, t)dt + g(t)dW_t\)) <strong>assuming</strong> that \(q(x_t \mid x_0) \approx p_t(x_t)\) at time \(t\) (which is a reasonable assumption).</p>

<p>This objective aims to minimize the difference:</p>

\[\mathbb{E}_{p(x_0), \epsilon \sim \mathcal{N}(0, I)} \left[ \left\| s_{\theta}(x_t, t) - \nabla_x \log p_t(x_t \mid x_0, \epsilon) \right\|^2 \right]\]

<h3 id="variational-perspective-and-kl-minimization">Variational Perspective and KL Minimization</h3>

<p>Another way to frame diffusion models is to consider them as a variational inference problem. The forward process \(q(x_{0:T})\) is a know noising chain of distributions, and the reverse process \(p_{\theta}(x_{0:T})\) is learned. Therefore, we can use the variational lower bound (ELBO) to train the model.</p>

\[\mathbb{E}_{q(x_{0:T})} \left[ D_{KL} \left( q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) \right]\]

<p><strong>ELBO</strong></p>

<p><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound"><strong>Evidence lower bound (ELBO)</strong></a> is a key concept in variational inference, which is used in VAEs to approximate the log-likelihood of the data.</p>

<p>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the marginal distribution of \(X\), and \(p_\theta(Z \mid X)\) is the conditional distribution of \(Z\) given \(X\). Then, for a sample \(x \sim p_{\text{data}}\), and any distribution \(q_\phi\), the ELBO is defined as</p>

\[L(\phi, \theta; x) := \mathbb{E}_{z\sim q_\phi(\cdot|x)} \left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>The ELBO can equivalently be written as</p>

\[\begin{aligned}
L(\phi, \theta; x) &amp;= \mathbb{E}_{z\sim q_\phi(\cdot|x)}[\ln p_\theta(x,z)] + H[q_\phi(z \mid x)] \\
&amp;= \ln p_\theta(x) - D_{KL}(q_\phi(z \mid x) || p_\theta(z \mid x)).
\end{aligned}\]

<p>In the first line, \(H[q_\phi(z \mid x)]\) is the entropy of \(q_\phi\), which relates the ELBO to the Helmholtz free energy. In the second line, \(\ln p_\theta(x)\) is called the evidence for \(x\), and \(D_{KL}(q_\phi(z \mid x) \mid\mid p_\theta(z \mid x))\) is the Kullback-Leibler divergence between \(q_\phi\) and \(p_\theta\). Since the Kullback-Leibler divergence is non-negative, \(L(\phi, \theta; x)\) forms a lower bound on the evidence (ELBO inequality)</p>

\[\ln p_\theta(x) \geq \mathbb{E}_{z\sim q_\phi(\cdot|x)}\left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>Deep-dive topics about VAE might including:</p>

<ul>
  <li>Reparameterization Trick: <a href="https://en.wikipedia.org/wiki/Reparameterization_trick">How to sample from a distribution in a differentiable way - Wiki</a></li>
  <li>The problem of KL divergence: <a href="https://andrewcharlesjones.github.io/journal/klqp.html">mode seeking vs mode covering</a> by Andy Jones</li>
  <li><a href="https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae">A nice property of VAEs: Disentanglement Representation Learning</a></li>
</ul>

<h3 id="tweedies-formula">Tweedie’s formula</h3>

<p>Finally, <strong>Tweedie’s formula</strong> gives a neat probabilistic justification for the denoising score matching objective:</p>

\[\mathbb{E} [ x_0 \mid x_t] = x_t + \sigma_t^2 s_{\theta}(x_t, t)\]

<p>where \(s_{\theta}(x_t, t)\) is the score function to be learned by the neural network. It shows that the posterior mean of clean data given a noisy data is just the noisy sample plus a correction term proportional to the score function.</p>

<p>In some papers such as ESD (Gandikota et al. 2023), where we need to fine-tune the pretrained model and match the score function of the original and the fine-tuned model, they use Tweedie’s formula to justify the matching term.</p>

<h2 id="variants-of-diffusion-models">Variants of Diffusion Models</h2>

<p>The original formulation of diffusion models can be implemented in several ways. Two of the most influential variants are <strong>Denoising Diffusion Probabilistic Models (DDPMs)</strong> and <strong>Denoising Diffusion Implicit Models (DDIMs)</strong>. Both share the same forward noising process but differ in how they perform the reverse (denoising) process during inference.</p>

<h3 id="ddpm">DDPM</h3>

<p>DDPM (Ho et al. 2020) is the classic diffusion model:</p>

<ul>
  <li>The Reverse process is defined as a <strong>Markov chain</strong>, where each step \(x_t \to x_{t-1}\) involves sampling from a Gaussian distribution conditioned on \(x_t\).</li>
  <li>Sampling os stochastic, even with the same starting noise \(x_T\), the generated data \(x_0\) is different.</li>
  <li>While highly effective and stable (compared to GANs), DDPMs require hundreds to thousands of steps to slowly add/remove noise, which makes inference slow.</li>
</ul>

<p>Read more about DDPM in another blog post <a href="/blog/2023/diffusion-tutorial/">here</a></p>

<h3 id="ddim">DDIM</h3>

<p>DDIM (Song et al. 2020) builds on DDPM but introduces a <strong>non-Markovian reverse process</strong>, enabling faster sampling.
It also allows us to use the same training process as DDPM, e.g., we can use pretrained DDPM models to generate data.</p>

<p>The sampling process of DDIM is as follows:</p>

\[x_{t-1} = \sqrt{\alpha_{t-1}} \left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right) + \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}(x_t) + \sigma_t\epsilon_t\]

<p>where the first term represents the “predicted \(x_0\)”, the second term is the “direction pointing to \(x_t\)”, and the last term is random noise.</p>

<p>By setting \(\sigma_t = 0\) for all \(t\), DDIM becomes a deterministic process given \(x_{t-1}\) and \(x_0\), except for \(t=1\). In other words, the intermediate steps \(x_{T-1}, x_{T-2}, \ldots, x_1\) are deterministic given starting noise \(x_T\).</p>

<p>Read more about DDIM in another blog post <a href="/blog/2023/diffusion-tutorial-p2/">here</a></p>

<h2 id="flow-matching">Flow Matching</h2>

<h3 id="fundammentals-concepts-in-flow-matching">Fundammentals Concepts in Flow Matching</h3>

<p><strong>Normalizing Flow</strong>: A class of generative models that learns a transformation (or “flow”) to map a know prior distribution \(p_0\) to a target distribution \(p_1\) through a family of intermediate marginal distributions \(p_t\), where \(t \in [0, 1]\). A key requirement is that the transformation must be invertible (bijective).</p>

<p><strong>Continuous Normalizing Flow</strong>: Uses ordinary differential equation (ODE) to define continuous-time transformations between distributions.</p>

<p><strong>Flow and Velocity Field</strong>:</p>

<ul>
  <li>The flow \(\psi_t(x)\) describes the trajectory of a point \(x\) over time.</li>
  <li>The velocity field \(u_t(x)\) specifies the instantaneous direction and speed of movement</li>
  <li>These are related by the ODE: \(\frac{d}{dt} \psi_t(x) = u_t (\psi_t (x))\)</li>
  <li>The induced density \(p_t(x)\) evolves according to the continuity equation: \(\frac{\partial p_t(x)}{\partial t} + \nabla_x \cdot \big( u_t(x) \, p_t(x) \big) = 0.\)</li>
</ul>

<p>This equation shows how a point moves along the flow path: \(x_t \rightarrow x_{t+1} = x_t + dt * u_t(x_t)\) at time \(t\).</p>

<p><strong>Key insight</strong>: The velocity field \(u_t(x)\) is the only component neccessary to sample from \(p_t\) by solving the ODE. Therefore, <strong>flow matching aims to learn the velocity field \(u_t(x)\)</strong>.</p>

<h3 id="derivation-of-the-flow-matching-objective">Derivation of the Flow Matching Objective</h3>

<p><strong>Starting Objective</strong>: Approximate the velocity field \(u_t(x)\) with the learned velocity field \(v_{\theta}(t, x)\).</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t) \|^2 \right]\]

<p><strong>Step 1</strong>: Expand the squared norm:</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t) \|^2 \right] = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) \|^2 - 2 \langle v_{\theta}(t, x_t), u_t(x_t) \rangle + \| u_t(x_t) \|^2 \right]\]

<p><strong>Step 2</strong>: Express the velocity field as a conditional expectation:</p>

\[u_t(x_t) = \int u_t(x_t \mid x_1) \frac{p_t (x_t \mid x_1) q(x_1)}{p_t(x_t)} dx_1\]

<p><strong>Interpretation</strong>: The velocity at \(x_t\) is a weighted average of conditional velocities \(u_t(x_t \mid x_1)\) from all possible data points \(x_1\). Point \(x_1\) that are “closer” to \(x_t\) (higher probability \(p_t (x_t \mid x_1)\)) contribute more to the velocity at \(x_t\).</p>

<p><strong>Step 3</strong>: Substitute into the cross-term expectation (correlation between \(v_{\theta}(t, x_t)\) and \(u_t(x_t)\))</p>

\[\mathbb{E}_{x_t \sim p_t(x)} \left[ \langle v_{\theta}(t, x_t), u_t(x_t) \rangle \right] = \int p_t(x_t) v_{\theta}(t, x_t) \cdot u_t(x_t) dx_t\]

<p>Substitute \(u_t(x_t)\) to the above equation:</p>

\[= \int \int v_{\theta}(t, x_t) \cdot u_t(x_t \mid x_1) \cdot p_t(x_t \mid x_1) \cdot q(x_1) dx_1 dx_t\]

\[= \mathbb{E}_{x_t \sim p_t(x_t \mid x_1), x_1 \sim q(x_1)} \left[ v_{\theta}(t, x_t) \cdot u_t(x_t \mid x_1) \right]\]

<p><strong>Step 4</strong>: Rewrite the full objective using conditional expectation:</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) \|^2 - 2 \langle v_{\theta}(t, x_t), u_t(x_t \mid x_1) \rangle + \| u_t(x_t) \|^2 \right]\]

<p><strong>Step 5</strong>: Add and subtract the term \(\| u_t(x_t \mid x_1) \|^2\)</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t \mid x_1) \|^2 \right] + \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| u_t(x_t) \|^2 - \| u_t(x_t \mid x_1) \|^2\right]\]

<p><strong>Step 6</strong>: Drop constant terms that are independent of \(\theta\):</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t \mid x_1) \|^2 \right]\]

<p><strong>Practical Implementation</strong>:</p>

<p>Simply choose the linear interpolation path \(X_t = (1 - t) X_0 + t X_1\), then the velocity field \(u_t(x_t)\) is:</p>

\[u_t(x_t \mid x_1) = \frac{d}{dt} X_t = X_1 - X_0\]

<p>This give us a tractable training objective where we sample:</p>

<ul>
  <li>A time step \(t \sim \mathcal{U}(0, 1)\)</li>
  <li>A data ppont \(x_1 \sim q(x_1)\)</li>
  <li>A noise sample \(x_0 \sim \mathcal{N}(0, I)\)</li>
  <li>Construct the interpolated sample \(x_t = (1 - t) x_0 + t x_1\)</li>
  <li>Train to predict the velocity field \(v_{\theta}(t, x_t) = x_1 - x_0\)</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-02-11-15-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-02-10-03-19.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h3 id="how-to-sampling-from-flow-matching-model">How to sampling from Flow Matching model</h3>

<p>The sampling process of Flow Matching model is similar to the diffusion model, where we start from a noise sample \(x_0 \sim \mathcal{N}(0, I)\) and iteratively sample the next step \(x_{t+dt}\) by Euler method:</p>

\[x_{t+dt} = x_t + dt * v_{\theta}(t+dt/2, x_t+dt/2 * v_{\theta}(t, x_t))\]

<p>where \(v_{\theta}(t, x_t)\) is the velocity field predicted by the neural network.</p>

<h3 id="flow-matching-code-example">Flow Matching Code Example</h3>

<p><a href="https://github.com/facebookresearch/flow_matching/blob/main/examples/standalone_flow_matching.ipynb">A Standalone Flow Matching code</a> - from [4]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Define the Flow
</span><span class="k">class</span> <span class="nc">Flow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_start</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_end</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_start</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_t</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="nf">self</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="nf">self</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">flow</span> <span class="o">=</span> <span class="nc">Flow</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">flow</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="nf">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x_1</span>
    <span class="n">dx_t</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="nf">loss_fn</span><span class="p">(</span><span class="nf">flow</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">),</span> <span class="n">dx_t</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="c1"># Sampling
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">t_start</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">t_end</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p>In the above code, the <code class="language-plaintext highlighter-rouge">forward</code> function is for the velocity field \(v_{\theta}(t, x)\), and the <code class="language-plaintext highlighter-rouge">step</code> function is to get the next step \(X_{t+dt}\) from the current step \(X_t\) by Euler method</p>

\[X_{t+dt} = X_t + dt * v_{\theta}(t+dt/2, X_t+dt/2 * v_{\theta}(t, X_t))\]

<h3 id="conditional-flow-matching">Conditional Flow Matching</h3>

<p>(Note that the <strong>Conditional</strong> in the name of Conditional Flow Matching is meaning the condition \(c\) is given, not the conditional vector field \(u_t(x \mid x_1)\) from previous step)</p>

<p>In conditional flow matching, we incorporate a condition \(c\) into the velocity field \(v_{\theta}(t, x, c)\).
In practice, the three inputs \(t, x, c\) are concatenated together as the input to the neural network.</p>

<p>Sampling function:</p>

\[X_{t+dt} = X_t + dt * v_{\theta}(t+dt/2, X_t+dt/2 * v_{\theta}(t, X_t, c), c)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Define the Flow
</span><span class="k">class</span> <span class="nc">Flow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_start</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_end</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_start</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_t</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="nf">self</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="nf">self</span><span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">flow</span> <span class="o">=</span> <span class="nc">Flow</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">flow</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">x_1</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nf">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x_1</span>
    <span class="n">dx_t</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="nf">loss_fn</span><span class="p">(</span><span class="nf">flow</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">),</span> <span class="n">dx_t</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Sampling
# --- evaluation / visualisation section --------------------------
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">256</span>                    

<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">x</span>      <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>     <span class="c1"># (n_samples, 2)
</span>
<span class="c1"># if you just want random labels –– otherwise load real labels here
</span><span class="n">c_eval</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (n_samples, 1)
</span>
<span class="c1"># colours for the scatter (same length as x)
</span><span class="n">colors</span>  <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span> <span class="k">if</span> <span class="n">lbl</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">'</span><span class="s">orange</span><span class="sh">'</span> <span class="k">for</span> <span class="n">lbl</span> <span class="ow">in</span> <span class="n">c_eval</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()]</span>

<span class="c1"># -----------------------------------------------------------------
</span><span class="n">n_steps</span>      <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_every</span>   <span class="o">=</span> <span class="mi">20</span>
<span class="n">plot_indices</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">plot_every</span><span class="p">))</span>
<span class="k">if</span> <span class="n">plot_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_steps</span><span class="p">:</span>
    <span class="n">plot_indices</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span>   <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">plot_indices</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">plot_indices</span><span class="p">),</span> <span class="mi">4</span><span class="p">),</span>
                           <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">time_steps</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># initial frame
</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">plot_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>                         <span class="c1"># no gradients while sampling
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                      <span class="n">t_start</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                      <span class="n">t_end</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">c_eval</span><span class="p">)</span>               <span class="c1"># 2️⃣ use the same‑sized label tensor
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">in</span> <span class="n">plot_indices</span><span class="p">:</span>
            <span class="n">plot_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<p>References:</p>

<ul>
  <li>[1]<a href="https://arxiv.org/abs/2210.02747">Flow Matching for Generative Modeling</a> paper</li>
  <li>[2]<a href="https://youtu.be/7cMzfkWFWhI?si=rRnZQKxs9p-_zjhZ">A cool explanation of Flow Matching</a></li>
  <li>[3]<a href="https://diffusionflow.github.io/">Diffusion Meets Flow Matching: Two Sides of the Same Coin</a></li>
  <li>[4]<a href="https://neurips.cc/media/neurips-2024/Slides/99531.pdf">A NeurIPS 2024 tutorial on Flow Matching</a></li>
</ul>

<h2 id="differences-between-score-matching-ddpm-and-flow-matching">Differences between Score Matching, DDPM and Flow Matching</h2>

<h3 id="summary-of-main-differences">Summary of Main Differences</h3>

<p>All three methods learn generative models by establishing a connection between a simple noise distribution and a complex data distribution, but they differ fundamentally in their formulation and training approach:</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Diffusion Models (DDPM/DDIM)</th>
      <th>Score Matching (SDE)</th>
      <th>Flow Matching (CFM)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Core Learning Target</strong></td>
      <td>Learn to predict noise \(\epsilon_t\) or data \(x_t\) from previous step \(x_{t-1}\)</td>
      <td>Learn score function \(\nabla_x \log p_t(x)\)</td>
      <td>Learn velocity field \(u_t(x)\)</td>
    </tr>
    <tr>
      <td><strong>Process Type</strong></td>
      <td>Discrete Markov chain</td>
      <td>Continuous SDE</td>
      <td>Continuous ODE</td>
    </tr>
    <tr>
      <td><strong>Forward Process</strong></td>
      <td>Add Gaussian noise step-by-step</td>
      <td>Stochastic diffusion (SDE with drift + noise)</td>
      <td>Deterministic interpolation path</td>
    </tr>
    <tr>
      <td><strong>Backward Process</strong></td>
      <td>Reverse Markov chain</td>
      <td>Reverse SDE</td>
      <td>ODE integration</td>
    </tr>
    <tr>
      <td><strong>Tractability</strong></td>
      <td>Forward process tractable</td>
      <td>Forward SDE tractable</td>
      <td>Conditional paths tractable</td>
    </tr>
    <tr>
      <td><strong>Training Paradigm</strong></td>
      <td>Denoising autoencoder</td>
      <td>Denoising score matching</td>
      <td>Conditional flow matching</td>
    </tr>
    <tr>
      <td><strong>Sampling</strong></td>
      <td>Iterative denoising (stochastic or deterministic)</td>
      <td>SDE/ODE integration</td>
      <td>ODE integration (typically straight paths)</td>
    </tr>
    <tr>
      <td><strong>Path Geometry</strong></td>
      <td>Curved noising trajectory</td>
      <td>Stochastic curved paths</td>
      <td>Straight/optimal transport paths</td>
    </tr>
    <tr>
      <td><strong>Key Advantage</strong></td>
      <td>Simple, well-understood</td>
      <td>Theoretically grounded, flexible</td>
      <td>Fast sampling, simple training</td>
    </tr>
  </tbody>
</table>

<p><strong>Relationship</strong>:</p>
<ul>
  <li>DDIM can be viewed as a discretization of a probability flow ODE derived from the Score Matching SDE</li>
  <li>Flow Matching with Gaussian probability paths recovers Score Matching formulations</li>
  <li>Flow Matching with linear interpolation paths gives the deterministic version similar to DDIM</li>
  <li>All three can be unified under a common framework of learning to transform distributions</li>
</ul>

<hr />

<h3 id="detailed-differences">Detailed Differences</h3>

<h4 id="1-time-convention">1. Time Convention</h4>

<p>Understanding time conventions is crucial for comparing these methods:</p>

<p><strong>Flow Matching:</strong></p>
<ul>
  <li><strong>Continuous time</strong> \(t \in [0, 1]\)</li>
  <li>\(t = 0\): <strong>noise</strong> distribution \(p_0(x) = \mathcal{N}(0, I)\)</li>
  <li>\(t = 1\): <strong>data</strong> distribution \(p_1(x) = q(x)\)</li>
  <li>Forward in time moves from noise → data</li>
</ul>

<p><strong>Diffusion Models (DDPM, DDIM):</strong></p>
<ul>
  <li><strong>Discrete time</strong> with steps \(t \in \{0, 1, 2, ..., T\}\)</li>
  <li>\(t = 0\): <strong>data</strong> distribution \(q(x_0)\)</li>
  <li>\(t = T\): <strong>noise</strong> distribution \(\mathcal{N}(0, I)\)</li>
  <li>Forward in time moves from data → noise (opposite of Flow Matching!)</li>
  <li>To align with Flow Matching convention, we use \(r = T - t\), so:
    <ul>
      <li>\(r = 0\) corresponds to noise</li>
      <li>\(r = T\) corresponds to data</li>
    </ul>
  </li>
</ul>

<p><strong>Score Matching (SDE):</strong></p>
<ul>
  <li><strong>Continuous time</strong> \(t \in [0, T]\) (often \(T = 1\))</li>
  <li>\(t = 0\): <strong>data</strong> distribution \(p_0(x) = q(x)\)</li>
  <li>\(t = T\): <strong>noise</strong> distribution \(p_T(x) \approx \mathcal{N}(0, \sigma^2 I)\)</li>
  <li>Forward in time moves from data → noise</li>
  <li>Using \(r = T - t\) for consistency: \(r = 0\) is noise, \(r = T\) is data</li>
</ul>

<hr />

<h4 id="2-forward-process-vs-probability-paths">2. Forward Process vs. Probability Paths</h4>

<p><strong>Diffusion Models (DDPM, DDIM):</strong></p>

<p>The forward process progressively corrupts data by adding Gaussian noise through a <strong>Markov chain</strong>:</p>

\[q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]

<p>With reparameterization:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\]

<p>where \(\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)\).</p>

<ul>
  <li><strong>Discrete steps</strong>: Each transition is a single Gaussian convolution</li>
  <li><strong>Tractable</strong>: \(q(x_t \mid x_0)\) has closed form</li>
  <li><strong>Markovian</strong>: Each step depends only on the previous state</li>
</ul>

<p><strong>Score Matching (SDE):</strong></p>

<p>The forward process is a <strong>continuous stochastic differential equation (SDE)</strong>:</p>

\[dx = f(x, t)dt + g(t)dW_t\]

<p>where:</p>
<ul>
  <li>\(f(x, t)\) is the <strong>drift coefficient</strong> (deterministic component)</li>
  <li>\(g(t)\) is the <strong>diffusion coefficient</strong> (stochastic component)</li>
  <li>\(W_t\) is the <strong>Wiener process</strong> (Brownian motion)</li>
</ul>

<p>Common example (Variance Exploding - VE):
\(dx = 0 \cdot dt + \sqrt{\frac{d\sigma_t^2}{dt}} dW_t\)</p>

<p>Common example (Variance Preserving - VP):
\(dx = -\frac{1}{2}\beta_t x \, dt + \sqrt{\beta_t} dW_t\)</p>

<ul>
  <li><strong>Continuous time</strong>: Infinitesimal noise additions</li>
  <li><strong>Stochastic</strong>: Includes random Brownian motion</li>
  <li><strong>Non-Markovian</strong> in discrete time but Markovian in continuous time</li>
</ul>

<p><strong>Flow Matching:</strong></p>

<p>The forward process defines <strong>probability paths</strong> that interpolate between distributions:</p>

\[p_t(x) = \int p_t(x \mid x_1) q(x_1) dx_1\]

<p>Where the <strong>conditional probability path</strong> is often chosen as:</p>

\[p_t(x_t \mid x_1) = \mathcal{N}(x_t; \mu_t(x_1), \sigma_t^2(x_1) I)\]

<p>For <strong>linear interpolation</strong> (Optimal Transport path):
\(x_t = (1-t)x_0 + t x_1, \quad x_0 \sim \mathcal{N}(0, I)\)</p>

<p>This gives:
\(\mu_t(x_1) = t x_1, \quad \sigma_t = 1 - t\)</p>

<ul>
  <li><strong>Deterministic paths</strong> (no stochastic component in the ODE)</li>
  <li><strong>Conditional paths</strong> are tractable by design</li>
  <li><strong>Straight trajectories</strong> (shortest path in many metrics)</li>
</ul>

<p><strong>Connections:</strong></p>
<ul>
  <li>The <strong>velocity field</strong> \(u_t(x)\) in Flow Matching corresponds to the <strong>drift term</strong> \(f(x, t)\) in Score Matching</li>
  <li>The <strong>conditional probability path</strong> \(p_t(x_t \mid x_1)\) in Flow Matching corresponds to the SDE solution initialized at \(x_1\)</li>
  <li>The <strong>marginal probability path</strong> \(p_t(x)\) in Flow Matching corresponds to the SDE marginal when initialized from data \(x_0 \sim q(x)\)</li>
</ul>

<p><strong>Special Cases:</strong></p>
<ul>
  <li>Flow Matching with <strong>Gaussian probability paths</strong> (with appropriate \(\mu_t, \sigma_t\)) recovers the forward SDE from Score Matching</li>
  <li>Flow Matching with <strong>linear interpolation</strong> gives the deterministic probability flow ODE, similar to DDIM</li>
</ul>

<hr />

<h4 id="3-training-objective">3. Training Objective</h4>

<p><strong>Diffusion Models (DDPM):</strong></p>

<p>Train a neural network to predict the noise added in the forward process:</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon_\theta(x_t, t) - \epsilon \|^2 \right]\]

<p>where \(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\) (<strong>\(\epsilon\)-prediction</strong>).</p>

<p>Alternative formulation (<strong>\(x_0\)-prediction</strong>):</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \hat{x}_\theta(x_t, t) - x_0 \|^2 \right]\]

<ul>
  <li><strong>Target</strong>: Noise \(\epsilon\) or clean data \(x_0\)</li>
  <li><strong>Simple</strong>: Direct regression on known quantities</li>
  <li><strong>Weighted MSE</strong>: Can add time-dependent weighting</li>
</ul>

<p><strong>Score Matching (DSM - Denoising Score Matching):</strong></p>

<p>The reverse SDE requires the <strong>score function</strong> \(\nabla_x \log p_t(x)\), which is intractable. Vincent (2011) proposed training a network \(s_\theta(x_t, t)\) to approximate the <strong>conditional score</strong>:</p>

\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t, x_0, x_t \mid x_0} \left[ \| s_\theta(x_t, t) - \nabla_{x_t} \log q(x_t \mid x_0) \|^2 \right]\]

<p>Under the assumption that \(q(x_t \mid x_0) \approx p_t(x_t)\) (reasonable for small noise), this approximates the true score.</p>

<p>For Gaussian perturbations \(q(x_t \mid x_0) = \mathcal{N}(\alpha_t x_0, \sigma_t^2 I)\):</p>

\[\nabla_{x_t} \log q(x_t \mid x_0) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2} = -\frac{\epsilon}{\sigma_t}\]

<p>So the objective becomes:</p>

\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \left\| s_\theta(\alpha_t x_0 + \sigma_t \epsilon, t) + \frac{\epsilon}{\sigma_t} \right\|^2 \right]\]

<ul>
  <li><strong>Target</strong>: Score function (gradient of log probability)</li>
  <li><strong>Theoretical</strong>: Grounded in score-based generative modeling theory</li>
  <li><strong>Flexible</strong>: Works with any forward SDE</li>
</ul>

<p><strong>Flow Matching (CFM):</strong></p>

<p>Train a network to predict the velocity field:</p>

\[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, x_1, x_t \mid x_1} \left[ \| v_\theta(t, x_t) - u_t(x_t \mid x_1) \|^2 \right]\]

<p>For linear interpolation \(x_t = (1-t)x_0 + t x_1\):</p>

\[u_t(x_t \mid x_1) = \frac{d x_t}{dt} = x_1 - x_0\]

<p>So:</p>

\[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, x_0, x_1} \left[ \| v_\theta(t, x_t) - (x_1 - x_0) \|^2 \right]\]

<ul>
  <li><strong>Target</strong>: Velocity (direction and magnitude of flow)</li>
  <li><strong>Direct</strong>: Straightforward regression on vector field</li>
  <li><strong>Efficient</strong>: Often requires fewer sampling steps</li>
</ul>

<p><strong>Equivalence:</strong></p>

<p>The training objectives are closely related through reparameterizations:</p>

<ul>
  <li><strong>Score to Noise</strong>: \(s_\theta(x_t, t) = -\frac{\epsilon_\theta(x_t, t)}{\sigma_t}\)</li>
  <li><strong>Velocity to Noise</strong>: \(v_\theta(t, x_t) = \frac{\alpha_t}{\sigma_t} \epsilon_\theta(x_t, t)\) (approximately, for certain schedulers)</li>
  <li>Score Matching with Gaussian paths is <strong>equivalent</strong> to Flow Matching with appropriate probability path parameterization</li>
</ul>

<hr />

<h4 id="4-sampling-process">4. Sampling Process</h4>

<p><strong>Diffusion Models:</strong></p>

<p><strong>DDPM (Stochastic Sampling):</strong></p>

<p>Iteratively denoise by reversing the forward Markov chain:</p>

\[x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z\]

<p>where \(z \sim \mathcal{N}(0, I)\) and \(\sigma_t\) is the noise variance.</p>

<ul>
  <li><strong>Stochastic</strong>: Adds noise at each step</li>
  <li><strong>Many steps</strong>: Typically 1000 steps (can be reduced with techniques)</li>
</ul>

<p><strong>DDIM (Deterministic Sampling):</strong></p>

<p>Use a deterministic update rule:</p>

\[x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{predicted } x_0} + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_\theta(x_t, t)\]

<ul>
  <li><strong>Deterministic</strong>: No added noise</li>
  <li><strong>Fewer steps</strong>: 10-50 steps often sufficient</li>
  <li><strong>Equivalent to probability flow ODE</strong></li>
</ul>

<p><strong>Score Matching:</strong></p>

<p><strong>Reverse-time SDE:</strong></p>

\[dx = [f(x, t) - g(t)^2 \nabla_x \log p_t(x)] dt + g(t) d\bar{W}_t\]

<p>where \(\bar{W}_t\) is a reverse-time Brownian motion.</p>

<ul>
  <li><strong>Stochastic</strong>: Includes diffusion term</li>
  <li><strong>Continuous</strong>: Integrated numerically (Euler-Maruyama, etc.)</li>
</ul>

<p><strong>Probability Flow ODE (Deterministic):</strong></p>

\[\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)\]

<ul>
  <li><strong>Deterministic</strong>: No stochastic component</li>
  <li><strong>Same marginals</strong> as the SDE</li>
  <li><strong>Flexible solvers</strong>: Use any ODE solver (Runge-Kutta, adaptive methods)</li>
</ul>

<p><strong>Flow Matching:</strong></p>

<p>Integrate the learned velocity field ODE:</p>

\[\frac{dx}{dt} = v_\theta(t, x)\]

<p>Starting from \(x_0 \sim \mathcal{N}(0, I)\) at \(t=0\), integrate to \(t=1\):</p>

<p><strong>References</strong></p>

<ul>
  <li>[1]<a href="https://neurips.cc/media/neurips-2024/Slides/99531.pdf">A NeurIPS 2024 tutorial on Flow Matching</a></li>
</ul>

<h2 id="noise-scheduling">Noise scheduling</h2>

<p>Noise scheduling in diffusion models refers to how noise is gradually added to data in the forward process and how it is removed in the reverse process. The choice of noise schedule significantly impacts the model’s performance, sample quality, and training efficiency.</p>

<p>We follow the DDIM convention, where \(0 &lt; \bar{\alpha}_t &lt; 1, \beta_t = 1 - \bar{\alpha}_t\) and \(\alpha_t = \prod_{i=1}^{t} \bar{\alpha}_i\) is the cumulative noise level at time \(t\), and \(\beta_t\) is the noise level at time \(t\). With this convention, \(x_t = \sqrt(\alpha_t) x_0 + \sqrt(1-\alpha_t) \epsilon\), and \(\alpha_T \approx 0\) when \(t \rightarrow T\) while \(\alpha_0 \approx 1\) when \(t \rightarrow 0\).</p>

<p>Common principles of noise scheduling:</p>

<ul>
  <li>Add large amount of noise at \(t\) large while small amount of noise at \(t\) small. \(t=0\) means clean data, \(t=T\) means pure noise.</li>
  <li>The speed of change (acceleration, or \(\frac{d\beta_t}{dt}\)) should also has some proper speed (but I am not sure :D)</li>
</ul>

<p>Common noise schedules:</p>

<ul>
  <li><strong>Linear</strong>: \(\alpha_t = \frac{t}{T}\) or \(\beta_t = \beta_{\min} + (\beta_{\max} - \beta_{\min})\frac{t}{T}\). Issue: early timesteps do not add enough noise, and late timesteps can add too much noise.</li>
  <li><strong>Cosine</strong>: \(\beta_t = \beta_{\min} + 0.5 (\beta_{\max} - \beta_{\min}) ( 1 + \cos(\frac{t}{T} \pi))\). Intuition is that adding more gradually at the start and <strong>faster at the end</strong>.</li>
  <li><strong>Exponential</strong>: \(\beta_t = \beta_{\max} (\beta_{\min} / \beta_{\max})^{\frac{t}{T}}\)</li>
</ul>

<h2 id="guidanced-diffusion">Guidanced Diffusion</h2>

<p>Resources:</p>

<ul>
  <li>A great blog from Sander Dieleman: <a href="https://sander.ai/2022/05/26/guidance.html">Guidance: a cheat code for diffusion models</a> and <a href="https://sander.ai/2023/08/28/geometry.html">the geometry of diffusion guidance</a>.</li>
</ul>

<p><strong>Why Guidance?</strong></p>

<p>Guidance is a method to control the generation process so that the ouput is sample from a conditional distribution \(p(x \mid y)\), where \(y\) is a condition - such as a text prompt - rather than a generic \(p(x)\).</p>

<h3 id="classifier-guidance">Classifier Guidance</h3>

<p>In order to get the conditional score function \(\nabla_x \ln p(x \mid y)\), we can use Bayes rule to decompose the score function into an unconditional component and a conditional one:</p>

\[p(x \mid y) = \frac{p(y \mid x) p(x)}{p(y)}\]

\[\log p(x \mid y) = \log p(y \mid x) + \log p(x) - \log p(y)\]

\[\nabla_x \log p(x \mid y) = \nabla_x \log p(y \mid x) + \nabla_x \log p(x) - \nabla_x \log p(y)\]

<p>where \(\nabla_x \log p(x)\) is the score function of the unconditional model. \(\nabla_x \log p(y) = 0\) since \(p(y)\) is independent of \(x\).</p>

<p>The term \(\nabla_x \log p(y \mid x)\) means the direction pointing to \(y\) given \(x\).</p>

<ul>
  <li>In the begining of the inference process, i.e., large \(t\), when \(x_t\) still has a lot of noise, \(\nabla_x \log p(y \mid x)\) is close to \(0\), means that there is no clear information of \(y\).</li>
  <li>In the later stages, i.e., small \(t\), when \(x_t\) is less noisy and closer to \(x_0\), \(\nabla_x \log p(y \mid x)\) is larger, means that \(x_t\) has more information of \(y\), i.e., larger \(p(y \mid x)\).</li>
</ul>

<p><strong>How to obtain \(\nabla_x \log p(y \mid x)\)?</strong></p>

<p>\(p(y \mid x)\) means the probability of a condition \(y\) given \(x\).
In a simple case, where \(y\) is just a image class, like a <code class="language-plaintext highlighter-rouge">cat</code>, the probability \(p(y=\text{cat} \mid x)\) can be simply obtained from a pre-trained classifier.</p>

<p>However, in a more complex case, where \(y\) is a text prompt like a black cat with red eyes and blue fur, a pre-trained classifier is not expressive enough, i.e., it cannot distinguish between \(y_1\) <code class="language-plaintext highlighter-rouge">a black cat with red eyes and blue fur</code> vs \(y_2\) <code class="language-plaintext highlighter-rouge">a white cat with blue eyes and red fur</code> or mathematically \(p(y_1 \mid x) \neq p(y_2 \mid x)\).</p>

<p>In other words, the quality - diversity of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\). For example:</p>

<ul>
  <li>If \(p_\phi(y \mid x)\) is a binary classifier <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code>, then output image \(x \sim p_\theta(x \mid y)\) can be either <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code> only, even \(p_\theta(x)\) was trained from a massive dataset with many more classes rather than just two classes.</li>
  <li>If you want to generate an image \(x\) from a complex prompt \(y\), you need a powerful model like CLIP as the conditional model \(p_\phi(y \mid x)\).</li>
</ul>

<p>To balance between the specificity (i.e., high \($p(y \mid x\))) and diversity/quality (i.e., \(p(x \mid y) \approx p(x)\)), we use a guidance scale \(\gamma\) to control the trade-off between the two.</p>

\[\nabla_x \log p(x \mid y) =  \nabla_x \log p(x) + \gamma \nabla_x \log p(y \mid x)\]

<p>where \(\gamma\) is the guidance scale. A big \(\gamma\) means the model is less creative but more following the condition \(y\).</p>

<h3 id="classifier-free-guidance">Classifier-free Guidance</h3>

<p>The main limitation of the above approach is that the quality of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\).</p>

<p>If your model \(p(x)\) was trained on Image-Net dataset, but you want to generate an CT-scan medical image, then even with a powerful conditional model \(p(y \mid x)\), you will not get that.</p>

<p>The idea of classifier-free guidance cames from the <strong>Bayes Classifier</strong> - if you have trained a powerful unconditional generative model \(p(x)\) then you can use it as a classifier \(p(y \mid x)\) as follows:</p>

\[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\]

<h2 id="latent-diffusion">Latent Diffusion</h2>

<h2 id="conditional-diffusion">Conditional Diffusion</h2>

<h3 id="control-net">Control-Net</h3>

<h3 id="image-prompt">Image Prompt</h3>

<p>Beyond controlling the generation process using text prompt, there is a hot topic in the community to control using image information/layout/prompt - which has a huge potential in applications, e.g., image inpainting, image-to-image generation, etc. In the standard Stable Diffusion, the condition embedding \(c_t\) is just a text embedding \(c_t = E_t(y)\) where \(y\) is the text prompt and \(E_t\) is a pre-trained text encoder such as CLIP.
IP-Adapter [1] proposes to use an additional image encoder to extract the image embedding from a reference image \(c_i = E_i(x)\) and then project it into the original condition space.
The objective function for IP-Adapter is:</p>

\[\mathcal{L}_{IP} = \mathbb{E}_{z, c, \epsilon, t} \left[ \mid \mid \epsilon - \epsilon_\theta(z_t \mid c_i, c_t, t) \mid \mid_2^2 \right]\]

<p>The cross-attention layer is also modified from the one in Stable Diffusion to include the image embedding \(c_i\) as a condition.</p>

\[\text{Attention}(Q, K_i, K_t, V_i, V_t) = \lambda \text{softmax}\left(\frac{QK_i^T}{\sqrt{d}} + c_i\right)V_i + \text{softmax}\left(\frac{QK_t^T}{\sqrt{d}}\right)V_t\]

<p>where \(Q=z W_Q\), \(K_i = c_i W_K^i\), \(K_t = c_t W_K^t\), \(V_i = c_i W_V^i\), \(V_t = c_t W_V^t\), and \(W_Q\), \(W_K^i\), \(W_K^t\), \(W_V^i\), \(W_V^t\) are the weights of the linear layers.
The model becomes the original Stable Diffusion when \(\lambda = 0\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-03-20-07-02-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2308.06721">IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</a></li>
  <li>[2] <a href="https://openreview.net/pdf?id=PJqP0wyQek#page=1.93">MS-DIFFUSION: MULTI-SUBJECT ZERO-SHOT IMAGE PERSONALIZATION WITH LAYOUT GUIDANCE</a></li>
</ul>

<h2 id="diffusion-transformers">Diffusion Transformers</h2>

<p>The Diffusion Transformers (DiTs) is a class of diffusion models that replace the traditional U-Net convolutional architecture with a Vision Transformer (ViT) architecture as a backbone.</p>

<h3 id="data-processing-in-dit">Data Processing in DiT</h3>

<p>Similar to Latent Diffusion model, the diffusion process in DiT is on the latent space. Therefore, the first step is using pre-trained convolutional Variational Autoencoder (VAE) as in LDM to convert the spatial input into the latent space (i.e., \(256 \times 256 \times 3\) to \(32 \times 32 \times 4\)).</p>

<p><strong>Patchifying</strong> converting the(latent) spatial input into a sequence of \(T\) tokens/patches, each of dimension \(d\), by linearly embedding each patch in the input with a linear layer.</p>

<p><strong>Positional Encoding</strong> the standard sinusoidal positional embeddings are added to the token embeddings to provide the model with the positional information.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-01-15-06-00.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Beside the visual tokens, the DiT also uses the <strong>conditional information</strong> such as timestep \(t\) and the textual prompt \(c\) associated with the input image. These information are added to the DiT block through a embedding layer.</p>

<h3 id="the-dit-architecture">The DiT Architecture</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-01-15-03-57.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>There are three types have been studied in the DiT paper including:</p>

<p><strong>In-Context Conditioning</strong> (The far right in the above figure) Append the vector embedding of \(t\) and \(c\) in the input sequence, <strong>treating them as additional visual tokens</strong>. This is similar to the <code class="language-plaintext highlighter-rouge">cls</code> tokens in ViT.</p>

<p><strong>Cross-Attention</strong> Concatenate the conditional embedding \(t\) and \(c\) into a length-two sequence, separate from the image token sequence. Then modify the cross-attention layer to inject these conditioning information into the visual path.</p>

<p><strong>Adaptive layer norm (adaLN) block</strong> Following the widespread success of Adaptive normalization layer in Diffusion with U-Net backbones, DiT also replaces the standard layer norm in transformer blocks with an adaptive layer norm. Rather than directly learn dimension-wise scaling and shift \(\gamma\) and \(\beta\), the adaLN block regresses them from the sum of the embedding of the conditioning information \(t\) and \(c\).</p>

<p><strong>adaLN-Zero block</strong> Prior work on ResNets has found that initializing each residual block as the identity function is beneficial. This version uses the same adaptive layer norm as the adaLN block but with zero initialization.</p>

<p><strong>Transformer Decoder</strong> After the final DiT block, we need to decode the sequence of image tokens into an output latent noise prediction and output diagonal covariance prediction (two outputs). This can be done by a standard linear layer with output dimension \(p \times p \times 2C\) where \(C\) is the number of channels of the image.</p>

<p>References:</p>

<ul>
  <li>DIT paper: <a href="https://arxiv.org/pdf/2212.09748">Scalable Diffusion Models with Transformers</a></li>
  <li>Official implementation: <a href="https://github.com/facebookresearch/DiT">https://github.com/facebookresearch/DiT</a></li>
</ul>

<h2 id="diffusion-flux">Diffusion Flux</h2>

<p>References:</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2507.09595v1">Demystifying Flux Architecture</a></li>
  <li>Flux official implementation: <a href="https://github.com/black-forest-labs/flux">https://github.com/black-forest-labs/flux</a></li>
</ul>

<h2 id="image-inpainting-with-diffusion-models">Image Inpainting with Diffusion Models</h2>

<h3 id="training-pipeline">Training Pipeline</h3>

<p><strong>Training data</strong> for inpainting is a combination of three components: <strong>original image</strong> as ground truth, <strong>masked image</strong> as input, and <strong>prompt</strong> as condition to provide the context of the missing region.</p>

<p>To ensure the model is robust, a variety of mask shapes and sizes can be used, including <strong>Rectangular</strong>, <strong>Free-form masks</strong>, and <strong>arbitrary shapes</strong></p>

<p><strong>Loss function</strong> for inpainting is a combination of <strong>pixel-wise reconstruction loss</strong> and <strong>perceptual loss</strong> (or <strong>Style loss</strong>). If using GANs, the <strong>adversarial loss</strong> is used to ensure the inpainted regions are perceptually realistic under the discriminator perspective.</p>

<h3 id="challenges-in-image-inpainting">Challenges in Image Inpainting</h3>

<p><strong>Semantic and Structural Consistency</strong>: A primary challenge for generative models is to fill in missing regions in a way that is not only visually plausible but also semantically and structurally consistent with the rest of the image.</p>

<p><strong>Semantic ambiguity</strong> means that the missing region can be filled in multiple ways, e.g., filling a gap in a street scene could be extending a road, adding a pedestrian, or a vehicle. Even when the input prompt is given, the task remains difficult, when <strong>concept leaking</strong> occurs, i.e., “a black cat on a white background” vs “a white cat on a black background”.</p>

<p><strong>Long-range dependency and global structure</strong>: is another significant hurdle. While generative models excel at local details, they can be struggling with the broader context, lighting, and perspective.</p>

<p><strong>Perceptual realism</strong>: is another key challenge. Even if the inpainted regions are visually consistent, they may not align with human perception. For example, an inpainting might produce unrealistic shadows or reflections or overly smooth, or having artificial artifacts.</p>

<p><strong>Large missing regions</strong>: The size of missing area is directly proportional to the difficulty of the task.</p>

<h2 id="accelerating-diffusion-models">Accelerating Diffusion Models</h2>

<h3 id="diffusion-distillation">Diffusion Distillation</h3>

<h3 id="rectified-diffusion">Rectified Diffusion</h3>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2209.03003">Flow straight and fast: Learning to generate and transfer data with rectified flow</a></li>
  <li>[2] <a href="https://arxiv.org/pdf/2403.03206">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a></li>
</ul>

<p>Rectified Flows define the forward process as straight paths between the data distribution and a standard normal distribution [2], i.e.,</p>

\[z_t = (1 - t) x_0 + t \epsilon\]

<p>where \(\epsilon\) is a standard normal random variable and \(t\) is the time step in [0, 1].</p>

<!-- mkdir -p assets/img/diffusion-foundation -->
<!-- mv _posts/2025-10-01-*.png assets/img/diffusion-foundation/ -->]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><summary type="html"><![CDATA[(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)]]></summary></entry><entry><title type="html">FineStyle - Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)</title><link href="https://tuananhbui89.github.io/blog/2025/finestyle/" rel="alternate" type="text/html" title="FineStyle - Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)" /><published>2025-02-28T00:00:00+11:00</published><updated>2025-02-28T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/finestyle</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/finestyle/"><![CDATA[<h2 id="finestyle-fine-grained-controllable-style-personalization-for-text-to-image-models-neurips-2024">FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-27-17-20-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-27-17-20-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-27-17-20-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-27-17-20-07.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h3 id="overview">Overview</h3>

<ul>
  <li><a href="https://openreview.net/pdf?id=1SmXUGzrH8">FineStyle paper</a></li>
  <li><a href="https://github.com/SHI-Labs/FineStyle">FineStyle Github</a></li>
</ul>

<p>The FineStyle method proposed in the paper addresses the content leakage problem in few-shot or one-shot fine-tuning by introducing concept-oriented data scaling, which decomposes a single reference image into multiple sub-image-text pairs, each focusing on different fine-grained concepts. This approach improves the model’s ability to separate content and style while reducing leakage.</p>

<h4 id="content-leakage-problem">Content Leakage Problem</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-27-17-21-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-27-17-21-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-27-17-21-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-27-17-21-56.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    An example of content leakage in the style transfer, i.e., the spindle leaves (from the reference image) in the background of “a sneaker”, even though it is not included in the text prompt
</div>

<p><strong>Content leakage</strong> in few-shot or one-shot fine-tuning happens because the model struggles to correctly associate visual concepts with corresponding text phrases when trained on only a few or a single image-text pair. The key reasons are:</p>

<ul>
  <li>
    <p><strong>Concept Entanglement</strong>: In large-scale training, models learn to decompose and associate individual visual concepts with text through extensive data diversity. However, with few-shot fine-tuning, the limited number of training examples makes it difficult to disentangle different visual elements, leading to unwanted content appearing in generated images.</p>
  </li>
  <li>
    <p><strong>Lack of Concept Alignment</strong>: When fine-tuning with only one or a few images, the model cannot effectively learn which parts of the image represent style versus specific objects. As a result, it may misinterpret background elements as essential style features, causing them to reappear in generated images even when not prompted.</p>
  </li>
  <li>
    <p><strong>Overfitting to Reference Image</strong>: The model tends to memorize the entire reference image, leading to a high risk of directly copying unwanted elements into generated images instead of generalizing style attributes properly.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-21-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-21-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-21-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-21-33.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    An example of content alignment problem. Even with a contextual prompt (as shown in the image), it is still difficult to disentangle and map pairs of visual concepts and text phrases, i.e., "a woman" to visual "a woman" concept, "laptop" to visual "laptop" concept, etc.
</div>

<h4 id="limitation-of-existing-methods">Limitation of Existing Methods</h4>

<p>Some approaches, like StyleDrop, attempt to mitigate content leakage through iterative fine-tuning with synthetic images curated by human or automated feedback. However, this process is computationally expensive and does not fully solve the underlying issue of disentangling style from content.</p>

<h4 id="key-contributions">Key Contributions</h4>

<ul>
  <li><strong>Concept-Oriented Data Scaling</strong>: Decomposes a single reference image into multiple sub-image-text pairs, each focusing on different fine-grained concepts. This helps disentangle style attributes from content.</li>
  <li><strong>Parameter-Efficient Fine-Tuning via Cross-Attention Adapters</strong>: FineStyle modifies only the key and value kernels in cross-attention layers. This improves fine-grained style control and better aligns visual concepts with textual prompts while keeping the model lightweight.</li>
</ul>

<h3 id="finestyle-framework">FineStyle Framework</h3>

<h4 id="background">Background</h4>

<p><a href="https://arxiv.org/abs/2301.00704">Muse</a> is a masked generative transformer for text-to-image generation, which is the foundation model of FineStyle.
It consists of four main components:</p>

<ul>
  <li>A pre-trained text encoder \(T\): encodes a text prompt into textual token space \(\tau\)</li>
  <li>An image encoder \(E\): encodes an image from pixel space to a sequence of discrete visual tokens \(v \in \epsilon\)</li>
  <li>A decoder \(D\): decodes the visual tokens back to pixel space</li>
  <li>A generative transformer \(G\): generates an image from the visual tokens, \(G: \epsilon \times \tau \rightarrow \mathcal{L}\)</li>
</ul>

\[L = \mathbb{E}_{(x,t)\sim\mathcal{D},m\sim\mathcal{M}}[\text{CE}(\text{E}(x), \text{G}(\mathcal{M}(\text{E}(x), m), \text{T}(t)))]\]

<p>where \(\mathcal{D}\) is the training set, \(\mathcal{M}\) is a uniformly distributed mask smapling strategy with a mask ratio as a coefficient, and \(\text{CE}\) is the weighted cross-entropy loss.</p>

<p><strong>Sampling Strategy in Muse</strong></p>

<p>During image synthesis, the model uses iterative decoding to generate images given a text prompt and initial visual tokens. The synthesis process is defined as:</p>

\[\mathcal{I} = \text{D}(v_K), v_k = \text{S}(\text{G}(v_{k-1}, \text{T}(t)) + \lambda(\text{G}(v_{k-1}, \text{T}(t)) - \text{G}(v_{k-1}, \text{T}(n))))\]

<p>where:</p>

<ul>
  <li>\(k \in [1, K]\) is the sampling step</li>
  <li>\(t\) is the text prompt</li>
  <li>\(n\) is the null prompt</li>
  <li>\(\text{S}\) is a sampling strategy for visual tokens</li>
  <li>\(\lambda\) represents the coefficient for classifier-free guidance</li>
  <li>\(\text{D}\) maps the final visual tokens to pixel space</li>
</ul>

<p>The sampling strategy \(\text{S}\) is an <strong>iterative masked decoding strategy</strong>, where visutal tokens are progressively predicted and refined.
The model starts with an initial sequence of visual tokens, some of which are masked. It then iteratively predicts the masked tokens, using the previous predictions to inform the next step.</p>

<p><strong>StyleDrop</strong></p>

<p><a href="https://arxiv.org/abs/2306.00983">StyleDrop</a> is an extension of Muse that introduces an adapter to the generative transformer \(G\) to have a better style control.</p>

<h4 id="proposed-method">Proposed Method</h4>

<h4 id="concept-oriented-data-scaling">Concept-Oriented Data Scaling</h4>

<p>Idea (Borrowed from <a href="https://arxiv.org/abs/2306.00983">StyleDrop</a>): Decompose a text prompt into multiple sub-text prompts, each focusing on a different fine-grained concept. For example</p>

<ul>
  <li>“woman”, “laptop”, “a pot of plant with spindle leaves”, and “bookshelf” for foreground subjects</li>
  <li>“flat cartoon vector art”, “a light blue circle”, and “white background” for style and background attributes</li>
</ul>

<p>Then combine the two sets into a single text prompt, <code class="language-plaintext highlighter-rouge">{concept phrase} in {style phrase} style</code>.</p>

<p><strong>Training with Concept-oriented Masking</strong></p>

<ul>
  <li>cropping around the area of interest associated with the concept-style text phrase</li>
  <li>Using a pre-trained Muse model to create the segmentation mask (as shown in Fig. 3 a-c)</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-27-18-04-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-27-18-04-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-27-18-04-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-27-18-04-29.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h4 id="classifier-free-guidance-for-style-control">Classifier-Free Guidance for Style Control</h4>

<p>FineStyle modifies Muse’s masked visual token prediction approach by introducing style and semantic guidance. The sampling strategy helps balance text fidelity and style adherence, mitigating content leakage.</p>

<p>Tunable parameters (\(λ_1, λ_2\)) allow users to control the strength of style influence versus prompt adherence, making the generation more flexible and controllable.</p>

<p>The sampling formula for visual tokens in FineStyle is</p>

\[v_k = \hat{G}(v_{k-1}, \text{T}(t)) + \lambda_1(\hat{G}(v_{k-1}, \text{T}(t)) - G(v_{k-1}, \text{T}(t))) + \lambda_2(\hat{G}(v_{k-1}, \text{T}(t)) - \hat{G}(v_{k-1}, \text{T}(n)))\]

<p>where:</p>

<ul>
  <li>\(\hat{G}\) is FineStyle adapted model</li>
  <li>\(G\) is the original Muse model</li>
  <li>\(t\) is the text prompt</li>
  <li>\(n\) is the null prompt for guidance</li>
  <li>\(\lambda_1\) is the coefficient for style guidance - Adjusts how strongly the generated image follows the reference style.</li>
  <li>\(\lambda_2\) is the coefficient for semantic guidance - Helps prevent content leakage by reinforcing adherence to the text prompt.</li>
</ul>

<h3 id="results">Results</h3>

<h4 id="qualitative-results">Qualitative Results</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-34-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-34-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-34-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-34-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Qualitative results of FineStyle. To me, the DreamStyler seems doing quite well, especially in the fourth and fifth rows, when the output images are aligned more with the negative prompt (i.e., "background not in gray" or "background not in white").
</div>

<h4 id="quantitative-results">Quantitative Results</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-41-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-41-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-41-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-41-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-42-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-42-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-42-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-42-41.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Quantitative results of FineStyle. To me, the quantitative results are not comprehensive enough to draw a conclusion, especially the lack of comparison with other methods like DreamStyler.
</div>

<h2 id="references">References</h2>

<ul>
  <li>[1] <a href="https://arxiv.org/abs/2309.06933">DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models</a></li>
  <li>[2] <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models</a></li>
</ul>

<!-- mkdir -p assets/img/personalization -->
<!-- mv _posts/2025-02-26-*.png assets/img/personalization/ -->]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><summary type="html"><![CDATA[FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)]]></summary></entry><entry><title type="html">DreamStyler - Paint by Style Inversion with Text-to-Image Diffusion Models (AAAI-2024)</title><link href="https://tuananhbui89.github.io/blog/2025/dreamstyler/" rel="alternate" type="text/html" title="DreamStyler - Paint by Style Inversion with Text-to-Image Diffusion Models (AAAI-2024)" /><published>2025-02-27T00:00:00+11:00</published><updated>2025-02-27T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/dreamstyler</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/dreamstyler/"><![CDATA[<h3 id="overview">Overview</h3>

<ul>
  <li><a href="https://nmhkahn.github.io/dreamstyler/">DreamStyler Project Page</a></li>
  <li><a href="https://github.com/webtoon/dreamstyler">DreamStyler Github</a></li>
</ul>

<p>DreamStyler introduces a novel approach to style transfer by leveraging a multi-stage textual embedding combined with a context-aware text prompt. The method aims to enhance the generation of images in a specific artistic style using text-to-image diffusion models.</p>

<h3 id="key-contributions">Key Contributions</h3>

<h4 id="problem-setting">Problem Setting</h4>

<p>Given a set of style images with an implicit personal style (e.g., Van Gogh’s style), the goal is to fine-tune a foundation model to mimic the style \(S^*\) such that it can generate images in that style when provided with a text prompt (e.g., “A painting of a bear in \(S^*\) style”). This is traditionally done using personalized methods like DreamBooth and Textual Inversion.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-41-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-41-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-41-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-41-45.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Examples of DreamStyler application. (a+b+c) Style-Guided Text-to-Image generation where input is a text prompt. (d+e+f) Style-Guided Image-to-Image generation where input is an image.
</div>

<h4 id="limitations-of-existing-methods">Limitations of Existing Methods</h4>

<p>Current methods face several challenges:</p>

<ul>
  <li>
    <p><strong>Dynamic Style Representation</strong>: Diffusion models require different capacities at various denoising steps, making it difficult for a single embedding vector to capture an entire style.</p>
  </li>
  <li>
    <p><strong>Local to Global Features</strong>: The denoising process moves from coarse to fine synthesis, meaning both global artistic elements (color tone) and fine-grained details (texture) need to be represented effectively.</p>
  </li>
  <li>
    <p><strong>Style-Content Separation</strong>: Without a structured way to distinguish style from content, generated images may unintentionally incorporate unwanted elements from reference images.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-12-11-09.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The disentanglement property of Diffusion models [2]. Example that requires different capacities at various diffusion steps.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-49-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-49-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-49-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-49-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Example of content leakage/style overfitting in the style transfer. If tranferring without context-aware text prompt (setting (a)), the model is overfitting to the reference image (copying people from the reference image to the generated image).
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-15-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-15-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-15-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-15-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-07-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-07-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-07-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-07-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Example of the challenge of Style-Content Separation, especially with fixed neutral textual templates (as in the right image). Specifically, given a reference input as in the left image and set of neutral templates, how the model knows S^* is represented for a style - color tone/texture or an object in the input image? The problem is even more severe on Few-shot/One-shot settings.
</div>

<h3 id="proposed-solution">Proposed Solution</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-43-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h4 id="context-aware-text-prompt">Context-Aware Text Prompt</h4>

<p>A style is often intertwined with content in a reference painting, making it difficult to extract only the stylistic elements. To address this, DreamStyler utilizes <strong>BLIP-2 and Human-in-the-loop methods to create a context-aware text prompt</strong> that explicitly describes non-style components (e.g., objects, composition, background).</p>

<p><strong>Benefits</strong></p>

<ul>
  <li><strong>Improved Style-Content Disentanglement</strong>: By explicitly describing the non-style elements of the reference image, the model can better focus on learning the stylistic features, leading to outputs that are more faithful to the user’s intent.</li>
  <li><strong>Reduced Unwanted Elements</strong>: The inclusion of context descriptions helps to prevent the model from incorporating irrelevant objects, compositions, or backgrounds from the reference image into the generated images.</li>
</ul>

<p><strong>Implementation</strong></p>

<p>The context-aware text prompt is manually assigned as an input argument:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">context_prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">A painting of pencil, pears and apples on a cloth, in the style of {}</span><span class="sh">"</span><span class="p">.</span>
<span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">template</span> <span class="k">if</span> <span class="n">context_prompt</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">context_prompt</span>
</code></pre></div></div>

<h4 id="multi-stage-textual-embedding">Multi-Stage Textual Embedding</h4>

<p><strong>Motivation</strong></p>

<p>Traditional Textual Inversion (TI) relies on a <strong>single embedding vector</strong>, which may not effectively represent complex artistic styles across the entire diffusion process. Prior research shows that diffusion models require <strong>different representational capacities at various timesteps</strong>.</p>

<p>As demonstrated in other works, there is a dynamic property throughout the diffusion process, which requires different capacities at various diffusion steps [2]. Therefore, using a single embedding vector for all diffusion steps is not ideal, especially for representing artistic styles that involve both global elements (like color tone) and fine-grained details (like texture).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-12-11-09.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The disentanglement property of Diffusion models [2].
</div>

<p><strong>Proposed Approach</strong></p>

<p>DreamStyler introduces a multi-stage textual embedding by utilizing multiple embedding vectors/tokens, each corresponding to a specific stage of the diffusion process.
More specifically, the entire diffusion process is broken down into \(T\) distinct stages, and a set of \(T\) style tokens \(S_1, S_2, \cdots, S_T\) are used to represent the style at each stage.</p>

<p><strong>Benefits</strong></p>

<ul>
  <li><strong>Enhanced Expressiveness</strong>: Captures both global elements (e.g., color tone) and fine details (e.g., brushstrokes, textures).</li>
  <li><strong>Better Adaptability</strong>: Adjusts to the changing nature of style representation during the denoising process.</li>
</ul>

<p><strong>Implementation of Multi-Stage Textual Embedding</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-14-37-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-14-37-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-14-37-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-14-37-20.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-14-38-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-14-38-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-14-38-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-14-38-50.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Decomposition of the style embedding into multiple stages.
</div>

<h4 id="style-and-context-guidance-with-classifier-free-guidance">Style and Context Guidance with Classifier-Free Guidance</h4>

<p><strong>Classifier-Free Guidance</strong></p>

<p><a href="https://arxiv.org/abs/2207.12598">Classifier-Free Guidance</a> is a popular technique in the diffusion model community to improve the quality of generated images. It is a simple yet effective method to improve the diversity of generated images.</p>

\[\hat{\epsilon}(v) = \epsilon(\emptyset) + \lambda(\epsilon(v) - \epsilon(\emptyset))\]

<p>where:</p>

<ul>
  <li>\(\epsilon\) is the denoising function</li>
  <li>\(\lambda\) is the coefficient for guidance</li>
  <li>\(\emptyset\) is the null prompt</li>
  <li>\(v\) is the text prompt</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-27-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-27-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-27-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-27-38.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of Classifier-Free Guidance. Image credit: <a href="https://www.researchgate.net/publication/379277262_MAM-E_Mammographic_Synthetic_Image_Generation_with_Diffusion_Models">MAM-E: Mammographic Synthetic Image Generation with Diffusion Models</a>.
</div>

<p><strong>Style and Context Guidance</strong></p>

<p>DreamStyler introduces a style and context guidance mechanism by incorporating the style and context prompts into the Classifier-Free Guidance ()</p>

\[\hat{\epsilon}(v) = \epsilon(\emptyset) + \lambda_{s}\left[ \epsilon(v) - \epsilon(v_c) \right] + \lambda_{c}\left[ \epsilon(v_c) - \epsilon(\emptyset) \right] + \lambda_{c}\left[ \epsilon(v) - \epsilon(v_s) \right] + \lambda_{s}\left[ \epsilon(v_s) - \epsilon(\emptyset) \right]\]

<p>where:</p>

<ul>
  <li>\(v_c\) is the context prompt and \(v_s\) is the style prompt that are decomposed from the text prompt \(v\) as \(v = v_c + v_s\).</li>
  <li>\(\lambda_c\) is the coefficient for context guidance. Increasing \(\lambda_c\) encourages the model to generate images that are more faithful to the context prompt.</li>
  <li>\(\lambda_s\) is the coefficient for style guidance. Increasing \(\lambda_s\) encourages the model to generate images that are more aligned with the style prompt.</li>
</ul>

<h4 id="utilizing-controlnet-for-style-preservation">Utilizing ControlNet for Style-Preservation</h4>

<p>DreamStyler also utilizes ControlNet to maintain the original content’s structure of the reference image in the Image-to-Image Style Transfer setting.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-43-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration the use of ControlNet for style-preservation (b - Sampling process). The content image is used to generate a encoding vector, which is used to guide the generation process.
</div>

<h3 id="results">Results</h3>

<h4 id="qualitative-results">Qualitative Results</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-17-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-17-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-17-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-17-17.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-18-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-18-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-18-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-18-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h4 id="quantitative-results">Quantitative Results</h4>

<p>The authors utilize three scores to evaluate the performance of the proposed method:</p>

<ul>
  <li><strong>Text Score and Image Score</strong>: measure the alignment with a given text prompt/reference image with the generated image.</li>
  <li><strong>Style Score</strong>: Assesses the style consistency by calculating the similarity of Gram features between the style and generated images.</li>
  <li><strong>User Score</strong>: Human evaluation score.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-46-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-46-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-46-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-46-58.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-51-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-51-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-51-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-51-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The Gram-based style score.
</div>]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Kolmogorov-Arnold Network (KAN)</title><link href="https://tuananhbui89.github.io/blog/2025/KAN/" rel="alternate" type="text/html" title="Kolmogorov-Arnold Network (KAN)" /><published>2025-02-21T00:00:00+11:00</published><updated>2025-02-21T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/KAN</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/KAN/"><![CDATA[<p>Resources:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/abs/2404.19756">KAN Paper</a></li>
  <li>[2] <a href="https://github.com/KindXiaoming/pykan">KAN Github</a></li>
  <li>[3] <a href="https://github.com/mintisan/awesome-kan">Awesome KAN(Kolmogorov-Arnold Network)</a></li>
  <li>[4] <a href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/">Philosophical thoughts on Kolmogorov-Arnold Networks by Ziming Liu</a></li>
  <li>[5] <a href="https://www.digitalocean.com/community/tutorials/kolmogorov-arnold-networks-kan-revolutionizing-deep-learning">Kolmogorov-Arnold Networks (KAN) Promising Alternative to Multi-Layer Perceptron? by DigitalOcean</a></li>
  <li>[6] <a href="https://arxiv.org/pdf/2407.16674">KAN or MLP: A Fairer Comparison</a></li>
</ul>

<h2 id="mlp-vs-kan">MLP vs KAN</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-21-16-23-16.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison between MLP and KAN.
</div>

<p><strong>Limitations of MLP</strong>:</p>

<ul>
  <li><strong>Interpretability</strong>: MLPs are often considered “black boxes” due to their complex internal workings, making it difficult to understand how they arrive at their predictions.</li>
  <li><strong>Curse of Dimensionality</strong>: MLPs can struggle with high-dimensional data, as the number of parameters required to capture complex relationships grows exponentially with the input dimension.</li>
  <li><strong>Local Optimization</strong>: MLPs rely on gradient-based optimization algorithms, which can get stuck in local minima, potentially leading to suboptimal solutions.</li>
  <li><strong>Catastrophic Forgetting</strong>: MLPs can be prone to catastrophic forgetting, where learning new information can overwrite previously learned knowledge, hindering their ability to perform continual learning.</li>
</ul>

<p><strong>Advantages of KAN over MLP</strong>:</p>

<ul>
  <li><strong>Interpretability</strong>: KANs are more interpretable than MLPs due to their structure and the use of learnable activation functions. The absence of linear weight matrices and the explicit representation of univariate functions make it easier to understand how KANs arrive at their predictions.</li>
  <li><strong>Neural Scaling Laws</strong>: KANs exhibit faster neural scaling laws than MLPs, meaning that their performance improves more rapidly with increasing model size. This faster scaling can lead to significant gains in accuracy by simply scaling up the model.</li>
  <li><strong>Continual Learning</strong>: KANs can naturally perform continual learning without catastrophic forgetting, unlike MLPs. This ability stems from the locality of spline basis functions, which allows KANs to update knowledge in specific regions without affecting previously learned information.</li>
</ul>

<p><strong>Limitations of KAN</strong>:</p>

<ul>
  <li><strong>Computational Efficiency</strong>: KANs can be computationally more expensive to train than MLPs due to the complexity of learning and evaluating spline functions. The current implementation of this spline function can be found <a href="https://github.com/KindXiaoming/pykan/blob/ecde4ec3274d3bef1ad737479cf126aed38ab530/kan/spline.py#L4">here</a>, which requires recursive computation of a higher-order spline from lower-order splines. This process does not leverage the parallelization of modern GPUs.</li>
  <li><strong>Theoretical Limitations</strong>: The Kolmogorov-Arnold Representation Theorem (KAT) primarily applies to single layer KANs, and therefore the multi-layer KANs are not guaranteed to be able to represent any continuous function. For example, the input of the activation function should be bounded, which is not trivial for multi-layer KANs.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-07-46-07.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Should we use KAN or MLP? Image from [1].
</div>

<h3 id="kan-or-mlp-a-fairer-comparison">KAN or MLP: A Fairer Comparison</h3>

<p>Paper [6] provides a <strong>fairer</strong> comparison between KAN and MLP by considering the same number of parameters and FLOPs to make sure that the computational complexity is the same.
The tasks for comparison are also more comprehensive, including tasks in ML, CV, NLP and symbolic formula representation.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-09-20-43.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Other comparison between KAN and MLP from a fairer perspective/setting.
</div>

<p>The key findings are follows, which somewhat contradict to the observation in the original KAN paper [1].</p>

<ul>
  <li><strong>Symbolic Formula Representation</strong>: KANs outperform MLPs when approximating symbolic formulas.</li>
  <li><strong>Other Tasks</strong>: MLPs generally outperform KANs on other tasks, including machine learning, computer vision, natural language processing, and audio processing.</li>
  <li><strong>Impact of B-spline Activation</strong>: KANs’ advantage in symbolic formula representation comes from their use of B-spline activation functions.  When MLPs use B-spline activation functions, their performance on symbolic formula representation matches or exceeds that of KANs.  However, B-spline activation functions do not significantly improve MLPs’ performance on other tasks.</li>
  <li><strong>Continual Learning</strong>: KANs do not outperform MLPs in continual learning tasks. In a standard class-incremental continual learning setting, KANs forget old tasks more quickly than MLPs.</li>
</ul>

<h2 id="kan">KAN</h2>

<!-- The **Universal Approximation Theorem** (UAT) states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary accuracy.
This is the foundation theorem of the Multi-Layer Perceptron (MLP). More specifically, the multivariate continuous function $$f: [0,1]^n \rightarrow \mathbb{R}$$ can be represented as follows in MLP:

$$
f(x) = f(x_1, x_2, \cdots, x_n) = \sum_{i=1}^{m} \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j + b_i \right)
$$

where $$\sigma$$ is the non-linear activation function, $$w_{i,j}$$ is the weight, and $$b_i$$ is the bias. -->

<p>Before we dive into the KAN, let’s first understand the two definitions <strong>“edge”</strong> and <strong>“node”</strong> in MLP and KAN.
Given a MLP with \(n\) input nodes and \(m\) output nodes, the MLP can be represented as a directed acyclic graph (DAG) as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-06-46-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    MLP layer
</div>

<p>Mathematically, the node \(y_i\) of the output (hidden) layer can be represented as \(y_i = \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j\right)\) where \(x_j\) is the input node, \(w_{i,j}\) is the weight. We ignore the bias term for simplicity.
The connection between the input <strong>nodes</strong> \(x_j\) and the output <strong>node</strong> \(y_i\) is called an <strong>edge</strong>, which is scaled by the <strong>learnable weight</strong> \(w_{i,j}\).
After applying the sum operation over all the edges, the output node \(y_i = \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j \right)\) is obtained by applying the non-linear activation function \(\sigma\) on the weighted sum.
Note that the activation function \(\sigma\) is pointwise applied and not learnable.</p>

<p>For the Kolmogorov-Arnold Network (KAN), it is based on the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov-Arnold Representation Theorem (KAT)</a>.
KAT states that any continuous function can be represented as a sum of a trigonometric polynomial and a spline function.
More specifically, the multivariate continuous function \(f: [0,1]^n \rightarrow \mathbb{R}\) can be represented as:</p>

\[f(x) = f(x_1, x_2, \cdots, x_n) = \sum_{q=0}^{2n+1} \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right)\]

<p>where \(\phi_{q,}:[0,1] \rightarrow \mathbb{R}\) are the learnable activation functions over <strong>edges</strong>, and the \(\Phi_q\) is the learnable activation function over output <strong>nodes</strong>.</p>

<p>In KAN, the <strong>edge</strong> connection between the input <strong>nodes</strong> \(x_p\) and the output <strong>node</strong> \(y_q\) is applied by the <strong>learnable activation function</strong> \(\phi_{q,p}\).
After applying the sum operation over all the edges, the output node \(y_q = \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right)\) is obtained by applying another learnable activation function \(\Phi_q\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-07-00-27.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    KAN layer
</div>

<p>So compared to MLP, while the process from input nodes to output nodes is quite similar (one output node connected to all the input nodes), and the activation function on edges <strong>\(\phi_{q,p}\)</strong> is also parameterized similar to \(w_{i,j}\) in MLP, the main difference lies in the activation function on output nodes <strong>\(\Phi_q\)</strong> that is learnable in KAN.</p>

<h3 id="implementation-of-kan">Implementation of KAN</h3>

<h4 id="residual-activation-function">Residual Activation Function</h4>

<p>Beside the spline function, the activation function also includes a basis function \(b(x)\) which gets the signal directly from the input nodes (without going through any weight matrix).</p>

\[\phi(x) = w_b b(x) + w_s \text{spline}(x)\]

<p>where \(w_b\) and \(w_s\) are the learnable weights. the basis function \(b(x) = \text{silu}(x) = x / (1 + e^{-x})\).</p>

<p>The most complicated part is the spline function, which is parameterized as a linear combination of <strong>B-splines</strong> such as:</p>

\[\text{spline}(x) = \sum_{i=1} c_i B_i(x)\]

<p>where \(B_i(x)\) is the \(i\)-th B-spline and \(c_i\) is the learnablecoefficient.</p>

<h4 id="b-spline">B-spline</h4>

<p>B-splines are essentially curves made up of polynomial segments, each with a specified level of smoothness. Picture each segment as a small curve, where multiple control points influence the shape. Unlike simpler spline curves, which rely on only two control points per segment, B-splines use more, leading to smoother and more adaptable curves.</p>

<p>The magic of B-splines lies in their local impact. Adjusting one control point affects only the nearby section of the curve, leaving the rest undisturbed. This property offers remarkable advantages, especially in maintaining smoothness and facilitating differentiability, which is crucial for effective backpropagation during training (From [4]).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    B-spline. Image from DigitalOcean [4].
</div>

<p>Mathematically, B-splines can be constructed by means of the Cox-de Boor recursion formula (<a href="https://en.wikipedia.org/wiki/B-spline#Definition">Wikipedia</a>), starting with the B-spline basis function of order 0. We start with the B-splines of degree \(p = 0\), i.e. piecewise constant polynomials:</p>

\[B_{i,0}(t) := \begin{cases}
1 &amp; \text{if } t_i \leq t &lt; t_{i+1}, \\
0 &amp; \text{otherwise.}
\end{cases}\]

<p>The higher \((p + 1)\)-degree B-splines are defined by recursion:</p>

\[B_{i,p}(t) := \frac{t - t_i}{t_{i+p} - t_i} B_{i,p-1}(t) + \frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} B_{i+1,p-1}(t).\]

<p>The implementation of the B-spline can be found <a href="https://github.com/KindXiaoming/pykan/blob/ecde4ec3274d3bef1ad737479cf126aed38ab530/kan/spline.py#L4">here</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-21-17-05-47.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Implementation of B-spline.
</div>

<p><strong>Computational Expensiveness</strong>: Because of the recursive computation of the B-spline, the computational complexity is much higher than that of MLP.</p>

<p><strong>Grid Extension</strong></p>

<p>The grid extension in KAN is the process of refining the spline function by adding more knots, so that the spline function can have a higher resolution, fit the data better. 
It can be done by using higher-order B-splines, which is calculated by the lower-order B-splines (therefore, it is called extension).</p>

<h3 id="philosophical-thoughts-on-kan-by-kans-author">Philosophical thoughts on KAN by Kan’s author</h3>

<p>I found the philosophical thoughts on KAN by the author <a href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/">here</a> very interesting and helpful to understand the KAN and its difference with MLP.
I just quote the part that I think is most relevant to the KAN here.</p>

<blockquote>
  <p><strong>Reductionism vs. Holism</strong> While MLPs are more aligned with holism, KANs are more aligned with reductionism. The design principle of MLPs is “more is different”. <strong>In an MLP, each neuron is simple</strong> because it has fixed activation functions. However, <strong>what matters is the complicated connection patterns among neurons</strong>. The magical power of MLPs performing a task is an emergent behavior which is attributed to collective contribution from all neurons. By contrast, <strong>in a KAN, each activation function is complicated</strong> because it has learnable functions. By sparsification and pruning, we hope the computation graph to be simple. In summary, MLPs have simple ‘atoms’ but complicated ways to combine these atoms; KANs have complicated (diverse) ‘atoms’ but simple ways to combine these atoms. In this sense, MLPs’ expressive power comes from the complicated connection patterns (fully-connected structure), which give rise to emergent bahavior (holism). In contrast, KANs’ expressive power comes from the complexity of fundamental units (learnable activation functions), but the way to decompose whole network to units is simple (reductionsim)</p>
</blockquote>

<h3 id="attention-mechanism">Attention Mechanism</h3>

<p>Because of the spline function \(\text{spline}(x)\), which is a linear combination of B-splines with different level of smoothness/resolution of the input \(x\), each resolution is weighted by the learnable coefficient \(c_i\), 
this mechanism can be regarded as a <strong>soft self attention</strong> mechanism, where the output attends to different parts of the input with different resolutions.</p>

<!-- img_path: /assets/img/KAN/ -->
<!-- mkdir -p ../assets/img/KAN/ -->
<!-- mv 2025-02-21-*.png ../assets/img/KAN/ -->]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Resources:]]></summary></entry><entry><title type="html">DeepSeek-R1</title><link href="https://tuananhbui89.github.io/blog/2025/deepseek/" rel="alternate" type="text/html" title="DeepSeek-R1" /><published>2025-01-28T00:00:00+11:00</published><updated>2025-01-28T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/deepseek</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/deepseek/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>DeepSeek-R1 is an open-source model that is developed by a Chinese quant company called <a href="https://www.deepseek.com/">DeepSeek AI</a>. This model has taken the AI community by storm as it is the first open-source solution capable of achieving performance comparable to premium OpenAI models (e.g., OpenAI-o1/o3) with <a href="https://www.forbes.com.au/news/investing/what-is-deepseek-new-chinese-ai-startup-rivals-openai/">a fraction of the training and inference costs</a>. It is also entirely free to use under an MIT license.</p>

<p><strong>Panic in Silicon Valley because of DeepSeek</strong></p>

<p>It is not a joke that Silicon Valley but not the whole tech industry is panicking about DeepSeek. Forbes even has a <strong>Panic Live update</strong> on <a href="https://www.forbes.com/sites/dereksaul/2025/01/27/deepseek-panic-live-updates-trump-calls-ai-development-positive-despite-tech-stock-plunge/">their website updating the latest loss</a> of the stock market.
Nvidia’s stock price dropped by 17%, a drop of <code class="language-plaintext highlighter-rouge">$589</code> billion in market cap - the biggest single-day loss in history (hint NVIDIA doesn’t like Test-time Computing).
And CEO of ScaleAI, <a href="https://www.tipranks.com/news/musk-and-scale-ais-ceo-suggest-that-deepseek-has-more-nvidia-chips-than-expected">a company that provides AI training data for LLMs models, who also doesn’t like the cost and data efficiency of DeepSeek, guessed that DeepSeek might has more GPU resources than they announced</a>.
President Trump called it a “wake-up call” for U.S. industries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/panic-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/panic-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/panic-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/panic.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/panic2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/panic2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/panic2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/panic2.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Models Released</strong></p>

<ul>
  <li><strong>DeepSeek-R1-Zero</strong>: This model, trained through large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally develops numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability and language mixing.</li>
  <li><strong>DeepSeek-R1</strong>: Incorporating multi-stage training and cold-start data before RL, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.</li>
  <li><strong>Distill-R1</strong>: A series of six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. Notably, the distilled 14B model outperforms state-of-the-art open-source models like Qwen-32B-Preview by a large margin. The 32B and 70B models set new records on reasoning benchmarks among dense models.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark?raw=true-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark?raw=true-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark?raw=true-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg?raw=true" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Benchmark of DeepSeek-R1. Image from [1].
</div>

<p><strong>Research Questions</strong></p>

<ul>
  <li>Can language model reasoning capabilities be improved purely through reinforcement learning without supervised fine-tuning?</li>
</ul>

<p><strong>Key Story Line</strong></p>

<ul>
  <li>
    <p>Base Model: The team uses DeepSeek-V3-Base and employs Group Relative Policy Optimization (GRPO) as the RL framework to enhance reasoning performance.</p>
  </li>
  <li>
    <p>Performance Gains: DeepSeek-R1-Zero achieves impressive reasoning benchmarks. For instance, the pass@1 score on AIME 2024 improves from 15.6% to 71.0%. With majority voting, the score further rises to 86.7%, matching OpenAI-o1-0912’s performance.</p>
  </li>
  <li>
    <p>Challenges and Solutions: While RL-only training produces strong reasoning capabilities, it introduces issues such as poor readability and language mixing. DeepSeek-R1 addresses these by incorporating cold-start data and multi-stage training pipelines.</p>
  </li>
  <li>
    <p>Pipeline Highlights:</p>

    <ul>
      <li>Collection of cold-start data for initial fine-tuning.</li>
      <li>Reasoning-oriented RL to refine reasoning skills.</li>
      <li>SFT using new datasets generated through rejection sampling and DeepSeek-V3 outputs.</li>
      <li>Final RL phase to align the model with human preferences across all scenarios.</li>
    </ul>
  </li>
</ul>

<p><strong>References</strong></p>

<ul>
  <li>Paper: <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
  <li>Code: <a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1</a></li>
  <li>All papers from DeepSeek-AI from <a href="https://huggingface.co/collections/Presidentlin/deepseek-papers-674c536aa6acddd9bc98c2ac">Huggingface</a> and <a href="https://github.com/orgs/deepseek-ai/repositories?type=all">DeepSeek’s Github</a></li>
  <li>Understanding Multi-Head Latent Attention from <a href="https://planetbanatt.net/articles/mla.html">Eryk Banatt</a></li>
</ul>

<h2 id="approach">Approach</h2>

<h3 id="overview">Overview</h3>

<ul>
  <li><strong>DeepSeek-R1-Zero</strong>: Applies RL directly to the base model without supervised fine-tuning. GRPO serves as the RL framework.</li>
  <li><strong>DeepSeek-R1</strong>: Employs a multi-stage process combining RL and SFT to address readability and language issues while enhancing performance.</li>
  <li><strong>Distill-R1</strong>: Features six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama, setting new records in reasoning benchmarks.</li>
</ul>

<h3 id="deepseek-r1-zero-rl-on-the-base-model">DeepSeek-R1-Zero: RL on the Base Model</h3>

<h4 id="group-relative-policy-optimization">Group Relative Policy Optimization</h4>

<p><strong>How does GRPO differ from PPO?</strong></p>

<p>Traiditional RL methods like PPO requires a pre-trained critic model to evaluate the performance of the policy model. However, to train a critic model, we need a pair of winning and losing outputs given a same input, normally from a human evaluator. These pairs are expensive to obtain, hard to scale. Moreover, if the task is complex, the human evaluator may be subjective, biased, or nuanced.</p>

<p>GRPO, on the other hand, removes the need for a pre-trained critic model by comparing responses within a group, therefore overcoming the above limitations of PPO.</p>

<p><strong>GRPO Objective Function</strong>
Specifically, for each question \(q\), GRPO samples a group of outputs \(\{o_1, o_2, \cdots, o_G\}\) from the old policy model \(\pi_{\theta_\text{old}}\).
It then optimizes the policy model \(\pi_{\theta}\) by maximizing the following objective function:</p>

\[\mathcal{J}_\text{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_\text{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \left(\min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_\text{old}}(o_i|q)}A_i, \text{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_\text{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right)A_i\right) - \beta\mathbb{D}_\text{KL}(\pi_\theta||\pi_\text{ref})\right)\right]\]

<p>where the KL divergence term \(\mathbb{D}_\text{KL}\) is defined as:</p>

\[\mathbb{D}_\text{KL}(\pi_\theta||\pi_\text{ref}) = \frac{\pi_\text{ref}(o_i|q)}{\pi_\theta(o_i|q)} - \log\frac{\pi_\text{ref}(o_i|q)}{\pi_\theta(o_i|q)} - 1\]

<p>and \(A_i\) is the advantage function defined as:</p>

\[A_i = \frac{r_i - \text{mean}({r_1, r_2, \cdots, r_G})}{ \text{std}({r_1, r_2, \cdots, r_G})}\]

<p>where \(r_i\) is the reward of the output \(o_i\) and \(\text{mean}\) and \(\text{std}\) are the mean and standard deviation of the rewards in the group.
The reward \(r_i\) is from a rule-based reward system (not from a human evaluator, therefore, it is scalable and might not be subjective).</p>

<p>The rule-based reward system mainly consists of two types of rewards:</p>

<ul>
  <li><strong>Accuracy rewards</strong>: evaluate whether the output is correct or not. There are plenty of existing datasets where the correct answer is known, for example, Math problems with deterministic answers or Leetcode problems with predefined test cases.</li>
  <li><strong>Format rewards</strong>: the output will be rewarded if it is in a predefined format. For example, the thinking process should be between <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code>.</li>
</ul>

<blockquote class="block-tip">
  <p>Template for DeepSeek-R1-Zero:</p>

  <p>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: <strong>prompt</strong>. Assistant:</p>
</blockquote>

<p><strong>Why is the rule-based reward system effective?</strong>
To me, the employed of rule-based reward system is another example of how self-supervised learning - where data can be generated automatically and massively - is the source of the success of large-scale deep learning models.
Similar to the success of ControlNet in image generation which also employs traditional CV techniques such as edge detection to create additional control signals, so that the model can leverage the existing rule-based knowledge in the dataset to improve its learning process, the rule-based reward system in this paper is a simple yet effective way that allows to create a large amount of data with structure/label, which is crucial for training a large-scale model, making the scaling law become still valid.</p>

<p>However, the rule-based reward system is not perfect and to my understanding, it is the reason why DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing.</p>

<p><strong>Breaking down the GRPO objective function</strong></p>

<p><strong>The expectation term</strong>
The expectation term \(\mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_\text{old}}(O \mid q)}\) says that for each question \(q\) sampled from a distribution of questions \(P(Q)\), we sample a group of outputs \(\{o_i\}_{i=1}^G\) from the old policy model \(\pi_{\theta_\text{old}}\).</p>

<p><strong>The KL divergence term</strong>
Minimizing the KL divergence term ensures that the policy model \(\pi_\theta\) does not deviate too much from the reference model \(\pi_\text{ref}\). Specifically, let \(t=\frac{\pi_\text{ref}(o_i \mid q)}{\pi_\theta(o_i \mid q)}\), then the KL divergence term can be rewritten as:</p>

\[\mathbb{D}_\text{KL}(\pi_\theta||\pi_\text{ref}) = t - \log (t) - 1\]

<p>And then \(\mathbb{D}_\text{KL}(\pi_\theta \mid \mid \pi_\text{ref}) \geq 0 ; \forall t &gt; 0\) and minima is 0 when \(t=1\).</p>

<p><strong>The advantage function</strong>
This term reflects how much better the output \(o_i\) is compared to the average output in the group, e.g., if \(A_i &gt; 0\), then \(o_i\) is better than the average output in the group or if \(A_i &gt; A_j\), then \(o_i\) is better than \(o_j\).</p>

<p>Therefore, maximizing the scaled advantage function \(\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_\text{old}}(o_i \mid q)}A_i\) encourages the policy model \(\pi_\theta\) to generate outputs that are better than the average output in the group, i.e., those with \(A_i &gt; 0\) while discouraging the worse outputs, i.e., those with \(A_i &lt; 0\).</p>

<h4 id="performance-self-evolution-process-and-aha-moment">Performance, Self-evolution Process and Aha Moment</h4>

<p>As mentioned in Section 2.2.4 of the paper, the performance of DeepSeek-R1-Zero is evaluated on the AIME 2024 benchmark (see <a href="#aime-2024">AIME 2024</a>) and impressively reaching comparable performance to OpenAI-o1-0912 - a premium OpenAI reasoning model - on the pass@1 score.
Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/table-2-aime-compare-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/table-2-aime-compare-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/table-2-aime-compare-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/table-2-aime-compare.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Self-evolution Process</strong></p>

<p>Beside the impressive performance, DeepSeek-R1-Zero also exhibits a fascinating self-evolution process as shown in Figure 3 of the paper, where the average response length per question increases over training time (from several hundred tokens to 10k+ tokens), again, with RL only.
DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended <strong>test-time computation</strong> (see <a href="#test-time-computing">Test time computing</a>).</p>

<p>One of the most remarkable aspects of this self-evolution is the <strong>emergence of sophisticated behaviors</strong> as the test-time computation increases. Behaviors such as <strong>reflection</strong>—where the model revisits and reevaluates its previous steps—and the <strong>exploration of alternative approaches</strong> to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model’s interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/fig-3-response-length-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/fig-3-response-length-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/fig-3-response-length-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/fig-3-response-length.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Aha Moment</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/table-3-aha-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/table-3-aha-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/table-3-aha-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/table-3-aha.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Another interesting phenomenon observed in DeepSeek-R1-Zero is the <strong>aha moment</strong> (of the model - as well as the authors or myself) where the model suddenly realizes that it needs to allocate more thinking time to solve the problem, by reevaluating its inital approach.
This reminds me of another <strong>aha moment</strong> in the history of RL, when a DeepMind’s DQN model explored an insane strategy to win the Atari game Breakout with the least effort by simply digging a hole in the wall.
Or <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Game_2">DeepMind’s AlphaGo move 37</a> - the move that no human player would have ever made.</p>

<p>This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.
It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.</p>

<div class="text-center">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/TmPfTpjtdgg?si=NWQ6377iCM50NJAr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<h3 id="deepseek-r1---rl-with-cold-start">DeepSeek-R1 - RL with Cold Start</h3>

<p>While DeepSeek-R1-Zero’s performance is impressive, it still encounters challenges such as poor readability, and language mixing.
To address these issues and further enhance reasoning performance, the authors introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a four-stage training pipeline.</p>

<h4 id="cold-start-with-cot-data">Cold Start with CoT data</h4>

<p>Unlike DeepSeek-R1-Zero, which begins with pure RL on the base model, DeepSeek-R1 incorporates a cold start phase. This stage involves collecting thousands of long Chain-of-Thought (CoT) data to fine-tune the base model (DeepSeek-V3-Base). This data is generated using methods such as few-shot prompting, direct prompting with reflection and verification, gathering DeepSeek-R1-Zero outputs, and <strong>refining with human annotators</strong>. The purpose of this step is to prevent an unstable start in the RL process and ensure the model produces more readable and coherent responses. The output format is designed to include a summary at the end of each response: <code class="language-plaintext highlighter-rouge">|special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;</code>.</p>

<h4 id="reasoning-oriented-rl">Reasoning-oriented RL</h4>

<p>After the cold start fine-tuning, the model undergoes a reasoning-oriented RL training process, which is similar to the one used for DeepSeek-R1-Zero. This stage focuses on enhancing the model’s ability to handle tasks in areas such as coding, mathematics, science, and logic. A language consistency reward is added during RL training, calculated as the proportion of target language words in the CoT, to mitigate language mixing issues, though this may slightly degrade performance. The final reward is a combination of reasoning accuracy and language consistency. The Group Relative Policy Optimization (GRPO) algorithm is employed for this stage, as mentioned in our previous conversation, to optimize the policy model, reduce training costs and estimate the baseline from group scores.</p>

<h4 id="sft-with-new-data">SFT with new data</h4>

<p>Once the reasoning-oriented RL has converged, the resulting checkpoint is used to collect SFT data for the next round. This stage incorporates both reasoning data and non-reasoning data. Rejection sampling, as discussed in our earlier conversation, is used to generate reasoning trajectories from the model’s output. The model is prompted to generate multiple responses, and only the correct and coherent responses are kept, and used as SFT data. This is also where a generative reward model is used, feeding both ground-truth and model predictions into DeepSeek-V3 for judgment. Non-reasoning data such as writing, factual QA, self-cognition, and translation, are added by adopting the DeepSeek-V3 pipeline and reusing portions of the DeepSeek-V3 SFT dataset. The DeepSeek-V3-Base model is then fine-tuned using this combined dataset</p>

<h4 id="rl-with-all-scenarios">RL with all scenarios</h4>

<p>The final stage consists of a secondary RL process to align the model with human preferences. This stage aims to improve the model’s helpfulness and harmlessness while refining its reasoning skills. For reasoning data, the process is similar to DeepSeek-R1-Zero, utilizing rule-based rewards. For general data, reward models are used to capture human preferences, where final summaries are assessed for helpfulness, while the entire response (including reasoning and summary) is evaluated for harmlessness</p>

<h2 id="conclusion">Conclusion</h2>

<p>To me, the most interesting part of this paper is the invention of DeepSeek-R1-Zero, whose introduction has had a profound impact on our understanding of RL and LLM training. More specifically, <strong>pure RL with rule-based rewards</strong> might represent a new paradigm for LLM training. The use of a rule-based reward system strikes me as another example of how self-supervised learning—where data can be generated automatically and on a massive scale—continues to underpin the success of large-scale deep learning models.</p>

<p>Similar to the success of ControlNet in image generation, which leverages traditional computer vision techniques like edge detection to provide additional control signals, the rule-based reward system in this paper offers a simple yet effective method to generate large amounts of structured, labeled data. This, in turn, plays a crucial role in training large-scale models, ensuring that the scaling laws remain valid.</p>

<p>The <strong>aha moment</strong> in DeepSeek-R1-Zero perfectly encapsulates the elegance and power of reinforcement learning: instead of explicitly teaching the model how to solve a problem, we simply design the right incentives, allowing the model to autonomously develop sophisticated problem-solving strategies.</p>

<hr />

<h2 id="appendix">Appendix</h2>

<h3 id="aime-2024">AIME 2024</h3>

<p>The American Invitational Mathematics Examination (AIME) is a prestigious mathematics competition in the United States, serving as an intermediary between the AMC 10/12 exams and the USA Mathematical Olympiad (USAMO). The AIME consists of 15 questions, each with an integer answer between 0 and 999, to be completed in 3 hours. Participants qualify for the AIME based on their performance in the AMC 10 or AMC 12 exams.</p>

<p>In 2024, the AIME I was administered on January 31, and the AIME II on February 7. The mean score for AIME I was 5.89, with a median of 5, while AIME II had a mean score of 5.45 and a median of 5.</p>

<p>The AIME 2024 benchmark employs two metrics:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">pass@1</code> score means the percentage of the questions that the model can solve correctly with the top-1 response (see Evaluation Setup - page 12).</li>
  <li>The <code class="language-plaintext highlighter-rouge">cons@64</code> score means the consensus (majority voting) result of the top-64 responses.</li>
</ul>

<h3 id="rejection-sampling">Rejection Sampling</h3>

<p><strong>Purpose</strong>: Rejection sampling is employed to generate reasoning trajectories from the model’s checkpoint after reasoning-oriented reinforcement learning (RL) has converged. The goal is to create a dataset that can improve the model’s ability in various areas, including writing, role-playing, and other general-purpose tasks, alongside its reasoning capabilities.</p>

<p><strong>Process</strong>:</p>

<ul>
  <li>A set of reasoning prompts are curated.</li>
  <li>The model generates multiple responses for each prompt.</li>
  <li>Only correct responses are retained, while incorrect or less desirable responses are rejected. This filtering step ensures that the SFT data consists of high-quality examples.</li>
  <li>The responses are also filtered to remove issues like mixed languages, long paragraphs, and code blocks, to ensure readability and relevance.</li>
</ul>

<p><strong>Expansion of Dataset</strong>: In the rejection sampling stage, the dataset expands beyond those that can be evaluated using rule-based rewards by including data that use a generative reward model. This is done by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.</p>

<p><strong>Output Quality</strong>: The overall goal is to produce higher quality training samples. This is done by filtering out low-quality responses and ensures that the model trains on consistent and reliable data.</p>

<p>In summary, rejection sampling plays a crucial role in the DeepSeek-R1 pipeline by generating a refined and expanded dataset for the second round of supervised fine-tuning. This process contributes to enhancing the model’s overall capabilities.</p>

<h3 id="test-time-computing">Test time computing</h3>

<p>Test Time Computing (TTC) refers to computational processes performed during the inference phase of machine learning models—that is, when the model is used to make predictions or solve problems after being trained. Unlike traditional inference, which usually involves a straightforward application of a pre-trained model, TTC allows for additional computations or adjustments to improve performance on specific tasks.</p>

<p><strong>Key Concepts in Test Time Computing</strong>:</p>

<ul>
  <li><strong>Adaptation at Inference</strong>: Some models dynamically adapt their behavior based on new inputs or environmental conditions. This can involve fine-tuning parts of the model or leveraging meta-learning techniques.</li>
  <li><strong>Iterative Reasoning</strong>: Instead of producing a single output, models perform multiple reasoning steps (e.g., generating intermediate explanations or calculations) to refine their predictions. This is common in large language models when solving complex problems.</li>
  <li><strong>On-the-Fly Learning</strong>: The model might use previously unseen data to improve its predictions in real time. This is particularly useful in tasks like personalization or domain adaptation.</li>
  <li><strong>Resource Allocation</strong>: TTC allows models to allocate varying amounts of computational resources to different inputs, depending on task complexity or uncertainty. For example, a model may run deeper reasoning loops for harder questions.</li>
  <li><strong>Applications</strong>:
    <ul>
      <li>Natural Language Processing (NLP): Iterative reasoning to solve logic or math problems.</li>
      <li>Computer Vision: Adjusting filters or segmentations for specific images.</li>
      <li>Personalization: Adapting user recommendations based on recent interactions.</li>
      <li>Robotics: Dynamically adjusting movements based on environmental feedback.</li>
    </ul>
  </li>
</ul>

<p><strong>Benefits</strong>:</p>

<ul>
  <li>Improved Accuracy: By refining outputs at test time, models often achieve higher performance on difficult tasks.</li>
  <li>Task-Specific Customization: Allows models to handle nuanced problems more effectively.</li>
  <li>Efficient Use of Resources: Computational effort can be adjusted based on task complexity.</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Increased Latency: Additional computations can slow down predictions.</li>
  <li>Higher Costs: Real-time adjustments require more computational resources.</li>
  <li>Complexity: Implementing TTC mechanisms can complicate model architecture.</li>
</ul>

<p>This approach is increasingly used in advanced AI systems, such as OpenAI’s GPT models, which employ techniques like iterative reasoning or chain-of-thought prompting to tackle complex tasks effectively.</p>

<p><strong>Why NVIDIA doesn’t like TTC</strong></p>

<p>NVIDIA’s GPUs are designed for parallel computing, which is not suitable for TTC which often involves sequential or iterative computation for individual inputs, underutilizing the GPU’s parallel architecture. TTC introduces variability and possibly higher latency, which isn’t ideal for traditional GPU pipelines.</p>

<h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3>

<p><strong>Monte Carlo Tree Search (MCTS)</strong> is an advanced search algorithm used primarily in decision-making processes, especially for games, simulations, and optimization problems. It is a method for making decisions by simulating many possible outcomes and using statistical analysis to find the most promising path.</p>

<p><strong>Key Components of MCTS</strong>
MCTS works by iteratively building a search tree, where nodes represent game states (or decision points) and edges represent actions. The process involves four main steps:</p>

<p>1.Selection</p>

<ul>
  <li>Starting from the root node, the algorithm selects child nodes recursively until it reaches a node that is not fully expanded (i.e., not all possible moves are explored).</li>
  <li>The selection is often guided by a strategy like the <strong>Upper Confidence Bound for Trees (UCT)</strong>, which balances exploration (trying less-visited nodes) and exploitation (focusing on nodes with high average rewards):</li>
</ul>

\[UCB = \text{win rate} + c \times \sqrt{\frac{\ln(\text{total visits})}{\text{visits to this node}}}\]

<p>2.Expansion</p>

<ul>
  <li>When a leaf node is reached, new child nodes are added for all possible moves from the current state.</li>
  <li>This step grows the search tree by exploring unvisited nodes.</li>
</ul>

<p>3.Simulation (Rollout)</p>

<ul>
  <li>From the newly added node, a simulation is run to the end of the game (or a predefined depth). The simulation often involves random or heuristic-based moves.</li>
  <li>The outcome (e.g., win, loss, or score) of this rollout provides an estimate of the value of the node.</li>
</ul>

<p>4.Backpropagation</p>

<ul>
  <li>The result of the simulation is propagated back up the tree, updating the statistics (e.g., win rate or average reward) for each node along the path to the root.</li>
  <li>This helps the algorithm prioritize the most promising branches in future iterations.</li>
</ul>

<p><strong>Applications of MCTS</strong></p>

<p>1.<strong>Games</strong>:</p>

<ul>
  <li>Widely used in game-playing AI, especially for games with large decision spaces (e.g., Go, Chess, Poker).</li>
  <li>Integral to the success of systems like AlphaGo, which combined MCTS with deep neural networks.</li>
</ul>

<p>2.<strong>Robotics and Planning</strong>:</p>

<ul>
  <li>Used to plan sequences of actions in dynamic environments where outcomes are uncertain.</li>
</ul>

<p>3.<strong>Optimization</strong>:</p>

<ul>
  <li>Applied in optimization problems where exploring the solution space is challenging due to its complexity or size.</li>
</ul>

<p>4.<strong>Simulations</strong>:</p>

<ul>
  <li>Used in Monte Carlo simulations to estimate probabilities or solve probabilistic decision-making problems.</li>
</ul>

<p><strong>Strengths of MCTS</strong></p>

<ul>
  <li><strong>Scalable</strong>: Handles very large state spaces effectively.</li>
  <li><strong>Adaptive</strong>: Focuses computational resources on the most promising parts of the tree.</li>
  <li><strong>Flexible</strong>: Can work without a full model of the game or problem and adapt as new information is added.</li>
</ul>

<p><strong>Limitations</strong></p>

<ul>
  <li><strong>Computationally Expensive</strong>: Requires many simulations, especially for complex problems.</li>
  <li><strong>Dependence on Rollout Policy</strong>: The quality of results depends heavily on how the simulations (rollouts) are performed.</li>
  <li><strong>Suboptimal for Short Decision Horizons</strong>: Less effective for problems requiring quick, shallow decisions.</li>
</ul>

<p>MCTS combines principles from reinforcement learning, probability, and decision-making, making it a powerful tool for complex tasks that involve uncertainty and large decision spaces.</p>

<h3 id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    SFT and RLHF. Image from [1].
</div>

<p><strong>Goal:</strong>
Fine-tuning a model using reinforcement learning where the reward signal is derived from human feedback to align the model with human preferences and values beyond task-specific objectives.</p>

<p><strong>Process:</strong></p>

<ul>
  <li><strong>Supervised Pretraining</strong>: Start with a pretrained and optionally fine-tuned model.</li>
  <li><strong>Feedback Collection</strong>: Collect human-labeled data ranking model outputs (e.g., ranking completions for prompts).</li>
  <li><strong>Reward Model (RM)</strong>: Train a reward model to predict rankings based on human feedback. The reward model is trained to mimic human preferences on a specific task.</li>
  <li><strong>Policy Optimization</strong>: Fine-tune the model (policy) using reinforcement learning (e.g., Proximal Policy Optimization, PPO) to maximize the reward from the RM.</li>
</ul>

<p>Loss Function:</p>

<ul>
  <li><strong>Combines reinforcement learning objectives (e.g., PPO loss) with supervised objectives to balance exploration and alignment.</strong></li>
</ul>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Aligns model behavior with human values, preferences, and ethical considerations.</li>
  <li>Reduces harmful or inappropriate responses.</li>
  <li>Improves usability in real-world scenarios (e.g., chatbot interactions).</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Requires expensive and time-consuming human feedback.</li>
  <li>May introduce biases based on the preferences of the feedback providers.</li>
  <li>Balancing alignment with generalization is challenging.</li>
</ul>

<p><strong>Comparison Table</strong></p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Unsupervised Pretraining</th>
      <th>Supervised Fine-Tuning</th>
      <th>RLHF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Purpose</td>
      <td>General-purpose language understanding</td>
      <td>Task-specific performance improvement</td>
      <td>Aligning outputs with human preferences</td>
    </tr>
    <tr>
      <td>Data Requirement</td>
      <td>Large-scale unlabeled text corpora</td>
      <td>Labeled datasets for specific tasks</td>
      <td>Human-labeled feedback or rankings</td>
    </tr>
    <tr>
      <td>Objective</td>
      <td>Learn language patterns and knowledge</td>
      <td>Optimize for specific task objectives</td>
      <td>Optimize for human alignment</td>
    </tr>
    <tr>
      <td>Training Cost</td>
      <td>High (large datasets, long training)</td>
      <td>Moderate (smaller labeled datasets)</td>
      <td>Very high (feedback collection + RL tuning)</td>
    </tr>
    <tr>
      <td>Model Usage</td>
      <td>Provides a base model</td>
      <td>Task-specific models</td>
      <td>Refines models for safer, more useful output</td>
    </tr>
    <tr>
      <td>Challenges</td>
      <td>Task-agnostic, needs fine-tuning</td>
      <td>Dependent on labeled data quality</td>
      <td>Expensive and subject to bias in feedback</td>
    </tr>
    <tr>
      <td>Example</td>
      <td>Training GPT, BERT, T5 from scratch</td>
      <td>Fine-tuning BERT for sentiment analysis</td>
      <td>Fine-tuning GPT with human ranking data</td>
    </tr>
  </tbody>
</table>

<h3 id="ppo-and-dpo">PPO and DPO</h3>

<p>References:</p>

<ul>
  <li><a href="https://medium.com/@bavalpreetsinghh/rlhf-ppo-vs-dpo-26b1438cf22b">RLHF(PPO) vs DPO</a></li>
</ul>

<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h4>

<p>The Bradley-Terry model is a probabilistic model used to rank items based on pairwise comparisons.</p>

\[p(y_1 \succ y_2 \mid x) = \frac{exp(r(x,y_1))}{exp(r(x,y_1)) + exp(r(x,y_2))}\]

<p>where \(p\) is the probability of \(y_1\) <strong>being better than</strong> \(y_2\) given \(x\) representing the true human preferences and \(r\) is the reward function.
\(y_1\) and \(y_2\) are the two items/responses being compared given \(x\) representing the input.</p>

<p>If \(r^*(x,y_1) &gt; r^*(x,y_2)\), then \(p^*(y_1 \succ y_2 \mid x) &gt; 0.5\), which means \(y_1\) is more likely to be better than \(y_2\) given \(x\).</p>

<p>To learn the reward function \(r\) from the data, we parameterize as a neural network \(r_{\phi}\) and optimize the following objective function:</p>

\[\mathcal{L}_{R}(r_{\phi}, \mathcal{D}) = - \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma (exp(r_{\phi}(x,y_w)) - exp(r_{\phi}(x,y_l))) \right]\]

<p>where \(\mathcal{D}\) is the dataset of human preferences and \(y_w\) and \(y_l\) are the winning and losing responses against the same input \(x\).
Minimizing the above objective function is equivalent to maximizing the probability of the winning response being better than the losing response given the input.</p>

<p>Note that the reward function \(r_{\phi}\) is usually initialized from the supervised fine-tuning (SFT) model same as the policy model.</p>

<p><strong>Fine-tuning the policy model</strong></p>

<p>Once we have the reward model \(r_{\phi}\), we can fine-tune the policy model \(\theta\) using the following objective function:</p>

\[\mathcal{L}_{P}(\theta, \mathcal{D}) = - \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \left[ r_{\phi}(x,y) + \beta D_{KL}(\pi_{\theta}(y \mid x) || \pi_{ref}(y \mid x)) \right]\]

<p>where \(\pi_{ref}\) is the reference model and \(\beta\) is a hyperparameter.
Minimizing the first term enforces the policy model to generate responses that are more preferred by the reward model,
while minimizing the second term ensures that the policy model does not deviate too much from the reference model.</p>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Aligns the model with human preferences.</li>
  <li>Reduces harmful or inappropriate responses.</li>
  <li>Improves usability in real-world scenarios (e.g., chatbot interactions).</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Requires expensive and time-consuming human feedback.</li>
  <li>May introduce biases based on the preferences of the feedback providers.</li>
  <li>Balancing alignment with generalization is challenging.</li>
</ul>

<h4 id="direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</h4>

<p>DPO simplifies PPO by directly optimizing the policy model to adhere to human preferences without the need for a reward model.</p>

<p><strong>Objective Function:</strong></p>

\[\pi_{r}(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp(r(x,y)/\beta)\]

<p>where \(\pi_r\) is the policy model, \(\pi_{\text{ref}}\) is the reference model, \(r\) is the reward function, and \(\beta\) is a hyperparameter.</p>

\[r(x,y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \log Z(x)\]

<p>where \(Z(x)\) is the partition function.</p>

<p>The final objective function is:</p>

\[\mathcal{L}_{DPO}(\theta, \mathcal{D}) = - \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma (\pi_{\theta}(x,y_w) - \pi_{\theta}(x,y_l)) \right]\]

<p>Where there is no need for the reward model \(r_{\phi}\) as the policy model \(\pi_{\theta}\) is directly optimized to adhere to human preferences.
Minimizing the above objective function is directly maximizing the probability of the winning response while minimizing the probability of the losing response.</p>

<h3 id="sft-vs-rlhf">SFT vs RLHF</h3>

<p>Reinforcement Learning from Human Feedback (RLHF) is a technique used to align large language models (LLMs) more closely with human preferences and expectations. While supervised fine-tuning (SFT) is a critical step in improving a model’s capabilities, it has limitations that RLHF can address.</p>

<h4 id="when-is-rlhf-needed">When is RLHF needed?</h4>

<p><strong>Ambiguity in Objectives</strong></p>

<ul>
  <li>SFT aligns the model with a dataset of “correct” outputs, but human preferences often involve subjective judgment or context-specific nuance that cannot be fully captured in a static dataset.</li>
  <li>Example: Chatbots generating empathetic or polite responses where the tone and context matter significantly.</li>
</ul>

<p><strong>Unclear or Complex Evaluation Metrics</strong></p>

<ul>
  <li>For some tasks, it’s difficult to define explicit evaluation metrics, but humans can intuitively judge quality.</li>
  <li>Example: Creative writing or generating humorous content.</li>
</ul>

<p><strong>Long-Term or Multi-Step Reasoning</strong></p>

<ul>
  <li>SFT trains models to produce correct outputs based on immediate context but might fail in scenarios requiring multi-step decision-making or long-term coherence.</li>
  <li>Example: Writing a coherent multi-paragraph essay or guiding a user through a series of troubleshooting steps.</li>
</ul>

<p><strong>Avoiding Harmful Outputs</strong></p>

<ul>
  <li>Static datasets used for SFT may not include all edge cases or potential pitfalls, and RLHF can help refine the model to avoid harmful or toxic outputs based on human preferences.</li>
  <li>Example: Ensuring a conversational agent avoids generating offensive or biased content.</li>
</ul>

<p><strong>Improving User Experience</strong></p>

<ul>
  <li>SFT often focuses on correctness, but RLHF can optimize for user satisfaction, such as balancing informativeness, politeness, and conciseness.</li>
  <li>Example: Personal assistants generating concise and helpful responses tailored to user needs.</li>
</ul>

<h4 id="why-is-supervised-fine-tuning-not-enough">Why is Supervised Fine-Tuning Not Enough?</h4>

<p><strong>Static Nature of Datasets</strong></p>

<ul>
  <li>SFT relies on pre-collected datasets that might not represent all real-world scenarios or evolving user preferences.</li>
  <li>Limitation: If the dataset lacks certain examples, the model cannot generalize well.</li>
</ul>

<p><strong>Difficulty in Capturing Preferences</strong></p>

<ul>
  <li>Human preferences are often complex and not directly labeled in datasets.</li>
  <li>Limitation: A model trained on SFT might produce technically correct but undesirable outputs (e.g., overly verbose or lacking empathy).</li>
</ul>

<p><strong>Overfitting to Training Data</strong></p>

<ul>
  <li>SFT can lead to overfitting, where the model learns to replicate the training data without adapting to unseen scenarios.</li>
  <li>Limitation: This can result in poor performance on out-of-distribution examples.</li>
</ul>

<p><strong>Reward Optimization</strong></p>

<ul>
  <li>RLHF optimizes a reward function designed to capture human preferences, which allows fine-grained control over the model’s behavior.</li>
  <li>Limitation: SFT does not involve direct optimization based on human evaluations.</li>
</ul>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">LLM Series - Part 5 - System Design for LLMs</title><link href="https://tuananhbui89.github.io/blog/2025/llm-system-design/" rel="alternate" type="text/html" title="LLM Series - Part 5 - System Design for LLMs" /><published>2025-01-19T00:00:00+11:00</published><updated>2025-01-19T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-system-design</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-system-design/"><![CDATA[<p>In this blog post, I will discuss the system design for applications that use LLMs as a core component. However, the goal is to prepare for a technical interview rather than to build a real product :D.</p>

<h2 id="fundamental-concepts">Fundamental Concepts</h2>

<h3 id="retriever-augmented-generation-rag">Retriever-augmented generation (RAG)</h3>

<h3 id="prompt-engineering">Prompt Engineering</h3>

<h3 id="faiss">FAISS</h3>

<p><strong>Indexing Vectors</strong></p>

<p>FAISS creates an index to store and organize vectors efficiently. The indexing method affects performance:</p>

<ul>
  <li>Flat Index (IndexFlatL2) → Exact k-NN search (slow but accurate).</li>
  <li>IVF (Inverted File Index) → Faster search with approximate results.</li>
  <li>HNSW (Hierarchical Navigable Small World) → Graph-based ANN search (fast &amp; accurate).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">faiss</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate random 512-dimension vectors for products
</span><span class="n">dimension</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_vectors</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="n">num_vectors</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Create a FAISS index
</span><span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="nc">IndexFlatL2</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span>  <span class="c1"># L2 distance (Euclidean)
</span><span class="n">index</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>  <span class="c1"># Add vectors to the index
</span></code></pre></div></div>

<p><strong>Searching for Similar Items</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate a random query vector
</span><span class="n">query_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Find the top 5 nearest neighbors
</span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">query_vector</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Nearest Neighbors:</span><span class="sh">"</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Distances:</span><span class="sh">"</span><span class="p">,</span> <span class="n">distances</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="core-components-of-an-llm-system">Core components of an LLM system</h2>

<h3 id="input-handling-and-processing">Input handling and processing</h3>

<p>Because each application has different input types, for example, code will differ from clinical notes, we need to have a module that can handle specific input types related to the application.</p>

<h3 id="knowledge-base-and-data-resources">Knowledge Base and Data Resources</h3>

<p>The purpose of this module is to:</p>

<ul>
  <li>Provide the necessary related knowledge to the LLM.</li>
  <li>To store user-specific data for personalized output/decision.</li>
</ul>

<p>This can be done by:</p>

<ul>
  <li>Vector database tools like <a href="https://www.pinecone.io/">Pinecone</a> or <a href="https://github.com/facebookresearch/faiss">Faiss</a>.</li>
  <li>Flat file system.</li>
</ul>

<h3 id="the-core-llm-powered-module">The core LLM-powered module</h3>

<p>This is the main module that uses the LLM to generate the output/decision.</p>

<h4 id="prompting-module">Prompting module</h4>

<p>This can be integrated into the core LLM-powered module to improve the quality of the output/decision.
We can have a sub-module to classify the input into different categories and use different prompt templates for each category.</p>

<p>We can also leverage the response from the LLM as well as the user feedback to improve the prompt.</p>

<h3 id="filtering-and-validation">Filtering and Validation</h3>

<ul>
  <li>Validation by rule-based logic check, to make sure the output/input is correct and valid. However, because of the rule-based nature, it is not always flexible to handle all cases.</li>
  <li>Validation by another machine learning model, for example, another LLM model or a uncertainty estimation model.</li>
  <li>Optional human-in-the-loop (HITL) validation by a human expert, especially in critical applications like medical diagnosis.</li>
</ul>

<h3 id="safe-guarding">Safe Guarding</h3>

<h3 id="agentic-framework">Agentic Framework</h3>

<h4 id="agent-tools">Agent Tools</h4>

<p>These are external tools or resources that the LLM can access to perform specitic actions or gather information. This could be a calculator, a search API, or a external database.</p>

<h4 id="multi-agent-system">Multi-agent system</h4>

<h2 id="data-distribution-shifts-and-monitoring">Data Distribution Shifts and Monitoring</h2>

<h2 id="continual-learning">Continual Learning</h2>

<h2 id="evaluation">Evaluation</h2>

<h3 id="offline-evaluation">Offline Evaluation</h3>

<h3 id="test-in-production">Test in Production</h3>

<h2 id="deployment-and-scaling">Deployment and Scaling</h2>

<h2 id="case-study">Case Study</h2>

<h3 id="discuss-on-user-journey">Discuss on User Journey</h3>

<p>User journey describes how a user interacts with a product, starting from the first touchpoint - i.e.,user’s input, to the final interaction - i.e., displaying the result to the user.</p>

<p>Discussing on the user journey helps us to understand the flow interaction between the user and the product, and identify the core components that are involved in the interaction.</p>

<h3 id="recommender-system">Recommender System</h3>

<p><strong>User Interaction Layer</strong></p>

<ul>
  <li>Chat Interface: Users can describe their interests, ask questions, and discuss product features.</li>
  <li>Input Handling: Supports text-based and voice-based interactions.</li>
</ul>

<p><strong>NLP Module</strong></p>

<ul>
  <li>Intent Recognition: Extracts user intent (e.g., “I want a lightweight laptop for travel”).</li>
  <li>Entity Extraction: Identifies key product attributes (e.g., “lightweight,” “laptop,” “travel”).</li>
  <li>Sentiment Analysis: Understands user sentiment to refine recommendations.</li>
</ul>

<p><strong>Product Database</strong></p>

<ul>
  <li>Structure: Contains product details, including:
    <ul>
      <li>Name, Category, Price</li>
      <li>Features &amp; Specifications</li>
      <li>User Reviews &amp; Ratings</li>
    </ul>
  </li>
</ul>

<p>The most important component of this module is the embedding model to convert all the data into vectors so that we can use the vector database to store and search for similar items. The ideal vector space should be able to capture the semantic meaning of the data, i.e., the more similar the data is, the closer the vectors are in the vector space.</p>

<p>The most commonly used embedding models fall into three categories:</p>

<p>1️⃣ General-Purpose Text Embeddings (Best for Q&amp;A, knowledge retrieval)</p>

<p>2️⃣ Domain-Specific Embeddings (Optimized for medical, legal, code, etc.)</p>

<p>3️⃣ Multimodal Embeddings (For text + images)</p>

<p><strong>Recommendation Engine</strong></p>

<ul>
  <li>Content-Based Filtering: Matches user preferences with product attributes.</li>
  <li>Collaborative Filtering: Uses customer behavior data to suggest items others with similar preferences liked.</li>
  <li>Hybrid Approach: Combines content-based and collaborative filtering.</li>
</ul>

<p><strong>RAG-based recommendation</strong></p>

<p>RAG is built on two main components:</p>

<p>1️⃣ Retriever (Information Fetching)</p>

<ul>
  <li>Dense Vector Search (FAISS, Annoy, Pinecone, Weaviate, ChromaDB) that uses embeddings (e.g., BERT, SBERT, DPR) to find semantically similar documents.</li>
  <li>Traditional Search (BM25, ElasticSearch, Google Search API) that retrieves documents using keyword-based matching.</li>
</ul>

<p>2️⃣ Generator (Text Generation)</p>

<ul>
  <li>Pre-trained LLMs (GPT, BART, T5, LLaMA) generate responses using the retrieved documents as additional context.</li>
  <li>Can use fine-tuned models for domain-specific responses (e.g., finance, medical).</li>
</ul>

<p><strong>Key Steps in RAG</strong>:</p>

<p>1️⃣ User Input → A query is given (e.g., “What are the latest gaming laptops?”).</p>

<p>2️⃣ Retrieval Module → Finds the most relevant documents using vector search (FAISS, BM25, ElasticSearch, etc.).</p>

<p>3️⃣ Context Injection → The retrieved documents are passed to the generation model.</p>

<p>4️⃣ Response Generation → The model generates a final, coherent answer using both the query and retrieved documents.</p>

<h2 id="references">References</h2>

<p>[1] Build your first LLM agent application: https://developer.nvidia.com/blog/building-your-first-llm-agent-application/</p>]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[How to break into $100k+ salary roles - part 5]]></summary></entry></feed>