<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tuananhbui89.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tuananhbui89.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-08T23:40:35+07:00</updated><id>https://tuananhbui89.github.io/feed.xml</id><title type="html">blank</title><subtitle>Researcher in Generative AI and Trustworthy AI
</subtitle><entry><title type="html">CS336 Lecture 17 - Alignment - RL - Part 2</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec17/" rel="alternate" type="text/html" title="CS336 Lecture 17 - Alignment - RL - Part 2" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec17</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec17/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/JdGFdViaOJk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="course-orientation-and-lecture-goals">Course orientation and lecture goals</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-00-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-00-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-00-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-00-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This session is positioned at the <strong>end of the quarter</strong> and will deep-dive into <strong>policy gradient methods</strong> for <strong>reinforcement learning (RL)</strong> applied to <strong>language models (LMs)</strong>.<br /></p>

<ul>
  <li>Goals:
    <ul>
      <li>Expand on prior coverage of <strong>RL with verifiable rewards</strong> rather than introduce fundamentally new theory.<br /></li>
      <li>Provide <strong>mathematical detail</strong> and <strong>code-oriented examples</strong> that connect theory to practice.<br /></li>
      <li>Focus on algorithm mechanics (e.g., <strong>policy gradients</strong>, <strong>GRPO</strong>) and on practical implementation concerns.<br /></li>
    </ul>
  </li>
  <li>Framing:
    <ul>
      <li>Treats the lecture as a <strong>continuation</strong> and an <strong>application-focused elaboration</strong> of earlier material.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="reinforcement-learning-formulation-for-language-models-states-actions-rewards-and-dynamics">Reinforcement learning formulation for language models: states, actions, rewards, and dynamics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-02-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-02-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-02-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-02-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>State (LM setting)</strong>: the <strong>prompt concatenated with the tokens generated so far</strong>.<br /></li>
  <li>
    <p><strong>Action</strong>: generating the <strong>next token</strong>.<br /></p>
  </li>
  <li>Reward characteristics:
    <ul>
      <li><strong>Outcome-based</strong> and <strong>deterministic</strong>: rewards depend on the entire generated response (verifiable).<br /></li>
      <li>This makes signals <strong>sparse</strong> and <strong>delayed</strong>, but conceptually simpler than interactive environments.<br /></li>
    </ul>
  </li>
  <li>Transition dynamics:
    <ul>
      <li>Trivial string concatenation: state’ = state + action.<br /></li>
      <li>This simplicity enables <strong>planning</strong> or <strong>test-time computation</strong> that physical robotics typically cannot use.<br /></li>
    </ul>
  </li>
  <li>State representation challenges:
    <ul>
      <li>The LM synthesizes arbitrary internal tokens, so states are <strong>ungrounded text</strong> and can include internal <strong>scratchpad</strong> behavior.<br /></li>
      <li>Main challenge: ensure those token sequences produce <strong>verifiable, correct outcomes</strong> rather than merely reaching reachable but meaningless states.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="policy-and-objective-in-outcome-reward-lm-reinforcement-learning">Policy and objective in outcome-reward LM reinforcement learning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-04-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-04-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-04-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-04-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>Policy</strong>: an LM conditioned on the current token sequence (prompt + generated tokens).<br /></li>
  <li>
    <p><strong>Training objective</strong>: maximize <strong>expected reward</strong> over the distribution of prompts and the policy’s token outputs.<br /></p>
  </li>
  <li>Outcome-reward simplification:
    <ul>
      <li>When rewards are outcome-based, treat the entire response as a <strong>single action a</strong> with a single reward <strong>R</strong>.<br /></li>
      <li>This yields a simpler expected-reward expression and implementation.<br /></li>
    </ul>
  </li>
  <li>Practical setup:
    <ul>
      <li>Policies often <strong>initialize from pretrained LMs</strong> and are then <strong>fine-tuned</strong> via policy optimization.<br /></li>
      <li><strong>Rollouts</strong> produce trajectories that yield a scalar reward at episode end.<br /></li>
      <li>The objective integrates over environment-provided prompts and the stochastic policy’s responses to compute the <strong>expected return</strong>, which is the target for gradient-based optimization.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="policy-gradient-derivation-gradient-of-expected-reward-via-log-derivative-trick">Policy gradient derivation: gradient of expected reward via log-derivative trick</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-06-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-06-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-06-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-06-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Derivation sketch:
    <ul>
      <li>Differentiate expected reward w.r.t. policy parameters and apply the <strong>log-derivative (score function) identity</strong> to move the gradient inside the expectation.<br /></li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Result: an expectation of **∇ log π(a</td>
              <td>s)** weighted by the <strong>return</strong> (reward).<br /></td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Properties:
    <ul>
      <li>This gives an <strong>unbiased Monte Carlo estimator</strong> when sampling prompts and actions.<br /></li>
      <li>In the outcome-reward setting, treating the entire response as a <strong>single action</strong> further simplifies notation and implementation.<br /></li>
      <li>This <strong>score-function estimator</strong> is the basis for sampling-based updates that seek to increase expected reward.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="naive-policy-gradient-as-reward-weighted-supervised-fine-tuning-and-implications-of-binary-rewards">Naive policy gradient as reward-weighted supervised fine-tuning and implications of binary rewards</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-08-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-08-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-08-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-08-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>Naive policy gradient update</strong>:
    <ul>
      <li>Sample prompts and model responses, then apply gradient steps proportional to the sampled reward.<br /></li>
      <li>Analogy: similar to <strong>supervised fine-tuning (SFT)</strong>, but responses are <strong>weighted by reward</strong> rather than matched to fixed human targets.<br /></li>
    </ul>
  </li>
  <li>Special cases and failure modes:
    <ul>
      <li>With <strong>binary rewards (0/1)</strong>, updates occur only for rewarded responses: effectively SFT on the subset of model outputs deemed correct.<br /></li>
      <li>In <strong>sparse-reward</strong> regimes, if the policy rarely produces positive responses, gradients are near zero and learning can <strong>stall</strong>.<br /></li>
      <li>Conclusion: naive sampling without <strong>variance reduction</strong> or <strong>reward shaping</strong> can leave the model stuck when initial performance is poor.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="dataset-non-stationarity-and-contrast-with-continuous-reward-rlhf">Dataset non-stationarity and contrast with continuous reward RLHF</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-11-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-11-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-11-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-11-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Non-stationarity:
    <ul>
      <li>Policy optimization changes the policy that generates training examples, so the <strong>empirical dataset evolves</strong> across iterations.<br /></li>
      <li>This can be beneficial if the policy discovers easy positives and generalizes to harder prompts, but it <strong>complicates analysis and monitoring</strong> because the data distribution shifts.<br /></li>
    </ul>
  </li>
  <li>Reward-model contrast:
    <ul>
      <li><strong>RL with human feedback</strong> typically uses a learned, <strong>continuous reward model</strong>, which provides graded values and reduces sparsity relative to binary verifiable rewards.<br /></li>
      <li>The choice between <strong>verifiable outcome rewards</strong> and <strong>learned reward models</strong> affects algorithm design, hyperparameter choices, and intuitions about training dynamics.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="baselines-for-variance-reduction-in-policy-gradient">Baselines for variance reduction in policy gradient</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-12-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-12-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-12-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-12-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>Baseline b(s)</strong>:
    <ul>
      <li>Any function of state (but <strong>not of action</strong>) that is subtracted from returns in the policy gradient estimator to <strong>reduce variance</strong> without biasing the gradient.<br /></li>
    </ul>
  </li>
  <li>Why it’s unbiased:
    <ul>
      <li>Subtracting b(s) leaves the expected gradient unchanged because the baseline term factors out of the expectation over actions for a given state and contributes zero in expectation.<br /></li>
    </ul>
  </li>
  <li>Practical notes:
    <ul>
      <li>Baselines are fundamental to <strong>stabilize</strong> and <strong>accelerate</strong> convergence by centering the reward signal.<br /></li>
      <li>Valid baselines include fixed constants or <strong>learned value estimators</strong>, provided they do not depend on the action.<br /></li>
      <li>The design of b(s) substantially affects gradient variance and empirical learning speed.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="toy-two-state-example-illustrating-variance-pitfalls-and-baseline-benefits">Toy two-state example illustrating variance pitfalls and baseline benefits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-15-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-15-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-15-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-15-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Two-state toy example (intended insight):
    <ul>
      <li>Two prompts (states) with different per-action rewards show how <strong>naive updates</strong> can favor misleadingly high absolute rewards in certain states and produce suboptimal policies.<br /></li>
      <li>If the policy randomly selects an action that yields high reward in an easy state, repeated updates can amplify that action and eliminate the true global optimum—an instance of <strong>high-variance, myopic updates</strong>.<br /></li>
    </ul>
  </li>
  <li>Remedy:
    <ul>
      <li>Introduce a <strong>state-dependent baseline</strong> (e.g., subtract per-state expected reward) to <strong>center rewards</strong>, dramatically reducing variance and preventing harmful relative gradients.<br /></li>
      <li>The example quantifies how appropriate baselines can shrink variance by multiple factors and improve convergence behavior.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="optimal-baseline-expression-and-practical-heuristic-choice">Optimal baseline expression and practical heuristic choice</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-21-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-21-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-21-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-21-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Optimal baseline properties:
    <ul>
      <li>For <strong>scalar-parameter models</strong>, the variance-optimal baseline has a closed-form involving expectations of squared score-function terms weighted by returns.<br /></li>
      <li>In <strong>higher dimensions</strong>, the optimal solution requires covariance matrices and becomes impractical to compute exactly.<br /></li>
    </ul>
  </li>
  <li>Practical heuristic:
    <ul>
      <li>Use the estimated <strong>expected reward given state (value estimate)</strong> as the baseline—this approximates the <strong>advantage function</strong> and typically yields substantial variance reduction.<br /></li>
      <li>Value estimates are tractable to obtain via sampling or simple learned value functions, so implementations favor these computable approximations balancing cost and variance reduction.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="advantage-function-and-unified-delta-notation-for-algorithms">Advantage function and unified delta notation for algorithms</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-27-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-27-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-27-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-27-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>Advantage function A(s,a)</strong>:
    <ul>
      <li>Defined as <strong>Q(s,a) − V(s)</strong>: how much better action a is relative to average behavior at state s.<br /></li>
      <li>Provides a principled baseline choice when available.<br /></li>
    </ul>
  </li>
  <li>Outcome-reward simplification:
    <ul>
      <li>In the outcome-reward LM setting, <strong>Q and the return R coincide</strong> when treating the entire response as the return, so <strong>A = return − expected return given state</strong>.<br /></li>
    </ul>
  </li>
  <li>Implementation abstraction:
    <ul>
      <li>Many algorithms adopt a unified scalar <strong>delta</strong> to represent whatever multiplier (reward, centered reward, normalized advantage, etc.) is used with the log-probability gradient.<br /></li>
      <li>Variants differ primarily in how <strong>delta</strong> is computed, clarifying that modern policy-gradient techniques scale the score-function gradient by a carefully chosen delta to control variance and bias.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="grpo-lineage-and-group-structured-variance-reduction-in-lm-settings">GRPO lineage and group-structured variance reduction in LM settings</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-31-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-31-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-31-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-31-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Lineage and structure exploitation:
    <ul>
      <li><strong>GRPO</strong> and related algorithms evolved from the <strong>PPO/DPO</strong> lineage but exploit structure specific to <strong>language modeling</strong>.<br /></li>
      <li>Key LM structure: you can <strong>sample multiple responses per prompt</strong>, forming natural groups for per-prompt baselines (<strong>group means</strong>).<br /></li>
    </ul>
  </li>
  <li>Benefits of grouped sampling:
    <ul>
      <li>Compute an empirical baseline across responses from the same prompt, which <strong>reduces variance</strong> without requiring a global learned value function.<br /></li>
      <li>When rollouts are naturally grouped (many responses per prompt), <strong>GRPO-style relative updates</strong> are practical and effective.<br /></li>
      <li>In environments without grouping, alternative value-estimation methods are required.<br /></li>
    </ul>
  </li>
  <li>Algorithmic consequences:
    <ul>
      <li>Group structure motivates <strong>clipping</strong> and <strong>normalization</strong> strategies that compare responses relative to their prompt-specific cohort.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="toy-sorting-task-and-reward-design-alternatives">Toy sorting task and reward design alternatives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-34-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-34-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-34-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-34-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Toy environment: <strong>sorting n numbers</strong>.<br />
    <ul>
      <li>Prompt: a sequence of numbers.<br /></li>
      <li>Desired response: the <strong>sorted sequence</strong>.<br /></li>
      <li>Reward must quantify closeness to ground truth.</li>
    </ul>
  </li>
  <li>Possible reward formulations:
    <ul>
      <li><strong>Binary correct/incorrect</strong>: severe sparsity.<br /></li>
      <li><strong>Position-match counts</strong>: partial credit by summing positions that match the sorted result.<br /></li>
      <li><strong>Inclusion-plus-adjacency</strong>: points for presence of prompt tokens and for correctly sorted adjacent pairs—richer shaping signals.<br /></li>
    </ul>
  </li>
  <li>Reward-engineering trade-offs:
    <ul>
      <li><strong>Too sparse</strong>: prevents learning.<br /></li>
      <li><strong>Too permissive</strong>: can be exploited or mislead optimization.<br /></li>
      <li>Designing a reward that balances <strong>informativeness</strong> and <strong>robustness</strong> is a critical practical decision.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="simplified-model-architecture-and-sampling-strategy-for-the-sorting-task">Simplified model architecture and sampling strategy for the sorting task</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-39-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-39-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-39-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-39-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Minimal parametric model for on-laptop experiments:
    <ul>
      <li>Fixed, equal prompt and response lengths to simplify indexing.<br /></li>
      <li><strong>Positional information</strong> captured via per-position parameter matrices.<br /></li>
      <li><strong>Encoding</strong> collapses position embeddings into a prompt summary.<br /></li>
      <li><strong>Decoding</strong> produces logits per response position <strong>independently</strong> (non-autoregressive) to simplify implementation.<br /></li>
    </ul>
  </li>
  <li>Forward pass:
    <ul>
      <li>Map batch-by-position inputs through embeddings and per-position linear transforms to produce logits over the vocabulary for each output position.<br /></li>
      <li>Use logits to <strong>sample multiple response trials</strong> per prompt.<br /></li>
    </ul>
  </li>
  <li>Rationale:
    <ul>
      <li>Trades realism for tractability but preserves core elements needed to illustrate <strong>policy-gradient training</strong>, <strong>grouped sampling</strong>, and <strong>reward-driven updates</strong>.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="computing-deltas-centering-normalization-and-optional-max-only-selection">Computing deltas: centering, normalization, and optional max-only selection</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-44-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-44-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-44-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-44-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>compute_deltas: converts raw rewards into scalar multipliers for gradient updates. Common choices include:
    <ul>
      <li><strong>Raw rewards</strong> (no transformation).<br /></li>
      <li><strong>Centering</strong> by subtracting per-prompt means.<br /></li>
      <li><strong>Normalization</strong> by dividing by standard deviation (with epsilon for numerical stability).<br /></li>
      <li><strong>Max-only</strong>: zero out any response that is not the batch maximum.<br /></li>
    </ul>
  </li>
  <li>Practical effects:
    <ul>
      <li><strong>Centering</strong> turns sparse 0/1 rewards into both positive and negative signals, so incorrect responses produce negative updates and correct ones positive—helps learning when positives are rare.<br /></li>
      <li><strong>Normalization</strong> yields scale invariance to multiplicative reward changes.<br /></li>
      <li><strong>Max-only selection</strong> enforces an all-or-nothing strategy to avoid rewarding trivial partial-credit solutions.<br /></li>
    </ul>
  </li>
  <li>Takeaway:
    <ul>
      <li>The chosen delta transformation profoundly affects <strong>update direction</strong>, <strong>stability</strong>, and <strong>sensitivity</strong> to reward scaling.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="log-probability-extraction-and-naive-policy-gradient-loss-computation">Log-probability extraction and naive policy-gradient loss computation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-48-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-48-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-48-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-48-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Computing log-probabilities and loss:
    <ul>
      <li>After sampling and scoring responses, compute the <strong>log-probability</strong> of each sampled token sequence by indexing model logits at the chosen token indices and <strong>summing or averaging across positions</strong>.<br /></li>
    </ul>
  </li>
  <li>Naive policy-gradient loss (outcome reward regime):
    <ul>
      <li>Broadcast the per-response <strong>delta</strong> to all positions for that response.<br /></li>
      <li>Multiply the delta by the corresponding per-position log-probabilities and form the (negative) expected delta-weighted log-probability averaged across batch and trials.<br /></li>
    </ul>
  </li>
  <li>Notes:
    <ul>
      <li>This direct score-function estimator implements the <strong>Monte Carlo policy gradient</strong> for both non-autoregressive and autoregressive decodings.<br /></li>
      <li><strong>Position broadcasting</strong> reflects the single-return-per-response assumption.<br /></li>
      <li>The loss is modular: different delta computations (centered, normalized, clipped) can plug into the same gradient pipeline.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="freezing-reference-quantities-no-grad-and-importance-ratio-pitfalls">Freezing reference quantities (no-grad) and importance-ratio pitfalls</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-51-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-51-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-51-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-51-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Importance-weighted ratios (pi_theta / pi_old):
    <ul>
      <li>Treat the <strong>denominator as a constant</strong> by disabling gradient flow through the old-policy computation.<br /></li>
      <li>If you differentiate through both numerator and denominator, you can <strong>nullify gradients</strong> (e.g., ratio of identical parameterizations yields 1 and zero gradient), defeating learning.<br /></li>
    </ul>
  </li>
  <li>Implementation guidance:
    <ul>
      <li>Wrap old-policy log-probabilities or probabilities in <strong>no-grad</strong> contexts or <strong>cache scalar log-probabilities</strong> from a frozen checkpoint so the backward pass only propagates through the current policy.<br /></li>
      <li>This engineering detail is critical for correctness when using <strong>PPO-style clipping</strong> or importance-weighted gradient estimators.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="credit-assignment-and-discounting-considerations-for-outcome-rewards">Credit assignment and discounting considerations for outcome rewards</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-52-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-52-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-52-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-52-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Credit assignment challenges:
    <ul>
      <li>Classical RL tools like <strong>discounting</strong> and <strong>bootstrapping</strong> are ambiguous when the reward is only available at episode end and many intermediate token decisions can matter.<br /></li>
      <li>Discounting earlier tokens is not obviously beneficial because <strong>early strategic choices</strong> may determine later correctness more than later tokens themselves.<br /></li>
    </ul>
  </li>
  <li>Practical approaches:
    <ul>
      <li>Often <strong>smear credit</strong> across the entire response (broadcast final reward to all token positions).<br /></li>
      <li>Or design <strong>process-level rewards</strong> if reliable intermediate signals exist.<br /></li>
    </ul>
  </li>
  <li>Ongoing challenge:
    <ul>
      <li>Designing effective <strong>credit-assignment mechanisms</strong> remains a core difficulty in sparse-reward LM tasks.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="grpo-clipped-objective-and-kl-regularization">GRPO clipped objective and KL regularization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/00-55-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/00-55-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/00-55-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/00-55-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>GRPO loss</strong> (group-relative, PPO-style):
    <ol>
      <li>Compute <strong>importance-weighted ratios</strong> between current and old policies for sampled responses.<br /></li>
      <li>Multiply ratios by <strong>deltas</strong> (reward/advantage variants).<br /></li>
      <li><strong>Clip</strong> ratios to [1 − ε, 1 + ε] to bound update magnitudes and prevent destructive policy shifts.<br /></li>
      <li>Take the <strong>minimum</strong> of the unclipped and clipped ratio-weighted deltas (with sign change to convert reward maximization into loss minimization).<br /></li>
    </ol>
  </li>
  <li>Regularization:
    <ul>
      <li>An auxiliary <strong>KL penalty</strong> between the current policy and a <strong>slowly updated reference policy</strong> provides additional stabilization.<br /></li>
      <li>Unbiased but lower-variance estimators of KL are possible via algebraic rearrangement (e.g., q/p − log(q/p) − 1 forms).<br /></li>
    </ul>
  </li>
  <li>Purpose:
    <ul>
      <li><strong>Clipping</strong> and <strong>KL regularization</strong> are practical mechanisms to stabilize policy updates in LM fine-tuning contexts.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="algorithm-components-innerouter-loop-structure-and-computational-workflow">Algorithm components, inner/outer loop structure, and computational workflow</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/01-01-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/01-01-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/01-01-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/01-01-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Typical training loop structure:
    <ol>
      <li>Outer loop: <strong>generate a fresh batch of responses</strong> (inference rollouts).<br /></li>
      <li>Inner loop: perform <strong>multiple gradient steps</strong> on that static set of rollouts to amortize expensive sampling costs.<br /></li>
    </ol>
  </li>
  <li>Key components:
    <ul>
      <li><strong>Current policy</strong> being trained.<br /></li>
      <li><strong>Frozen old policy</strong> used to compute importance ratios for clipping.<br /></li>
      <li><strong>Slower-moving reference policy</strong> for KL regularization (updated less frequently).<br /></li>
    </ul>
  </li>
  <li>Practical considerations:
    <ul>
      <li>Cache log-probabilities from the rollout stage to avoid recomputing frozen-model outputs and reduce compute/memory needs.<br /></li>
      <li>Architecting inference workers, checkpointing, and distributed pipelines is a major engineering concern beyond the scalar algorithmic choices.<br /></li>
      <li>The inner/outer loop design reflects a trade-off between <strong>sample efficiency</strong> and <strong>inference cost</strong> at scale.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="why-a-separate-kl-reference-vs-old-policy-and-initialization-considerations">Why a separate KL reference vs. old policy and initialization considerations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/01-05-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/01-05-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/01-05-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/01-05-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Role of a slowly updated reference model for KL regularization:
    <ul>
      <li><strong>Decouples</strong> the long-term regularization target from the short-term importance-ratio stabilization provided by the old policy.<br /></li>
      <li>Helps define a <strong>stable optimization objective</strong> over inner-loop updates.<br /></li>
    </ul>
  </li>
  <li>Practical notes:
    <ul>
      <li>If the KL target were the immediately previous policy (changing on the same timescale), regularization would shift too rapidly and could undermine convergence.<br /></li>
      <li>Common practices: use a frozen checkpoint or periodic parameter copies as a constant <strong>regularization anchor</strong>; compute pi_old quantities from cached log-probabilities.<br /></li>
      <li>Even when pi_old equals the current policy in initial iterations, valid updates occur because pi_old is treated as a <strong>constant</strong> in gradient computations.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="empirical-sorting-task-experiments-and-learning-curve-interpretation">Empirical sorting-task experiments and learning curve interpretation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/01-10-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/01-10-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/01-10-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/01-10-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Empirical observations on the toy sorting task:
    <ul>
      <li><strong>Centered-reward</strong> updates often yield <strong>modest mean-reward improvements</strong>, pushing the policy away from lower-reward responses within a prompt cohort.<br /></li>
      <li>Limitations:
        <ul>
          <li>If all sampled responses for a prompt have identical rewards, centered deltas become zero and produce <strong>no update</strong> for that batch.<br /></li>
          <li><strong>Loss curves</strong> can be misleading because the data distribution changes as the policy evolves.<br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Monitoring recommendations:
    <ul>
      <li>Track reward metrics on <strong>held-out prompts</strong> or <strong>regenerate rollouts</strong> to assess true progress.<br /></li>
      <li>Experiments illustrate sensitivity to <strong>reward design</strong>, <strong>delta transformations</strong>, <strong>sampling variance</strong>, and the need for careful hyperparameter tuning.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="conclusions-promise-of-rl-for-lms-and-scaling-challenges">Conclusions: promise of RL for LMs and scaling challenges</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec17/01-13-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec17/01-13-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec17/01-13-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec17/01-13-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Why use RL for LMs:
    <ul>
      <li><strong>Optimizes behaviors</strong> that exceed imitation-limited supervised data by directly optimizing measurable objectives when <strong>verifiable rewards</strong> exist.<br /></li>
      <li>Powerful for improving LM performance on tasks with <strong>quantifiable outcomes</strong>.<br /></li>
    </ul>
  </li>
  <li>Persistent challenges:
    <ul>
      <li><strong>Sparse and delayed rewards</strong>.<br /></li>
      <li>High <strong>variance</strong> in Monte Carlo estimators.<br /></li>
      <li><strong>Reward-design vulnerabilities</strong> (hackability/exploitation).<br /></li>
      <li>Complex <strong>credit assignment</strong> across long token sequences.<br /></li>
    </ul>
  </li>
  <li>Systems-level complexity:
    <ul>
      <li>Building scalable RL for LMs adds engineering burden beyond pretraining: <strong>inference cost</strong>, <strong>multi-model orchestration</strong> (policy, old policy, reference, reward models), and <strong>distributed execution</strong> must be managed.<br /></li>
    </ul>
  </li>
  <li>Outlook:
    <ul>
      <li>Continued research on <strong>reward specification</strong>, <strong>variance reduction</strong>, and <strong>system-level infrastructure</strong> is necessary to realize RL’s potential for large-scale language-model improvement.<br /></li>
    </ul>
  </li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 15 - Alignment - SFT/RLHF</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec15/" rel="alternate" type="text/html" title="CS336 Lecture 15 - Alignment - SFT/RLHF" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec15</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec15/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Dfu7vC9jo4w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-and-objectives">Lecture overview and objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-01-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-01-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-01-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-01-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment outlines the scope and schedule of the lecture series, highlighting a transition from <strong>pre-training</strong> to <strong>post-training</strong> techniques with a primary focus on <strong>reinforcement learning from human feedback (RLHF)</strong> and <strong>safety/alignment</strong>.<br /></p>

<p>The immediate goals are defined as:</p>
<ul>
  <li>Convert large <strong>pre-trained language models</strong> into <strong>useful</strong>, <strong>controllable</strong>, and <strong>safe instruction-following systems</strong>.</li>
  <li>Preview follow-up material on <strong>verifiable reward-based training</strong>.<br /></li>
</ul>

<p>This lecture is positioned within a <strong>two-part post-training module</strong> and frames the central problem statement that subsequent sections address:</p>
<ol>
  <li>How to transform a general pre-trained model into a <strong>deployable instruction-following system</strong>.</li>
  <li>How to add <strong>guard rails</strong> and achieve <strong>product-quality behavior</strong>.<br /></li>
</ol>

<hr />

<h1 id="motivation-for-post-training-and-the-chatgpt-transition">Motivation for post-training and the ChatGPT transition</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-03-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-03-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-03-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-03-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment motivates <strong>post-training</strong> by contrasting large-scale <strong>pre-training</strong> (e.g., GPT-3) with product-ready <strong>instruction-following systems</strong> (e.g., ChatGPT).<br /></p>

<p>Key points:</p>
<ul>
  <li><strong>Pre-training</strong> encodes many capabilities distributed across parameters, but it does <strong>not reliably produce instruction-following behavior</strong> without targeted post-training.</li>
  <li>Product constraints like <strong>usability</strong>, <strong>toxicity control</strong>, and <strong>guard rails</strong> make post-training practically essential.<br /></li>
</ul>

<p>The section highlights broader considerations that drive post-training work:</p>
<ul>
  <li><strong>Societal impact</strong> of deployed models.</li>
  <li>Research questions on <strong>data collection</strong>, <strong>algorithms</strong>, and <strong>scalability</strong> required to make pre-trained models safe and useful in real products.<br /></li>
</ul>

<hr />

<h1 id="instructgpt-three-step-pipeline-and-lecture-structure">InstructGPT three-step pipeline and lecture structure</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-05-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-05-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-05-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-05-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces the <strong>InstructGPT three-step post-training pipeline</strong> as the organizing framework:</p>
<ol>
  <li><strong>Supervised fine-tuning (SFT)</strong> on demonstrations.</li>
  <li><strong>Reward modeling</strong> from pairwise feedback.</li>
  <li><strong>Reinforcement learning</strong> for policy optimization.<br /></li>
</ol>

<p>The lecture will follow this structure, covering:</p>
<ul>
  <li><strong>SFT</strong> (leftmost block) first.</li>
  <li>Then <strong>pairwise feedback</strong> and <strong>RLHF</strong>.</li>
  <li>Finally practical and algorithmic details.<br /></li>
</ul>

<p>It sets up two axes of concern for SFT:</p>
<ul>
  <li>The <strong>nature and cost of training data</strong>.</li>
  <li>The <strong>adaptation method</strong> (e.g., gradient-based fine-tuning vs. alternative approaches).<br /></li>
</ul>

<hr />

<h1 id="importance-of-post-training-data-quality-and-collection-considerations">Importance of post-training data quality and collection considerations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-06-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-06-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-06-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-06-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment argues that <strong>post-training data matters more than ever</strong> due to <strong>small-data, high-leverage effects</strong>: noisy instruction-tuning data can provably harm model behavior and desirable behaviors must be encoded precisely.<br /></p>

<p>Practical concerns for data collection include:</p>
<ul>
  <li><strong>Annotator expertise</strong> and variability.</li>
  <li><strong>Annotation cost</strong> and incentives.</li>
  <li><strong>Style and length variability</strong> across examples.</li>
  <li><strong>Safety coverage</strong> and handling edge cases.</li>
  <li>Trade-offs between <strong>quantity</strong> and <strong>per-example quality</strong>.<br /></li>
</ul>

<p>The segment positions these considerations as prerequisites for selecting downstream algorithmic approaches, noting that some data types (e.g., <strong>expert demonstrations</strong>, <strong>pairwise feedback</strong>) are more directly usable than others.<br /></p>

<hr />

<h1 id="flan-dataset-paradigm-aggregating-existing-nlp-datasets">FLAN dataset paradigm: aggregating existing NLP datasets</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-07-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-07-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-07-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-07-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment documents the <strong>FLAN</strong> paradigm: many standard supervised NLP datasets (QA, classification, NLI, etc.) are reformatted and aggregated into a single <strong>instruction-style corpus</strong> for instruction tuning.<br /></p>

<p>How FLAN works and why it’s used:</p>
<ul>
  <li>It <strong>leverages existing labeled benchmarks</strong> (e.g., Natural Instructions, adversarial QA) to provide broad task coverage at scale.</li>
  <li>Requires <strong>minimal human authoring</strong> of new chat-style examples since it reformats existing datasets.<br /></li>
</ul>

<p>Strengths and weaknesses:</p>
<ul>
  <li>Strengths: <strong>scale</strong>, <strong>task diversity</strong>.</li>
  <li>Weaknesses: <strong>surgical reformatting</strong> can produce <strong>unnatural prompt-response distributions</strong> that differ from real chat interactions and introduce distributional artifacts.<br /></li>
</ul>

<hr />

<h1 id="openassistant-and-alpaca-paradigms-for-instruction-data">OpenAssistant and Alpaca paradigms for instruction data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-07-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-07-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-07-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-07-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment contrasts two other paradigms: <strong>OpenAssistant</strong> and <strong>Alpaca</strong>.<br /></p>

<p>OpenAssistant:</p>
<ul>
  <li>A <strong>crowdsourced, human-authored</strong> instruction dataset.</li>
  <li>Produces <strong>long, detailed responses</strong> and often includes <strong>citations</strong>.</li>
  <li>Emphasizes <strong>human quality</strong> and richness.<br /></li>
</ul>

<p>Alpaca:</p>
<ul>
  <li>Starts from a <strong>small seed of human-written instructions</strong> that are expanded by a <strong>language model</strong> and filled by a stronger model (a <strong>distillation-style</strong> process).</li>
  <li>Emphasizes <strong>cheap synthetic instruction data</strong> that closely matches conversational inputs.<br /></li>
</ul>

<p>Trade-offs and differences:</p>
<ul>
  <li><strong>Diversity</strong> and <strong>authoring cost</strong> differ dramatically.</li>
  <li>Artifact profiles vary by paradigm: <strong>response length</strong>, <strong>citation behavior</strong>, and <strong>multimodal style</strong> are distinct across them.<br /></li>
</ul>

<hr />

<h1 id="concrete-examples-from-flan-and-implications-for-instruction-tuning">Concrete examples from FLAN and implications for instruction tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-09-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-09-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-09-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-09-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment inspects representative <strong>FLAN</strong> examples (article highlights, multiple-choice classification, email subject generation, E2E text generation) to show how reformatted benchmark data appears when repurposed for instruction tuning.<br /></p>

<p>Key observations:</p>
<ul>
  <li>Much of FLAN-style data is <strong>task-centric</strong>, often <strong>brief</strong>, and <strong>visibly constructed</strong>.</li>
  <li>This brevity and artificial construction can create a <strong>mismatch</strong> with real chat-style user interactions.<br /></li>
</ul>

<p>Conclusion: aggregated benchmark data provides <strong>scale</strong> but introduces <strong>distributional artifacts</strong> that can bias instruction-following behavior and style.<br /></p>

<hr />

<h1 id="alpaca-generation-pipeline-and-response-style-differences">Alpaca generation pipeline and response style differences</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-11-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-11-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-11-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-11-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment describes the <strong>Alpaca pipeline</strong> in detail:</p>
<ol>
  <li>Start with a set of <strong>human-written instructions</strong>.</li>
  <li>Use a <strong>language model</strong> to generate augmented instructions.</li>
  <li>Use an <strong>instruction-capable model</strong> (e.g., InstructGPT) to produce long-form completions.<br /></li>
</ol>

<p>Characteristics and trade-offs:</p>
<ul>
  <li><strong>Alpaca-style data</strong> tends to produce <strong>conversational, long-form, chat-like</strong> inputs and outputs compared to terse FLAN outputs.</li>
  <li>Generated data can be <strong>less diverse</strong> in instruction forms and may <strong>preserve biases or limitations</strong> of the generator.<br /></li>
</ul>

<p>Emphasis: there is a trade-off between <strong>human-authored richness</strong> and <strong>model-generated scale and uniformity</strong>.<br /></p>

<hr />

<h1 id="crowdsourcing-interactive-exercise-and-annotator-response-patterns">Crowdsourcing interactive exercise and annotator response patterns</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-13-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-13-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-13-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-13-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment reports on a crowdsourcing-style interactive exercise where participants author instruction-following responses, illustrating typical annotator behaviors and quality differences.<br /></p>

<p>Empirical findings:</p>
<ul>
  <li>Many annotators produce <strong>short, low-effort responses</strong> under time pressure.</li>
  <li>Obtaining <strong>high-quality, long-form human responses</strong> is <strong>costly</strong> and difficult to scale.<br /></li>
</ul>

<p>Common artifacts in crowd data:</p>
<ul>
  <li><strong>Emoji insertion</strong>, <strong>templated answers</strong>, <strong>shallow factuality checking</strong>.</li>
  <li>The practical challenge of eliciting <strong>consistent, detailed demonstrations</strong> at scale is substantial.<br /></li>
</ul>

<hr />

<h1 id="annotator-difficulties-and-the-incentives-for-ai-generated-feedback">Annotator difficulties and the incentives for AI-generated feedback</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-15-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-15-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-15-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-15-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment analyzes why annotators often produce short or low-quality responses: <strong>time constraints</strong>, <strong>lack of motivation</strong>, and <strong>task unfamiliarity</strong>.<br /></p>

<p>Empirical observation: human and AI evaluators tend to <strong>prefer longer, more detailed outputs ~60–70% of the time</strong>, which introduces bias toward verbosity if evaluation or reward correlates with length.<br /></p>

<p>This partly explains the rise of <strong>AI-generated feedback</strong>: models like <strong>GPT-4</strong> produce long, polished outputs at low marginal cost, making <strong>synthetic supervision</strong> attractive to practitioners.<br /></p>

<hr />

<h1 id="evaluation-trade-offs-chat-style-judgments-versus-benchmark-metrics">Evaluation trade-offs: chat-style judgments versus benchmark metrics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-17-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-17-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-17-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-17-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment compares <strong>open-ended chat-style evaluation</strong> methods (user engagement, chatbot arena, AlpacaEval) with <strong>traditional benchmark evaluations</strong> (e.g., MMLU).<br /></p>

<p>Trade-offs:</p>
<ul>
  <li>Chat-style evaluations capture <strong>user-facing quality</strong> and <strong>preference</strong>, but introduce confounders such as <strong>output length</strong> and <strong>style</strong>.</li>
  <li>Benchmarks provide <strong>stable, task-specific fidelity metrics</strong> that are less sensitive to surface stylistic changes.<br /></li>
</ul>

<p>Recommendation: use a <strong>diverse evaluation suite</strong> combining interactive preference tests and standardized benchmarks to mitigate evaluation biases during post-training.<br /></p>

<hr />

<h1 id="risk-of-teaching-hallucination-via-rich-sft-data-and-the-citation-failure-mode">Risk of teaching hallucination via rich SFT data and the citation failure mode</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-19-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-19-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-19-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-19-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment articulates a key failure mode: when <strong>SFT</strong> displays high-quality, citation-rich targets that the base model does not already know, the model can learn <strong>superficial structural patterns</strong> (e.g., append a citation) rather than correct factual retrieval.<br /></p>

<p>Two simultaneous effects of such SFT examples:</p>
<ul>
  <li>When the model <strong>already encodes</strong> the fact, the SFT can produce <strong>correct knowledge acquisition</strong>.</li>
  <li>When the model <strong>lacks</strong> the underlying knowledge, SFT encourages <strong>hallucinatory fabrications</strong> of supporting details.<br /></li>
</ul>

<p>This is a <strong>distributional mismatch</strong>: SFT can incentivize inappropriate surface-level completions if the model cannot <strong>reliably ground</strong> answers.<br /></p>

<hr />

<h1 id="mitigation-perspective-abstention-tools-and-reinforcement-approaches">Mitigation perspective: abstention, tools, and reinforcement approaches</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-21-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-21-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-21-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-21-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explores mitigation strategies for <strong>hallucination</strong> and inappropriate surface behavior:</p>
<ul>
  <li>Train models to <strong>abstain</strong> when uncertain.</li>
  <li><strong>Augment</strong> models with <strong>retrieval/tooling</strong> to ground claims.</li>
  <li>Use <strong>reinforcement learning</strong> methods to selectively teach behaviors the model can reliably perform.<br /></li>
</ul>

<p>Practical notes:</p>
<ul>
  <li><strong>Abstention</strong> and <strong>tool-use</strong> require explicit dataset design or tool-augmented training.</li>
  <li><strong>RLHF</strong> and reward-based methods can in principle shape models to prefer abstention or verification when factuality is not assured.<br /></li>
</ul>

<p>Empirical point: models reproduce <strong>known facts</strong> more readily than they learn <strong>new unknown facts</strong> through small SFT datasets.<br /></p>

<hr />

<h1 id="counterintuitive-nature-of-high-quality-instruction-data">Counterintuitive nature of ‘high-quality’ instruction data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-24-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-24-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-24-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-24-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment summarizes a counterintuitive conclusion: <strong>highly correct and knowledge-rich SFT datasets</strong> can nevertheless <strong>degrade downstream behavior</strong> because they encourage outputs that mimic high-level structures without true grounding.<br /></p>

<p>Why this paradox matters:</p>
<ul>
  <li>Exposing a model to expert outputs is <strong>insufficient</strong> if the model lacks the internal representation to support those outputs truthfully.</li>
  <li>Motivates <strong>cautious dataset engineering</strong>, careful <strong>distillation practices</strong>, and <strong>alignment-aware training</strong>.<br /></li>
</ul>

<p>Recommendation: prefer techniques that allow the model to <strong>abstain</strong> or <strong>request tools</strong> rather than forcing proximate but potentially false completions.<br /></p>

<hr />

<h1 id="safety-tuning-and-refusal-trade-offs">Safety tuning and refusal trade-offs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-28-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-28-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-28-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-28-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment addresses <strong>safety-specific post-training</strong> considerations, describing <strong>safety tuning</strong> as targeted instruction-tuning that mixes refusal, content moderation, and safer completions into training data.<br /></p>

<p>Central trade-off:</p>
<ul>
  <li><strong>Excessive refusal</strong> reduces utility by blocking legitimate, nuanced queries.</li>
  <li><strong>Insufficient refusal</strong> allows harmful or toxic outputs to surface.<br /></li>
</ul>

<p>Evidence and practice:</p>
<ul>
  <li>Even <strong>small curated safety datasets</strong> (on the order of hundreds of examples) can meaningfully alter behavior.</li>
  <li><strong>Mixing safety data</strong> into broader instruction tuning is a pragmatic mitigation strategy.<br /></li>
</ul>

<hr />

<h1 id="scaling-instruction-tuning-via-mid-training-data-mixing">Scaling instruction tuning via mid-training data mixing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-30-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-30-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-30-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-30-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains the practice of <strong>mixing instruction-tuning or high-quality supervision datasets</strong> into later stages of large-scale pre-training (a <strong>mid-training</strong> or <strong>decay-stage mix</strong>).<br /></p>

<p>Operational recipe:</p>
<ol>
  <li>Conduct standard <strong>pre-training</strong>.</li>
  <li>Transition into a <strong>decay stage</strong> where the learning rate is annealed.</li>
  <li>Progressively add higher-quality or <strong>instructional data</strong> into the training mix to integrate instruction-following behavior without catastrophic forgetting.<br /></li>
</ol>

<p>Discussion: mid-training blending blurs the boundary between a <strong>base</strong> pre-trained model and a <strong>post-trained</strong> instruction-tuned model and provides empirical leverage by integrating behavioral data at scale.<br /></p>

<hr />

<h1 id="mini-cpm-example-of-two-stage-training-and-data-mix-composition">Mini-CPM example of two-stage training and data mix composition</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-31-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-31-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-31-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-31-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment uses the <strong>mini-CPM</strong> example to illustrate concrete data-mix engineering: a primary pre-training phase of common-crawl, code, and large corpora followed by a decay/second stage that injects Wikipedia, code-SFT, chat data, and instruction-adjacent corpora.<br /></p>

<p>Design rationale:</p>
<ul>
  <li>The <strong>decay stage</strong> intentionally contains a <strong>non-pure mixture</strong> (retaining some pre-training data) to avoid catastrophic forgetting.</li>
  <li>This smoothly shifts model behavior toward <strong>instruction-following</strong> while preserving core capabilities.<br /></li>
</ul>

<p>Empirical note: this two-stage approach is widely used in practice and is effective at integrating instruction-style behaviors into large transformers.<br /></p>

<hr />

<h1 id="practical-implication-the-ambiguous-meaning-of-base-models">Practical implication: the ambiguous meaning of ‘base models’</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-32-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-32-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-32-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-32-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment highlights that widespread mid-training and data mixing complicate the notion of a canonical <strong>“base model”</strong>: many labeled base checkpoints are already influenced by instruction-like data.<br /></p>

<p>Consequences for evaluation and comparison:</p>
<ul>
  <li>Claims about model capabilities should account for whether the claimed base model includes <strong>mid-training instruction mixtures</strong> or represents <strong>pure pre-training</strong>.</li>
  <li>Recommend <strong>careful documentation</strong> of data mixtures and training stages when reporting model baselines and public releases.<br /></li>
</ul>

<hr />

<h1 id="concept-thinking-tokens-and-adaptive-pre-training-limitations">Concept: thinking tokens and adaptive pre-training limitations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-35-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-35-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-35-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-35-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment discusses an idea to embed <strong>‘thought’ or verification tokens</strong> in pre-training to make the model check its knowledge during training, then assesses practical limitations.<br /></p>

<p>Key observations:</p>
<ul>
  <li>Adaptive schemes that check whether the model <strong>knows</strong> a fact before applying supervision begin to resemble <strong>reinforcement learning</strong> because they require conditional updates based on current model state.</li>
  <li>Engineering constraints: static pre-training datasets and batched computation make <strong>adaptive per-example decisions</strong> difficult at pre-training scale without an RL-style training loop and dynamic data generation.<br /></li>
</ul>

<hr />

<h1 id="star-and-quiet-star-style-approaches-for-training-internal-thought-processes">STAR and Quiet-STAR-style approaches for training internal thought processes</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-38-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-38-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-38-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-38-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment connects internal verification ideas to existing methods such as <strong>STAR</strong> and <strong>Quiet-STAR</strong>, which train models to generate intermediate <strong>‘thought’ traces</strong> and then reinforce those traces conditionally on final correctness.<br /></p>

<p>Operational pattern:</p>
<ol>
  <li>Predict a <strong>chain-of-thought</strong> or internal reasoning sequence.</li>
  <li>Evaluate the outcome.</li>
  <li>Reinforce thought traces that lead to <strong>correct outputs</strong>.<br /></li>
</ol>

<p>Interpretation: these methods effectively implement a form of <strong>reward-guided supervised learning</strong>, akin to early RL-style interventions that shape internal reasoning behaviors.<br /></p>

<hr />

<h1 id="style-imprinting-and-token-pattern-learning-in-sft-emoji-example">Style imprinting and token-pattern learning in SFT (emoji example)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-39-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-39-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-39-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-39-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment analyzes how <strong>instruction tuning</strong> reliably transfers <strong>output style</strong> and <strong>token-level signatures</strong>—using the emoji example to illustrate that routine post-training tokens get reproduced.<br /></p>

<p>Main takeaways:</p>
<ul>
  <li>Instruction tuning most reliably teaches the <strong>‘type signature’</strong> or <strong>surface style</strong> of outputs (formatting, length, token presence).</li>
  <li><strong>Deeper grounded behavior</strong> requires sufficiently diverse and content-rich supervision relative to pre-trained knowledge.<br /></li>
</ul>

<p>The segment emphasizes: surface-style learning is a <strong>predictable outcome of SFT</strong>, while substantive content learning is <strong>scale- and distribution-dependent</strong>.<br /></p>

<hr />

<h1 id="instruction-tuning-and-knowledge-acquisition-limits">Instruction tuning and knowledge acquisition limits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-41-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-41-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-41-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-41-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment clarifies that <strong>instruction tuning can teach new factual knowledge</strong> if it is <strong>scaled and diversified</strong> sufficiently—especially when integrated via <strong>mid-training</strong>—but smaller SFT-only routines typically have limited capacity to reliably instill novel world knowledge.<br /></p>

<p>Key distinctions:</p>
<ul>
  <li>Differences hinge on <strong>data scale</strong>, <strong>diversity</strong>, and whether instruction examples are exposed in a <strong>pre-training-like mixing regime</strong>.</li>
  <li><strong>Mid-training hybrids</strong> bridge pre-training knowledge acquisition and SFT behavioral shaping, enabling both content learning and style transfer.<br /></li>
</ul>

<hr />

<h1 id="transition-to-reinforcement-learning-from-human-feedback-rlhf-conceptual-framing">Transition to reinforcement learning from human feedback (RLHF) conceptual framing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-42-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-42-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-42-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-42-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment sets up the conceptual transition from <strong>probabilistic generative modeling</strong> to a <strong>reward-maximization perspective</strong>: instead of estimating a generative reference distribution p*, <strong>RLHF</strong> treats the LM as a <strong>policy π(y|x)</strong> to be optimized for expected reward <strong>R(x,y)</strong>.<br /></p>

<p>Formal objective shift:</p>
<ul>
  <li>Training target becomes any policy that attains <strong>high expected reward</strong> under a <strong>human-aligned reward function</strong>.</li>
  <li>This <strong>decouples</strong> training from the goal of matching a specific data distribution and reframes the problem as <strong>policy optimization</strong>.<br /></li>
</ul>

<p>The segment motivates RLHF on conceptual grounds and contrasts <strong>distributional imitation</strong> with <strong>policy optimization</strong>.<br /></p>

<hr />

<h1 id="two-primary-rationales-for-rlhf-data-cost-and-generator-validator-asymmetry">Two primary rationales for RLHF: data cost and generator-validator asymmetry</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-44-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-44-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-44-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-44-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment articulates two practical motivations for <strong>RLHF</strong>:</p>
<ol>
  <li><strong>Expert demonstrations</strong> are expensive to collect at scale.</li>
  <li><strong>Human validators</strong> can often <strong>judge or compare outputs</strong> more cheaply and sometimes more reliably than producing full demonstrations, creating a <strong>generator–validator gap</strong>.<br /></li>
</ol>

<p>Empirical evidence: validators sometimes <strong>prefer model outputs</strong> to their own generations, making <strong>preference data</strong> an efficient supervisory signal.<br /></p>

<p>The segment frames <strong>pairwise preference collection</strong> and <strong>reward modeling</strong> as approaches that leverage cheaper validator comparisons to guide policy optimization.<br /></p>

<hr />

<h1 id="rlhf-pipeline-pairwise-feedback-reward-modeling-and-policy-optimization">RLHF pipeline: pairwise feedback, reward modeling, and policy optimization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-47-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-47-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-47-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-47-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment outlines the standard <strong>RLHF pipeline</strong>:</p>
<ol>
  <li>Collect model outputs (<strong>rollouts</strong>).</li>
  <li>Collect <strong>pairwise human preferences</strong> comparing outputs.</li>
  <li>Train a <strong>reward model</strong> to predict scalar scores that explain pairwise judgments (e.g., Bradley–Terry/logistic differences).</li>
  <li>Optimize the policy to increase expected reward, often with a <strong>KL penalty</strong> to limit divergence.<br /></li>
</ol>

<p>Practical details:</p>
<ul>
  <li>Data collection interfaces include <strong>side-by-side comparisons</strong> and guidelines emphasizing <strong>helpfulness/truthfulness/harmlessness</strong>.</li>
  <li><strong>Annotation guidelines</strong> shape reward model semantics; the reward model operationalizes human preferences into a differentiable scalar objective for RL.<br /></li>
</ul>

<hr />

<h1 id="annotation-guidelines-and-practicalities-for-pairwise-preference-collection">Annotation guidelines and practicalities for pairwise preference collection</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-51-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-51-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-51-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-51-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment reviews concrete <strong>annotation guidelines</strong> used in practice (e.g., InstructGPT, leaked Bard guidelines) that operationalize evaluation into pillars such as <strong>helpfulness</strong>, <strong>truthfulness</strong>, and <strong>harmlessness</strong>, plus style considerations and rating scales.<br /></p>

<p>Practical constraints and sensitivities:</p>
<ul>
  <li>Annotator time budgets are often <strong>around one minute per example</strong>.</li>
  <li>Early RLHF efforts used <strong>small annotator pools</strong>, increasing susceptibility to bias.</li>
  <li><strong>Clear instructions</strong>, rater training, and vendor management materially affect the reliability and systematic biases of preference data.<br /></li>
</ul>

<hr />

<h1 id="interactive-pairwise-evaluation-exercise-outcomes-and-empirical-difficulty">Interactive pairwise evaluation exercise outcomes and empirical difficulty</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-55-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-55-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-55-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-55-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment reports on an in-class <strong>pairwise evaluation exercise</strong>, documenting empirical challenges:</p>
<ul>
  <li>Limited time drastically reduces factual checking.</li>
  <li>Annotators often choose <strong>longer but hallucinated responses</strong>.</li>
  <li>Distinguishing correctness requires <strong>decomposing outputs into verifiable claims</strong>, which is labor-intensive.<br /></li>
</ul>

<p>Conclusion: large-scale, high-quality pairwise annotation is <strong>hard, expensive</strong>, and susceptible to annotator shortcuts (e.g., copying model outputs into comparator tools).<br /></p>

<hr />

<h1 id="ethical-demographic-and-bias-considerations-in-rlhf-annotation">Ethical, demographic, and bias considerations in RLHF annotation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/00-57-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/00-57-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/00-57-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/00-57-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment discusses ethical and bias considerations in <strong>RLHF data collection</strong>, including annotator compensation, demographic composition effects, and cultural/regional value alignment shifts induced by annotator pools.<br /></p>

<p>Empirical evidence: annotator nationality and background can <strong>systematically alter model preferences</strong> (alignment shifts correlated with annotator locations), and different annotator cohorts sometimes <strong>prioritize format over factuality</strong>.<br /></p>

<p>Recommendations: careful <strong>annotator selection</strong>, fair <strong>payment practices</strong>, and <strong>transparency about annotator demographics</strong> to mitigate unwanted alignment shifts.<br /></p>

<hr />

<h1 id="ai-generated-feedback-adoption-datasets-and-length-confounds">AI-generated feedback adoption, datasets, and length confounds</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/01-00-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/01-00-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/01-00-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/01-00-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment surveys the adoption of <strong>AI-generated feedback</strong> for preference labeling (e.g., using GPT-4 or other LMs as cheaper raters), noting strong empirical agreement between AI judges and humans and cost advantages of synthetic raters.<br /></p>

<p>Examples and strategies:</p>
<ul>
  <li>Projects that heavily use LM-generated preference labels include <strong>UltraFeedback</strong>, <strong>Tulu3</strong>.</li>
  <li><strong>Constitutional AI</strong> is cited as a rule-based synthetic supervision strategy.<br /></li>
</ul>

<p>Warning: a dominant confound is that both human preferences and automated judges <strong>correlate with output length</strong>, causing reward signals to favor <strong>verbosity</strong> unless explicitly controlled.<br /></p>

<hr />

<h1 id="on-policy-versus-off-policy-preference-data-and-prompt-sourcing">On-policy versus off-policy preference data and prompt sourcing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/01-03-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/01-03-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/01-03-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/01-03-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines <strong>on-policy preference data</strong> as comparisons sampled from the model being trained and <strong>off-policy data</strong> as comparisons drawn from other models or pre-existing corpora.<br /></p>

<p>Roles and trade-offs:</p>
<ul>
  <li><strong>Off-policy data</strong> help characterize the broader landscape of possible outputs and can bootstrap reward models.</li>
  <li><strong>On-policy data</strong> provide targeted feedback for local policy improvement.<br /></li>
</ul>

<p>Prompt sourcing note: using prompts with known answers or existing ground truth helps in some domains (e.g., math) but does not generalize to many open-ended tasks where correctness is subjective or hard to adjudicate.<br /></p>

<hr />

<h1 id="capabilities-of-self-improvement-loops-and-practical-limits">Capabilities of self-improvement loops and practical limits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/01-05-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/01-05-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/01-05-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/01-05-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment examines <strong>self-improvement</strong> strategies where models generate outputs and then judge or refine them iteratively, exploiting latent pre-training information via clever prompting and self-refinement.<br /></p>

<p>Key caveats:</p>
<ul>
  <li>Models theoretically contain vast pre-training information, but <strong>empirical upper bounds and scaling behavior are uncertain</strong>.</li>
  <li>Effectiveness depends heavily on <strong>prompt engineering</strong>, <strong>model size</strong>, and the nature of the self-critique loop.<br /></li>
</ul>

<p>Framing: self-improvement is both an <strong>information-theoretic</strong> and empirical question; practical effectiveness varies across tasks.<br /></p>

<hr />

<h1 id="formal-rlhf-objective-and-reward-modeling-instructgpt-equation">Formal RLHF objective and reward modeling (InstructGPT equation)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/01-06-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/01-06-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/01-06-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/01-06-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment presents the formal <strong>RLHF optimization objective</strong> popularized by InstructGPT: maximize expected reward under a learned reward model while constraining the policy with a <strong>KL divergence</strong> term relative to a reference policy (often the SFT policy).<br /></p>

<p>Objective components:</p>
<ul>
  <li><strong>Expected reward</strong> term: Eπ[R(x,y)].</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>KL penalty</strong>: β KL(π</td>
          <td> </td>
          <td>π_ref) to prevent excessive deviation and catastrophic forgetting.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>The reward model is learned from pairwise comparisons via a <strong>Bradley–Terry/logistic</strong> observation model mapping comparative judgments to scalar utilities.<br /></li>
</ul>

<p>Additional regularization may be added to maintain desirable pre-training behaviors.<br /></p>

<hr />

<h1 id="policy-optimization-via-policy-gradients-and-ppo-po-overview">Policy optimization via policy gradients and PPO (PO) overview</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/01-09-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/01-09-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/01-09-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/01-09-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces <strong>policy-gradient-based methods</strong> used in RLHF (e.g., the PPO family), deriving gradients for expected reward and describing variance-reduction via advantage estimates and baseline subtraction.<br /></p>

<p>Practical algorithmic elements:</p>
<ol>
  <li>Collect rollouts from the current policy.</li>
  <li>Compute <strong>advantages</strong> (reward minus baseline).</li>
  <li>Apply importance-weighted gradient updates for off-policy corrections if needed.</li>
  <li>Constrain updates using <strong>ratio clipping</strong> or <strong>KL penalties</strong> to stabilize training.<br /></li>
</ol>

<p>Note: <strong>PPO-style clipping</strong> replaces explicit constrained optimization with a practical surrogate that limits policy update magnitudes.<br /></p>

<hr />

<h1 id="direct-preference-optimization-dpo-derivation-and-practical-advantages">Direct Preference Optimization (DPO) derivation and practical advantages</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec15/01-12-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec15/01-12-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec15/01-12-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec15/01-12-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment derives <strong>Direct Preference Optimization (DPO)</strong> by making a nonparametric mapping between policy and implied reward, using the <strong>Bradley–Terry</strong> model for pairwise comparisons, and converting the RLHF objective into a supervised, maximum-likelihood-like loss over preferences.<br /></p>

<p>Core steps and rationale:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Assume the optimal policy satisfies π*(y</td>
          <td>x) ∝ p_ref(y</td>
          <td>x) exp(α R(x,y)).</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Invert this relationship to express <strong>R</strong> in terms of <strong>π</strong> and <strong>π_ref</strong>.</li>
  <li>Optimize <strong>π</strong> directly by minimizing a <strong>preference-based likelihood</strong> without an explicit learned reward model or on-policy importance correction.<br /></li>
</ul>

<p>Pragmatic benefits of DPO:</p>
<ul>
  <li><strong>Removes the explicit reward model</strong>.</li>
  <li>Offers <strong>simpler training dynamics</strong>.</li>
  <li>Empirically effective as a <strong>less complex alternative</strong> to PPO-style RLHF.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 14 - Data - Part 2</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec14/" rel="alternate" type="text/html" title="CS336 Lecture 14 - Data - Part 2" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec14</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec14/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/9Cd0THLS1t0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-and-objectives">Lecture overview and objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-00-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-00-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-00-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-00-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This lecture presents a detailed treatment of <strong>data collection</strong>, <strong>preprocessing</strong>, <strong>quality filtering</strong>, and <strong>deduplication</strong> techniques used to prepare <strong>large web-scale corpora</strong> for <strong>language model training</strong>.<br /></p>

<p>It frames the problem as extracting a <strong>high-quality target subset</strong> from massive raw sources and outlines the lecture plan:<br /></p>

<ol>
  <li><strong>Review model-based filtering algorithms</strong> — survey approaches that score or classify documents for inclusion.<br /></li>
  <li><strong>Show how a single primitive supports multiple filtering tasks</strong> — demonstrate reuse of core operations across different filters.<br /></li>
  <li><strong>Describe algorithms for exact and near-duplicate detection</strong> — cover methods for identifying identical and similar documents at scale.<br /></li>
</ol>

<p>The discussion emphasizes practical constraints:</p>
<ul>
  <li><strong>Compute cost</strong> and the need to minimize expensive operations<br /></li>
  <li><strong>Scalability to web-scale data</strong> and engineering trade-offs for throughput and storage<br /></li>
  <li>Preference for <strong>fast, approximate methods</strong> over costly <strong>per-document model inference</strong> when processing billions of documents<br /></li>
</ul>

<p>The presentation positions the material as <strong>algorithmic building blocks</strong> for <strong>production data pipelines</strong>, not a purely theoretical treatment — focusing on techniques that are practical, efficient, and deployable at web scale.<br /></p>

<hr />

<h1 id="filtering-primitive-target-set-selection-from-raw-data">Filtering primitive: target set selection from raw data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-02-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-02-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-02-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-02-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Filtering for model training is cast as selecting a subset <strong>T’</strong> from a large raw dataset <strong>R</strong> that is similar to a small, high-quality target dataset <strong>T</strong>.<br />
The primitive must <strong>generalize beyond exact copies of T</strong> and execute at <strong>web-scale</strong> under strict performance constraints.<br /></p>

<p>The selection workflow (typical implementation):<br /></p>
<ol>
  <li>Fit one or more <strong>models</strong> on <strong>T</strong> and/or <strong>R</strong> to capture relevance or quality signals.<br /></li>
  <li>Use those models to produce a <strong>per-document score</strong> (the <strong>scoring function</strong>).<br /></li>
  <li>Convert scores into a subset <strong>T’</strong> via <strong>thresholding</strong> or <strong>stochastic sampling</strong>.<br /></li>
</ol>

<p>Two primary operational requirements:<br /></p>
<ul>
  <li><strong>Statistical generalization</strong> from a limited <strong>T</strong> — the filter must identify useful, non-identical examples that match the target distribution.<br /></li>
  <li><strong>Computational efficiency</strong> sufficient to process <strong>billions of web documents</strong> — the method must meet strict throughput and cost constraints.<br /></li>
</ul>

<p>These requirements drive the compute vs. selection-quality trade-off:<br /></p>
<ul>
  <li><strong>Lightweight statistical models</strong> — low compute, fast at scale, sometimes weaker selection fidelity.<br /></li>
  <li><strong>Linear classifiers</strong> — middle ground in cost and accuracy for many problems.<br /></li>
  <li><strong>Neural methods</strong> — higher selection quality but much higher compute and infrastructure demands.<br /></li>
</ul>

<p>The filtering abstraction therefore guides method choice by balancing <strong>selection quality</strong> against <strong>scalability and cost</strong> for web-scale dataset curation.<br /></p>

<hr />

<h1 id="n-gram-statistical-language-models-for-fast-scoring">N-gram (statistical) language models for fast scoring</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-04-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-04-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-04-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-04-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>N-gram language models</strong> estimate token <strong>conditional probabilities</strong> by counting <strong>n-gram</strong> occurrences and normalizing by <strong>(n−1)-gram</strong> context counts—this yields a <strong>maximum likelihood estimate</strong> for local sequential structure.<br /></p>

<ol>
  <li>Count occurrences of each <strong>n-gram</strong>.</li>
  <li>Divide by the count of the corresponding <strong>(n−1)-gram</strong> context to obtain the conditional probability.<br />
This is the MLE for short-range dependencies.<br /></li>
</ol>

<p>Practical implementations use <strong>smoothing/backoff</strong> or <strong>interpolation</strong> to address data sparsity as n grows:</p>
<ul>
  <li>Common techniques include <strong>Kneser–Ney</strong> and <strong>CANLM</strong>.</li>
  <li>These methods enable <strong>nonzero probability</strong> for unseen n-grams by leveraging <strong>lower-order statistics</strong> (e.g., backing off to smaller n).<br /></li>
</ul>

<p>Advantages:</p>
<ul>
  <li><strong>Computationally efficient</strong> to train and evaluate relative to large neural models.</li>
  <li>Produce per-document <strong>perplexity</strong> scores that serve as a proxy for how well data matches a target corpus.<br /></li>
</ul>

<p>Limitations:</p>
<ul>
  <li><strong>Myopic locality</strong> — evaluates only short contexts, so long-range dependencies are not captured.</li>
  <li>Vulnerable to <strong>adversarially constructed high-probability but low-quality</strong> examples.</li>
  <li><strong>Reduced expressivity</strong> compared with <strong>contextual neural models</strong>, which can model richer, longer-range structure.</li>
</ul>

<hr />

<h1 id="using-n-gram-perplexity-for-dataset-filtering-ccnet-example">Using n-gram perplexity for dataset filtering (CCNet example)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-06-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-06-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-06-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-06-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Perplexity</strong> computed from an <strong>n-gram model</strong> provides a scalar quality score for paragraphs or documents.<br />
<strong>Low perplexity</strong> indicates a high likelihood under the target distribution and is commonly used to rank raw text data.<br /></p>

<p>A practical pipeline for filtering text by perplexity typically works as follows:<br /></p>
<ol>
  <li>Score each passage with an <strong>n-gram perplexity</strong> measure.<br /></li>
  <li>Sort passages by <strong>increasing perplexity</strong> (lower = better).<br /></li>
  <li>Retain a top fraction of passages (e.g., the top one‑third) as training data or a curated corpus.<br /></li>
</ol>

<p>This simple heuristic has been effective for creating domain-aligned corpora such as <strong>CCNet</strong> and early <strong>LLaMA</strong> training splits.<br /></p>

<p>Example outcomes and caveats:<br /></p>
<ul>
  <li>Well-formed <strong>Wikipedia</strong> content generally receives <strong>relatively low perplexity</strong>.<br /></li>
  <li><strong>Gibberish</strong> or badly formed text typically receives <strong>very high perplexity</strong>.<br /></li>
  <li>However, there are <strong>edge-cases</strong> caused by <strong>n-gram limitations</strong> where scores can mislead (e.g., rare but valid constructions or repeated local patterns).<br /></li>
</ul>

<p>Practical trade-offs:<br /></p>
<ul>
  <li>Principal benefit: <strong>speed and simplicity</strong> — easy to compute at corpus scale and cheap to apply.<br /></li>
  <li>Principal drawback: <strong>coarseness</strong> and susceptibility to <strong>local-context artifacts</strong>, which can lead to misranking in some cases.</li>
</ul>

<hr />

<h1 id="fasttext-linear-classifiers-with-hashed-n-gram-features-for-filtering">FastText linear classifiers with hashed n-gram features for filtering</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-10-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-10-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-10-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-10-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>FastText</strong> implements a <strong>near-linear bag-of-(hashed)-n-grams classifier</strong> that maps <strong>high-dimensional sparse token/ngram feature vectors</strong> into a <strong>lower-dimensional embedding space</strong>, then applies a <strong>linear classifier</strong>—yielding a fast and memory-efficient text classifier.<br /></p>

<ul>
  <li><strong>Hashed n-grams</strong>: Hashing n-grams into a <strong>fixed number of bins</strong> (e.g., <strong>10 million</strong>) bounds vocabulary growth and controls model size.<br />
    <ul>
      <li><strong>Collisions</strong> occur, but are often tolerable in aggregate learning and are <strong>implicitly handled by optimization</strong>.<br /></li>
    </ul>
  </li>
  <li>
    <p><strong>Embedding + linear classification</strong>: The model projects sparse token/ngram features into a compact embedding space and runs a linear classifier from that space, enabling both <strong>speed</strong> and <strong>memory efficiency</strong> without storing massive vocabularies.<br /></p>
  </li>
  <li>
    <p><strong>Use cases and scaling</strong>: The architecture scales well for <strong>binary or multi-class filtering tasks</strong> (e.g., <strong>language ID</strong>, <strong>quality</strong>, <strong>toxicity</strong>) and is especially attractive when throughput and resource constraints matter.<br /></p>
  </li>
  <li>
    <p><strong>Expressiveness vs speed trade-off</strong>: FastText trades some expressive power for <strong>orders-of-magnitude speed improvements</strong> compared to <strong>deep contextual models</strong>.<br /></p>
  </li>
  <li><strong>Practical pipeline trade-off</strong>: The central decision for filtering pipelines is <strong>compute per-example versus selection quality</strong>:
    <ol>
      <li><strong>Faster linear models</strong> let you process vastly more raw data for the same compute budget.<br /></li>
      <li><strong>Larger or deeper models</strong> increase per-document cost and may be suboptimal if compute is limited relative to data volume.<br /></li>
    </ol>
  </li>
</ul>

<hr />

<h1 id="importance-resampling-and-distributional-data-selection">Importance resampling and distributional data selection</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-16-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-16-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-16-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-16-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Importance resampling</strong> treats the target as a distribution <strong>P</strong> and the raw dataset as a proposal distribution <strong>Q</strong>, estimating <strong>importance weights w(x) = P(x)/Q(x)</strong> to resample raw items proportionally to how well they match the target distribution.<br /></p>

<p>When the target dataset is too small to fit a full generative model, lightweight <strong>hashed n-gram probability estimators</strong> or other tractable <strong>density approximations</strong> are used to estimate <strong>P</strong> and <strong>Q</strong>, producing <strong>per-document weights</strong> that prioritize <strong>diversity</strong> and <strong>distributional matching</strong> rather than binary classification.<br /></p>

<p>How it works (high-level steps):<br /></p>
<ol>
  <li>Estimate the target density <strong>P(x)</strong> and the proposal density <strong>Q(x)</strong> (e.g., with hashed unigram/ngram models or small generative approximations).<br /></li>
  <li>Compute <strong>importance weights w(x) = P(x)/Q(x)</strong> for each raw item/document.<br /></li>
  <li>Resample items proportionally to those weights to produce a dataset that better matches the target distribution.<br /></li>
</ol>

<p>Key properties and trade-offs:<br /></p>
<ul>
  <li>Provides a <strong>theoretical grounding</strong> for sampling data that matches a desired target distribution.<br /></li>
  <li>Often yields better <strong>diversity</strong> and smoother <strong>distributional matching</strong> than discriminative <strong>accept/reject classifiers</strong>.<br /></li>
  <li>Requires <strong>reliable density estimates or approximations</strong>; performance depends on the quality of P and Q estimators.<br /></li>
</ul>

<p>Practical notes:<br /></p>
<ul>
  <li>Common implementations use <strong>hashed unigram/ngram models</strong> or small, tractable generative approximations to estimate densities.<br /></li>
  <li>The resulting <strong>resampled datasets</strong> are typically fed into downstream model training to better align training data with the target distribution.<br /></li>
</ul>

<hr />

<h1 id="unified-scoring-recipe-for-model-based-filtering">Unified scoring recipe for model-based filtering</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-20-26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-20-26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-20-26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-20-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>All practical filtering approaches share a common pipeline:<br /></p>

<ol>
  <li><strong>Estimate models or scoring functions</strong> from <strong>T</strong> and/or <strong>R</strong>.<br /></li>
  <li><strong>Compute a per-document score</strong> using those models.<br /></li>
  <li><strong>Retain documents</strong> based on a <strong>threshold</strong> or <strong>probabilistic sampling rule</strong> to form <strong>T’</strong>.<br /></li>
</ol>

<p><strong>Scoring functions</strong> can take several forms:<br /></p>
<ul>
  <li><strong>Generative</strong> — use <strong>per-token probability</strong> or <strong>perplexity</strong> as the score.<br /></li>
  <li><strong>Discriminative</strong> — estimate the <strong>probability that a sample comes from T rather than R</strong>.<br /></li>
  <li><strong>Importance weights</strong> — use the <strong>ratio of two generative models</strong>.<br />
Each approach produces different trade-offs for <strong>diversity</strong> and <strong>precision</strong>.<br /></li>
</ul>

<p><strong>Thresholding</strong> strategies:<br /></p>
<ul>
  <li><strong>Deterministic thresholding</strong> — apply a fixed cutoff to retain items.<br /></li>
  <li><strong>Stochastic thresholding</strong> — sample around the cutoff to control dataset <strong>size</strong> and <strong>variance</strong>.<br /></li>
</ul>

<p>All methods benefit from improved <strong>modeling fidelity</strong> (for example, <strong>higher-order n-grams</strong> or <strong>neural models</strong>) within available compute constraints.<br /></p>

<p>This abstraction enables <strong>interchangeable implementation choices</strong> across languages, domains, and quality objectives, while emphasizing <strong>scalability</strong> and <strong>reproducibility</strong>.<br /></p>

<hr />

<h1 id="limitations-of-local-statistical-filters-and-adversarial-risks">Limitations of local statistical filters and adversarial risks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-22-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-22-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-22-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-22-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Local-context filters</strong>, such as <strong>n-gram models</strong>, primarily evaluate <strong>short-range coherence</strong> and <strong>grammaticality</strong>—so they can be <strong>gamed</strong> by manipulations that preserve local statistics but break overall quality.<br /></p>

<ul>
  <li>Examples of attacks or failure modes:
    <ul>
      <li><strong>Reorderings</strong> that keep n-gram counts but destroy document-level flow</li>
      <li><strong>Paraphrases</strong> that change global meaning while retaining local patterns</li>
      <li><strong>Adversarially constructed sequences</strong> that preserve n-gram statistics yet lack <strong>global coherence</strong> or <strong>factual correctness</strong><br /></li>
    </ul>
  </li>
</ul>

<p>Increasing <strong>n</strong> makes these filters more sensitive to larger context, but this comes with clear trade-offs:<br /></p>
<ol>
  <li>Benefit: better sensitivity to <strong>larger context</strong> and reduced susceptibility to some local attacks.</li>
  <li>Cost: rapidly increasing <strong>sparsity</strong> and <strong>compute costs</strong> as n grows.<br /></li>
</ol>

<p>By contrast, <strong>discriminative classifiers</strong> trained on <strong>higher-level features</strong> or <strong>neural representations</strong> can capture more semantics and detect issues beyond local patterns.<br /></p>
<ul>
  <li>Pros: better at modeling document-level meaning and factual inconsistencies</li>
  <li>Cons: substantially <strong>costlier to run at web scale</strong> and harder to deploy as a first-pass filter<br /></li>
</ul>

<p>In practice, teams combine these tools pragmatically:<br /></p>
<ul>
  <li>Use <strong>model-based filters</strong> (including n-gram checks) to remove <strong>gross nonsense</strong> and obviously low-quality pages</li>
  <li>Tolerate some residual errors or address them via <strong>downstream curation</strong> and <strong>human evaluation</strong><br /></li>
</ul>

<p>The engineering posture is a pragmatic trade-off: balance the risks of <strong>false accepts</strong> and <strong>false rejects</strong> against available <strong>processing capacity</strong> and the intended role of the filtered data in <strong>training pipelines</strong>.</p>

<hr />

<h1 id="language-identification-with-fasttext-for-corpus-selection">Language identification with FastText for corpus selection</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-26-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-26-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-26-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-26-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Language identification</strong> is a focused application of <strong>model-based filtering</strong>: a <strong>classifier</strong> predicts document language and filters raw data to a target language to conserve compute and improve per-language performance.<br /></p>

<ul>
  <li><strong>FastText</strong> provides <strong>pre-trained multi-language classifiers</strong> that operate on <strong>hashed n-gram features</strong>.<br /></li>
  <li>These classifiers produce <strong>per-document language probabilities</strong> that can be <strong>thresholded</strong> (e.g., keep English if P(English) &gt; 0.5) to construct <strong>monolingual corpora</strong>.<br /></li>
</ul>

<p><strong>Practical workflow:</strong><br /></p>
<ol>
  <li>Run the classifier to obtain <strong>per-document language probabilities</strong>.<br /></li>
  <li>Apply a <strong>threshold</strong> to decide inclusion (example: keep if P(target) &gt; 0.5).<br /></li>
  <li>Add <strong>post-processing heuristics</strong> and tune thresholds based on <strong>sentence length</strong> and <strong>domain</strong> to reduce false positives/negatives.<br /></li>
</ol>

<p><strong>Challenges and limitations:</strong><br /></p>
<ul>
  <li><strong>Short sentences</strong>, <strong>code-mixed text</strong>, <strong>dialects</strong>, and <strong>low-resource languages</strong> reduce automatic language ID reliability.<br /></li>
  <li>Thresholds and heuristics must be tuned for <strong>sentence length</strong> and <strong>domain</strong> to maintain precision.<br /></li>
</ul>

<p><strong>Why this matters:</strong><br /></p>
<ul>
  <li><strong>Language-specific filtering</strong> prevents dilution of the compute budget across unwanted languages.<br /></li>
  <li>It is especially important in <strong>compute-limited multilingual training regimes</strong> where focusing resources on target languages improves per-language performance.<br /></li>
</ul>

<hr />

<h1 id="domain-specific-data-curation-case-study-openwebmath">Domain-specific data curation case study: OpenWebMath</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-29-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-29-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-29-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-29-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>OpenWebMath</strong> demonstrates targeted <strong>domain curation</strong> by treating <strong>mathematical writing</strong> as a distinct style and building a specialized corpus through a multi-stage pipeline.<br /></p>

<ol>
  <li><strong>Rule-based prefilters</strong>: apply syntactic heuristics to identify obvious mathematical content.<br /></li>
  <li><strong>Perplexity-based selection</strong>: use a <strong>CANLM</strong> model trained on a proof dataset (<strong>ProofPile</strong>) to score documents by how well they fit mathematical language patterns.<br /></li>
  <li><strong>Discriminative classification</strong>: run a <strong>FastText</strong> classifier trained specifically to identify mathematical text and refine selection.<br /></li>
  <li><strong>Adaptive thresholds</strong>: set different acceptance thresholds depending on whether a document passed the rule-based math identification, yielding stricter or looser inclusion criteria as appropriate.<br /></li>
</ol>

<ul>
  <li>These steps combine <strong>syntactic heuristics</strong>, <strong>n-gram density models</strong> (via CANLM), and <strong>discriminative classification</strong> to efficiently gather domain data.<br /></li>
  <li>The result is a <strong>curated corpus of high-density mathematical content</strong> (e.g., <strong>15 billion tokens</strong>) that is highly focused compared with generic web-scale corpora.<br /></li>
</ul>

<p>Impact and takeaways:<br /></p>
<ul>
  <li>Training on this targeted corpus yields <strong>significant downstream improvements on math-specific tasks</strong> compared with training on much larger generic corpora.<br /></li>
  <li>The case illustrates that combining simple heuristics with language-model scoring and fast classifiers can produce <strong>data-efficient</strong> collections for specialized models.<br /></li>
  <li>Effective domain curation improves <strong>sample efficiency</strong> and <strong>task performance</strong>, especially when <strong>model capacity</strong> and <strong>compute</strong> are constrained.</li>
</ul>

<hr />

<h1 id="quality-filtering-strategies-and-synthetic-labeling">Quality filtering strategies and synthetic labeling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-32-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-32-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-32-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-32-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Quality filtering is implemented via <strong>positive/negative examples</strong> drawn from <strong>curated sources (positives)</strong> and <strong>broad web crawl (negatives)</strong> to train classifiers that score documents by <strong>educational value</strong>, <strong>textbook-like quality</strong>, or <strong>topical relevance</strong>.<br /></p>

<p>Approaches include:<br /></p>
<ul>
  <li>Training <strong>linear classifiers</strong> on high-quality sources (for example, <strong>non-CommonCrawl</strong> content and <strong>Wikipedia-referenced pages</strong>).<br /></li>
  <li>Using <strong>synthesized labeled sets</strong> generated by a strong model (e.g., <strong>prompting GPT-4</strong> to label documents) to produce training data for a fast downstream classifier (for example, a <strong>random forest</strong> on <strong>precomputed embeddings</strong>).<br /></li>
</ul>

<p>This is commonly organized as a <strong>two-stage distillation</strong> process:<br /></p>
<ol>
  <li>A <strong>large model</strong> (teacher) labels or ranks documents to produce high-quality semantic labels.<br /></li>
  <li>A <strong>lightweight classifier</strong> (student) is trained on those labels and precomputed features to enable <strong>high-throughput</strong>, low-cost inference.<br /></li>
</ol>

<p>Benefits:<br /></p>
<ul>
  <li>Enables <strong>high-throughput filtering</strong> while preserving stronger <strong>semantic criteria</strong> than simple heuristics.<br /></li>
  <li>Keeps <strong>inference costs low</strong> by deploying efficient downstream classifiers.<br /></li>
  <li>Empirical results indicate this approach can substantially improve <strong>downstream training efficiency</strong> and <strong>evaluation metrics</strong> on specialized tasks compared with <strong>unfiltered</strong> or <strong>naively curated</strong> data.<br /></li>
</ul>

<hr />

<h1 id="toxicity-filtering-with-annotated-datasets-and-lightweight-classifiers">Toxicity filtering with annotated datasets and lightweight classifiers</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-36-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-36-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-36-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-36-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Toxicity filtering</strong> uses curated, labeled datasets to train classifiers that detect unwanted content categories.<br /></p>

<ul>
  <li>Common datasets: <strong>Jigsaw Toxic Comments</strong> (and similar curated corpora).<br /></li>
  <li>Typical labeled categories: <strong>toxic</strong>, <strong>severe toxic</strong>, <strong>obscene</strong>, <strong>threat</strong>, <strong>insult</strong>, <strong>hate speech</strong>.<br /></li>
</ul>

<p><strong>Lightweight classifiers</strong> (for example, <strong>FastText</strong>) are commonly trained on these labels to produce <strong>per-document probabilities</strong> for each safety-related category—enabling efficient, large-scale screening of web data.<br /></p>

<ol>
  <li><strong>Train</strong> on curated labeled data to predict the target safety categories and output per-document probability scores.<br /></li>
  <li><strong>Apply</strong> models at scale to remove or downweight content that exceeds predefined thresholds.<br /></li>
  <li><strong>Mitigate</strong> remaining risk by combining automated flags with human review, rule-based checks, and post-hoc mitigation strategies.<br /></li>
</ol>

<p>Practical deployments usually set <strong>conservative thresholds</strong> to prioritize reducing unsafe content while acknowledging classifier limitations:<br /></p>

<ul>
  <li>Classifier imperfections are common on <strong>short utterances</strong>.<br /></li>
  <li>Performance degrades with <strong>ambiguous context</strong>.<br /></li>
  <li>Labels and judgments can vary due to <strong>cultural variance in annotations</strong>.<br /></li>
</ul>

<p>Overall, toxicity filters are one component of a broader <strong>content-safety pipeline</strong> and are typically integrated with human oversight, deterministic rules, and additional mitigation layers to achieve robust safety outcomes.</p>

<hr />

<h1 id="deduplication-exact-and-near-duplicate-categories-and-motivations">Deduplication: exact and near-duplicate categories and motivations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-39-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-39-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-39-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-39-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Deduplication</strong> addresses both <strong>exact duplicates</strong> (identical content mirrored across URLs) and <strong>near-duplicates</strong> (texts differing by a few tokens due to copies, edits, or templating) that otherwise waste <strong>training tokens</strong> and increase <strong>memorization</strong> risk.<br /></p>

<p><strong>Exact duplicates</strong> often arise from <strong>site mirroring</strong> and <strong>distributed hosting</strong>, producing enormous redundant repetition of identical text that <strong>distorts sampling during pretraining</strong> and overweights repeated content.<br /></p>

<p><strong>Near-duplicates</strong> include <strong>license text</strong>, <strong>templated pages</strong>, and <strong>lightly edited copies</strong> — these variations provide little additional signal despite consuming tokens.<br /></p>

<p>Removing or downsampling duplicates improves <strong>training efficiency</strong> and helps mitigate <strong>memorization-related risks</strong> (e.g., <strong>copyright leakage</strong>, <strong>privacy</strong>), while still allowing sufficient repetition for <strong>mid-training regimes</strong> where multiple epochs over high-quality sources may be desired.<br /></p>

<p>Key <strong>deduplication design choices</strong>:<br /></p>
<ul>
  <li><strong>Unit of comparison</strong> — choose the granularity to compare: <strong>sentence</strong>, <strong>paragraph</strong>, or <strong>document</strong>.<br /></li>
  <li><strong>Matching criterion</strong> — decide between <strong>exact match</strong> or a <strong>similarity threshold</strong> for near-duplicate detection.<br /></li>
  <li><strong>Action</strong> — determine the post-match policy: <strong>keep one instance</strong>, <strong>cap counts</strong>, or apply other downsampling rules.<br /></li>
</ul>

<hr />

<h1 id="design-space-and-hashing-primitives-for-scalable-deduplication">Design space and hashing primitives for scalable deduplication</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-43-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-43-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-43-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-43-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Scalable deduplication</strong> requires converting expensive <strong>pairwise similarity comparisons</strong> into fast <strong>unary operations</strong> via <strong>hashing</strong>—hash functions map items to compact values that enable grouping and approximate membership queries instead of performing quadratic pairwise comparisons.<br /></p>

<p><strong>Hash-family tradeoffs:</strong><br /></p>
<ul>
  <li><strong>Cryptographic hashes</strong>: minimize collision probability but are relatively <strong>slow</strong>.<br /></li>
  <li><strong>Non-cryptographic hashes</strong> (e.g., <strong>MurmurHash</strong>): much <strong>faster</strong> and typically <strong>sufficient</strong> for large-scale deduplication tasks, at the cost of a higher collision probability.<br /></li>
</ul>

<p><strong>Exact deduplication pipeline (MapReduce-style):</strong><br /></p>
<ol>
  <li><strong>Canonicalize items</strong> so equivalent records have the same normalized form.<br /></li>
  <li>Compute a <strong>hash</strong> for each canonicalized item.<br /></li>
  <li><strong>Group items into hash bins</strong> by their hash value (shuffle stage).<br /></li>
  <li>Keep a <strong>single representative per bin</strong> (reduce stage).<br />
This yields a <strong>high-precision</strong>, <strong>parallelizable</strong> pipeline that is straightforward to implement and memory-efficient.<br /></li>
</ol>

<p><strong>Limitations:</strong><br /></p>
<ul>
  <li>This exact, hash-bin approach <strong>does not capture near-duplicates</strong> that differ by small <strong>token edits</strong> or by <strong>templated substitutions</strong>—those cases require fuzzy or locality-sensitive techniques beyond simple hashing.<br /></li>
</ul>

<hr />

<h1 id="bloom-filter-approximate-set-membership-for-exact-duplication-detection">Bloom filter approximate set membership for exact-duplication detection</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/00-51-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/00-51-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/00-51-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/00-51-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Bloom filter</strong>: a <strong>space-efficient probabilistic set representation</strong> for answering membership queries with <strong>zero false negatives</strong> and a <strong>controllable false positive rate</strong>.<br /></p>

<p><strong>How it works (insertion &amp; query)</strong>:<br /></p>
<ol>
  <li><strong>Insertion</strong>: hash each inserted item <strong>k</strong> times into an <strong>m-bit</strong> array and set the corresponding bits.<br /></li>
  <li><strong>Query</strong>: check the same <strong>k</strong> bit positions; if any bit is 0, the answer is <strong>definitely “no”</strong> (no false negatives). If all are 1, the answer is <strong>“yes”</strong>, which may be a <strong>false positive</strong>.<br /></li>
</ol>

<p>Key properties and trade-offs:<br /></p>
<ul>
  <li><strong>False positives</strong> occur with probability that depends on <strong>m</strong>, <strong>k</strong>, and <strong>n</strong> (the number of inserted items).<br /></li>
  <li><strong>Zero false negatives</strong> (membership misses do not occur).<br /></li>
  <li>Supports <strong>streaming insertion</strong> and <strong>constant-time membership checks</strong> (O(k) hash checks, usually treated as constant).<br /></li>
  <li><strong>Tunable memory vs. false-positive trade-off</strong>: increase <strong>m</strong> (bits) to reduce false positives, or adjust <strong>k</strong> to balance rates.<br /></li>
  <li>Widely used for <strong>large-scale exact-duplication detection</strong> and other applications where occasional false positives are acceptable.<br /></li>
</ul>

<p>Design guidance:<br /></p>
<ul>
  <li>Use design formulas and approximations to select <strong>m</strong> and <strong>k</strong> for a target false-positive rate given expected dataset size <strong>n</strong>.<br /></li>
  <li>A common rule-of-thumb for the <strong>optimal number of hash functions</strong> is <strong>k ≈ (m / n) ln 2</strong>.<br /></li>
  <li>Choose <strong>m</strong> (bits) so that, with the optimal <strong>k</strong>, the false-positive probability meets your target (e.g., <strong>1e-5</strong>) given the expected <strong>n</strong>.<br /></li>
</ul>

<hr />

<h1 id="similarity-measures-minhash-and-probabilistic-reduction-of-jaccard">Similarity measures, MinHash and probabilistic reduction of Jaccard</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/01-01-32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/01-01-32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/01-01-32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/01-01-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Near-duplicate detection uses set similarity measures such as the <strong>Jaccard index</strong> (|A ∩ B| / |A ∪ B|).<br /></p>

<p>A probabilistic reduction to unary sketches is provided by <strong>MinHash</strong>: repeated independent applications of a randomized hash family map a set to its <strong>minimum hashed element</strong>, and the resulting collision probability equals the <strong>Jaccard similarity</strong> between the original sets.<br /></p>

<ol>
  <li><strong>Constructing a MinHash sketch</strong>:<br />
    <ol>
      <li>Choose k independent randomized hash functions from a hash family.<br /></li>
      <li>For each hash, map the set to the element with the minimum hash value.<br /></li>
      <li>Collect the k minima into a <strong>k‑coordinate sketch</strong> (k scalar values).<br /></li>
    </ol>
  </li>
</ol>

<ul>
  <li>The fraction of matching coordinates between two MinHash sketches estimates the <strong>Jaccard similarity</strong>.<br /></li>
  <li>
    <p>That estimator is <strong>unbiased</strong> and has a <strong>known variance</strong>, so accuracy improves predictably with k.<br /></p>
  </li>
  <li>Practical implications:<br />
    <ul>
      <li><strong>Compact representation</strong>: sets are compressed into k scalars rather than full pairwise comparisons.<br /></li>
      <li><strong>Linear-time sketching</strong>: large corpora can be sketched in time proportional to the data size.<br /></li>
      <li><strong>Cheap comparisons</strong>: sketches can be compared quickly, forming the basis for scalable near-duplicate pipelines that avoid O(N^2) pairwise comparisons.<br /></li>
    </ul>
  </li>
</ul>

<p><strong>MinHash</strong> is particularly effective for <strong>bag-of-shingled</strong> representations (token <strong>n-grams</strong>) that capture surface similarity between documents.</p>

<hr />

<h1 id="locality-sensitive-hashing-lsh-with-banding-to-amplify-thresholds">Locality-Sensitive Hashing (LSH) with banding to amplify thresholds</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/01-08-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/01-08-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/01-08-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/01-08-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Locality-Sensitive Hashing (LSH)</strong> sharpens <strong>MinHash</strong> collisions into a practical approximate neighbor-retrieval rule by partitioning <strong>k</strong> MinHash values into <strong>B</strong> bands of <strong>R</strong> hashes each and declaring a <strong>candidate match</strong> if any band is identical between two items.<br /></p>

<p>The <strong>banding</strong> construction yields a tunable, <strong>sigmoid-like mapping</strong> from true similarity to <strong>collision probability</strong>:<br /></p>
<ul>
  <li>Increasing <strong>R</strong> makes band equality rarer — the curve <strong>shifts right</strong> and <strong>sharpens</strong> (higher selectivity).<br /></li>
  <li>Increasing <strong>B</strong> raises the chance of at least one band match — the curve <strong>shifts left</strong> (higher sensitivity).<br /></li>
</ul>

<p>By selecting <strong>B</strong> and <strong>R</strong> appropriately you can concentrate collision probability around a chosen similarity threshold so that pairs above the threshold are very likely to be retrieved and pairs below are unlikely.<br /></p>

<p>Typical retrieval workflow (sublinear via hash tables):<br /></p>
<ol>
  <li>Compute <strong>k</strong> MinHash signatures for each item.<br /></li>
  <li>Partition the signature into <strong>B</strong> bands of <strong>R</strong> hashes.<br /></li>
  <li>Hash each band into a hash table (band signature → bucket).<br /></li>
  <li>At query time, probe the band hash tables and union candidates from matching buckets; then verify candidates as needed.<br /></li>
</ol>

<p>Practical pipelines use large <strong>k</strong> and tune <strong>B/R</strong> to match application-specific near-duplicate criteria (for example, ~<strong>0.99 Jaccard</strong> for strict paragraph deduplication), accepting <strong>probabilistic retrieval guarantees</strong> and trading off <strong>recall</strong> versus <strong>precision</strong> when choosing parameters.</p>

<hr />

<h1 id="practical-considerations-embeddings-for-paraphrase-sensitive-deduplication-and-duplicate-handling-policies">Practical considerations: embeddings for paraphrase-sensitive deduplication and duplicate handling policies</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/01-15-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/01-15-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/01-15-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/01-15-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Paraphrase-</strong> or <strong>semantics-sensitive deduplication</strong> requires <strong>dense embeddings</strong> and <strong>approximate nearest neighbor (ANN)</strong> search rather than surface n-gram methods like <strong>Jaccard/MinHash</strong>—this enables detection of semantically similar paraphrases but comes with higher <strong>computational cost</strong> for <strong>embedding generation</strong> and <strong>index maintenance</strong>.<br /></p>

<p>Embedding-based LSH or ANN structures fit into the same high-level framework as <strong>MinHash LSH</strong>: compute a vector per document, then index/query to find neighbors above a similarity threshold. Key components and steps are:<br /></p>

<ol>
  <li><strong>Vectorize each document</strong> into a dense embedding (semantic representation).<br /></li>
  <li><strong>Build an index</strong> using methods such as <strong>product quantization</strong>, <strong>HNSW</strong>, or <strong>LSH</strong> to support fast neighbor queries.<br /></li>
  <li><strong>Query for neighbors</strong> using a similarity metric (e.g., <strong>cosine similarity</strong>) and a chosen threshold to detect paraphrases/near-duplicates.<br /></li>
  <li><strong>Tune hyperparameters</strong> (index size, PQ/graph settings, LSH bands, threshold) to trade <strong>recall</strong> for <strong>efficiency</strong> and storage costs.<br /></li>
</ol>

<p>Policy choices about what to do with detected duplicates are important and depend on objectives:<br /></p>

<ul>
  <li><strong>Full deduplication</strong>: remove all duplicates to maximize dataset uniqueness and reduce memorization risk.<br /></li>
  <li><strong>Retain multiple occurrences</strong>: keep repeats when the source is genuinely high-quality or when <strong>multi-epoch training</strong> over the same data is desired.<br /></li>
  <li><strong>Count-capping heuristics</strong>: instead of full elimination, apply caps (e.g., <strong>sqrt</strong> or <strong>log</strong> scaling of counts) to preserve useful signal while avoiding extreme overrepresentation.<br /></li>
</ul>

<p>These design choices balance three main concerns:<br /></p>

<ul>
  <li><strong>Dataset diversity</strong> (favoring deduplication),<br /></li>
  <li><strong>Memorization risk</strong> (favoring stricter deduplication), and<br /></li>
  <li>The <strong>intended training regimen</strong> (e.g., <strong>single-pass pretraining</strong> vs. <strong>multi-epoch fine-tuning</strong>), which may justify retaining some duplicates.<br /></li>
</ul>

<p>Choose the embedding model, indexing method, similarity threshold, and deduplication policy together to meet your target trade-offs in quality, compute, and risk.</p>

<hr />

<h1 id="lecture-summary-and-next-steps">Lecture summary and next steps</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec14/01-17-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec14/01-17-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec14/01-17-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec14/01-17-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture consolidates algorithmic primitives for preparing training corpora:<br /></p>

<ul>
  <li><strong>n-gram models</strong><br /></li>
  <li><strong>fast linear classifiers</strong><br /></li>
  <li><strong>importance resampling for filtering</strong><br /></li>
  <li><strong>Hashing-based techniques</strong> for deduplication, including:
    <ul>
      <li><strong>exact hashing</strong><br /></li>
      <li><strong>Bloom filters</strong><br /></li>
      <li><strong>MinHash</strong><br /></li>
      <li><strong>LSH</strong> (Locality-Sensitive Hashing)<br /></li>
    </ul>
  </li>
</ul>

<p>Each primitive is characterized by three complementary aspects:<br /></p>

<ul>
  <li><strong>Computational cost</strong> — runtime, memory, and scalability trade-offs<br /></li>
  <li><strong>Statistical properties</strong> — bias, variance, and impact on distributional fidelity<br /></li>
  <li><strong>Practical role</strong> — how the primitive contributes to extracting a <strong>high-quality target dataset</strong> from raw web data<br /></li>
</ul>

<p>Practical pipelines combine these methods to perform concrete corpus-preparation tasks:<br /></p>

<ol>
  <li><strong>Language identification</strong> — filter and route data by language<br /></li>
  <li><strong>Quality and toxicity filtering</strong> — remove low-quality or harmful content<br /></li>
  <li><strong>Targeted domain curation</strong> — select domain-specific subsets from broad web crawls<br /></li>
  <li><strong>Exact and near-duplicate removal</strong> — eliminate redundant content using hashing and similarity techniques<br /></li>
</ol>

<ul>
  <li><strong>Parameter tuning</strong> is applied across the pipeline to balance <strong>false-positive vs. recall</strong> and to meet <strong>compute constraints</strong> (e.g., memory, throughput).<br /></li>
</ul>

<p>The next instructional unit transitions to <strong>reinforcement learning</strong> and <strong>alignment</strong> topics.</p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 13 - Data</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec13/" rel="alternate" type="text/html" title="CS336 Lecture 13 - Data" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec13</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec13/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/WePxmeXU1xg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="data-is-the-primary-determinant-of-language-model-behavior">Data is the primary determinant of language model behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-01-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-01-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-01-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-01-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Data quality, composition, and curation</strong> materially determine the capabilities and risks of <strong>foundation models</strong>.<br /></p>

<p>High-level system choices like <strong>architecture</strong>, <strong>optimization</strong>, and <strong>tokenization</strong> matter, but the empirical performance and <strong>emergent behaviors</strong> of large language models are primarily driven by what they are trained on:<br /></p>
<ul>
  <li><strong>data sources</strong> (web crawls, books, code, papers, etc.)<br /></li>
  <li><strong>filtering heuristics</strong> (rule-based or model-based)<br /></li>
  <li><strong>deduplication</strong> strategies<br /></li>
  <li><strong>downstream instructional tuning</strong> (instruction data, RLHF)<br /></li>
</ul>

<p>Commercial disclosure about training data is often limited for competitive or legal reasons, which complicates <strong>reproducibility</strong> and independent evaluation.<br /></p>

<p>Organizing and staffing data efforts is a distinct, highly scalable operational function separate from architecture development, enabling specialized teams to produce targeted corpora for capabilities like <strong>multilinguality</strong>, <strong>code</strong>, or <strong>multimodal learning</strong>.<br /></p>

<hr />

<h1 id="pre-training-mid-training-and-post-training-form-distinct-stages-of-model-training">Pre-training, mid-training, and post-training form distinct stages of model training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-03-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-03-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-03-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-03-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Model training</strong> is commonly organized into three phases:<br /></p>
<ol>
  <li><strong>Pre‑training</strong> — large, raw corpora to learn general language priors.<br /></li>
  <li><strong>Mid‑training</strong> — smaller, curated corpora to develop targeted capabilities (e.g., math, code, long‑context).<br /></li>
  <li><strong>Post‑training</strong> — instruction tuning and reinforcement methods to align models for safe, useful interaction.<br /></li>
</ol>

<p>Checkpoints after pre‑ and mid‑training are often called <strong>base models</strong>; models after post‑training are <strong>production</strong> or <strong>instruction‑tuned</strong> checkpoints.<br /></p>

<p>In practice, these boundaries blur: organizations use multiple iterative stages and mixtures of data to reach desired behaviors.<br /></p>

<p>Each stage imposes different curation and filtering trade-offs because early stages prioritize <strong>scale</strong>, while later stages emphasize <strong>higher signal‑to‑noise</strong> and <strong>specific task formats</strong>.<br /></p>

<hr />

<h1 id="example-training-mixes-illustrate-the-premidpost-pipeline-and-token-scales">Example training mixes illustrate the pre/mid/post pipeline and token scales</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-04-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-04-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-04-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-04-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Released open‑source examples show a common pipeline:<br /></p>
<ul>
  <li>Massive pre‑training mixes dominated by <strong>web crawls</strong> plus books, code, academic papers, math, and <strong>Wikipedia</strong>.<br /></li>
  <li>Mid‑training on <strong>filtered subsets</strong> and <strong>synthetic data</strong> to focus capabilities.<br /></li>
  <li>Post‑training on <strong>chat</strong> and <strong>instruction‑like</strong> data for alignment.<br /></li>
</ul>

<p>Token counts in public reproductions typically scale down across stages:<br /></p>
<ul>
  <li>Trillions of tokens in <strong>pre‑training</strong><br /></li>
  <li>Tens–hundreds of billions in <strong>mid‑training</strong><br /></li>
  <li>Billions in <strong>post‑training</strong><br /></li>
</ul>

<p>This illustrates a progressive focusing from <strong>scale → quality</strong>.<br /></p>

<p><strong>Synthetic generation</strong> is increasingly used at mid and post stages to produce targeted instruction or reasoning traces, and dataset composition choices plus token budgets materially affect which capabilities improve at each stage.<br /></p>

<hr />

<h1 id="there-is-no-simple-formalism-for-dataset-selection-curated-practices-are-inductive-and-empirical">There is no simple formalism for dataset selection; curated practices are inductive and empirical</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-05-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-05-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-05-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-05-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>There is no established theory prescribing exact dataset composition for optimal model behavior; choices remain <strong>empirical, heuristic, and context‑dependent</strong>.<br /></p>

<p>Data preparation is therefore an <strong>inductive process</strong>: study historical dataset designs, observe how they map to capabilities, and form principled heuristics for:<br /></p>
<ul>
  <li><strong>filtering</strong><br /></li>
  <li><strong>deduplication</strong><br /></li>
  <li><strong>augmentation</strong><br /></li>
</ul>

<p>Practical dataset engineering combines domain knowledge (e.g., including math or code sources) with scalable collection and quality pipelines.<br /></p>

<p>The lack of formal principles motivates reproducibility efforts, ablation studies, and benchmarks that compare filtering strategies and data mixes.<br /></p>

<hr />

<h1 id="bert-demonstrated-early-pre-training-on-books-and-wikipedia-and-revealed-issues-like-data-poisoning-risk">BERT demonstrated early pre-training on books and Wikipedia and revealed issues like data poisoning risk</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-09-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-09-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-09-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-09-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Early transformer pre‑training relied on curated, high‑quality corpora such as <strong>books</strong> and <strong>Wikipedia</strong> to learn robust language representations and long‑context structure.<br /></p>

<p>Public book collections (e.g., Smashwords‑derived corpora, Project Gutenberg) and Wikipedia provide coherent long‑form and factual content but do <strong>not</strong> cover informal or opinionated genres like recipes or personal blogs.<br /></p>

<p>Public datasets that mirror live services can be vulnerable to <strong>data poisoning</strong>, where adversarial edits are timed to appear in dumps—showing that training data from live services contains manipulation risks and requires pipeline‑level safeguards.<br /></p>

<p>Adversarial or erroneous content in foundational sources can produce undesirable model behaviors if not detected and removed.<br /></p>

<hr />

<h1 id="webtext-used-link-popularity-heuristics-to-extract-higher-quality-web-pages">WebText used link popularity heuristics to extract higher-quality web pages</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-12-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-12-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-12-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-12-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>WebText</strong> (the GPT‑2 corpus) used a pragmatic heuristic to get a diverse but higher‑quality subset of the web: extract pages linked from <strong>Reddit</strong> posts that exceeded a small karma threshold.<br /></p>

<p>This produced a compact set of higher‑signal documents (≈8 million pages and tens of gigabytes of text in the original work) compared to undifferentiated web crawls.<br /></p>

<p>The approach highlights the value of using <strong>social signals</strong> or other external proxies to select content likely to be high quality and human‑valued.<br /></p>

<p>However, proprietary crawls and heuristics used by many developers are often not fully disclosed, limiting reproducibility.<br /></p>

<hr />

<h1 id="common-crawl-provides-large-monthly-web-snapshots-but-requires-extensive-processing">Common Crawl provides large monthly web snapshots but requires extensive processing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-15-05-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-15-05-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-15-05-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-15-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Common Crawl</strong> is an open, monthly web crawl that provides raw HTTP responses and WARC archives for billions of pages—an academic approximation of the web.<br /></p>

<p>Raw data requires heavy preprocessing before training, including:<br /></p>
<ul>
  <li><strong>HTML→text conversion</strong> (WET files or custom parsers)<br /></li>
  <li><strong>deduplication</strong><br /></li>
  <li><strong>language identification</strong><br /></li>
  <li><strong>filtering</strong> for boilerplate, dynamic URLs, and near‑duplicates<br /></li>
</ul>

<p>The HTML‑to‑text toolchain materially affects downstream model quality: different extractors yield measurable performance differences in ablations.<br /></p>

<p>Common Crawl is deliberately conservative in crawling behavior and is not a comprehensive mirror of the full web, so downstream datasets often supplement it with targeted crawls or third‑party sources.<br /></p>

<hr />

<h1 id="common-crawl-contains-offensive-and-copyrighted-material-and-respects-robotstxt-policies-variably">Common Crawl contains offensive and copyrighted material and respects robots.txt policies variably</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-19-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-19-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-19-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-19-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Common Crawl does not perform aggressive semantic filtering by default, so raw dumps include <strong>offensive, toxic, and copyrighted content</strong>, and handling of illegal or disallowed pages is limited.<br /></p>

<ul>
  <li>Web hosts can request exclusion via <strong>robots.txt</strong> or other crawler rules, but adherence varies and is partly heuristic.<br /></li>
  <li>Major model developers often run bespoke crawlers beyond Common Crawl.<br /></li>
  <li>Media (images, etc.) may appear in raw responses but are frequently ignored by text pipelines unless explicit media crawlers are used.<br /></li>
</ul>

<p>The prevalence of copyrighted material complicates legal usage; teams typically address this with licensing agreements or fair‑use arguments, while content owners may restrict downstream uses via <strong>terms of service</strong>.<br /></p>

<hr />

<h1 id="two-principal-approaches-to-web-filtering-are-model-based-scoring-and-rule-based-heuristics">Two principal approaches to web filtering are model-based scoring and rule-based heuristics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-24-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-24-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-24-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-24-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Filtering large web crawls into high‑quality corpora has been implemented via two main paradigms:<br /></p>
<ul>
  <li><strong>Model‑based approaches</strong> (e.g., CCNet) train a classifier or language model to score documents against curated positive examples (Wikipedia, books). These enable language‑aware quality judgments but can concentrate content toward the positive example distribution.<br /></li>
  <li><strong>Rule‑based heuristics</strong> (e.g., C4) apply deterministic filters (punctuation counts, sentence thresholds, blacklists). They preserve broader linguistic diversity but can admit well‑formed spam or boilerplate.<br /></li>
</ul>

<p>Both paradigms have trade‑offs: model‑based selection amplifies similarity to positives; rule‑based selection preserves diversity at the cost of admitting noise.<br /></p>

<hr />

<h1 id="gpt-3-used-a-mixture-of-common-crawl-filtered-web-text-books-and-wikipedia-with-a-learned-quality-classifier">GPT-3 used a mixture of Common Crawl, filtered web text, books, and Wikipedia with a learned quality classifier</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-28-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-28-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-28-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-28-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>GPT‑3 era</strong> dataset combined Common Crawl processed into <strong>WebText2</strong>, curated book corpora (Books1/Books2), and <strong>Wikipedia</strong>, then applied a quality classifier to surface high‑signal documents.<br /></p>

<p>Key aspects of the classifier‑based pipeline:<br /></p>
<ul>
  <li>Select positive examples representative of desired quality.<br /></li>
  <li>Train a classifier to find similar material across a much larger pool.<br /></li>
  <li>Produce a focused ~400B‑token training mix by amplifying scarce high‑quality sources.<br /></li>
</ul>

<p>This demonstrates the utility of <strong>supervised quality scoring</strong> to extract high‑signal content from massive raw pools, but proprietary training mixes and classifier details are often incompletely disclosed, limiting reproducibility and independent evaluation.<br /></p>

<hr />

<h1 id="the-pile-aggregated-diverse-high-quality-domains-to-maximize-open-source-training-material">The Pile aggregated diverse high-quality domains to maximize open-source training material</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-32-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-32-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-32-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-32-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>The Pile</strong> is an open, community‑assembled dataset aggregating multiple high‑quality domains: Common Crawl subsets, OpenWebText, arXiv, PubMed Central, Stack Exchange, GitHub, Project Gutenberg, and curated book collections.<br /></p>

<p>Goals and practices:<br /></p>
<ul>
  <li>Maximize <strong>domain diversity</strong> and include permissively licensed material where possible.<br /></li>
  <li>Perform <strong>licensing checks</strong>, <strong>deduplication</strong>, and conversion choices to preserve code and long‑form attributes.<br /></li>
</ul>

<p>The Pile shows how curated open datasets can replicate—and sometimes exceed—proprietary mixes when volunteers and institutions coordinate collection and processing.<br /></p>

<hr />

<h1 id="stack-exchange-and-github-provide-structurally-rich-application-relevant-corpora-but-require-careful-licensing-and-processing">Stack Exchange and GitHub provide structurally rich, application-relevant corpora but require careful licensing and processing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-39-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-39-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-39-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-39-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Stack Exchange</strong> dumps and <strong>GitHub</strong> repositories contribute QA‑style conversational data and code, respectively—both valuable for instruction‑following and reasoning behaviors.<br /></p>

<p>Important considerations:<br /></p>
<ul>
  <li>These sources include metadata (votes, comments, commit history) that support fine‑grained filtering and high‑signal selection.<br /></li>
  <li>They often carry <strong>licensing constraints</strong> and commercial‑use caveats (license‑aware inclusion policies are required).<br /></li>
  <li>Converting repo snapshots into tokenizable training data needs careful handling of non‑code files, deduplication, and licensing; permissive licenses facilitate open redistribution (e.g., The Stack).<br /></li>
</ul>

<p>Random sampling reveals large heterogeneity in quality and content distribution, so preprocessing choices strongly affect final dataset composition.<br /></p>

<hr />

<h1 id="gopher-and-other-research-era-large-datasets-emphasized-curated-filters-and-manual-heuristics">Gopher and other research-era large datasets emphasized curated filters and manual heuristics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-44-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-44-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-44-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-44-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Research models such as DeepMind’s <strong>Gopher</strong> collected massive text sources and applied language‑specific filters, rule‑based heuristics, and toxicity checks to improve signal‑to‑noise in the pre‑training pool.<br /></p>

<p>Early decisions favored manual rule‑based filters because model understanding was limited, and conservative filters reduced the risk of excluding marginalized or non‑standard linguistic forms.<br /></p>

<p>Massive curated pools were often larger than required by training runs, enabling selective training on the best subsets and iterative dataset design.<br /></p>

<p>Design choices in these datasets influenced later <strong>model‑based filtering</strong> approaches and the community trade‑offs between <strong>coverage</strong> and <strong>quality</strong>.<br /></p>

<hr />

<h1 id="llama-redpajama-reproductions-and-repav2-illustrate-dataset-reproduction-link-aware-quality-and-large-scale-signal-computation">LLaMA, RedPajama reproductions, and RepAv2 illustrate dataset reproduction, link-aware quality, and large-scale signal computation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-47-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-47-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-47-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-47-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>LLaMA</strong> combined Common Crawl processed with CCNet‑style filters, <strong>C4</strong>, permissively licensed GitHub, Wikipedia, and multiple book sources to produce a multi‑trillion‑token training mix; the exact proprietary mix was not released but has been materially reproduced (e.g., RedPajama).<br /></p>

<p>Variants like <strong>RepAv2</strong> process multiple Common Crawl snapshots and compute many quality signals at scale to produce candidate corpora for research into filtering strategies.<br /></p>

<p>Alternative signals include <strong>link‑structure classifiers</strong> that predict whether pages are cited by Wikipedia, leveraging web link graphs as proxies for quality.<br /></p>

<p>These reproductions highlight challenges of exact replication and the benefits of publishing both data and filtering code for the research community.<br /></p>

<hr />

<h1 id="refinedweb-and-fineweb-provide-minimally-processed-web-datasets-intended-for-further-curation">RefinedWeb and FineWeb provide minimally processed web datasets intended for further curation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-52-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-52-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-52-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-52-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>RefinedWeb</strong> and <strong>FineWeb</strong> produce lightly filtered, deduplicated, and extract‑cleaned Common Crawl derivatives that preserve broad coverage while removing obvious noise.<br /></p>

<p>Typical priorities:<br /></p>
<ul>
  <li>High‑quality <strong>HTML→text extraction</strong><br /></li>
  <li><strong>Fuzzy deduplication</strong><br /></li>
  <li>Conservative rule‑based removal of low‑signal documents<br /></li>
</ul>

<p>These datasets yield multi‑trillion‑token pools with released subsets intended as researcher‑friendly starting points for subsequent model‑based filtering or task‑specific selection, preserving linguistic diversity that Wikipedia‑centric filters might eliminate.<br /></p>

<hr />

<h1 id="datacomp-defined-a-standard-benchmarking-and-competition-framework-and-demonstrated-aggressive-model-based-filtering-gains">DataComp defined a standard benchmarking and competition framework and demonstrated aggressive model-based filtering gains</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-57-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-57-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-57-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-57-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>DataComp</strong> produced a standardized infrastructure to process all Common Crawl dumps into a massive DCM pool, then applied a staged recipe to filter down to a DCM baseline using a learned quality classifier plus rule‑based filters.<br /></p>

<p>Key elements:<br /></p>
<ul>
  <li>Label positive examples (e.g., instruction‑like data from Open Hermes, ELI5).<br /></li>
  <li>Train a fast classifier to score candidates.<br /></li>
  <li>Aggressively filter the large raw pool to a small high‑quality subset.<br /></li>
</ul>

<p>Aggressive model‑in‑the‑loop filtering delivered substantial downstream gains on language modeling benchmarks and catalyzed reproducible comparisons of filtering strategies through open artifacts and baselines.<br /></p>

<hr />

<h1 id="nemoneatroncc-ensembles-model-based-scorers-and-rewrites-to-increase-usable-token-volume-while-preserving-quality">NeMo/NeatronCC ensembles model-based scorers and rewrites to increase usable token volume while preserving quality</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-02-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-02-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-02-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-02-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>NeMo/NeatronCC</strong> addressed the scale‑quality trade‑off by combining multiple scoring models, bucket‑based sampling, and token‑preserving extraction to recover more usable tokens from Common Crawl while maintaining benchmark performance.<br /></p>

<p>Techniques used:<br /></p>
<ul>
  <li>Use large models to score documents for educational value.<br /></li>
  <li>Ensemble scores with DCM classifiers and sample from score buckets to preserve coverage.<br /></li>
  <li>In some cases, use models to <strong>rewrite</strong> lower‑quality texts into higher‑quality forms or generate task‑like input‑output pairs from high‑quality documents.<br /></li>
</ul>

<p>This increased the available high‑quality token pool to several trillion tokens and produced subsets that outperformed aggressively pruned baselines on standard benchmarks—illustrating ensemble and rewrite techniques as practical mechanisms to expand training budgets without sacrificing curated signal.<br /></p>

<hr />

<h1 id="copyright-law-and-licensing-are-central-constraints-for-dataset-collection-and-sharing">Copyright law and licensing are central constraints for dataset collection and sharing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-07-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-07-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-07-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-07-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Copyright</strong> protects original expressive works fixed in a tangible medium, and most internet text is copyrighted even without registration.<br /></p>

<p>Legal pathways for dataset use include:<br /></p>
<ul>
  <li><strong>Licenses</strong> from rights holders<br /></li>
  <li><strong>Fair use</strong> defenses (assessed by purpose, nature, amount used, and market effect)<br /></li>
</ul>

<p>Platform licensing agreements and <strong>terms of service</strong> (YouTube, Reddit, GitHub, etc.) can restrict automated crawling and downstream model training even when content is publicly accessible.<br /></p>

<p>For research and production datasets, explicit licensing choices or risk assessments about fair use and market effects are essential to determine lawful inclusion and redistribution strategies.<br /></p>

<hr />

<h1 id="training-models-on-copyrighted-content-raises-complex-legal-and-technical-issues-including-memorization-and-transformative-use-arguments">Training models on copyrighted content raises complex legal and technical issues including memorization and transformative use arguments</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-13-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-13-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-13-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-13-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Using copyrighted material for training can implicate reproduction rights because training copies data into storage and models can sometimes <strong>memorize and regurgitate</strong> verbatim excerpts.<br /></p>

<p>Common legal and technical considerations:<br /></p>
<ul>
  <li>Defenses include arguing training is <strong>transformative</strong>—extracting abstract patterns rather than reproducing expression—but the legal status is unsettled and fact‑specific.<br /></li>
  <li>Platform <strong>terms of service</strong> may independently prohibit automated extraction, so datasets must account for contractual constraints as well as copyright law.<br /></li>
  <li>Technical mitigations: <strong>deduplication</strong>, removal of verbatim copyrighted passages, and auditing to limit extractable memorized content.<br /></li>
</ul>

<hr />

<h1 id="mid-training-and-post-training-use-targeted-corpora-and-synthetic-data-to-instill-capabilities-such-as-long-context-and-instruction-following">Mid-training and post-training use targeted corpora and synthetic data to instill capabilities such as long context and instruction following</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-18-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Mid‑training</strong> often focuses on capability extension, for example training on longer documents (books, mathematical text) to enable <strong>long‑context modeling</strong>, since attention complexity makes long‑context training expensive and better deferred until model capacity is adequate.<br /></p>

<p>Instruction tuning and post‑training exploit task‑structured datasets by converting benchmarks into unified prompt‑response formats (e.g., <strong>FLAN</strong>, <strong>SuperNaturalInstructions</strong>) or by synthesizing instruction data using existing models (<strong>self‑instruct</strong>, <strong>Alpaca</strong>, <strong>Vicuna</strong>).<br /></p>

<p>Notes on synthetic pipelines and human alignment:<br /></p>
<ul>
  <li>Synthetic generation can produce large amounts of conversational or reasoning‑trace data by prompting stronger models, but generator license terms may restrict their use.<br /></li>
  <li><strong>Human annotation</strong> and <strong>RLHF</strong> remain important for alignment, though they are more expensive and slower than synthetic methods.<br /></li>
</ul>

<hr />

<h1 id="dataset-engineering-is-heuristic-consequential-and-a-primary-axis-for-improving-language-models">Dataset engineering is heuristic, consequential, and a primary axis for improving language models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-18-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Effective language model development requires an end‑to‑end data pipeline that includes:<br /></p>
<ol>
  <li>Acquiring live service snapshots.<br /></li>
  <li>Converting raw responses to tokenizable text.<br /></li>
  <li>Language identification.<br /></li>
  <li>Deduplication.<br /></li>
  <li>Quality filtering and license checking.<br /></li>
  <li>Constructing staged mixes for <strong>pre‑</strong>, <strong>mid‑</strong>, and <strong>post‑training</strong>.<br /></li>
</ol>

<p>Small implementation choices—HTML extractor, deduplication thresholds, positive example selection for classifiers, sampling from score buckets—have measurable downstream impacts on model behavior and benchmarks.<br /></p>

<p>Because many decisions remain heuristic, there are substantial opportunities for systematic research on:<br /></p>
<ul>
  <li>Principled filtering<br /></li>
  <li>Fairness‑aware curation<br /></li>
  <li>Copyright‑safe pipelines<br /></li>
  <li>Reproducible dataset benchmarks<br /></li>
</ul>

<p>Investing in dataset engineering and open reproducible pipelines often yields outsized returns relative to marginal architectural changes when scale and compute are sufficient.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 11 - Scaling laws</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec11/" rel="alternate" type="text/html" title="CS336 Lecture 11 - Scaling laws" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec11</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec11/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/OSYuUqGBQxw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="this-lecture-presents-case-studies-of-scaling-laws-in-modern-large-language-model-development-and-a-mathematical-deep-dive-into-maximum-update-parameterization-mup">This lecture presents case studies of scaling laws in modern large language model development and a mathematical deep dive into Maximum Update Parameterization (MUP).</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-02-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-02-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-02-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-02-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture frames two primary goals: to examine detailed case studies of how contemporary LLM builders apply <strong>scaling laws</strong> in practice, and to present a mathematical treatment of <strong>Maximum Update Parametrization (MUP)</strong> that makes hyperparameters <strong>scale-invariant</strong>.<br /></p>

<p>It highlights common practitioner concerns about scaling-law use, including whether <strong>log–log curve fits</strong> reliably predict optimal <strong>token/model trade-offs</strong> and hyperparameters such as <strong>learning rate</strong>.<br /></p>

<p>Motivation and approach:<br /></p>
<ul>
  <li>Study both published <strong>scaling-law</strong> analyses and less-documented industrial practices to derive best practices for <strong>model sizing</strong>, <strong>data budgets</strong>, and <strong>stable training parameterizations</strong>.<br /></li>
  <li>Use case studies (Cerebras-GPT, MiniCPM, DeepSeek) plus a formal derivation and empirical evaluation of <strong>MUP</strong> to link theory and practice.<br /></li>
</ul>

<p>The section sets expectations for subsequent material: practical case studies, concrete recipes, and a mathematical foundation for <strong>MUP</strong> with empirical validation.<br /></p>

<hr />

<h1 id="cerebras-gpt-applies-maximum-update-parametrization-mup-to-stabilize-hyperparameter-behavior-and-improve-predictability-of-scaling-curves">Cerebras-GPT applies Maximum Update Parametrization (MUP) to stabilize hyperparameter behavior and improve predictability of scaling curves.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-08-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-08-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-08-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-08-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Cerebras-GPT</strong> is a family of models spanning ~<strong>0.1B to 13B</strong> parameters trained with <strong>Chinchilla-style compute/data ratios</strong> and using <strong>MUP</strong> to reduce sensitivity of optimal learning rates across scale.<br /></p>

<p>Key practical points:<br /></p>
<ul>
  <li><strong>MUP initialization:</strong> nearly all non-embedding parameters scaled proportional to <strong>1 / layer width</strong>.<br /></li>
  <li><strong>Per-layer LR scaling:</strong> apply learning-rate multipliers inversely proportional to width, producing much smoother adherence to predicted <strong>scaling-law</strong> fits.<br /></li>
  <li><strong>Validation strategy:</strong> perform extensive small-proxy hyperparameter sweeps (down to <strong>40M</strong> parameter models), then scale those hyperparameters upward under <strong>MUP</strong>, showing reduced oscillation around predicted scaling points compared with standard parameterization.<br /></li>
  <li><strong>Reproducibility artifacts:</strong> detailed appendix tables enumerate initialization differences and per-layer learning-rate multipliers between standard parameterization and <strong>MUP</strong>, enabling straightforward implementation.<br /></li>
</ul>

<hr />

<h1 id="minicpm-combines-mup-with-a-warmup-stable-decay-wsd-learning-rate-schedule-to-enable-single-run-chin-chilla-style-data-scaling-experiments-and-efficient-proxy-model-hyperparameter-selection">MiniCPM combines MUP with a Warmup-Stable-Decay (WSD) learning-rate schedule to enable single-run Chin chilla-style data-scaling experiments and efficient proxy-model hyperparameter selection.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-21-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-21-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-21-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-21-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>MiniCPM</strong> aims to use substantial compute to produce highly optimized <strong>small models</strong>, leveraging <strong>MUP</strong> to keep optimal hyperparameters stable when scaling width and using a <strong>WSD (Warmup–Stable–Decay)</strong> learning-rate schedule to make data-scaling reproducible from single long runs.<br /></p>

<p>WSD schedule (three phases):<br /></p>
<ol>
  <li><strong>Short warmup</strong> to ramp to the target LR.<br /></li>
  <li><strong>Long stable plateau</strong> at the target LR to concentrate most of the token budget.<br /></li>
  <li><strong>Rapid decay</strong> to a small termination LR to finish and stabilize weights.<br /></li>
</ol>

<p>Practical features and analyses:<br /></p>
<ul>
  <li>Checkpoint <strong>rewinding</strong> and selective decay-phase application allow emulating training to different token counts without retraining from scratch.<br /></li>
  <li>Measure <strong>critical batch size scaling</strong> versus <strong>terminal loss</strong> (an isoflops-style analysis) to choose batch sizes.<br /></li>
  <li>Use two fitting methods for <strong>Chinchilla-style</strong> token-to-parameter trade-offs:
    <ul>
      <li>Take the <strong>lower envelope</strong> of training curves.<br /></li>
      <li>Jointly fit a <strong>two-variable power-law</strong> scaling law.<br /></li>
    </ul>
  </li>
  <li>Report unusually high token-to-parameter ratios compared to original Chinchilla estimates, while cautioning that these ratios are sensitive to <strong>architecture</strong> and <strong>data quality</strong>, illustrating both the power and fragility of curve-fitting approaches.<br /></li>
</ul>

<hr />

<h1 id="deepseek-performs-explicit-grid-searches-for-optimal-batch-size-and-learning-rate-across-compute-scales-fits-scaling-laws-to-these-optima-and-extrapolates-performance-to-large-target-models-using-wsd-schedules-and-isoflops-analysis">DeepSeek performs explicit grid searches for optimal batch size and learning rate across compute scales, fits scaling laws to these optima, and extrapolates performance to large target models using WSD schedules and isoflops analysis.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-35-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-35-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-35-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-35-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>DeepSeek</strong> obtains optimal hyperparameter estimates by running grids of <strong>batch-size</strong> and <strong>learning-rate</strong> sweeps at multiple compute scales, fitting scaling relationships to the optima, and extrapolating those fits to set hyperparameters for <strong>7B</strong> and <strong>67B</strong> models.<br /></p>

<p>Practical methodology:<br /></p>
<ul>
  <li>Adopt <strong>WSD-style</strong> schedules with multiple decay phases to permit cheap Chinchilla-style experiments and concentrate compute on the effective cooldown that yields most terminal-loss improvements.<br /></li>
  <li>Replicate <strong>isoflops/Chinchilla</strong> analyses (fit quadratics to training curves, take the lower envelope) and demonstrate that <strong>extrapolated scaling laws</strong> can accurately predict held-out larger-model performance when experiments are well executed.<br />
Caveats:<br /></li>
  <li><strong>Hyperparameter-scaling relations</strong> are noisier than isoflops trends, so careful experimental design and validation are required when extrapolating <strong>learning-rate</strong> and <strong>batch-size</strong> schedules.<br /></li>
</ul>

<hr />

<h1 id="recent-releases-llama-3-hunuan-1-minimax-01-reapply-isoflopschinchilla-style-analyses-and-report-varying-optimal-token-to-parameter-ratios-and-architectural-trade-offs-for-linearhybrid-attentions">Recent releases (Llama 3, Hunuan 1, Minimax-01) reapply isoflops/Chinchilla-style analyses and report varying optimal token-to-parameter ratios and architectural trade-offs for linear/hybrid attentions.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-43-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-43-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-43-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-43-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Several recent releases revisit <strong>Chinchilla-style isoflops</strong> fits and demonstrate variability in optimal token-to-parameter ratios across projects:<br /></p>

<ul>
  <li><strong>Llama 3</strong>: redoes isoflops fits and reports an optimal token-to-parameter ratio around <strong>~39:1</strong>, higher than the original <strong>~20:1</strong> Chinchilla heuristic—possible causes include architecture improvements and better data quality.<br /></li>
  <li><strong>Hunuan 1</strong>: reports even larger ratios (e.g., <strong>~96:1</strong>) under its training regime, showing strong dependence on architecture, data, and methodology.<br /></li>
  <li><strong>Minimax-01</strong>: investigates <strong>linear (lightning) attention</strong> and hybrid architectures and finds that under isoflops analysis, linear/hybrid attention families can achieve <strong>similar scaling performance</strong> to full softmax attention, supporting their use for long-context, linear-time models.<br /></li>
</ul>

<p>Across these releases, <strong>isoflops-style quadratic fits</strong> and <strong>lower-envelope</strong> methods remain reliable tools for inferring compute-to-token/model trade-offs, even though absolute ratio estimates vary by project.<br /></p>

<hr />

<h1 id="common-practical-scaling-ingredients-are-stable-parameterizations-eg-mup-wsd-schedules-for-efficient-data-scaling-isoflopschinchilla-analyses-and-fixed-aspect-ratio-scaling-of-model-dimensions">Common practical scaling ingredients are stable parameterizations (e.g., MUP), WSD schedules for efficient data-scaling, isoflops/Chinchilla analyses, and fixed aspect-ratio scaling of model dimensions.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-48-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-48-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-48-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-48-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Production-grade scaling workflows typically combine several reproducible elements to reduce expensive per-scale hyperparameter searches and enable controlled extrapolation from small proxies:<br /></p>

<ul>
  <li>A <strong>stable parameterization or initialization</strong> (e.g., <strong>MUP</strong>) to reduce hyperparameter drift across width or depth.<br /></li>
  <li>An <strong>LR schedule</strong> such as <strong>Warmup–Stable–Decay (WSD)</strong> to permit checkpoint-based data-scaling and reuse.<br /></li>
  <li><strong>Isoflops/Chinchilla analyses</strong> to set token/model trade-offs and estimate critical batch sizes.<br /></li>
  <li>A <strong>fixed aspect-ratio</strong> approach when increasing total parameter counts to keep architecture proportions consistent.<br /></li>
</ul>

<p>These elements enable small-proxy sweeps and controlled extrapolation to target compute budgets, reducing overall experimental cost. The combination is not universal but recurs across prominent open studies and high-performance model recipes.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 10 - Inference</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec10/" rel="alternate" type="text/html" title="CS336 Lecture 10 - Inference" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec10</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec10/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/fcgPYo3OtV0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-introduction-and-scope-of-inference">Lecture introduction and scope of inference</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-01-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-01-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-01-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-01-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Inference denotes the process of generating model outputs from a fixed, pre-trained model given prompts and appears across multiple use cases including <strong>chat</strong>, <strong>code completion</strong>, <strong>batch processing</strong>, <strong>evaluation</strong>, and <strong>reinforcement-learning-based training</strong>.<br /></p>

<p>Inference is distinct from <strong>training</strong> because it is executed repeatedly at runtime and therefore often <strong>dominates operational cost</strong> and the <strong>user experience</strong> for deployed systems.<br /></p>

<p>This lecture positions <strong>inference</strong> as a broad topic that spans:</p>
<ul>
  <li><strong>Workload characterization</strong> — understanding request shapes, sequence lengths, and batching patterns<br /></li>
  <li><strong>System-level optimizations</strong> — kernels, memory layout, and scheduling<br /></li>
  <li><strong>Architectural changes</strong> — model modifications that trade compute, memory, and accuracy for improved latency and throughput<br /></li>
</ul>

<hr />

<h1 id="inference-importance-and-primary-metrics-ttft-latency-throughput">Inference importance and primary metrics (TTFT, latency, throughput)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-03-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-03-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-03-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-03-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical evaluation of inference hinges on three primary metrics:<br /></p>

<ol>
  <li><strong>Time-to-first-token (TTFT)</strong> — the elapsed time until any token is produced; a key determinant of interactivity.<br /></li>
  <li><strong>Latency</strong> — the per-request token arrival speed that affects perceived responsiveness for an individual request.<br /></li>
  <li><strong>Throughput</strong> — the aggregate tokens produced per unit time, which matters for batch and high-volume workloads.<br /></li>
</ol>

<p>These metrics trade off with one another:</p>
<ul>
  <li>Optimizing <strong>throughput</strong> (e.g., via large batching) can worsen per-request <strong>latency</strong> and <strong>TTFT</strong>.<br /></li>
  <li>Minimizing <strong>TTFT</strong> typically reduces achievable <strong>throughput</strong>.<br /></li>
</ul>

<p>Quantifying and balancing these metrics guides architecture and system decisions for production deployments.<br /></p>

<hr />

<h1 id="training-parallelism-versus-sequential-nature-of-autoregressive-inference">Training parallelism versus sequential nature of autoregressive inference</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-05-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-05-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-05-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-05-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>Training</strong> typically exploits <strong>full-sequence parallelism</strong> because all tokens are available at once, enabling efficient tensorized operations across sequence length.<br /></li>
  <li><strong>Autoregressive inference</strong>, by contrast, requires <strong>sequential token generation</strong>: each output token conditions on prior outputs, preventing simple parallelization over the output sequence.<br /></li>
</ul>

<p>That sequential dependence creates two fundamental challenges:</p>
<ul>
  <li><strong>Utilization</strong> problems — accelerators cannot be kept fully busy when generation is stepwise.<br /></li>
  <li><strong>Memory</strong> pressure — per-step state (e.g., KV caches) must be stored and moved frequently.<br /></li>
</ul>

<p>As a result, inference is often more resource-constrained and operationally different from training.<br /></p>

<hr />

<h1 id="transformer-notation-and-computational-graph-review">Transformer notation and computational graph review</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-07-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-07-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-07-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-07-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Transformer inference computation is organized by the key symbols:</p>
<ul>
  <li><strong>B</strong> — batch size<br /></li>
  <li><strong>L</strong> — number of layers<br /></li>
  <li><strong>S</strong> — conditioning (prompt) sequence length<br /></li>
  <li><strong>T</strong> — generated sequence length<br /></li>
  <li><strong>D</strong> — model dimension<br /></li>
  <li><strong>F</strong> — MLP hidden dimension<br /></li>
  <li><strong>H</strong> — head dimension<br /></li>
  <li><strong>N</strong>, <strong>K</strong> — query and key/value head counts respectively<br /></li>
</ul>

<p>Typical transformer computation alternates <strong>attention</strong> and <strong>MLP</strong> blocks, each expressed via large matrix multiplications and tensor contractions.<br /></p>

<ul>
  <li><strong>Attention</strong> introduces an O(S*T) term because queries interact with keys/values across conditioning and generated tokens.<br /></li>
  <li><strong>MLPs</strong> contribute roughly O(B<em>T</em>D*F) floating-point operations per layer.<br /></li>
</ul>

<p>Expressing compute and memory in these symbols enables precise flop and byte-transfer accounting for later <strong>arithmetic intensity</strong> and performance analysis.<br /></p>

<hr />

<h1 id="arithmetic-intensity-analysis-for-matrix-multiplication-and-accelerator-limits">Arithmetic intensity analysis for matrix multiplication and accelerator limits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-11-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-11-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-11-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-11-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Arithmetic intensity</strong> is the ratio of floating-point operations to bytes transferred and determines whether a kernel is <strong>compute-bound</strong> or <strong>memory-bound</strong> on a given accelerator.<br /></p>

<p>For a B×D times D×F matrix multiply:</p>
<ul>
  <li>Flops scale as approximately <strong>2·B·D·F</strong>.<br /></li>
  <li>Bytes transferred include reads/writes of the input X, weights W, and the output, so the intensity simplifies to a quantity that is roughly <strong>proportional to the batch size B</strong> under common scalings.<br /></li>
</ul>

<p>Comparing this intensity to an accelerator’s peak <strong>flop-to-bandwidth ratio</strong> identifies thresholds (for example, an H100 requires <strong>B</strong> on the order of hundreds) below which the operation is <strong>memory-bound</strong> and cannot saturate compute.<br /></p>

<hr />

<h1 id="implication-of-low-arithmetic-intensity-b1-for-generation">Implication of low arithmetic intensity (B=1) for generation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-13-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-13-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-13-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-13-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When <strong>B = 1</strong> (matrix-vector products) or when sequence parallelism is absent, <strong>arithmetic intensity collapses toward one</strong>, implying severe memory-boundedness.<br /></p>

<p>Consequences:</p>
<ul>
  <li>Memory-bound kernels perform few flops per byte moved, so <strong>data transfers dominate runtime</strong>.<br /></li>
  <li>Autoregressive generation often operates with small effective <strong>B</strong> and with <strong>T = 1</strong> per generation step, making it <strong>inherently memory-limited</strong> and inefficient on modern accelerators unless mitigated.<br /></li>
</ul>

<hr />

<h1 id="kv-caching-to-eliminate-prefix-recomputation-during-autoregressive-generation">KV caching to eliminate prefix recomputation during autoregressive generation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-16-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-16-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-16-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-16-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Autoregressive generation repeats much of the same prefix computation for each new token; <strong>KV caching</strong> avoids recomputing those prefix contributions by storing per-layer key and value projections in high-bandwidth device memory.<br /></p>

<p>Two phases:</p>
<ul>
  <li><strong>Prefill</strong> — compute and store the KV cache for the prompt in parallel across tokens.<br /></li>
  <li><strong>Generation</strong> — append new KV vectors per generated token and reuse the cached values to avoid recomputing prefix contributions.<br /></li>
</ul>

<p>Proper <strong>KV caching</strong> reduces computational complexity per generated token in naive analysis from <strong>O(T^2)</strong> to <strong>O(T)</strong> and is essential for practical transformer inference.<br /></p>

<hr />

<h1 id="transformer-mlp-and-attention-ioflop-accounting-and-resulting-intensities">Transformer MLP and attention IO/flop accounting and resulting intensities</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-21-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-21-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-21-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-21-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Per-layer accounting separates <strong>MLP</strong> and <strong>attention</strong> contributions:</p>

<ul>
  <li><strong>MLP layers</strong>
    <ul>
      <li>Perform large matrix multiplies with arithmetic intensity roughly proportional to <strong>B·T</strong> under typical parameter scalings.<br /></li>
      <li>Batching and longer sequence lengths increase intensity, helping reach compute-bound regimes.</li>
    </ul>
  </li>
  <li><strong>Attention layers</strong>
    <ul>
      <li>Require Q×K matrix multiplies that scale as <strong>B·S·T·D</strong> and have memory reads proportional to the per-sequence <strong>KV cache</strong> size.<br /></li>
      <li>A simplified attention intensity evaluates to approximately <strong>S·T / (S + T)</strong>:
        <ul>
          <li>During <strong>prefill</strong> this is O(S) (good — high intensity).<br /></li>
          <li>During <strong>generation</strong> (T≈1) this collapses to O(1) (bad — low intensity).<br /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Thus, attention inherently imposes a low, often-unimprovable arithmetic intensity during token-by-token generation because KV caches are sequence-specific and scale with batch.<br /></p>

<hr />

<h1 id="prefill-is-compute-limited-while-generation-is-memory-limited">Prefill is compute-limited while generation is memory-limited</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-26-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-26-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-26-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-26-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When encoding the prompt (<strong>prefill</strong>), computation can be parallelized across sequence and batches, so arithmetic intensity can be made large enough to be <strong>compute-bound</strong>, allowing near-saturated accelerators.<br /></p>

<p>By contrast, the <strong>generation</strong> stage produces tokens sequentially (T≈1), which:</p>
<ul>
  <li>Reduces effective arithmetic intensity.<br /></li>
  <li>Renders inference <strong>memory-bound</strong> due to frequent KV cache transfers.<br /></li>
</ul>

<p>Consequently, optimization strategies differ:</p>
<ul>
  <li><strong>Prefill</strong> benefits from heavyweight compute kernels and large batch/sequence parallelism.<br /></li>
  <li><strong>Generation</strong> must minimize memory transfers and the <strong>KV footprint</strong> to improve latency and throughput.<br /></li>
</ul>

<hr />

<h1 id="latency-and-throughput-modeling-with-a-concrete-llama-2-13b-h100-example">Latency and throughput modeling with a concrete Llama 2 13B H100 example</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-30-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-30-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-30-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-30-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A stylized model that assumes perfect overlap of compute and communication provides tractable latency and throughput estimates by dividing required byte transfers by device memory bandwidth to estimate per-token latency and scaling throughput by batch size <strong>B</strong>.<br /></p>

<p>Example instantiation (qualitative takeaways):</p>
<ul>
  <li>For a <strong>Llama-2-like 13B</strong> model on an <strong>H100</strong>, single-request (<strong>B = 1</strong>) generation can yield <strong>millisecond-range per-token latencies</strong> and modest tokens/sec throughput.<br /></li>
  <li>Increasing <strong>B</strong> raises throughput but also increases memory and per-request latency, exhibiting <strong>diminishing returns</strong> and hard caps set by on-device memory capacity.<br /></li>
</ul>

<p>This quantitative instantiation demonstrates the latency/throughput trade-offs and memory ceilings that govern practical inference.<br /></p>

<hr />

<h1 id="scaling-via-replicated-model-instances-and-model-sharding">Scaling via replicated model instances and model sharding</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-35-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-35-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-35-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-35-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A simple parallelism strategy for serving is <strong>model replication</strong>:</p>
<ul>
  <li>Run <strong>M</strong> independent copies of the model to increase overall throughput by <strong>M</strong> without inter-model communication.<br /></li>
  <li>Replication preserves per-request latency but multiplies memory requirements.</li>
</ul>

<p>When a single model replica cannot fit on-device, <strong>parameter and KV cache sharding</strong> across accelerators becomes necessary:</p>
<ul>
  <li>Sharding introduces communication and coordination overheads.<br /></li>
  <li><strong>KV cache layout</strong> and sharding strategy materially affect memory transfers and performance.</li>
</ul>

<p>Both <strong>replication</strong> and <strong>sharding</strong> are viable levers to tune throughput subject to memory and communication constraints.<br /></p>

<hr />

<h1 id="group-query-attention-gqa-reduces-kv-cache-size-by-sharing-keyvalue-heads">Group Query Attention (GQA) reduces KV cache size by sharing key/value heads</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-39-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-39-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-39-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-39-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Group Query Attention (GQA)</strong> reduces KV cache memory by using fewer <strong>key/value (K/V) heads</strong> than <strong>query</strong> heads.<br /></p>

<p>Key ideas and effects:</p>
<ul>
  <li>Multiple query heads <strong>share</strong> the same K/V representations, shrinking per-token KV storage proportional to the reduction in K/V head count.<br /></li>
  <li>This preserves <strong>query expressivity</strong> while reducing KV memory, enabling larger batch sizes and higher throughput on the same device memory budget.<br /></li>
  <li>Empirical results show substantial latency and throughput gains with <strong>negligible accuracy loss</strong> up to moderate grouping ratios, making GQA an effective low-loss inference optimization.<br /></li>
</ul>

<hr />

<h1 id="multi-head-latent-mla-projection-for-compact-kv-representations">Multi-Head Latent (MLA) projection for compact KV representations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-44-21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-44-21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-44-21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-44-21.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Multi-Head Latent (MLA)</strong> methods compress KV storage by projecting per-token KV vectors into a lower-dimensional <strong>latent</strong> space rather than reducing head count.<br /></p>

<p>Characteristics:</p>
<ul>
  <li>MLA dramatically shrinks the KV footprint (e.g., tens of thousands → a few hundred dimensions) while retaining useful representational capacity.<br /></li>
  <li>Practical implementations must reconcile <strong>positional encodings</strong> (such as <strong>RoPE</strong>) with dimensionality reduction to preserve sequence information.<br /></li>
  <li>When properly implemented, MLA delivers latency and throughput benefits comparable to head-count reduction techniques while maintaining accuracy with modest architectural adjustments.<br /></li>
</ul>

<hr />

<h1 id="cross-layer-attention-cla-shares-kv-projections-across-layers-to-reduce-cache">Cross-Layer Attention (CLA) shares KV projections across layers to reduce cache</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-46-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-46-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-46-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-46-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Cross-Layer Attention (CLA)</strong> collapses per-layer KV projections into shared projections that are reused across multiple consecutive layers, reducing KV cache storage across the stack.<br /></p>

<p>Trade-offs and effects:</p>
<ul>
  <li>By amortizing KV storage across layers, <strong>CLA lowers on-device memory pressure</strong> and can improve the latency/throughput frontier.<br /></li>
  <li>It trades off some representational flexibility and can slightly affect perplexity.<br /></li>
  <li>Empirical evidence indicates favorable trade-offs in many settings, especially when combined with other KV-reduction techniques.<br /></li>
</ul>

<hr />

<h1 id="local-attention-and-hybrid-architectures-to-cap-kv-growth">Local attention and hybrid architectures to cap KV growth</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-49-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-49-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-49-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-49-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Local attention</strong> restricts attention to a fixed window of the past <strong>K</strong> tokens so KV cache size remains bounded and does not grow with sequence length, turning per-sequence KV cost into a constant rather than O(S).<br /></p>

<p>To recover long-range modeling capacity, hybrid designs:</p>
<ul>
  <li>Interleave local attention layers with occasional <strong>global attention</strong> layers.<br /></li>
  <li>Combine local patterns with KV-sharing techniques.<br /></li>
</ul>

<p>Such <strong>local-and-global hybridization</strong> is a pragmatic balance between memory-efficient inference and long-range dependency modeling and is used in production systems to preserve much of transformer expressivity while greatly reducing KV memory and quadratic cost.<br /></p>

<hr />

<h1 id="alternative-architectures-state-space-models-hyenamamba-and-linear-attention-at-scale">Alternative architectures: state-space models, Hyena/Mamba and linear attention at scale</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/00-55-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/00-55-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/00-55-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/00-55-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>State-space models (SSMs)</strong> and recent variants such as <strong>Hyena</strong> and <strong>Mamba</strong> reinterpret sequence modeling through linear dynamical systems or convolutional kernels to avoid full O(S^2) attention and to enable RNN-like or convolutional implementations.<br /></p>

<p>Key points:</p>
<ul>
  <li>These architectures were motivated by long-context efficiency and have been adapted with transformer components or occasional full-attention layers to handle associative recall tasks needed for language.<br /></li>
  <li>Linear-attention variants and hybrid stacks have been scaled to very large parameter counts (hundreds of billions), showing that much of the transformer can be replaced with lower-KV, linear-or-local components while preserving competitive performance.<br /></li>
</ul>

<hr />

<h1 id="diffusion-based-and-parallel-generation-approaches-for-faster-sampling">Diffusion-based and parallel-generation approaches for faster sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/01-01-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/01-01-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/01-01-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/01-01-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Diffusion-style text generation</strong> iteratively refines full-sequence drafts in parallel rather than generating tokens autoregressively, enabling strong parallelism and very high tokens-per-second throughput.<br /></p>

<p>How it works and trade-offs:</p>
<ul>
  <li>The approach produces initially noisy outputs that are progressively denoised or refined over multiple iterations; each iteration operates on the whole sequence and can be executed in parallel across devices, eliminating per-token sequential KV transfers.<br /></li>
  <li>Early results, particularly on <strong>coding tasks</strong>, show dramatic speed advantages versus autoregressive transformers.<br /></li>
  <li>Achieving parity in general language accuracy remains an active research challenge.<br /></li>
</ul>

<hr />

<h1 id="quantization-techniques-for-inference-memory-and-bandwidth-reduction">Quantization techniques for inference memory and bandwidth reduction</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/01-06-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/01-06-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/01-06-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/01-06-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Quantization</strong> reduces numeric precision of weights and activations (e.g., <strong>BF16 → FP8/INT8/INT4</strong>) to lower memory footprint and bandwidth requirements, directly reducing the dominant memory transfer cost of generation.<br /></p>

<p>Practical strategies include:</p>
<ul>
  <li><strong>Post-training quantization</strong> and dynamic schemes that isolate and preserve outlier parameters or activations in higher precision (for example, keeping a small set of large-magnitude columns in FP16 while quantizing the rest to INT8).<br /></li>
  <li><strong>Activation-aware</strong> and <strong>mixed-precision</strong> pipelines that enable aggressive compression with modest accuracy loss.<br /></li>
</ul>

<p>When implemented with hardware-aware kernels, quantization yields substantial runtime and memory benefits.<br /></p>

<hr />

<h1 id="model-pruning-and-structured-sparsity-with-distillation-to-recover-accuracy">Model pruning and structured sparsity with distillation to recover accuracy</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/01-13-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/01-13-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/01-13-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/01-13-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Structured pruning</strong> removes entire layers, attention heads, or hidden dimensions judged unimportant by calibration, producing smaller, faster variants of a trained model.<br /></p>

<p>To mitigate accuracy degradation, the pruned architecture is typically:</p>
<ol>
  <li><strong>Reinitialized</strong>, and then<br /></li>
  <li><strong>Distilled</strong> from the full model to transfer behavior and calibrate the reduced parameterization.<br /></li>
</ol>

<p>This combination of selection and distillation has enabled meaningful parameter reductions (for example, from <strong>15B → 8B</strong>) with small accuracy losses on common benchmarks, offering a practical path for cost-effective inference.<br /></p>

<hr />

<h1 id="speculative-decoding-using-a-cheap-draft-model-to-accelerate-exact-sampling">Speculative decoding using a cheap draft model to accelerate exact sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/01-18-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/01-18-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/01-18-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/01-18-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Speculative decoding</strong> uses a small, fast draft model <strong>P</strong> to generate multiple look-ahead tokens and then verifies those tokens under the target model <strong>Q</strong> via scoring (a prefill-like parallel evaluation) to accept or reject proposals.<br /></p>

<p>Procedure and guarantees:</p>
<ul>
  <li>Accepted tokens are returned immediately; rejected proposals are corrected by sampling from <strong>Q</strong>.<br /></li>
  <li>The overall procedure is constructed so that accepted outputs are <strong>exact samples from Q</strong>, preserving model correctness.<br /></li>
  <li>Because verification uses parallel prefill-like operations and drafting is cheap, speculative decoding can yield significant wall-clock speedups while producing mathematically exact samples from the expensive target distribution.<br /></li>
</ul>

<hr />

<h1 id="serving-dynamics-batching-heterogeneity-and-memory-allocation-strategies-page-attention">Serving dynamics, batching heterogeneity, and memory allocation strategies (page attention)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec10/01-21-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec10/01-21-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec10/01-21-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec10/01-21-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Real-world serving traffic is heterogeneous with varying arrival times, request lengths, and <strong>shared prefixes</strong>, creating fragmentation and scheduling challenges that differ from dense training workloads.<br /></p>

<p>Systems strategies to address these issues include:</p>
<ul>
  <li><strong>Selective batching</strong> — flatten independent MLP computations while handling attention per-request.<br /></li>
  <li><strong>Copy-on-write prefix sharing</strong> — avoid duplicating KV blocks for requests that share prefixes.<br /></li>
  <li><strong>Page-based KV allocation</strong> — break the KV cache into contiguous blocks for flexible reuse and compaction.<br /></li>
</ul>

<p>Applying operating-system-inspired memory management and dynamic scheduling reduces fragmentation, improves memory utilization, and increases effective throughput under live traffic loads.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 9 - Scaling laws</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec09/" rel="alternate" type="text/html" title="CS336 Lecture 9 - Scaling laws" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec09</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec09/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/6Q-ESEmDf4Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="scaling-laws-provide-a-predictive-framework-for-extrapolating-small-model-experiments-to-guide-large-model-design">Scaling laws provide a predictive framework for extrapolating small-model experiments to guide large-model design</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-00-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-00-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-00-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-00-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Scaling laws formalize using systematic <strong>small-scale experiments</strong> to predict the behavior of much larger models, reducing the compute and human cost of hyperparameter and architecture search.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> train many small models across a range of sizes or compute budgets, fit simple functional forms to observed <strong>loss</strong> or <strong>metric trajectories</strong>, and extrapolate those fits to select configurations for a single large-scale run.<br /></li>
  <li><strong>Rationale:</strong> avoid prohibitively expensive brute‑force tuning at full scale by relying on empirically validated extrapolations.<br /></li>
  <li><strong>Implementation notes:</strong> ensure small-scale experiments cover the relevant regime (e.g., the <strong>power-law region</strong>) and use robust distributed training, data pipelines, and infra so extrapolation isn’t dominated by engineering artifacts.<br /></li>
</ul>

<p>This approach is especially useful in frontier labs where a single successful large-scale training run is the objective.<br /></p>

<hr />

<h1 id="scaling-laws-aim-to-produce-simple-predictive-relationships-between-model-resources-data-parameters-compute-and-model-performance">Scaling laws aim to produce simple, predictive relationships between model resources (data, parameters, compute) and model performance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-02-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-02-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-02-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-02-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Scaling laws</strong> seek simple mathematical relationships that predict how performance metrics change when resources (dataset size, model parameters, compute) increase.<br /></p>

<ul>
  <li><strong>Central idea:</strong> fit compact empirical functions on small-scale runs and use them to make engineering decisions for larger runs.<br /></li>
  <li><strong>Empirical regime:</strong> rely on monotonic, often approximately <strong>power-law</strong> dependencies where the model is not yet saturated by <strong>irreducible error</strong>.<br /></li>
  <li><strong>Guidance produced:</strong> choices such as optimal parameter counts, data budgets, and compute allocations.<br /></li>
  <li><strong>Practical requirement:</strong> choose resource ranges that avoid pathological regimes (random guessing or asymptotic irreducible error) so fitted forms are valid for extrapolation.<br /></li>
  <li><strong>Program design:</strong> couple careful experimental design with regression or surface‑fitting techniques to quantify uncertainty in extrapolations.<br /></li>
</ul>

<hr />

<h1 id="scaling-laws-connect-to-classical-statistical-learning-theory-but-operate-on-empirical-realized-losses-rather-than-worst-case-upper-bounds">Scaling laws connect to classical statistical learning theory but operate on empirical realized losses rather than worst-case upper bounds</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-04-21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-04-21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-04-21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-04-21.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Classical theory</strong> (VC dimension, Rademacher complexity, nonparametric rates) gives asymptotic/worst-case bounds (e.g., O(1/√n) or n^{-β/(2β+1})) but these are upper bounds rather than observed loss trajectories.<br /></p>

<ul>
  <li><strong>Role of scaling laws:</strong> empirically fit functional relationships between dataset size, model capacity, and realized test loss to provide operational predictions for modern high-capacity models that violate simple theoretical assumptions.<br /></li>
  <li><strong>Mechanism:</strong> treat empirical loss as the observable and use parametric or semi‑parametric curve fitting (e.g., <strong>power laws</strong>) across many runs to estimate exponents and offsets.<br /></li>
  <li><strong>Rationale:</strong> empirical fits often deliver much tighter, actionable guidance than generic theory, especially for highly overparameterized neural networks.<br /></li>
  <li><strong>Implementation considerations:</strong> carefully average over random seeds and use consistent evaluation metrics so fitted curves reflect true model behavior rather than noise.<br /></li>
</ul>

<hr />

<h1 id="early-empirical-scaling-law-thinking-predates-modern-deep-learning-and-proposed-fitting-predictive-performance-curves-to-avoid-full-scale-training">Early empirical scaling-law thinking predates modern deep learning and proposed fitting predictive performance curves to avoid full-scale training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-06-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-06-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-06-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-06-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Work since the early 1990s proposed predicting classifier performance from partial or small-scale runs to select promising configurations without training full models.<br /></p>

<ul>
  <li><strong>Historical mechanism:</strong> decompose test error into <strong>irreducible error</strong> plus polynomially decaying terms, train many small models, fit decay curves (polynomial/power-law), and extrapolate to larger regimes — directly analogous to modern scaling-law practice.<br /></li>
  <li><strong>Practical context:</strong> these historical results validate that systematic small-scale experiments can guide large-scale decisions and that power-law‑like fits often capture dominant behavior in the productive regime.<br /></li>
  <li><strong>Implementation note:</strong> validate fitted functional forms against held-out experiments to avoid overconfident or biased extrapolation.<br /></li>
</ul>

<hr />

<h1 id="empirical-neural-scaling-laws-exhibit-three-phasesrandominitial-power-law-scaling-and-asymptotic-irreducible-errorand-are-surprisingly-predictive-across-domains">Empirical neural scaling laws exhibit three phases—random/initial, power-law scaling, and asymptotic irreducible error—and are surprisingly predictive across domains</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-09-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-09-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-09-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-09-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Empirical studies across translation, speech, and vision identify a characteristic three‑phase curve shape for loss versus resources:<br /></p>

<ol>
  <li><strong>Initial region near random performance</strong> — difficult to extrapolate.<br /></li>
  <li><strong>Middle regime</strong> where loss decreases approximately as a <strong>power law</strong> with resources — the predictive domain for scaling laws.<br /></li>
  <li><strong>Asymptotic region</strong> approaching <strong>irreducible error</strong> where gains taper off and extrapolation is unreliable.<br /></li>
</ol>

<ul>
  <li><strong>Mechanistic insight:</strong> scaling-law procedures restrict fits to the intermediate regime where predictions are most reliable.<br /></li>
  <li><strong>Practical implication:</strong> design experiments to avoid model saturation so the middle regime is well sampled.<br /></li>
  <li><strong>Robustness:</strong> this three‑phase behavior appears across architectures and tasks and underpins confidence that small-scale experiments can inform large-scale outcomes when the right regime is targeted.<br /></li>
</ul>

<hr />

<h1 id="scaling-behavior-can-break-down-for-out-of-distribution-or-narrowly-defined-failure-modes-producing-non-scaling-or-inverse-scaling-phenomena">Scaling behavior can break down for out-of-distribution or narrowly defined failure modes, producing non-scaling or inverse-scaling phenomena</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-11-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-11-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-11-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-11-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Scaling is natural for metrics like <strong>held-out training loss</strong> because improvement is typically monotonic with more data or capacity, but some capabilities or pathological behaviors can degrade with scale.<br /></p>

<ul>
  <li><strong>Examples:</strong> tasks from inverse-scaling studies where certain capabilities become worse as models grow, or where extreme out‑of‑distribution (OOD) performance does not improve monotonically.<br /></li>
  <li><strong>Mechanism for non-scaling:</strong> occurs when the evaluation domain drifts far from the training distribution or when tasks depend on brittle heuristics that do not benefit from capacity in conventional ways.<br /></li>
  <li><strong>Practical implication:</strong> extrapolation must be <strong>task-aware</strong> — don’t assume universal log‑log linearity for all downstream behaviors; directly measure the phenomenon of interest across scale to verify the assumed scaling.<br /></li>
</ul>

<hr />

<h1 id="large-language-model-scaling-exhibits-consistent-log-log-linear-relationships-across-compute-data-and-parameters-when-measured-in-appropriate-regimes">Large language model scaling exhibits consistent log-log linear relationships across compute, data, and parameters when measured in appropriate regimes</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-12-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-12-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-12-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-12-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When evaluated on <strong>next-token prediction</strong> or other log‑loss metrics in the middle power‑law regime, models show approximately linear trends on log‑log plots between resource axes (compute, data, parameters) and test loss.<br /></p>

<ul>
  <li><strong>Empirical mechanism:</strong> hold one resource large enough to avoid saturation and vary the other to reveal the power‑law region (e.g., keep model size large while varying data to study data scaling).<br /></li>
  <li><strong>Rationale:</strong> log‑log linearity (power laws) is a compact functional form that fits many empirical curves, enabling interpolation and extrapolation across orders of magnitude.<br /></li>
  <li><strong>Implementation caveat:</strong> ensure the held‑constant resource is actually large enough and that evaluation is done in a regime where the power‑law model is valid before extrapolating.<br /></li>
</ul>

<hr />

<h1 id="data-scaling-laws-map-dataset-size-n-to-excess-error-and-typically-show-log-log-linear-power-law-decay-in-the-productive-regime">Data-scaling laws map dataset size n to excess error and typically show log-log linear (power-law) decay in the productive regime</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-14-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-14-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-14-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-14-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Data‑scaling analysis</strong> focuses on the <strong>excess error</strong> (error above irreducible noise) as a function of the number of unique training tokens n and often finds error ∝ n^{-α} in the power‑law regime.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> train models with fixed high capacity while varying dataset sizes, plot test loss versus n on log‑log axes, and fit a straight line to estimate the exponent α.<br /></li>
  <li><strong>Rationale:</strong> identifying α and the offset enables predictions about how much additional data is needed to achieve a target reduction in loss and supports tradeoffs like data collection vs. compute allocation.<br /></li>
  <li><strong>Implementation notes:</strong> focus on <strong>unique‑token counts</strong> (account for repeated‑epoch effects separately) and validate that the fitted exponent is stable across model sizes within the power‑law region.<br /></li>
</ul>

<hr />

<h1 id="simple-parametric-estimation-tasks-yield-power-law-rates-eg-mean-estimation-scales-as-1n-while-nonparametric-function-estimation-introduces-intrinsic-dimension-dependent-slower-rates">Simple parametric estimation tasks yield power-law rates (e.g., mean estimation scales as 1/n) while nonparametric function estimation introduces intrinsic-dimension-dependent slower rates</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-15-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-15-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-15-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-15-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Elementary tasks explain why polynomial decay in n is natural:<br /></p>

<ul>
  <li><strong>Gaussian mean estimation:</strong> mean‑squared error ∝ σ^2/n, so log‑error is linear in log n with slope −1.<br /></li>
  <li>
    <p><strong>Nonparametric regression:</strong> covering input domain with bins yields error scaling like n^{-1/d} for data uniformly distributed in d dimensions; effective learning rate decays more slowly as intrinsic dimension increases.<br /></p>
  </li>
  <li><strong>Mechanistic takeaway:</strong> flexible function classes require exponentially more samples to resolve fine structure in higher intrinsic dimensions, reflected as smaller exponents on log‑log error plots.<br /></li>
  <li><strong>Rationale:</strong> shallow exponents in language modeling can be understood as consequences of high intrinsic dimensionality; inducing strong <strong>inductive biases</strong> or estimating intrinsic dimension can change the empirical exponent.<br /></li>
  <li><strong>Implementation caveat:</strong> intrinsic dimension is hard to estimate robustly, so interpret exponents cautiously and validate across datasets and architectures.<br /></li>
</ul>

<hr />

<h1 id="observed-empirical-exponents-for-language-related-tasks-are-often-much-smaller-than-simple-parametric-rates-reflecting-high-intrinsic-complexity">Observed empirical exponents for language-related tasks are often much smaller than simple parametric rates, reflecting high intrinsic complexity</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-18-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-18-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-18-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-18-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Empirical studies report exponents substantially below classical parametric expectations (examples: ~0.13 for MT, ~0.3 for speech, and ≈0.95 in some LM settings), meaning errors decline much more slowly with dataset size than naive 1/n or 1/√n rates.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> modern language and speech tasks involve highly nonparametric target functions with large effective dimensionality and complex structure, increasing sample complexity.<br /></li>
  <li><strong>Rationale:</strong> shallow empirical exponents imply enormous increases in tokens are needed for modest loss gains, motivating strategies that optimize model architecture and data mixtures rather than naive data accumulation.<br /></li>
  <li><strong>Implementation note:</strong> estimate empirical exponents using a broad range of dataset sizes and carefully control confounders such as model capacity and evaluation protocol.<br /></li>
</ul>

<hr />

<h1 id="generating-synthetic-data-with-controlled-intrinsic-dimensionality-is-straightforward-but-estimating-real-data-intrinsic-dimension-is-challenging">Generating synthetic data with controlled intrinsic dimensionality is straightforward, but estimating real data intrinsic dimension is challenging</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-20-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-20-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-20-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-20-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>To create data with a chosen intrinsic dimension for experiments, specify a function of k latent variables plus noise to produce manifolds of known dimension.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> synthetic manifolds validate predicted n^{-1/d} behavior in nonparametric estimation.<br /></li>
  <li><strong>Practical implication:</strong> synthetic experiments are useful for theory validation, but translating results to complex real data (code, natural text) requires caution because true generative factors and interactions are unknown and noisy.<br /></li>
  <li><strong>Implementation note:</strong> use additional empirical diagnostics when moving from synthetic to real‑world corpora.<br /></li>
</ul>

<hr />

<h1 id="data-scaling-laws-enable-practical-engineering-decisions-such-as-dataset-composition-optimal-mixing-and-assessing-diminishing-returns-from-repetitions">Data-scaling laws enable practical engineering decisions such as dataset composition, optimal mixing, and assessing diminishing returns from repetitions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-22-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-22-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-22-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-22-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Scaling‑law fits</strong> can quantify how dataset composition shifts the <strong>loss offset</strong> without substantially changing the exponent, enabling evaluation of relative data‑quality decisions on smaller models and extrapolation to large regimes.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> fit scaling laws per data source and estimate their contributions to mixed‑data performance, enabling regression‑based optimal mixing strategies for trillion‑token regimes.<br /></li>
  <li><strong>Extension:</strong> scaling laws extend to multi‑epoch training by defining an <strong>effective unique‑token count</strong> that diminishes with repetition, helping decide whether to repeat high‑quality data or add lower‑quality new data.<br /></li>
  <li><strong>Implementation caveats:</strong> data‑selection research is empirically difficult; validate that per‑mixture power‑law assumptions hold in the target regime.<br /></li>
</ul>

<hr />

<h1 id="when-analyzing-data-scaling-behavior-the-model-size-used-for-evaluation-must-be-large-enough-to-avoid-parameter-limited-saturation">When analyzing data-scaling behavior, the model size used for evaluation must be large enough to avoid parameter-limited saturation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-25-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-25-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-25-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-25-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Data‑scaling experiments assume the <strong>model is not the bottleneck</strong>; therefore experimenters select model sizes sufficiently larger than the task’s effective complexity so loss is dominated by data scarcity rather than model capacity.<br /></p>

<ul>
  <li><strong>Mechanic:</strong> keep model parameters high and vary only dataset size when fitting data‑scaling exponents.<br /></li>
  <li><strong>Rationale:</strong> if the model is saturated, fitted data‑scaling laws will reflect model limitations rather than true data‑driven gains, invalidating extrapolations.<br /></li>
  <li><strong>Implementation note:</strong> when presenting data‑scaling plots, document the fixed model size and verify that increasing the model further does not substantially change the fitted exponent in the regime of interest.<br /></li>
</ul>

<hr />

<h1 id="model-scaling-analysis-addresses-trade-offs-among-architectures-optimizers-hyperparameters-and-resource-allocation-for-deployment">Model-scaling analysis addresses trade-offs among architectures, optimizers, hyperparameters, and resource allocation for deployment</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-27-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-27-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-27-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-27-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Model‑scaling law methodology</strong> evaluates architectures (transformers, LSTMs, state‑space models, gated units, MoE), optimizers (SGD, Adam), and hyperparameters by training families of models across compute budgets and comparing their loss‑versus‑compute trajectories.<br /></p>

<ul>
  <li><strong>Mechanistic pattern:</strong> curves for different architectures often appear as non‑crossing, parallel‑like lines on log‑log plots, indicating constant‑factor compute efficiency differences and enabling extrapolation that lower‑offset architectures perform better across scales.<br /></li>
  <li><strong>Rationale:</strong> reveals which innovations are worth scaling (e.g., gated linear units, mixture‑of‑experts) and which provide limited benefit when compute is abundant.<br /></li>
  <li><strong>Implementation considerations:</strong> account for parameter types (embeddings vs. non‑embedding parameters) and normalize sparse/conditional parameterizations into equivalent dense‑parameter measures for fair comparisons.<br /></li>
</ul>

<hr />

<h1 id="optimizer-choice-and-depthwidthaspect-ratio-choices-can-be-evaluated-via-scaling-curves-to-find-scale-robust-hyperparameter-regimes">Optimizer choice and depth/width/aspect-ratio choices can be evaluated via scaling curves to find scale-robust hyperparameter regimes</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-30-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-30-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-30-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-30-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Scaling studies comparing optimizers show approximately constant‑factor differences in compute efficiency across dataset sizes, and depth‑versus‑width sweeps reveal broad basins of near‑optimal aspect ratios rather than sharp optima.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> train many models at multiple sizes and plot loss versus compute to test whether slopes and offsets of scaling curves vary across hyperparameter choices; conserved slopes with differing offsets imply small‑scale tuning can transfer to large scale.<br /></li>
  <li><strong>Rationale:</strong> scaling‑law sweeps avoid repeated costly retuning at each scale and identify hyperparameter settings robust across sizes.<br /></li>
  <li><strong>Implementation notes:</strong> sweep multiple sizes (e.g., 50M, 270M, 1.5B) and fit surfaces or envelopes to identify consistent minima.<br /></li>
</ul>

<hr />

<h1 id="not-all-parameters-are-equivalent-embedding-and-sparsely-activated-parameters-change-scaling-law-behavior-and-require-normalization">Not all parameters are equivalent; embedding and sparsely activated parameters change scaling-law behavior and require normalization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-33-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-33-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-33-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-33-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Certain parameter classes (e.g., <strong>embedding tables</strong>) do not participate in scaling the same way as non‑embedding transformer parameters, producing bends or deviations in naive parameter‑versus‑loss curves.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> counting embeddings as ordinary parameters can distort fitted exponents and offsets, so analyses either exclude embeddings or convert sparse/specialized parameters into an equivalent dense‑parameter metric.<br /></li>
  <li><strong>For conditional/sparse architectures (MoE):</strong> derive an <strong>equivalent dense‑parameter count</strong> to enable fair comparisons and meaningful scaling predictions.<br /></li>
  <li><strong>Implementation implication:</strong> reports should specify which parameter subsets were counted and how sparsity/activation patterns were normalized for cross‑architecture scaling comparisons.<br /></li>
</ul>

<hr />

<h1 id="batch-size-exhibits-a-critical-threshold-beyond-which-returns-diminish-and-the-critical-batch-size-itself-scales-with-target-loss">Batch size exhibits a critical threshold beyond which returns diminish, and the critical batch size itself scales with target loss</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-36-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-36-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-36-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-36-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Increasing batch size up to the <strong>gradient‑noise scale</strong> yields near‑linear wall‑clock/step equivalence (doubling batch size ≈ taking two gradient steps) and is therefore an effective systems optimization; past a <strong>critical batch size</strong>, additional samples per step stop reducing stochastic gradient noise and yield diminishing returns.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> the noise scale defines the regime boundary where optimization transitions from noise‑dominated to curvature‑dominated; the critical batch size can be estimated empirically or modeled theoretically.<br /></li>
  <li><strong>Empirical observation:</strong> the critical batch size tends to decrease as the target loss becomes smaller, so schedules and parallelization should adapt during training.<br /></li>
  <li><strong>Implementation consequences:</strong> plan batch‑size schedules and balance data‑parallel throughput with optimization efficiency; many large training reports adjust batch sizes during runs.<br /></li>
</ul>

<hr />

<h1 id="learning-rate-optima-vary-with-model-width-under-standard-parameterizations-but-scale-stable-reparameterizations-eg-μpnew-p-can-make-the-optimal-learning-rate-transfer-across-widths">Learning-rate optima vary with model width under standard parameterizations, but scale-stable reparameterizations (e.g., μP/new-p) can make the optimal learning rate transfer across widths</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-39-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-39-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-39-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-39-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Under conventional transformer parameterizations the optimal learning rate typically decreases as width increases, producing different tuned optima at different scales and complicating hyperparameter transfer from small to large models.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> reparameterizations that scale initializations and layer‑wise update magnitudes as a function of width (e.g., <strong>μP</strong> or variants) normalize the optimization landscape so a single learning‑rate choice tuned at small scale remains near‑optimal at larger widths.<br /></li>
  <li><strong>Rationale:</strong> scale‑stable parameterizations reduce the need for repeated learning‑rate sweeps across model sizes and simplify large‑scale training planning.<br /></li>
  <li><strong>Implementation caveat:</strong> μP‑like schemes improve transferability but are not a panacea; multiple variants exist (e.g., Meta’s metap) and require empirical validation in each training stack.<br /></li>
</ul>

<hr />

<h1 id="batch-size-and-learning-rate-interact-via-gradient-noise-considerations-and-their-optimal-pairing-can-change-with-loss-targets-and-modality">Batch size and learning rate interact via gradient-noise considerations, and their optimal pairing can change with loss targets and modality</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-42-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-42-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-42-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-42-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Optimal batch‑size choices are linked to <strong>noise scale</strong> and learning‑rate schedules; as training progresses toward lower loss targets, practitioners often increase batch size to reduce gradient noise that would otherwise force smaller learning rates or conservative updates.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> batch size and learning rate affect gradient variance and step size in opposing ways, so their joint tuning is necessary for stable large‑scale training.<br /></li>
  <li><strong>Practical considerations:</strong> modality‑specific behavior (NLP vs. vision) matters and noise‑scale theory gives qualitative guidance rather than a universal prescription.<br /></li>
  <li><strong>Implementation advice:</strong> empirically estimate critical batch sizes and validate learning‑rate transfer or reparameterization choices for the specific model and dataset.<br /></li>
</ul>

<hr />

<h1 id="log-loss-next-token-prediction-scaling-is-robust-and-predictable-but-downstream-task-scaling-and-capability-emergence-can-be-far-less-predictable">Log-loss (next-token prediction) scaling is robust and predictable, but downstream task scaling and capability emergence can be far less predictable</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-45-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-45-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-45-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-45-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Cross‑entropy</strong> or negative log‑likelihood scales smoothly and is highly amenable to log‑log linear modeling across architectures and compute levels, explaining why many scaling‑law results focus on perplexity or test loss.<br /></p>

<ul>
  <li><strong>Mechanism for mismatch:</strong> mapping log‑loss improvements to downstream benchmarks (accuracy, QA, in‑context learning) is often nonlinear and can vary widely across architectures and hyperparameters because task capabilities depend on subtle behaviors not fully captured by perplexity.<br /></li>
  <li><strong>Practical implication:</strong> use direct empirical evaluation on the downstream task of interest rather than assuming that perplexity scaling automatically translates to better task performance.<br /></li>
</ul>

<hr />

<h1 id="scaling-law-based-design-prescribes-training-small-models-across-orders-of-magnitude-fitting-scaling-relationships-and-extrapolating-optimal-hyperparameters-and-architectures-to-full-scale">Scaling-law-based design prescribes training small models across orders of magnitude, fitting scaling relationships, and extrapolating optimal hyperparameters and architectures to full scale</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-48-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-48-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-48-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-48-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical recipe for using scaling laws:<br /></p>

<ol>
  <li>Run small‑to‑medium scale experiments spanning multiple orders of magnitude in compute.<br /></li>
  <li>Verify loss versus resource relationships show stable log‑log linearity (or another valid functional form).<br /></li>
  <li>Fit relationships to extract slopes and offsets.<br /></li>
  <li>Use fits to choose model size, data budget, batch size, and hyperparameters for a single large‑scale run.<br /></li>
</ol>

<ul>
  <li><strong>Mechanism:</strong> relies on conserved slopes (non‑crossing curves) so small‑scale minima and offsets transfer to larger scales.<br /></li>
  <li><strong>Rationale:</strong> reduces risk and cost compared to brute‑force large‑scale hyperparameter searches.<br /></li>
  <li><strong>Caveats:</strong> some parameters (e.g., learning rate) may not transfer without reparameterization.<br /></li>
  <li><strong>Best practices:</strong> span a broad compute range, report fit uncertainty, and validate extrapolations with a limited number of larger intermediate runs when feasible.<br /></li>
</ul>

<hr />

<h1 id="joint-datamodel-scaling-laws-iso-flop-trade-offs-describe-how-loss-decomposes-into-data-dependent-and-model-dependent-components-and-enable-optimal-compute-allocation">Joint data–model scaling laws (iso-flop trade-offs) describe how loss decomposes into data-dependent and model-dependent components and enable optimal compute allocation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-52-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-52-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-52-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-52-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Joint scaling models</strong> posit additive or multiplicative decompositions of error into terms that decay polynomially with data (n) and model size (m), plus irreducible error, often written as Loss(n,m) ≈ c_n n^{-α_n} + c_m m^{-α_m} + c_0.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> fit such a two‑variable surface across grids of (n,m) to compute <strong>iso‑flop curves</strong> that show optimal parameter/token trade‑offs for a fixed compute budget.<br /></li>
  <li><strong>Rationale:</strong> answer whether to allocate compute to more parameters or more data and derive prescriptions (e.g., tokens‑per‑parameter ratios) that minimize loss for fixed flops.<br /></li>
  <li><strong>Implementation notes:</strong> perform careful surface fitting, account for learning‑rate and schedule interactions, and validate extrapolations with held‑out larger‑scale runs.<br /></li>
</ul>

<hr />

<h1 id="the-chinchilla-analysis-operationalized-iso-flop-optimization-and-indicated-an-approximately-20-tokens-per-parameter-budget-under-its-assumptions">The Chinchilla analysis operationalized iso-flop optimization and indicated an approximately 20 tokens-per-parameter budget under its assumptions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-55-32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-55-32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-55-32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-55-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Chinchilla used three empirical methods to infer the optimal <strong>token‑to‑parameter ratio</strong> for minimizing loss at fixed training flops and found a consistent recommendation near ~20 tokens per parameter under their training protocol and schedules.<br /></p>

<ul>
  <li><strong>Mechanisms used:</strong> lower envelope of performance across models, isoflop minima extraction via quadratic fits per compute slice, and direct joint functional form fitting; the first two produced similar coefficients while the third required careful regression handling.<br /></li>
  <li><strong>Rationale:</strong> the tokens‑per‑parameter guideline provided a practical balance between parameter count and dataset size for a given compute budget and guided many subsequent training budgets.<br /></li>
  <li><strong>Implementation caveat:</strong> the precise optimal ratio depends on learning‑rate schedule, optimization details, and evaluation protocol, so reproducing the exact number requires matching training recipes.<br /></li>
</ul>

<hr />

<h1 id="method-1-minimum-envelope-finds-the-optimal-model-size-and-data-allocation-by-taking-the-lower-envelope-across-many-training-curves">Method 1 (minimum-envelope) finds the optimal model-size and data allocation by taking the lower envelope across many training curves</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-57-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-57-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-57-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-57-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>minimum‑envelope method</strong> overlays training curves for models of different parameter counts across compute budgets and selects the lower envelope of achievable loss for each compute level.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> read off the parameter count and token count at the envelope to get an empirical optimal token‑to‑parameter ratio; the approach is nonparametric and leverages the smooth curve formed by observed optima.<br /></li>
  <li><strong>Rationale:</strong> makes minimal modeling assumptions and directly uses observed optima, making it robust when grid coverage is sufficient.<br /></li>
  <li><strong>Implementation requirements:</strong> train a dense grid of model sizes and checkpoints across compute so the envelope is well‑resolved; use interpolation or local quadratic fits to reduce noise when identifying minima.<br /></li>
</ul>

<hr />

<h1 id="method-2-isoflop-analysis-finds-the-optimal-parameterdata-trade-off-by-minimizing-loss-along-fixed-flop-slices-and-reading-out-the-minima">Method 2 (isoflop analysis) finds the optimal parameter/data trade-off by minimizing loss along fixed-flop slices and reading out the minima</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/00-59-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/00-59-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/00-59-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/00-59-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Isoflop analysis</strong> fixes total compute (flops) and, for each compute level, sweeps model sizes and corresponding token counts to find the configuration that minimizes loss within that budget.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> implement by fitting local quadratics to loss‑versus‑parameter curves at each isoflop and extracting minima to reduce measurement noise.<br /></li>
  <li><strong>Rationale:</strong> directly answers how to allocate a given compute budget between model size and dataset size.<br /></li>
  <li><strong>Implementation notes:</strong> require careful curve smoothing and validation across multiple compute levels to ensure minima form a stable power‑law curve amenable to extrapolation.<br /></li>
</ul>

<hr />

<h1 id="method-3-direct-joint-functional-fitting-fits-a-parametric-two-variable-surface-to-observed-runs-but-requires-careful-regression-to-avoid-biased-coefficients">Method 3 (direct joint functional fitting) fits a parametric two-variable surface to observed runs but requires careful regression to avoid biased coefficients</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/01-01-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/01-01-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/01-01-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/01-01-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>An alternative approach fits a global parametric form Loss(n,m)=c_n n^{-α_n}+c_m m^{-α_m}+c_0 to all observed runs and extracts optimal trade‑offs from the fitted surface.<br /></p>

<ul>
  <li><strong>Mechanistic risk:</strong> sensitive to regression setup and residual structure; non‑zero‑mean residuals or heteroscedasticity can bias estimated exponents and conflict with envelope or isoflop methods.<br /></li>
  <li><strong>Empirical lesson:</strong> documented replication found that correcting residual issues reconciled the global fit with envelope/isoflop methods, highlighting the importance of statistically careful fitting.<br /></li>
  <li><strong>Implementation takeaway:</strong> inspect residuals, consider weighted regression, and validate global fits against lower‑envelope or isoflop minima to ensure consistent conclusions.<br /></li>
</ul>

<hr />

<h1 id="inference-cost-and-deployment-considerations-have-shifted-preferred-training-regimes-toward-much-higher-tokens-per-parameter-ratios">Inference cost and deployment considerations have shifted preferred training regimes toward much higher tokens-per-parameter ratios</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/01-02-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/01-02-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/01-02-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/01-02-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>As models became deployed products with recurring inference costs, the operational trade‑off shifted: pay more one‑time training cost (increased tokens per parameter) to produce <strong>smaller models</strong> that are cheaper to serve at inference time.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> increasing tokens per parameter while reducing parameter counts lowers steady‑state inference FLOPs and monetary serving cost even if training cost stays similar or increases modestly.<br /></li>
  <li><strong>Historical trend:</strong> GPT‑3 used smaller token/parameter ratios while Chinchilla and later models pushed toward ~20 tokens/parameter; recent models experiment with even larger token budgets to prioritize compact inference.<br /></li>
  <li><strong>Implementation implication:</strong> product teams should treat tokens‑per‑parameter as a tunable hyperparameter driven by the relative costs of training versus inference and by service‑level targets.<br /></li>
</ul>

<hr />

<h1 id="scaling-law-methodologies-generalize-to-different-generative-architectures-eg-diffusion-text-models-and-often-reproduce-similar-iso-flop-minima-separated-by-constant-offsets">Scaling-law methodologies generalize to different generative architectures (e.g., diffusion text models) and often reproduce similar iso-flop minima separated by constant offsets</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/01-03-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/01-03-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/01-03-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/01-03-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When applied to novel modeling paradigms (e.g., <strong>text diffusion models</strong>), the same empirical playbook (isoslop/isoflop analyses) often produces predictable scaling behavior with curves that mirror autoregressive results up to constant offsets.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> training grids for alternative architectures and plotting minimum loss envelopes often reveals parallel power‑law behavior, indicating scaling phenomena are not idiosyncratic to one objective or decoder.<br /></li>
  <li><strong>Rationale:</strong> cross‑model reproducibility supports using scaling‑law analysis as a general engineering tool when exploring new model families, subject to careful experimental control.<br /></li>
  <li><strong>Implementation note:</strong> confirm offset magnitudes and that the productive regime exists for the new architecture before extrapolating to extreme scales.<br /></li>
</ul>

<hr />

<h1 id="log-linear-scaling-across-data-parameters-and-compute-provides-a-unifying-empirical-foundation-for-many-practical-model-design-and-resource-allocation-decisions">Log-linear scaling across data, parameters, and compute provides a unifying empirical foundation for many practical model-design and resource-allocation decisions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec09/01-04-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec09/01-04-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec09/01-04-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec09/01-04-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Power‑law (log‑log linear) relationships often hold across multiple axes — dataset size, parameter count, compute — enabling unified analyses for architecture selection, hyperparameter transfer, iso‑flop optimization, and batch/learning‑rate scheduling.<br /></p>

<ul>
  <li><strong>Mechanism:</strong> these empirical regularities permit dimensionality reduction of design choices into simple curves or surfaces that teams can fit on small‑to‑moderate experiments and extrapolate to production scales.<br /></li>
  <li><strong>Practical impact:</strong> guidance on tokens‑per‑parameter ratios, when to prefer more data versus more parameters, and how to design scale‑stable parameterizations for optimizer transfer.<br /></li>
  <li><strong>Caveats:</strong> validate scaling assumptions for downstream tasks, account for training‑schedule interactions, and quantify fit uncertainty.<br /></li>
  <li><strong>Summary implication:</strong> when properly validated, scaling laws materially reduce the risk and cost of building large‑scale generative models by turning extensive empirical sweeps into concise, actionable extrapolations.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 8 - Parallelism - Part 2</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec08/" rel="alternate" type="text/html" title="CS336 Lecture 8 - Parallelism - Part 2" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec08</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec08/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/LHpr5ytssLo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="multi-gpu-training-requires-structuring-computation-to-minimize-inter-device-data-transfer">Multi-GPU training requires structuring computation to minimize inter-device data transfer.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-01-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-01-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-01-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-01-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modern multi-GPU systems expose a multi-level <strong>memory and communication hierarchy</strong> where compute units (SMs) perform arithmetic that fetches inputs from different tiers:</p>

<ul>
  <li><strong>On-chip caches</strong> (e.g., <strong>L1</strong>) for the fastest, smallest accesses</li>
  <li><strong>High-bandwidth on-device memory</strong> (<strong>HBM</strong>) for larger working sets</li>
  <li><strong>Remote device memory</strong> reached over interconnects (PCIe, NVLink, network) for the slowest accesses<br /></li>
</ul>

<p>Data transfers across these levels are <strong>orders of magnitude slower</strong> than on-chip arithmetic, so maximizing <strong>arithmetic intensity</strong> and avoiding unnecessary transfers is central to performance.<br /></p>

<p>Common techniques to reduce memory traffic:</p>

<ul>
  <li><strong>Fusion</strong>: combine operations to eliminate intermediate writes/reads</li>
  <li><strong>Tiling</strong>: work on local tiles in a scratchpad before committing results to slower tiers</li>
</ul>

<p>When scaling across GPUs and nodes, <strong>model parameters</strong> and <strong>optimizer state</strong> are either <strong>replicated</strong> or <strong>sharded</strong>, and the chosen distribution determines communication volume and latency.<br /></p>

<p>Design goal: keep data <strong>local as long as possible</strong> — this is the primary systems objective for efficient multi-GPU training.<br /></p>

<hr />

<h1 id="lecture-content-is-organized-into-two-parts-collective-operations-implementation-and-distributed-training-strategies-with-benchmarking-and-code-examples">Lecture content is organized into two parts: collective operations implementation and distributed training strategies with benchmarking and code examples.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-04-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-04-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-04-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-04-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The material is organized into two complementary parts:</p>

<ol>
  <li><strong>Building blocks</strong> — collective communication primitives and their low-level implementations</li>
  <li><strong>Higher-level distributed training strategies</strong> — data, tensor, and pipeline parallelism<br /></li>
</ol>

<p>Teaching approach:</p>

<ul>
  <li>Emphasize <strong>concrete code examples</strong> so concepts map directly to implementation</li>
  <li>Use <strong>benchmarks</strong> to reveal performance effects of different designs</li>
  <li>Combine theory (primitives) with practice (real code) so students see both how and why performance differs<br /></li>
</ul>

<hr />

<h1 id="collective-operations-provide-standardized-multi-device-primitives-such-as-broadcast-scatter-gather-reduce-all-gather-and-reduce-scatter-for-coordinating-distributed-computation">Collective operations provide standardized multi-device primitives such as broadcast, scatter, gather, reduce, all-gather, and reduce-scatter for coordinating distributed computation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-06-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-06-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-06-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-06-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Collective operations</strong> are foundational primitives in parallel programming that hide point-to-point bookkeeping and provide efficient coordinated data movement across devices.<br /></p>

<p>Core terms:</p>

<ul>
  <li><strong>World size</strong>: number of participating devices</li>
  <li><strong>Rank</strong>: device index within the group<br /></li>
</ul>

<p>Common collectives:</p>

<ul>
  <li><strong>Broadcast</strong>: copy a tensor from one rank to all ranks</li>
  <li><strong>Scatter</strong>: distribute distinct pieces of a tensor to different ranks</li>
  <li><strong>Gather</strong>: collect pieces from ranks onto a single destination</li>
  <li><strong>Reduce</strong>: apply an associative, commutative operation (sum, min, max) while gathering results</li>
  <li><strong>All-gather</strong>: concatenate/stack data from every rank onto every rank</li>
  <li><strong>Reduce-scatter</strong>: reduce across ranks and scatter disjoint portions of the reduced result</li>
  <li><strong>All-reduce</strong>: functionally equivalent to reduce followed by all-gather; commonly used for synchronizing gradients in training<br /></li>
</ul>

<p>Correct use requires consistent matching of calls across ranks and agreement on shapes and orderings.<br /></p>

<hr />

<h1 id="gpu-system-topologies-form-a-hierarchy-sm-l1-hbm-pcie-nvlink-nvswitch-and-influence-where-and-how-collective-communication-is-executed">GPU system topologies form a hierarchy (SM L1, HBM, PCIe, NVLink, NVSwitch) and influence where and how collective communication is executed.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-09-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-09-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-09-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-09-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>GPU nodes contain multiple memory and interconnect levels:</p>

<ul>
  <li>Tiny on-SM <strong>L1</strong> caches for immediate operands</li>
  <li>Larger high-bandwidth <strong>device memory</strong> (<strong>HBM</strong>) for layer state and activations</li>
  <li>Host-level links (<strong>PCIe</strong>) and cluster interconnects for cross-node transfers<br /></li>
</ul>

<p>Traditional GPU-to-GPU transfers used the host (PCIe + Ethernet), incurring kernel and copy overheads that throttled throughput.<br /></p>

<p>Modern designs use <strong>device-aware interconnects</strong> such as <strong>NVLink</strong> and <strong>NVSwitch</strong> to bypass the host, providing much higher aggregate bandwidth and lower latency and thereby changing optimal communication strategies.<br /></p>

<p>The relative costs of on-chip accesses vs. inter-node links determine whether <strong>replication</strong>, <strong>sharding</strong>, or <strong>recomputation</strong> is preferable in a design.<br /></p>

<hr />

<h1 id="topology-discovery-tools-reveal-inter-gpu-links-such-as-nvlink-and-show-how-network-cards-and-pcie-paths-connect-gpus-and-cpus-which-affects-communication-strategies">Topology-discovery tools reveal inter-GPU links such as NVLink and show how network cards and PCIe paths connect GPUs and CPUs, which affects communication strategies.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-12-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-12-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-12-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-12-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Topology visibility</strong> matters: system utilities report which GPU pairs are directly connected by high-bandwidth links and which require routing over PCIe or network interfaces.<br /></p>

<p>Practical implications:</p>

<ul>
  <li>In an eight-GPU node, many pairs are connected by <strong>NVLink lanes</strong> while other traffic still traverses <strong>PCIe</strong> or network adapters</li>
  <li>This <strong>heterogeneous connectivity</strong> influences collective implementations and performance optimizations</li>
  <li><strong>Network interface cards</strong> and the <strong>host CPU</strong> remain part of the topology for coordination and non-GPU backends<br /></li>
</ul>

<p>Correctly interpreting topology lets libraries and applications choose minimal-latency / maximum-bandwidth paths for collective transfers.<br /></p>

<hr />

<h1 id="nccl-nvidia-collective-communications-library-implements-low-level-gpu-aware-collectives-and-torchdistributed-provides-a-higher-level-multi-backend-interface-for-python-workloads">NCCL (NVIDIA Collective Communications Library) implements low-level GPU-aware collectives and torch.distributed provides a higher-level, multi-backend interface for Python workloads.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-14-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-14-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-14-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-14-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>NCCL</strong> translates high-level collective semantics into optimized, hardware-aware packet transfers and CUDA kernels that move and reduce data across GPUs.<br /></p>

<p>Key behaviors:</p>

<ul>
  <li>Performs <strong>topology discovery</strong> and <strong>path optimization</strong> during initialization</li>
  <li>Emits device-side kernels and transfers that exploit link topology and in-network features where available<br /></li>
</ul>

<p>Higher-level frameworks expose these primitives:</p>

<ul>
  <li><strong>torch.distributed</strong> wraps backend libraries and exposes ops such as <strong>all-reduce</strong>, <strong>all-gather</strong>, <strong>reduce-scatter</strong>, and point-to-point sends/receives to Python programs</li>
  <li>Supports multiple backends (e.g., <strong>NCCL</strong> for GPUs, <strong>Gloo</strong> for CPUs), enabling portability while retaining backend-specific performance characteristics<br /></li>
</ul>

<p>These libraries also offer <strong>asynchronous operations</strong> and expert features for overlapping communication and computation, but exact performance depends on hardware, tensor sizes, and library internals.<br /></p>

<hr />

<h1 id="distributed-processes-must-initialize-a-process-group-and-use-synchronization-primitives-like-barrier-to-coordinate-multi-process-execution">Distributed processes must initialize a process group and use synchronization primitives like barrier to coordinate multi-process execution.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-17-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-17-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-17-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-17-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A typical distributed workload spawns multiple processes (commonly one per device) that run the same function with different rank indices.<br /></p>

<p>Initialization process:</p>

<ul>
  <li>Processes <strong>rendezvous</strong> via a host-based coordinator to establish a <strong>process group</strong></li>
  <li>Initialization exchanges topology and membership info — distinct from the heavy tensor traffic handled by the collective library</li>
  <li>A <strong>barrier</strong> primitive provides an explicit synchronization point, useful for ordered side effects (logging), deterministic benchmarking, and coordination before launching collectives<br /></li>
</ul>

<p>Proper initialization and synchronization prevent mismatched collectives that would otherwise hang or deadlock.<br /></p>

<hr />

<h1 id="all-reduce-aggregates-values-across-ranks-using-an-associative-operation-and-places-the-same-reduced-result-on-every-participating-rank-typically-in-place-on-the-input-tensor">All-reduce aggregates values across ranks using an associative operation and places the same reduced result on every participating rank, typically in-place on the input tensor.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-19-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-19-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-19-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-19-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>All-reduce</strong> applies a commutative, associative operation (typically sum or average) across corresponding tensor elements from every rank and writes the aggregated result back to the provided output (often in-place).<br /></p>

<p>Practical notes:</p>

<ul>
  <li>Implementations support <strong>synchronous</strong> and <strong>asynchronous</strong> variants; async all-reduce returns a handle to overlap communication with computation</li>
  <li>Canonical use: <strong>gradient synchronization</strong> in synchronous data-parallel training — all ranks obtain identical averaged gradients after backward pass</li>
  <li>Correct use requires matching collective calls and careful attention to tensor <strong>shapes</strong> and <strong>dtypes</strong> across ranks<br /></li>
</ul>

<hr />

<h1 id="reduce-scatter-computes-a-reduction-across-ranks-and-distributes-disjoint-portions-of-the-reduced-tensor-to-different-ranks-producing-a-smaller-local-output-per-rank">Reduce-scatter computes a reduction across ranks and distributes disjoint portions of the reduced tensor to different ranks, producing a smaller local output per rank.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-20-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-20-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-20-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-20-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Reduce-scatter</strong> combines reduction and scattering:</p>

<ul>
  <li>Takes an input whose one dimension corresponds to <strong>world size</strong> and performs a per-segment reduction across ranks</li>
  <li>Delivers each reduced segment to the corresponding destination rank; the output is the per-rank portion (often smaller than the full tensor)<br /></li>
</ul>

<p>When to use:</p>

<ul>
  <li>Useful when algorithms expect <strong>partitioned reduced outputs</strong> rather than full replication</li>
  <li>Can reduce total communication compared to separate reduce and scatter steps</li>
</ul>

<p>Implementation caveats:</p>

<ul>
  <li>Ensure <strong>dimension alignment</strong> and consistent element ordering so segments map to ranks deterministically<br /></li>
</ul>

<hr />

<h1 id="all-gather-concatenates-tensors-from-all-ranks-so-that-each-rank-ends-up-with-the-full-collection-and-when-paired-with-reduce-scatter-it-composes-to-an-all-reduce">All-gather concatenates tensors from all ranks so that each rank ends up with the full collection, and when paired with reduce-scatter it composes to an all-reduce.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-21-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-21-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-21-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-21-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>All-gather</strong> assembles a full tensor from per-rank inputs by concatenating or stacking so every rank receives the complete assembled tensor.<br /></p>

<p>Usage patterns:</p>

<ul>
  <li>Reconstruct full activation or parameter vectors when preceding stages computed only a <strong>shard</strong></li>
  <li>Combined with <strong>reduce-scatter</strong> (reduce-scatter followed by all-gather) the composition is functionally equivalent to an <strong>all-reduce</strong> because the reduced segments are redistributed so every rank obtains the complete reduced result<br /></li>
</ul>

<p>All-gather is therefore a key building block for implementing higher-level collective behaviors from lower-level primitives.<br /></p>

<hr />

<h1 id="collective-operations-require-consistent-tensor-shapes-across-ranks-and-a-convention-for-which-tensor-dimension-maps-to-devices-in-sharded-operations">Collective operations require consistent tensor shapes across ranks and a convention for which tensor dimension maps to devices in sharded operations.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-23-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-23-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-23-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-23-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Many collectives infer a mapping from a tensor dimension to <strong>world size</strong> (for example, reduce-scatter often assumes one dimension equals the number of devices and partitions along that axis).<br /></p>

<p>Practical guidance:</p>

<ul>
  <li>All ranks must supply tensors with <strong>compatible shapes</strong> and consistent partitioning rules</li>
  <li>Mismatched dimensionality or stride assumptions lead to runtime errors or incorrect placements</li>
  <li>Use <strong>small synthetic examples</strong> to test shape conventions before integrating collectives into larger code paths<br /></li>
</ul>

<p>Careful shape management simplifies debugging and ensures deterministic mapping from tensor segments to device ranks.<br /></p>

<hr />

<h1 id="reliable-benchmarking-of-collectives-requires-warm-up-synchronization-and-precise-byte-accounting-to-compute-effective-bandwidth">Reliable benchmarking of collectives requires warm-up, synchronization, and precise byte accounting to compute effective bandwidth.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-26-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-26-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-26-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-26-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Benchmark methodology essentials:</p>

<ul>
  <li><strong>Warm up</strong> kernels and CUDA context to avoid first-call overheads</li>
  <li><strong>Synchronize</strong> all ranks before timing to ensure consistent start and end points<br /></li>
</ul>

<p>Byte accounting:</p>

<ul>
  <li>Depends on the operation: for <strong>all-reduce</strong> each rank contributes tensor bytes and typically sends and receives data such that a factor-of-two term appears in total traffic accounting (send plus receive)</li>
  <li><strong>Reduce-scatter</strong> omits the broadcast-back component in its accounting<br /></li>
</ul>

<p>Compute aggregate bandwidth as total transferred bytes divided by measured wall-clock duration. Results depend on tensor size, world size, backend optimizations, and topology — interpret benchmark results as <strong>relative comparisons</strong> across configurations rather than absolute hardware maxima.<br /></p>

<hr />

<h1 id="reduce-scatter-benchmarks-follow-the-same-methodological-steps-as-all-reduce-but-typically-report-different-effective-bandwidth-due-to-different-traffic-patterns-and-less-broadcast-traffic">Reduce-scatter benchmarks follow the same methodological steps as all-reduce but typically report different effective bandwidth due to different traffic patterns and less broadcast traffic.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-30-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-30-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-30-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-30-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Reduce-scatter benchmarking notes:</p>

<ul>
  <li>Construct an input whose relevant dimension <strong>scales with world size</strong>, warm up kernels, synchronize, and measure elapsed time for the operation</li>
  <li>Because reduce-scatter reduces and scatters disjoint portions without a global broadcast back, its traffic accounting <strong>omits the round-trip factor</strong> present in all-reduce calculations — often yielding numerically smaller bandwidth estimates for the same element counts<br /></li>
</ul>

<p>Variations arise from backend optimizations (e.g., in-network aggregation) and hardware features, so interpretation requires careful comparison to theoretical link rates and other measurements.<br /></p>

<p>End-to-end benchmarking is necessary to reveal real-world throughput characteristics for different collectives.<br /></p>

<hr />

<h1 id="distributed-training-strategies-data-tensor-pipeline-can-be-understood-by-applying-them-to-a-deep-mlp-as-a-representative-compute-bound-workload">Distributed training strategies (data, tensor, pipeline) can be understood by applying them to a deep MLP as a representative compute-bound workload.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-32-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-32-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-32-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-32-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>MLPs</strong> are a simple yet representative workload:</p>

<ul>
  <li>Each layer is dominated by <strong>matrix multiplies</strong> and nonlinearities, making MLPs useful pedagogical examples for distributed strategies</li>
  <li>Parallelization strategies:
    <ul>
      <li><strong>Data parallelism</strong>: split the batch dimension</li>
      <li><strong>Tensor parallelism</strong>: shard model dimensions (e.g., hidden width)</li>
      <li><strong>Pipeline parallelism</strong>: partition layers across devices<br /></li>
    </ul>
  </li>
</ul>

<p>Examining these approaches on MLPs reveals core patterns that generalize to transformers while keeping implementation and reasoning tractable.<br /></p>

<hr />

<h1 id="data-parallel-ddp-training-replicates-the-model-on-each-device-partitions-the-batch-across-ranks-and-synchronizes-gradients-via-all-reduce-after-the-backward-pass">Data-parallel (DDP) training replicates the model on each device, partitions the batch across ranks, and synchronizes gradients via all-reduce after the backward pass.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-37-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-37-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-37-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-37-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Data-parallel training</strong> flow:</p>

<ol>
  <li>Each rank receives a disjoint slice of the global minibatch and runs forward/backward on a full replica of the model</li>
  <li>After backward, an <strong>all-reduce</strong> averages gradients across ranks</li>
  <li>Each rank updates parameters with the same aggregated gradient, resulting in synchronized parameters across devices<br /></li>
</ol>

<p>Trade-offs:</p>

<ul>
  <li>Scales straightforwardly with replicas</li>
  <li>Requires communicating gradients of the <strong>full model</strong> each step and synchronizing at the collective barrier point<br /></li>
</ul>

<hr />

<h1 id="tensor-parallelism-shards-parameter-matrices-for-example-across-the-hidden-dimension-computes-local-partial-activations-and-uses-all-gather-to-assemble-full-activations-for-subsequent-layers">Tensor parallelism shards parameter matrices (for example across the hidden dimension), computes local partial activations, and uses all-gather to assemble full activations for subsequent layers.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-43-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-43-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-43-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-43-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Tensor parallelism</strong>:</p>

<ul>
  <li>Divides model tensors along one or more model dimensions so each rank stores and computes on only a <strong>slice</strong> of each layer’s parameters</li>
  <li>Forward pass: each rank computes <strong>partial activations</strong> for its shard; an <strong>all-gather</strong> assembles the full activation required by the next stage</li>
  <li>Backward pass: symmetric communication propagates gradients to the appropriate parameter shards and typically mirrors the forward communication pattern<br /></li>
</ul>

<p>Trade-offs: reduces memory footprint per device but increases interconnect traffic for activations and intermediates, so it benefits from <strong>high-bandwidth, low-latency links</strong>.<br /></p>

<hr />

<h1 id="pipeline-parallelism-partitions-layers-across-devices-and-streams-microbatches-through-the-partitioned-model-using-point-to-point-sends-and-receives-to-improve-utilization">Pipeline parallelism partitions layers across devices and streams microbatches through the partitioned model using point-to-point sends and receives to improve utilization.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-47-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-47-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-47-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-47-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Pipeline parallelism</strong> assigns contiguous layer groups to different ranks so each rank is responsible for a stage of the network.<br /></p>

<p>Streaming with microbatches:</p>

<ol>
  <li>Subdivide the global batch into <strong>microbatches</strong></li>
  <li>Stream microbatches through stages; each stage receives activations from the previous stage, computes local layers, and forwards activations to the next stage via point-to-point sends/receives</li>
  <li>Interleave forward and backward microbatches to keep stages busy and reduce pipeline bubbles<br /></li>
</ol>

<p>Implementation notes:</p>

<ul>
  <li>Uses <strong>send/receive</strong> primitives rather than collectives</li>
  <li>Naive implementations suffer from idle pipeline bubbles and synchronous blocking</li>
  <li>Advanced implementations use asynchronous sends/receives, microbatching, and careful scheduling to overlap compute and communication and minimize stalls<br /></li>
</ul>

<hr />

<h1 id="overlapping-communication-and-computation-via-asynchronous-sendsreceives-and-handles-reduces-stalls-and-improves-gpu-utilization-in-pipeline-and-other-parallel-schemes">Overlapping communication and computation via asynchronous sends/receives and handles reduces stalls and improves GPU utilization in pipeline and other parallel schemes.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-52-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-52-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-52-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-52-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modern collective and point-to-point APIs provide <strong>asynchronous</strong> variants that return completion handles so communication can be launched while computation continues on other streams.<br /></p>

<p>Common pattern:</p>

<ul>
  <li>Launch <strong>nonblocking sends/receives</strong> (device-side kernels move data independently of the CPU thread)</li>
  <li>Continue useful computation (process other microbatches, local work)</li>
  <li><strong>Synchronize on handles</strong> before accessing transferred data to ensure correctness<br /></li>
</ul>

<p>Caveats:</p>

<ul>
  <li>Ordering of multiple sends to the same destination is preserved by stream semantics, but matching receives and careful coordination are required to avoid mismatches.<br /></li>
</ul>

<hr />

<h1 id="full-pipeline-training-requires-coordinating-forwards-and-backwards-across-ranks-handling-last-stage-loss-computation-and-implementing-backward-stage-sends-to-propagate-gradients-upstream">Full pipeline training requires coordinating forwards and backwards across ranks, handling last-stage loss computation, and implementing backward-stage sends to propagate gradients upstream.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-56-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-56-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-56-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-56-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>In a complete pipeline implementation the final stage computes the loss for a microbatch and initiates backward propagation by computing gradients and sending contributions back to the previous stage.<br /></p>

<p>Backward scheduling:</p>

<ul>
  <li>Must be interleaved with forward microbatches to avoid deadlocks and minimize stalls — more complex than forward-only streaming logic</li>
  <li>Requires bookkeeping for which microbatch corresponds to which backward activation</li>
  <li>Often uses asynchronous operations to overlap communications and computation<br /></li>
</ul>

<p>Production systems typically encapsulate this complexity in libraries that manage ordering, handles, and partial failures instead of leaving manual control to application code.<br /></p>

<hr />

<h1 id="high-level-frameworks-such-as-jax-expose-sharding-primitives-so-users-can-specify-how-tensor-dimensions-are-partitioned-and-let-the-compiler-generate-the-underlying-communication-and-kernel-launches">High-level frameworks such as JAX expose sharding primitives so users can specify how tensor dimensions are partitioned and let the compiler generate the underlying communication and kernel launches.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/00-59-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/00-59-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/00-59-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/00-59-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>JAX</strong> and similar systems let users declare data and model sharding by mapping tensor axes to device axes; the compiler synthesizes the required collectives and kernel code to realize the sharding across available devices.<br /></p>

<p>Tooling:</p>

<ul>
  <li>Higher-level toolkits (e.g., the referenced <strong>Lavanter</strong> example) allow concise specifications of <strong>FSDP</strong> or tensor-parallel strategies in only a few lines of code</li>
  <li>Benefit: conceptual simplicity and portability — users describe partitioning and the system optimizes mapping and communication</li>
  <li>Trade-off: less direct low-level control compared to hand-crafted <strong>PyTorch + NCCL</strong> implementations, though compiler-generated code is often sufficiently performant and much easier to maintain<br /></li>
</ul>

<hr />

<h1 id="parallelization-choices-trade-off-computation-communication-and-memory-recomputation-checkpointing-and-sharding-are-recurring-techniques-and-hardware-evolution-maintains-a-multi-level-hierarchy-of-limits">Parallelization choices trade off computation, communication, and memory; recomputation, checkpointing, and sharding are recurring techniques, and hardware evolution maintains a multi-level hierarchy of limits.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/01-02-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/01-02-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/01-02-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/01-02-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Parallelization can be viewed as cutting either the data or various model dimensions:</p>

<ul>
  <li>Batch (data)</li>
  <li>Width/hidden (tensor parallelism)</li>
  <li>Depth/layers (pipeline parallelism)</li>
  <li>Context/sequence length (e.g., for sequences)<br /></li>
</ul>

<p>Memory vs. compute trade-offs:</p>

<ul>
  <li><strong>Activation checkpointing</strong> (recompute on demand) trades extra compute for reduced memory and fewer transfers</li>
  <li>Alternatively, storing activations or moving them to remote device memory trades memory pressure for communication overhead<br /></li>
</ul>

<p>Hardware trends:</p>

<ul>
  <li>Increases in on-chip memory and link bandwidth shift but do not eliminate trade-offs, since model size tends to grow in parallel with hardware capabilities</li>
  <li>Understanding the trade space and available library primitives is necessary to design scalable training systems rather than relying solely on future hardware advances<br /></li>
</ul>

<hr />

<h1 id="practical-implementation-questions-include-handling-data-dependent-state-batchnorm-using-high-level-libraries-fsdp-activation-checkpointing-apis-specialized-hardware-trade-offs-incremental-training-and-how-collectives-are-executed-at-runtime">Practical implementation questions include handling data-dependent state (batchnorm), using high-level libraries (FSDP), activation checkpointing APIs, specialized hardware trade-offs, incremental training, and how collectives are executed at runtime.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec08/01-09-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec08/01-09-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec08/01-09-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec08/01-09-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Batch-dependent statistics (e.g., <strong>BatchNorm</strong>) introduce global state that complicates naive data-parallel schemes, whereas normalization methods that do not depend on batch-wide statistics (e.g., <strong>LayerNorm</strong>) avoid that problem.<br /></p>

<p>Distributed solutions and tooling:</p>

<ul>
  <li>Synchronize batch statistics across ranks or use normalization variants suited to sharded training</li>
  <li>PyTorch provides higher-level solutions such as <strong>FSDP</strong> for parameter sharding and memory-efficient training</li>
  <li>Both PyTorch and JAX expose <strong>activation checkpointing</strong> APIs to selectively recompute activations and trade compute for memory<br /></li>
</ul>

<p>Other considerations:</p>

<ul>
  <li>Specialized inference/training hardware (Cerebras, GROQ) emphasizes large on-chip memory and different trade-offs, reducing communication but possibly limiting flexibility</li>
  <li>Incremental/continued training is conceptually straightforward (gradient steps and checkpoints) but requires practical handling of <strong>optimizer state placement</strong> and consistency<br /></li>
</ul>

<p>Runtime orchestration:</p>

<ul>
  <li>The CPU typically invokes libraries (NCCL, Gloo) to orchestrate collectives; those libraries launch GPU kernels and manage device-side transfers so data movement and reductions execute efficiently on the devices themselves.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 7 - Parallelism</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec07/" rel="alternate" type="text/html" title="CS336 Lecture 7 - Parallelism" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec07</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec07/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/l1RJcDjzK8M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-and-objectives">Lecture overview and objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-00-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-00-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-00-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-00-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture introduces <strong>multi-machine optimization</strong> for training very large machine learning models, focusing on <strong>parallelism across machines</strong> to address compute and memory constraints.<br /></p>

<p>It reframes the problem as moving beyond single-GPU throughput optimization to using <strong>multiple servers and accelerators</strong>, and stresses that <strong>communication patterns</strong> and <strong>hardware topology</strong> critically determine which parallelization techniques are effective.<br /></p>

<p>Planned coverage:</p>
<ul>
  <li><strong>Networking basics</strong> (interconnect hierarchies and their costs)</li>
  <li><strong>Parallelization paradigms</strong> (data, model, activation/sequence axes)</li>
  <li><strong>Case studies</strong> and practical heuristics for combining techniques to fit models that do not fit on a single GPU while maximizing throughput and respecting memory limits<br /></li>
</ul>

<p>Objective: combine different parallelization strategies to train models that exceed single-GPU capacity while maximizing throughput and honoring memory limits.<br /></p>

<hr />

<h1 id="motivation-for-multi-machine-scaling-from-compute-requirements">Motivation for multi-machine scaling from compute requirements</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-02-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-02-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-02-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-02-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Large-scale language models require aggregate compute that outpaces single-GPU FLOPS growth, so <strong>distributed multi-machine systems</strong> are necessary to reach exascale-class training throughput.<br /></p>

<p>Key points:</p>
<ul>
  <li>High-end supercomputers demonstrate that state-of-the-art training requires distributing compute across many nodes rather than waiting for single-device advances.</li>
  <li>Investing in <strong>multi-node parallelism</strong> unlocks orders-of-magnitude more FLOPS than any single accelerator.<br /></li>
</ul>

<p>This compute motivation justifies the forthcoming discussion of distributed algorithms and system-level tradeoffs that enable large-model training.<br /></p>

<hr />

<h1 id="memory-scaling-motivates-model-sharding-across-devices">Memory scaling motivates model sharding across devices</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-03-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-03-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-03-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-03-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Model parameter counts are growing faster than individual accelerator memory, so <strong>billions-parameter</strong> models routinely exceed a single GPU’s capacity and must be distributed across devices.<br /></p>

<p>Implications and strategies:</p>
<ul>
  <li>Accelerator memory increases slowly relative to model size → a hard constraint requiring <strong>sharding techniques</strong> for parameters, optimizer state, and activations.</li>
  <li>Addressing <strong>memory scaling</strong> is as important as compute scaling: models need both capacity for parameters and working memory for optimizer state and activations.</li>
  <li>Common strategies motivated by memory pressure: <strong>optimizer-state sharding</strong>, <strong>parameter sharding</strong>, activation management, and other memory-saving techniques.<br /></li>
</ul>

<hr />

<h1 id="hardware-hierarchy-and-interconnect-tiers-influence-parallelization-choices">Hardware hierarchy and interconnect tiers influence parallelization choices</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-04-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-04-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-04-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-04-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Hardware topology creates a strong communication-performance gradient that drives placement decisions.<br /></p>

<p>Highlights:</p>
<ul>
  <li><strong>Node-local interconnects</strong> (e.g., <strong>NVLink</strong>, <strong>NVSwitch</strong>) provide very high bandwidth and low latency between GPUs within a machine.</li>
  <li><strong>Inter-node links</strong> (e.g., <strong>HDR InfiniBand</strong>) are substantially slower; cross-rack or large-scale switches can be slower still.</li>
  <li>Result: <strong>intra-node collectives</strong> are much cheaper than <strong>inter-node collectives</strong>, and performance can shift again beyond ~256 GPUs due to differing switch fabrics.<br /></li>
</ul>

<p>Design rule: place bandwidth-hungry synchronizations inside the fast connectivity domain and minimize or restructure communication across slower links. This explains rules of thumb like applying <strong>tensor-parallel</strong> techniques within a node and using <strong>data</strong> or <strong>pipeline parallelism</strong> across nodes.<br /></p>

<hr />

<h1 id="collective-communication-primitives-and-their-equivalences">Collective communication primitives and their equivalences</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-06-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-06-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-06-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-06-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Distributed training communication cost can be modeled by counting collective primitives, because most parallel algorithms compose these operations.<br /></p>

<p>Common collectives (with short descriptions):</p>
<ul>
  <li><strong>All-reduce</strong>: compute a global reduction (e.g., sum) and distribute the result to all ranks.</li>
  <li><strong>Broadcast</strong>: copy a single rank’s tensor to all ranks.</li>
  <li><strong>Reduce</strong>: gather and reduce inputs to a single rank.</li>
  <li><strong>All-gather</strong>: concatenate shards from all ranks into full tensors on every rank.</li>
  <li><strong>Reduce-scatter</strong>: perform a reduction and distribute different output shards to different ranks.<br /></li>
</ul>

<p>Practical note: in bandwidth-limited regimes, <strong>all-reduce</strong> ≈ <strong>reduce-scatter</strong> followed by <strong>all-gather</strong>, so counting these primitives gives a good first-order model for communication overhead in parallel training.<br /></p>

<hr />

<h1 id="differences-between-gpu-and-tpu-networking-and-implications-for-collectives">Differences between GPU and TPU networking and implications for collectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-09-32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-09-32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-09-32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-09-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Different accelerator/network architectures favor different communication patterns and therefore different parallel algorithms.<br /></p>

<p>Comparison:</p>
<ul>
  <li><strong>GPU-based clusters</strong>: node-centric high-speed links plus slower inter-node fabrics → arbitrary communication patterns up to a scale.</li>
  <li><strong>TPU-style toroidal mesh</strong>: neighbor-to-neighbor high-bandwidth links favor locality and neighbor-based collective algorithms.<br /></li>
</ul>

<p>Implications:</p>
<ul>
  <li><strong>Collective-heavy workloads</strong> can benefit from torus-like topologies at scale.</li>
  <li><strong>Heterogeneous or irregular communication</strong> patterns often favor GPU cluster fabrics with broader connectivity.<br /></li>
</ul>

<p>Choice of accelerator and network topology influences optimal system design and parallelism choices.<br /></p>

<hr />

<h1 id="data-center-as-the-new-unit-of-compute-and-collective-centric-performance-reasoning">Data center as the new unit of compute and collective-centric performance reasoning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-11-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-11-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-11-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-11-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>At datacenter scale, the unit of computation becomes the <strong>entire cluster</strong>, and scalable algorithms aim for linear scaling of both usable model size (memory) and aggregate compute with the number of devices.<br /></p>

<p>Design goals and modeling approach:</p>
<ul>
  <li>Achieve <strong>linear memory scaling</strong> (train larger models) and <strong>linear compute scaling</strong> (increase effective throughput).</li>
  <li>Use collective-primitive counting as the primary performance model because many algorithms are built from those building blocks.<br /></li>
</ul>

<p>Reasoning about the number and type of collectives is sufficient for estimating bandwidth-limited performance at scale.<br /></p>

<hr />

<h1 id="three-high-level-parallelism-axes-data-model-and-activation">Three high-level parallelism axes: data, model, and activation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-13-21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-13-21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-13-21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-13-21.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Scaling strategies decompose into three fundamental axes: <strong>data parallelism</strong>, <strong>model parallelism</strong>, and <strong>activation (sequence) parallelism</strong>.<br /></p>

<p>Definitions:</p>
<ul>
  <li><strong>Data parallelism</strong>: replicate parameters and shard minibatches across replicas; synchronize gradients.</li>
  <li><strong>Model parallelism</strong>: shard parameters across devices (e.g., pipeline or tensor parallelism); transfer activations between devices.</li>
  <li><strong>Activation (sequence) parallelism</strong>: shard activations across devices or time (sequence positions) to reduce activation memory.<br /></li>
</ul>

<p>Combining these axes provides the tools to jointly scale compute and memory while balancing communication, computation, and batch-size constraints.<br /></p>

<hr />

<h1 id="naive-data-parallelism-implements-synchronous-sgd-with-full-parameter-replication">Naive data parallelism implements synchronous SGD with full-parameter replication</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-14-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-14-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-14-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-14-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Naive data parallelism workflow (synchronous):<br /></p>

<ol>
  <li>Split each minibatch into M micro-batches and send one micro-batch to each device.</li>
  <li>Place identical model replicas on each device and compute per-device gradients.</li>
  <li>Synchronize gradients via an <strong>all-reduce</strong> across replicas.</li>
  <li>Perform the parameter update on each replica.<br /></li>
</ol>

<p>Properties:</p>
<ul>
  <li>Near-linear compute scaling when each device receives a sufficiently large micro-batch to saturate compute.</li>
  <li>Poor memory scaling because parameters and optimizer state are replicated on every device.</li>
  <li>Communication overhead ≈ twice the model parameter size per update in bandwidth-limited all-reduce regimes.</li>
  <li>Assumes sufficiently large global batch sizes to amortize synchronization costs.<br /></li>
</ul>

<hr />

<h1 id="data-parallel-tradeoffs-compute-saturation-versus-communication-and-memory-replication">Data-parallel tradeoffs: compute saturation versus communication and memory replication</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-15-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-15-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-15-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-15-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical limits of naive data parallelism:<br /></p>

<ul>
  <li>Good compute scaling only when micro-batches per device are large enough to utilize accelerators.</li>
  <li>Communication cost grows with model size and occurs every synchronous step.</li>
  <li>Memory scaling is unfavorable: every device stores full parameters and optimizer state (often multiple copies), which is problematic for optimizers like <strong>Adam</strong>.</li>
  <li>Conclusion: naive data parallelism is simple and effective up to a point but insufficient when model+optimizer-state exceed single-device capacity or when communication dominates.<br /></li>
</ul>

<hr />

<h1 id="optimizer-state-explosion-and-its-impact-on-memory">Optimizer-state explosion and its impact on memory</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-17-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-17-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-17-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-17-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modern optimizers increase per-parameter memory requirements significantly.<br /></p>

<p>Details:</p>
<ul>
  <li>Optimizers like <strong>Adam</strong> require storing first and second moments and often <strong>master weights</strong>, which can multiply per-parameter memory by factors approaching <strong>eight</strong> compared to one parameter copy.</li>
  <li>In mixed-precision training, master weights + gradients + moments can produce effective overheads on the order of <strong>~16 bytes per parameter</strong> in some implementations.</li>
  <li>Because of this <strong>optimizer-state dominance</strong>, replicating parameters across devices is frequently infeasible for very large models; addressing optimizer-state memory is a prerequisite to multi-billion-parameter scaling.<br /></li>
</ul>

<hr />

<h1 id="zero-stage-1-optimizer-state-sharding-reduces-per-device-optimizer-memory-by-sharding-state">ZeRO stage 1 (optimizer state sharding) reduces per-device optimizer memory by sharding state</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-20-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-20-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-20-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-20-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>ZeRO Stage 1</strong> shards optimizer-state tensors (e.g., Adam moments) across devices while still replicating parameters and gradients, reducing per-device memory without changing gradient semantics.<br /></p>

<p>Core mechanism (high-level steps):</p>
<ol>
  <li>Gradients are computed on all devices.</li>
  <li>Use <strong>reduce-scatter</strong> to aggregate the summed gradients for each parameter shard onto its owning device.</li>
  <li>The owning device updates its local shard with its local optimizer state.</li>
  <li>Use <strong>all-gather</strong> to distribute updated parameter shards back to replicas.<br /></li>
</ol>

<p>Notes: in bandwidth-limited regimes this sequence (reduce-scatter → local update → all-gather) matches the communication cost of a traditional all-reduce but lowers memory by removing replicated optimizer state.<br /></p>

<hr />

<h1 id="zero-stage-2-shards-gradients-incrementally-during-backward-pass-to-limit-peak-memory">ZeRO stage 2 shards gradients incrementally during backward pass to limit peak memory</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-26-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-26-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-26-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-26-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>ZeRO Stage 2</strong> extends stage 1 by also <strong>sharding gradients</strong> so no device materializes the full gradient vector, bounding peak memory usage.<br /></p>

<p>Key behavior:</p>
<ul>
  <li>During the backward pass, layers are processed sequentially and gradient contributions for a layer are immediately sent (via reductions) to the device that owns the corresponding parameter shard.</li>
  <li>Once a layer’s gradients are communicated and incorporated, local gradient buffers are freed to keep memory low.</li>
  <li>This <strong>streaming</strong> approach requires more frequent, fine-grained synchronization (layer-by-layer reduces and frees) but keeps the same total communication volume while lowering peak memory compared to stage 1.<br /></li>
</ul>

<hr />

<h1 id="zero-stage-3--fsdp-fully-shards-parameters-gradients-and-optimizer-state-with-on-demand-parameter-communication">ZeRO stage 3 / FSDP fully shards parameters, gradients, and optimizer state with on-demand parameter communication</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-33-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-33-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-33-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-33-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>ZeRO Stage 3</strong> (fully-sharded data parallel; e.g., <strong>FSDP</strong>) shards parameters, gradients, and optimizer state so each device holds only its parameter shard.<br /></p>

<p>Runtime behavior and optimizations:</p>
<ul>
  <li>During forward/backward, parameters are requested and communicated <strong>on demand</strong> for local computation.</li>
  <li>The runtime <strong>overlaps communication and computation</strong> by prefetching parameter shards before they are needed.</li>
  <li>Operations occur at layer granularity using <strong>all-gather</strong> and <strong>reduce-scatter</strong>, and shards/gradients are freed immediately after use to minimize memory.<br /></li>
</ul>

<p>Trade-offs:</p>
<ul>
  <li>Stage 3 increases the number of collectives (roughly <strong>3×</strong> parameter size total bandwidth work vs naive all-reduce’s ~<strong>2×</strong>), but careful overlap and pipelining keep runtime overhead low in practice while delivering maximal per-device memory savings.<br /></li>
</ul>

<hr />

<h1 id="practical-performance-of-zero-stages-and-memory-efficiency-examples">Practical performance of ZeRO stages and memory efficiency examples</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-40-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-40-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-40-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-40-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Summary of <strong>ZeRO</strong> trade-offs:<br /></p>

<ul>
  <li><strong>Stage 1</strong>: memory wins with no extra bandwidth cost compared to standard all-reduce.</li>
  <li><strong>Stage 2</strong>: further reduces peak memory via layer-granularity synchronization at the cost of more frequent collects.</li>
  <li><strong>Stage 3 (FSDP)</strong>: largest per-device memory reduction but higher total collective count; practical overlap/pipelining often keeps runtime overhead modest.<br /></li>
</ul>

<p>Practical consequence: full-sharding can enable orders-of-magnitude larger model fits on a fixed node count (e.g., enabling tens of billions of parameters where naive replication would not fit), making ZeRO variants the standard approach when memory is the bottleneck and the extra implementation/communication complexity is acceptable.<br /></p>

<hr />

<h1 id="batch-size-is-a-limited-resource-that-constrains-data-parallel-scalability">Batch size is a limited resource that constrains data parallel scalability</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-43-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-43-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-43-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-43-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Global batch size constrains how many devices can be used under data parallelism because each device must process at least one micro-batch.<br /></p>

<p>Important points:</p>
<ul>
  <li>Data parallelism cannot exceed global batch size without <strong>gradient accumulation</strong>.</li>
  <li>There are <strong>diminishing returns</strong>: beyond a critical batch size, the marginal benefit of increasing batch size (variance reduction per optimization step) drops sharply.</li>
  <li>Treat batch size as a constrained resource to allocate across parallelism axes.</li>
  <li><strong>Gradient accumulation</strong> trades additional temporal computation for an effectively larger batch size when memory prevents larger per-step batches.<br /></li>
</ul>

<hr />

<h1 id="model-parallelism-partitions-model-state-across-devices-to-reduce-memory-and-activation-cost">Model parallelism partitions model state across devices to reduce memory and activation cost</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-45-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-45-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-45-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-45-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Model parallelism</strong> places different parameters on different devices (rather than replicating them), transferring activations between devices instead of parameters.<br /></p>

<p>Main categories:</p>
<ul>
  <li><strong>Pipeline parallelism</strong>: cut the network along depth, assign contiguous layer groups to devices, pass activations forward/backward.</li>
  <li><strong>Tensor parallelism</strong>: split large matrix multiplies across devices and perform partial sums via collectives.<br /></li>
</ul>

<p>Use cases: model or activation sizes exceed single-device capacity; often combined with data parallelism to achieve both memory and throughput scaling.<br /></p>

<hr />

<h1 id="pipeline-parallelism-partitions-layers-across-devices-and-exposes-pipeline-bubbles">Pipeline parallelism partitions layers across devices and exposes pipeline bubbles</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-47-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-47-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-47-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-47-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Pipeline parallelism</strong> assigns contiguous layer groups to different devices and streams micro-batches through the device pipeline so stages can work concurrently.<br /></p>

<p>Practical considerations:</p>
<ul>
  <li>Naive scheduling produces <strong>pipeline bubbles</strong> (idle periods) because stages wait for upstream activations, causing poor utilization when micro-batch counts are small relative to pipeline stages.</li>
  <li>Mitigations: <strong>micro-batching</strong> (split minibatch into micro-batches) and scheduling strategies like <strong>1F1B</strong> to overlap forward/backward passes.</li>
  <li>Pipeline efficiency remains sensitive to micro-batch size and scheduling complexity.<br /></li>
</ul>

<hr />

<h1 id="zero-bubble-dual-pipelining-reduces-idle-time-by-rescheduling-weight-gradient-computations">Zero-bubble (dual) pipelining reduces idle time by rescheduling weight-gradient computations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-52-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-52-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-52-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-52-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Advanced pipeline optimizations hide idle time by scheduling independent work into pipeline bubbles.<br /></p>

<p>Techniques and trade-offs:</p>
<ul>
  <li>Split backward work into <strong>activation-backpropagation</strong> and <strong>weight-gradient computation</strong>; weight-gradient updates are independent and can fill original pipeline bubbles.</li>
  <li>Methods like <strong>dual-pipelining</strong> put weight-gradient calculations into otherwise idle slots to improve utilization.</li>
  <li>These techniques increase implementation complexity: manipulating autodiff order, maintaining fine-grained task queues, and ensuring correctness under dynamic scheduling—powerful but operationally costly.<br /></li>
</ul>

<hr />

<h1 id="tensor-parallelism-splits-large-matrix-multiplies-into-submatrices-and-uses-collective-sums">Tensor parallelism splits large matrix multiplies into submatrices and uses collective sums</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/00-57-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/00-57-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/00-57-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/00-57-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Tensor parallelism</strong> partitions the width (or height) of large linear transforms across devices: each device holds submatrices, computes local partial results, and uses collectives (typically <strong>all-reduce</strong>) to sum partial activations or gradients.<br /></p>

<p>Characteristics:</p>
<ul>
  <li>Parallelizes dominant linear algebra kernels and avoids pipeline bubbles because each layer runs in parallel across devices.</li>
  <li>Requires very high interconnect <strong>bandwidth</strong> and low <strong>latency</strong>.</li>
  <li>Most efficient <strong>within a single node</strong> (e.g., up to 8 GPUs over NVLink/NVSwitch) and shows rapidly diminishing throughput across slower inter-node links.<br /></li>
</ul>

<hr />

<h1 id="comparing-pipeline-and-tensor-parallelism-and-common-combinations">Comparing pipeline and tensor parallelism and common combinations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-01-05-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-01-05-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-01-05-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-01-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Combining parallelism in real systems:<br /></p>

<ul>
  <li><strong>Tensor parallelism</strong>: applied inside nodes to split matrix computation.</li>
  <li><strong>Pipeline parallelism</strong> (and/or model sharding): applied across nodes to stretch model capacity.</li>
  <li><strong>Data parallelism</strong>: layered on top to scale aggregate throughput.<br /></li>
</ul>

<p>Choice depends on topology, available batch size, and implementation complexity; hybrid strategies are the practical norm.<br /></p>

<hr />

<h1 id="activation-memory-is-dynamically-large-and-can-dominate-peak-memory-usage">Activation memory is dynamically large and can dominate peak memory usage</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-03-26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-03-26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-03-26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-03-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Activation tensors accumulate during the forward pass and are freed during backward, producing dynamic memory profiles where peak memory often occurs mid-backward.<br /></p>

<p>Consequences and mitigations:</p>
<ul>
  <li>For deep or long-sequence models, <strong>activation storage</strong> can dominate per-device memory; parameter/optimizer sharding alone does not remove this pressure.</li>
  <li>Techniques to manage activation memory: <strong>activation sharding</strong> (sequence/position partitioning), <strong>recomputation</strong> (trade extra compute for reduced storage), and <strong>attention-specific optimizations</strong> (e.g., <strong>flash attention</strong>).<br /></li>
</ul>

<hr />

<h1 id="activation-memory-per-layer-formula-and-the-residual-straggler-terms-after-tensor-parallelism">Activation memory per-layer formula and the residual ‘straggler’ terms after tensor parallelism</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-07-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-07-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-07-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-07-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Per-layer activation memory can be approximated by the expression <strong>SBH<em>34 + 5</em>A*S/H</strong>, where:</p>
<ul>
  <li><strong>S</strong> = sequence length</li>
  <li><strong>B</strong> = batch size</li>
  <li><strong>H</strong> = hidden size</li>
  <li><strong>A</strong> = attention-cost factor<br /></li>
</ul>

<p>Interpretation:</p>
<ul>
  <li>The first term corresponds to MLP and pointwise costs; the second term captures quadratic attention costs.</li>
  <li><strong>Tensor parallelism</strong> divides many matrix-related activation terms by the tensor-parallel factor <strong>T</strong>, but pointwise operations (layer norms, dropouts, small residuals) create a residual <strong>SBH*10</strong>-like term that does not split cleanly and scales with model size.</li>
  <li>To reduce these straggler terms, apply <strong>sequence-parallel</strong> techniques (partition activations across sequence positions) and <strong>attention recomputation</strong> (e.g., flash attention) to lower quadratic memory footprints.<br /></li>
</ul>

<hr />

<h1 id="sequence-parallel-activation-sharding-and-recomputation-minimize-activation-memory">Sequence-parallel (activation) sharding and recomputation minimize activation memory</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-09-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-09-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-09-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-09-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Sequence-parallel and recomputation techniques:<br /></p>

<ul>
  <li><strong>Sequence parallelism</strong>: partition sequence positions across devices so pointwise ops act on disjoint slices; requires <strong>all-gather</strong> / <strong>reduce-scatter</strong> at specific points to assemble/distribute tensors for dense ops.</li>
  <li><strong>Activation recomputation</strong>: trade extra FLOPS for memory by recomputing intermediate activations on demand instead of storing them.</li>
  <li><strong>Attention optimizations</strong> (flash attention): reduce S^2 memory and compute costs.<br /></li>
</ul>

<p>Combining tensor parallelism, sequence parallelism, and recomputation approaches a near-minimal activation memory lower bound (roughly <strong>SBH*34/T</strong> after partitioning), enabling much larger effective models per device.<br /></p>

<hr />

<h1 id="additional-parallelism-variants-ring-context-attention-and-expert-sparse-parallelism">Additional parallelism variants: ring (context) attention and expert (sparse) parallelism</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-10-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-10-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-10-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-10-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Variants addressing specific bottlenecks:<br /></p>

<ul>
  <li><strong>Ring / context-parallel attention</strong>: computes attention by circulating keys/values in a ring so each device handles a subset of queries and receives streamed key-value tiles; reduces per-device memory by using an online tiling pattern (good for long-context attention).</li>
  <li><strong>Expert parallelism (Mixture of Experts, MoE)</strong>: partitions MLP capacity into many experts across devices and activates only a sparse subset per input. It resembles tensor parallelism but requires <strong>routing</strong> mechanisms and <strong>load balancing</strong> because routing is input-dependent and communication patterns are irregular.<br /></li>
</ul>

<p>These variants tackle long-context memory (ring attention) and parameter-count scaling with sparsity (MoE), but introduce routing/communication complexity.<br /></p>

<hr />

<h1 id="tradeoffs-summary-across-distributed-parallel-strategies">Tradeoffs summary across distributed-parallel strategies</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-12-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-12-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-12-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-12-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Trade-offs across parallelization strategies:<br /></p>

<ul>
  <li><strong>Data-parallel (DDP)</strong>: simple and bandwidth-friendly but replicates memory.</li>
  <li><strong>ZeRO / FSDP</strong>: reduces memory at modest bandwidth cost and integrates cleanly with existing models.</li>
  <li><strong>Pipeline parallelism</strong>: reduces parameter/activation memory per device and can be spread across nodes, but consumes batch-size and is complex to implement.</li>
  <li><strong>Tensor parallelism</strong>: scales matrix computation without consuming batch-size but requires high-bandwidth, low-latency interconnects and frequent collectives.<br /></li>
</ul>

<p>Selecting a hybrid strategy requires evaluating: network topology, per-device memory limits, global batch-size constraints, and operational cost of implementation and maintenance.<br /></p>

<hr />

<h1 id="batch-size-to-device-ratio-determines-which-hybrid-parallelism-is-optimal">Batch-size-to-device ratio determines which hybrid parallelism is optimal</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-15-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-15-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-15-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-15-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Simple performance model mapping global batch size to parallelism mixes:<br /></p>

<ul>
  <li><strong>Tiny batch size per device</strong>: communication dominates; no technique is efficient.</li>
  <li><strong>Moderate batch sizes</strong>: combine <strong>ZeRO/FSDP</strong> with <strong>tensor parallelism</strong> to reach compute-bound operation.</li>
  <li><strong>Large batch sizes</strong>: pure data parallelism (or ZeRO with data parallelism) suffices for high utilization.<br /></li>
</ul>

<p>Practical lever: increasing effective batch size via <strong>gradient accumulation</strong> trades wall-clock time for improved communication efficiency, so batch-size management is central when tuning parallelism for a hardware fleet.<br /></p>

<hr />

<h1 id="practical-rule-of-thumb-for-multi-dimensional-parallelism-3d4d">Practical rule-of-thumb for multi-dimensional parallelism (3D/4D)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-17-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-17-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-17-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-17-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A practical sequence (heuristic) for combining parallelism:<br /></p>

<ol>
  <li>Fit model and activations in memory using <strong>tensor parallelism</strong> first (apply up to the number of GPUs per node).</li>
  <li>If needed, use <strong>ZeRO Stage 3 (FSDP)</strong> or <strong>pipeline parallelism</strong> across machines to further reduce per-device memory.</li>
  <li>Scale aggregate throughput with <strong>data parallelism</strong> across many replicas.</li>
  <li>If batch size is insufficient to hide pipeline latency, use <strong>gradient accumulation</strong> to increase effective batch size and reduce synchronization frequency.<br /></li>
</ol>

<p>This heuristic often yields near-linear aggregate throughput and provides a reproducible path for choosing which parallelism axes to use at each scale.<br /></p>

<hr />

<h1 id="case-studies-megatron-lm-deepseek-llama-3-and-tpu-based-examples">Case studies: Megatron-LM, DeepSeek, LLaMA 3, and TPU-based examples</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-20-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-20-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-20-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-20-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Case study patterns from large-scale training papers and release notes:<br /></p>

<ul>
  <li><strong>Megatron-LM</strong>: tensor parallelism (commonly 1–8-way) combined with pipeline and data parallelism to scale from billions to trillions of parameters.</li>
  <li><strong>DeepSeek variants</strong>: mix tensor, sequence, pipeline, and ZeRO Stage 1.</li>
  <li><strong>LLaMA 3</strong>: practical combination of tensor parallelism (e.g., 8-way), pipeline stages, FSDP-like sharding, and context-parallel techniques for long-context phases.</li>
  <li><strong>TPU-based systems (e.g., GMA2)</strong>: leverage toroidal mesh networking to expand model-parallel extents.<br /></li>
</ul>

<p>These case studies validate earlier rules of thumb: practical systems almost always combine multiple parallelism axes tailored to hardware and workload.<br /></p>

<hr />

<h1 id="operational-challenges-at-scale-including-hardware-failures-and-data-integrity">Operational challenges at scale including hardware failures and data integrity</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-23-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-23-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-23-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-23-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Operational risks and reliability concerns for large-scale distributed runs:<br /></p>

<ul>
  <li>Common failures: GPU hardware failures, node maintenance interruptions, and <strong>silent data corruption</strong>. Production runs often see hundreds of interruptions.</li>
  <li>Essential mechanisms: <strong>checkpointing</strong>, redundancy, validation, and observability to detect silent numerical corruption.</li>
  <li>Operational robustness (fault tolerance, monitoring, and validation) is as important as algorithmic scaling when running multi-week, multi-thousand-GPU training campaigns.<br /></li>
</ul>

<hr />

<h1 id="final-synthesis-combine-parallelism-axes-and-follow-simple-rules-of-thumb">Final synthesis: combine parallelism axes and follow simple rules of thumb</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec07/01-24-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec07/01-24-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec07/01-24-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec07/01-24-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical summary and topology-aware heuristics:<br /></p>

<ul>
  <li>Combine <strong>data</strong>, <strong>model</strong> (tensor or pipeline), and <strong>activation</strong> (sequence) parallelism to balance limited resources—memory, bandwidth, compute, and batch size—while respecting hardware topology.</li>
  <li>Heuristics: apply <strong>tensor parallelism</strong> within nodes; use <strong>ZeRO/pipeline</strong> to fit models across nodes; scale throughput with <strong>data parallelism</strong>; use <strong>sequence/activation sharding</strong> and <strong>recomputation</strong> to minimize activation memory.</li>
  <li>Use <strong>gradient accumulation</strong> to trade time for larger effective batch sizes when necessary.</li>
  <li>Implementation notes: careful overlap of communication and computation plus attention to operational robustness enable near-linear aggregate throughput in practice.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry><entry><title type="html">CS336 Lecture 5 - GPUs</title><link href="https://tuananhbui89.github.io/blog/2025/cs336-lec05/" rel="alternate" type="text/html" title="CS336 Lecture 5 - GPUs" /><published>2025-12-08T00:00:00+07:00</published><updated>2025-12-08T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/cs336-lec05</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/cs336-lec05/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/6OBtO9niT00" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-objectives-and-roadmap">Lecture objectives and roadmap</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-01-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-01-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-01-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-01-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The lecture introduces the goals: demystify <strong>CUDA / GPUs</strong> and equip practitioners to accelerate model components with <strong>GPU-aware algorithms and implementations</strong>.<br /></p>

<p>Roadmap (three parts):<br /></p>
<ul>
  <li><strong>Understand GPU hardware and execution model</strong> — learn SMs, warps, memory hierarchy, latency vs throughput tradeoffs.<br /></li>
  <li><strong>Analyze performance characteristics</strong> — why GPUs are fast on some workloads and slow on others, and how to measure bottlenecks.<br /></li>
  <li><strong>Apply lessons to implement high-performance primitives</strong> — practical examples such as <strong>Flash Attention</strong>.<br /></li>
</ul>

<p>The segment also frames assignments and resources for deeper study, and sets expectations for the remainder of the talk.<br /></p>

<hr />

<h1 id="compute-scaling-motivates-hardware-focused-optimization">Compute scaling motivates hardware-focused optimization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-04-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-04-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-04-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-04-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Compute scaling and historical semiconductor trends explain why <strong>parallel hardware</strong> matters for modern deep learning workloads.<br /></p>

<ul>
  <li><strong>Dennard scaling</strong> and <strong>Moore’s Law</strong> drove decades of single-thread frequency and IPC increases. <br /></li>
  <li>When single-thread throughput plateaued, the performance story shifted to <strong>parallel scaling across many cores</strong>. <br /></li>
  <li>The result: exponential growth in <strong>aggregate integer / FLOP throughput</strong> across GPU generations. <br /></li>
  <li>Consequence: efficient utilization of <strong>massively parallel accelerators</strong> is the key lever for model scaling and performance.<br /></li>
</ul>

<hr />

<h1 id="cpus-optimize-latency-while-gpus-optimize-throughput">CPUs optimize latency while GPUs optimize throughput</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-08-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-08-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-08-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-08-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>CPUs and GPUs are architected for different goals, which dictates different programming and algorithmic approaches.<br /></p>

<ul>
  <li><strong>CPUs</strong>:<br />
    <ul>
      <li>Allocate significant die area to <strong>control logic</strong>, <strong>branch prediction</strong>, and single-thread performance.<br /></li>
      <li>Optimized to minimize <strong>latency for individual tasks</strong> and handle complex control flow.<br /></li>
    </ul>
  </li>
  <li><strong>GPUs</strong>:<br />
    <ul>
      <li>Trade control for <strong>massive numbers of ALUs</strong> to maximize <strong>aggregate throughput</strong>.<br /></li>
      <li>Expose many simple processing elements that execute the same instruction across many data elements (<strong>SIMT</strong>).<br /></li>
      <li>Deliver extremely high parallel <strong>FLOPS</strong> at the cost of higher per-thread latency and simplified control.<br /></li>
    </ul>
  </li>
</ul>

<p>Implication: design for <strong>latency-sensitive</strong> workloads on CPUs and <strong>throughput-oriented</strong> workloads on GPUs, with algorithms and kernels matched accordingly.<br /></p>

<hr />

<h1 id="gpu-physical-chip-structure-and-memory-proximity-determine-performance">GPU physical chip structure and memory proximity determine performance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-11-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-11-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-11-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-11-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A GPU chip is organized into many <strong>Streaming Multiprocessors (SMs)</strong> with a layered memory system and strong locality considerations.<br /></p>

<ul>
  <li>Per-SM fast memory: <strong>registers</strong>, <strong>L1</strong>, and <strong>shared memory</strong> (very low latency).<br /></li>
  <li>Chip-level caches and external memory: <strong>L2</strong> and external DRAM (HBM or GDDR) (much higher latency).<br /></li>
  <li>Physical proximity matters: on‑SM accesses take <strong>tens of cycles</strong>, global memory accesses can take <strong>hundreds of cycles</strong>.<br /></li>
</ul>

<p>Design implication: <strong>minimize transfers from global memory</strong> and exploit SM-local storage to keep compute units busy.<br /></p>

<hr />

<h1 id="gpu-execution-granularity-blocks-warps-and-threads">GPU execution granularity: blocks, warps and threads</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-14-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-14-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-14-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-14-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>GPU execution organizes work into blocks, warps, and threads — understanding this mapping is critical for high utilization.<br /></p>

<ul>
  <li><strong>Block</strong>: the unit scheduled onto an SM; contains many threads and can use shared memory visible to the block.<br /></li>
  <li><strong>Warp</strong>: typically 32 threads that execute the <strong>same instruction</strong> in lockstep (SIMT).<br /></li>
  <li><strong>Thread</strong>: the smallest program unit, with private registers.<br /></li>
</ul>

<p>Key points:<br /></p>
<ul>
  <li>A block is assigned to a <strong>single SM</strong> and is executed in warps.<br /></li>
  <li>All threads in a warp must follow the <strong>same instruction stream</strong>; divergence penalizes utilization.<br /></li>
  <li>Kernel design must consider mapping from <strong>blocks → SMs</strong> and <strong>threads → warps</strong> to achieve memory coalescing and high occupancy.<br /></li>
</ul>

<hr />

<h1 id="logical-memory-model-registers-shared-and-global-memory">Logical memory model: registers, shared, and global memory</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-15-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-15-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-15-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-15-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The GPU programming model exposes a hierarchy of logical memories — use each level for its intended purpose.<br /></p>

<ul>
  <li><strong>Per-thread registers</strong>: fastest storage; hold frequently used scalar and temporary values.<br /></li>
  <li><strong>Per-block shared memory</strong>: programmable on-SM scratchpad for collaborative reuse across threads in a block.<br /></li>
  <li><strong>Global memory</strong>: high-latency, visible across blocks; used for large data and inter-block communication.<br /></li>
</ul>

<p>Kernel design rules:<br /></p>
<ul>
  <li>Keep <strong>hot working sets</strong> in registers / shared memory.<br /></li>
  <li>Minimize <strong>global memory</strong> reads/writes and use it mainly for large persistent storage or cross-block communication.<br /></li>
  <li>Structure kernels to exploit on-SM storage for reuse and locality.<br /></li>
</ul>

<hr />

<h1 id="tpu-architecture-is-conceptually-similar-to-gpus-with-mxu-specialization">TPU architecture is conceptually similar to GPUs with MXU specialization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-17-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-17-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-17-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-17-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>TPUs share several high-level concepts with GPUs but emphasize specialized matrix hardware.<br /></p>

<ul>
  <li>Commonalities:<br />
    <ul>
      <li>Many local compute units, <strong>on-chip fast memory</strong>, and external <strong>high-bandwidth memory</strong>.<br /></li>
      <li>Importance of data locality and tiling strategies.<br /></li>
    </ul>
  </li>
  <li>Distinctive TPU features:<br />
    <ul>
      <li>Large, highly-optimized <strong>matrix multiply unit (MXU)</strong> for batched matrix multiplies.<br /></li>
      <li>Scalar/vector units for control and element-wise ops, plus specialized on-core memory for tensor operands and accumulators.<br /></li>
    </ul>
  </li>
</ul>

<p>Practical takeaway: many GPU optimization patterns (tiling, data locality, memory-hierarchy awareness) carry over to TPU programming, though <strong>interconnect and multi-chip parallelism</strong> differ and must be handled separately.<br /></p>

<hr />

<h1 id="matrix-multiplies-and-tensor-cores-drive-gpu-value-for-ml">Matrix multiplies and tensor cores drive GPU value for ML</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-20-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-20-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-20-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-20-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modern accelerators became dominant because <strong>matrix multiply throughput</strong> outpaced general-purpose FLOPS growth, especially after adding specialized hardware like <strong>tensor cores</strong>.<br /></p>

<ul>
  <li><strong>Tensor cores</strong> provide very high-throughput <strong>mixed-precision matrix multiplication</strong> primitives.<br /></li>
  <li>These units deliver orders-of-magnitude higher <strong>MAP (matrix arithmetic) FLOPS</strong> for matrix operations compared to scalar/non-matrix ops.<br /></li>
  <li>Result: neural architectures and kernels are designed to make the bulk of computation <strong>MAP-friendly</strong> to exploit hardware specialization and achieve superior end-to-end performance.<br /></li>
</ul>

<hr />

<h1 id="compute-has-outpaced-memory-growth-making-memory-movement-the-bottleneck">Compute has outpaced memory growth, making memory movement the bottleneck</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-24-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-24-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-24-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-24-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>External memory bandwidth (host interconnects and device global memory) has grown much more slowly than raw compute capacity, creating a compute-versus-memory imbalance.<br /></p>

<ul>
  <li>Many GPUs are now <strong>compute-rich but memory-starved</strong>.<br /></li>
  <li>End-to-end performance is increasingly determined by how efficiently an algorithm moves data between <strong>DRAM and on-chip buffers</strong>, not just raw FLOPS.<br /></li>
  <li>Designing hardware-efficient algorithms therefore focuses on <strong>minimizing global memory traffic</strong> and <strong>maximizing reuse</strong> in fast on-chip storage.<br /></li>
</ul>

<hr />

<h1 id="summary-of-gpu-characteristics-to-guide-optimization">Summary of GPU characteristics to guide optimization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-25-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-25-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-25-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-25-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Key GPU facts and the recurring optimization goals they imply:<br /></p>

<ul>
  <li><strong>Massively parallel SIMT execution</strong> — exploits regular, lockstep work across many threads.<br /></li>
  <li><strong>SM-local fast memory vs. slower global memory</strong> — locality and proximity drive performance.<br /></li>
  <li><strong>Hardware preference for matrix multiplies</strong> — matrix-friendly code maps best to peak throughput.<br /></li>
</ul>

<p>Repeated optimization checklist:<br /></p>
<ul>
  <li>Avoid <strong>branch divergence</strong>.<br /></li>
  <li>Maximize <strong>data locality</strong> in shared memory and registers.<br /></li>
  <li>Optimize memory access patterns (coalescing and alignment).<br /></li>
  <li>Structure work to match SM and warp granularities.<br />
Keeping these characteristics in mind provides a consistent toolkit for improving GPU kernel performance.<br /></li>
</ul>

<hr />

<h1 id="roofline-model-explains-memory--vs-compute-bound-regimes">Roofline model explains memory- vs compute-bound regimes</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-27-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-27-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-27-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-27-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The roofline model partitions performance into <strong>memory-bound</strong> and <strong>compute-bound</strong> regimes using <strong>arithmetic intensity</strong> (FLOPS per byte).<br /></p>

<ul>
  <li>Low arithmetic intensity or small problems → <strong>memory-bound</strong>: performance limited by global memory bandwidth (diagonal slope on the roofline).<br /></li>
  <li>High arithmetic intensity and large problems → <strong>compute-bound</strong>: kernel can saturate compute and operate at the flat peak of the roofline.<br /></li>
</ul>

<p>Practical implication: increase arithmetic intensity (for example, via <strong>tiling</strong>, <strong>fusion</strong>, or <strong>lower-precision storage</strong>) to move kernels from the memory-limited to the compute-limited regime and unlock higher throughput.<br /></p>

<hr />

<h1 id="warp-divergence-from-conditionals-degrades-throughput">Warp divergence from conditionals degrades throughput</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-29-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-29-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-29-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-29-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Warps execute the same instruction; divergent branches inside a warp serialize execution and reduce effective parallelism.<br /></p>

<ul>
  <li>If threads in a warp take different control paths, the GPU runs each branch path in turn while <strong>masking</strong> inactive threads.<br /></li>
  <li>Divergence causes utilization and throughput to drop dramatically because only a subset of threads execute at a time.<br /></li>
</ul>

<p>Strategies to preserve SIMT efficiency:<br /></p>
<ul>
  <li>Avoid <strong>intra-warp control-flow divergence</strong> where possible.<br /></li>
  <li>Structure workloads so divergent behavior is <strong>segregated across warps</strong> (e.g., group similar-control-flow threads together).<br /></li>
</ul>

<hr />

<h1 id="lower-precision-arithmetic-increases-effective-memory-bandwidth">Lower-precision arithmetic increases effective memory bandwidth</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-31-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-31-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-31-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-31-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Reducing numeric precision (FP32 → FP16/BF16 → INT8) provides substantial bandwidth and capacity benefits when done carefully.<br /></p>

<ul>
  <li>Lower precision reduces per-value storage and transfer costs, effectively multiplying usable <strong>memory bandwidth</strong> and on-chip cache capacity.<br /></li>
  <li><strong>Mixed-precision</strong> commonly stores matrix inputs in low precision while keeping <strong>accumulators in higher precision (FP32)</strong> to preserve numerical stability.<br /></li>
  <li>Special care is required for operations with <strong>high dynamic range</strong> or sensitivity to rounding.<br /></li>
</ul>

<p>When memory is the bottleneck, moving to lower-precision representations often yields large throughput gains with modest algorithmic changes and careful numerical engineering.<br /></p>

<hr />

<h1 id="operator-fusion-reduces-global-memory-traffic-by-composing-kernels">Operator fusion reduces global memory traffic by composing kernels</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-35-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-35-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-35-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-35-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Operator fusion combines multiple sequential small operations into a single kernel so data stays in registers or shared memory.<br /></p>

<ul>
  <li>Fusion eliminates intermediate writes to and reads from <strong>global memory</strong>, drastically reducing memory traffic and launch overhead for small operators.<br /></li>
  <li>Compilers and frameworks (for example, <strong>torch.compile</strong> or fused CUDA kernels) can perform fusion automatically.<br /></li>
  <li>Result: increased <strong>arithmetic intensity</strong> and improved end-to-end throughput for fused operator sequences.<br /></li>
</ul>

<hr />

<h1 id="recomputation-trades-compute-for-memory-to-reduce-global-accesses">Recomputation trades compute for memory to reduce global accesses</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-39-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-39-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-39-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-39-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Recomputation (checkpointing) trades extra FLOPS for reduced memory traffic by recomputing intermediates instead of storing them in global memory.<br /></p>

<ul>
  <li>Useful when a workload is <strong>memory-bound and compute-rich</strong>: use idle compute cycles to regenerate data that would otherwise require expensive DRAM reads.<br /></li>
  <li>Trade-offs to consider:<br />
    <ul>
      <li>Wall-clock improvement depends on the relative cost of recompute FLOPS vs. DRAM access latency and bandwidth.<br /></li>
      <li>Numerical determinism and reproducibility constraints.<br /></li>
      <li>Memory footprint vs. recompute overhead balance.<br />
Implementations must carefully choose which activations to checkpoint and which to recompute during the backward pass.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="dram-burst-transfers-and-memory-coalescing-shape-effective-bandwidth">DRAM burst transfers and memory coalescing shape effective bandwidth</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-44-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-44-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-44-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-44-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>DRAM returns contiguous bursts of data per access; GPUs exploit this via <strong>coalesced memory accesses</strong>.<br /></p>

<ul>
  <li>A single DRAM access typically fetches a contiguous <strong>burst section</strong>, bringing neighboring addresses into an on-chip buffer.<br /></li>
  <li>GPUs <strong>coalesce</strong> memory requests from threads in a warp so that nearby accesses combine into a single transfer, greatly amplifying throughput.<br /></li>
</ul>

<p>Kernel design implications:<br /></p>
<ul>
  <li>Layout arrays and choose memory strides so warp threads request <strong>nearby addresses</strong> (match row-major vs column-major to access pattern).<br /></li>
  <li>Proper alignment and stride choices maximize coalescing and eliminate superfluous DRAM transactions.<br /></li>
</ul>

<hr />

<h1 id="tiling-reuses-on-chip-memory-to-amortize-global-memory-cost">Tiling reuses on-chip memory to amortize global memory cost</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-50-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-50-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-50-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-50-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Tiling partitions large matrix or tensor computations into small sub-blocks that fit into shared memory to maximize reuse.<br /></p>

<ol>
  <li>Load a tile from global memory into <strong>shared memory</strong> once.<br /></li>
  <li>Reuse that tile for many FLOPS inside the SM.<br /></li>
  <li>Write results back to DRAM once when finished.<br /></li>
</ol>

<p>Benefits and considerations:<br /></p>
<ul>
  <li>Tiling reduces global reads by a factor proportional to the tile dimension <strong>T</strong>, increasing arithmetic intensity.<br /></li>
  <li>Effective tiling requires tuning <strong>tile sizes</strong> to shared memory capacity, ensuring coalesced loads, and matching tile shapes to the hardware’s preferred micro-tiles.<br /></li>
</ul>

<hr />

<h1 id="tiling-arithmetic-global-reads-scale-with-nt-and-shared-reads-scale-with-t">Tiling arithmetic: global reads scale with N/T and shared reads scale with T</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/00-55-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/00-55-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/00-55-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/00-55-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>For an N×N matrix multiply with tile size T, global memory loads per operand are reduced from O(N) per result to approximately O(N/T) per tile plus O(T) reuse inside shared memory.<br /></p>

<ul>
  <li>The total number of global memory reads is reduced by roughly a factor of <strong>T</strong> compared to the naive non-tiled algorithm (modulo tiling overhead and alignment constraints).<br /></li>
  <li>This reduction in DRAM traffic is the primary reason tiled matrix multiplication dramatically increases arithmetic intensity and overall device utilization on modern GPUs.<br /></li>
</ul>

<hr />

<h1 id="tile-size-discretization-and-alignment-cause-large-throughput-variance">Tile-size discretization and alignment cause large throughput variance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-00-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-00-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-00-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-00-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Tile sizing, matrix dimension divisibility, and hardware granularities produce quantized performance behavior that must be managed.<br /></p>

<ul>
  <li><strong>Non-divisible dimensions</strong> create partially filled tiles that underutilize threads and SM resources.<br /></li>
  <li><strong>Misaligned tiles</strong> can break DRAM burst coalescing and double memory transactions.<br /></li>
  <li>These discretization effects produce wavelike performance curves as problem sizes change.<br /></li>
</ul>

<p>Mitigation strategies:<br /></p>
<ul>
  <li>Carefully select or <strong>pad dimensions</strong> to align with tile sizes, warp size, and burst alignment.<br /></li>
  <li>Tune tile sizes to avoid pathological underutilization on typical problem sizes.<br /></li>
</ul>

<hr />

<h1 id="wave-quantization-tiles-sm-count-and-occupancy-create-cliffs">Wave quantization: tiles, SM count and occupancy create cliffs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-03-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-03-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-03-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-03-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The number of tiles (blocks) relative to the physical SM count yields <strong>occupancy quantization</strong> and wave-based dispatch behavior.<br /></p>

<ul>
  <li>If total tiles barely exceed or fall short of the SM count, work dispatch happens in <strong>waves</strong>, creating utilization cliffs.<br /></li>
  <li>Small changes in matrix dimension can change the number of tiles from evenly mapping across SMs to leaving many SMs idle while a few execute leftover tiles, producing large throughput drops.<br /></li>
</ul>

<p>Avoidance techniques:<br /></p>
<ul>
  <li>Choose tile sizes and partitions that <strong>distribute work evenly</strong> across all SMs.<br /></li>
  <li>Increase the number of independent blocks (more fine-grained partitioning) to <strong>smooth occupancy</strong> and avoid wave cliffs.<br /></li>
</ul>

<hr />

<h1 id="practical-optimization-checklist-for-gpu-kernels">Practical optimization checklist for GPU kernels</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-04-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-04-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-04-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-04-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Effective GPU optimization centers on minimizing global memory accesses and maximizing on-chip reuse via coalescing, fusion, and shared-memory tiling.<br /></p>

<p>Key tactics:<br /></p>
<ul>
  <li>Design access patterns for <strong>burst-coalesced reads</strong> and proper alignment.<br /></li>
  <li><strong>Fuse</strong> small operators into single kernels to avoid intermediate global writes.<br /></li>
  <li><strong>Tile</strong> computations to fit shared memory and reuse loaded data for many FLOPS.<br /></li>
  <li><strong>Trade compute for memory</strong> with recomputation/checkpointing when DRAM is the bottleneck.<br /></li>
  <li>Choose <strong>numeric precision</strong> that balances bandwidth savings with numerical stability.<br /></li>
</ul>

<p>Collectively, these strategies increase arithmetic intensity and move kernels toward the compute-bound roofline.<br /></p>

<hr />

<h1 id="flash-attention-uses-tiling-and-recomputation-to-reduce-hbm-accesses-for-attention">Flash Attention uses tiling and recomputation to reduce HBM accesses for attention</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-05-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-05-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-05-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-05-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Flash Attention</strong> implements attention with tiled matrix multiplies and an <strong>online softmax</strong>, enabling exact attention with sub-quadratic HBM accesses.<br /></p>

<ul>
  <li>Keeps as much intermediate data in <strong>shared memory</strong> as possible.<br /></li>
  <li>Computes softmax normalization terms <strong>incrementally per tile</strong> (online), avoiding materializing large N×N matrices in global memory.<br /></li>
  <li>Trades extra local computation and careful tiling for dramatically fewer global reads/writes, yielding large speedups for long-context transformer attention.<br /></li>
</ul>

<hr />

<h1 id="attention-computation-is-tiled-but-softmax-is-a-global-normalization-challenge">Attention computation is tiled but softmax is a global normalization challenge</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-08-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-08-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-08-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-08-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Attention decomposes into three steps: compute <strong>QK^T</strong>, apply a row-wise <strong>softmax</strong>, then multiply by <strong>V</strong> — the softmax normalization is the algorithmic challenge for tiled execution.<br /></p>

<ul>
  <li>The KQ product can be tiled and accumulated across tiles.<br /></li>
  <li>The <strong>row-wise softmax normalization</strong> requires the sum of exponentials per row — a global reduction if done naively.<br /></li>
  <li>Materializing full N×N logits violates on-chip memory limits, so a <strong>tile-by-tile online algorithm</strong> that maintains per-row state is required to evaluate softmax without full-matrix storage.<br /></li>
</ul>

<hr />

<h1 id="online-softmax-enables-tile-by-tile-softmax-computation">Online softmax enables tile-by-tile softmax computation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-11-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-11-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-11-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-11-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The online softmax algorithm maintains a running maximum and a running sum of exponentials corrected for changes in the maximum as new elements arrive — enabling streaming, numerically stable normalization.<br /></p>

<ul>
  <li>For each incoming tile of logits the algorithm:<br />
    <ul>
      <li>Updates the <strong>running max</strong> for the row.<br /></li>
      <li><strong>Rescales the running sum</strong> to account for any max change and adds the tile’s contributions.<br /></li>
    </ul>
  </li>
  <li>This telescoping/online formulation preserves numerical stability and computes the normalization factor from per-tile state, avoiding the need to store full N×N logits in global memory.<br /></li>
</ul>

<hr />

<h1 id="flash-attention-integrates-tiled-matmuls-and-recomputation-for-forward-and-backward-passes">Flash Attention integrates tiled matmuls and recomputation for forward and backward passes</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-12-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-12-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-12-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-12-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Flash Attention forward and backward mechanics combine tiling, online reduction, and recomputation to avoid N^2 storage while remaining exact.<br /></p>

<p>Forward pass (high level):<br /></p>
<ol>
  <li>Tile Q and K, compute dot-product tiles and accumulate per-row logits.<br /></li>
  <li>Update <strong>online softmax</strong> statistics per row tile (running max and sum).<br /></li>
  <li>Apply normalized weights to V in tiled form — <strong>no N×N matrix</strong> is materialized in HBM.<br /></li>
</ol>

<p>Backward pass strategy:<br /></p>
<ul>
  <li>Use <strong>recomputation</strong>: re-evaluate tiles on the fly during gradient propagation instead of storing N^2 activations to global memory.<br /></li>
  <li>Trade extra FLOPS for reduced DRAM traffic and memory footprint.<br /></li>
</ul>

<p>Combined effect: tiling, coalesced accesses, online reduction, selective low precision, and recomputation yield significant throughput and memory-efficiency improvements in Flash Attention implementations.<br /></p>

<hr />

<h1 id="final-takeaway-optimize-data-movement-first-then-compute">Final takeaway: optimize data movement first, then compute</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec05/01-13-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec05/01-13-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec05/01-13-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec05/01-13-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Hardware trends make <strong>memory movement</strong> the dominant limiter for large models, so algorithm and implementation efforts should prioritize minimizing global memory traffic and maximizing on-chip reuse.<br /></p>

<p>Practical toolkit for high performance:<br /></p>
<ul>
  <li>Lower precision when safe (mixed-precision).<br /></li>
  <li>Fuse operators to avoid intermediate DRAM writes.<br /></li>
  <li>Coalesce memory accesses and align buffers for bursts.<br /></li>
  <li>Tile to exploit shared memory and increase reuse.<br /></li>
  <li>Recompute activations selectively to trade FLOPS for reduced DRAM access.<br /></li>
</ul>

<p>Applied systematically, these techniques raise arithmetic intensity, move kernels toward compute-bound performance, and enable state-of-the-art primitives such as <strong>Flash Attention</strong> for scalable transformer training and inference.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary of CS336 Lecture]]></summary></entry></feed>