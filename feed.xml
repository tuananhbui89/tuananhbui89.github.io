<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tuananhbui89.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tuananhbui89.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-15T13:19:11+07:00</updated><id>https://tuananhbui89.github.io/feed.xml</id><title type="html">blank</title><subtitle>Researcher in Generative AI and Trustworthy AI
</subtitle><entry><title type="html">Agent 07 - Building an Agent from Scratch by AI Engineer</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec07/" rel="alternate" type="text/html" title="Agent 07 - Building an Agent from Scratch by AI Engineer" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec07</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec07/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/xzXdLRUyjUg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="talk-goals-demonstrate-a-minimal-agent-enable-experimentation-with-code-and-build-intuition-for-agent-components">Talk goals: demonstrate a minimal agent, enable experimentation with code, and build intuition for agent components</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-00-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-00-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-00-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-00-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The talk sets three primary objectives to orient the audience and the repository work:<br /></p>

<ol>
  <li><strong>Expose a minimal, operational definition of an agent</strong> — a compact, implementable specification that you can run and reason about.<br /></li>
  <li><strong>Encourage hands-on experimentation</strong> — clone, run, and deliberately break the included code so learners see behavior change in real time.<br /></li>
  <li><strong>Provide intuition for foundational building blocks</strong> used by agent frameworks — so subsequent examples map directly to implementable components.<br /></li>
</ol>

<ul>
  <li>Emphasis is placed on the <strong>pedagogical value of iterating on code</strong> to observe the transition from deterministic, scripted behavior to emergent, agent-like behavior.<br /></li>
  <li>The goal framing prioritizes <strong>practical learning and conceptual clarity</strong>, setting expectations for the rest of the talk and motivating active engagement with the provided repository and slides.<br /></li>
</ul>

<hr />

<h1 id="slides-and-code-are-published-and-attendees-are-encouraged-to-try-the-repository">Slides and code are published and attendees are encouraged to try the repository.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-00-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-00-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-00-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-00-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The presenter shares the canonical artifacts and a call to action:<br /></p>

<ul>
  <li><strong>Slides, code, and social/contact links</strong> are provided up front so attendees can follow along.<br /></li>
  <li>Attendees are explicitly encouraged to <strong>clone, run, and modify the example repository</strong> as the primary learning path.<br /></li>
</ul>

<p>Recommended learning strategies emphasized in this segment:<br /></p>
<ul>
  <li><strong>Rapid iteration</strong> — make small changes and observe outcomes quickly.<br /></li>
  <li><strong>Fault injection</strong> — introduce errors intentionally to reveal where deterministic code starts behaving like an agent.<br /></li>
  <li>Treat the rest of the presentation as a <strong>guided tour of the codebase</strong>, not just theory, and consult the published artifacts for hands-on practice.<br /></li>
</ul>

<hr />

<h1 id="an-agent-is-composed-of-an-llm-memory-planning-tools-and-a-loop-or-conditional-control-flow">An agent is composed of an LLM, memory, planning, tools, and a loop or conditional control flow.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-01-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-01-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-01-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-01-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Agent definition (operational):</strong> an architecture that combines a <strong>large language model (LLM)</strong> with persistent <strong>memory</strong>, <strong>planning logic</strong>, and <strong>external tools</strong>, all orchestrated by a looping control structure.<br /></p>

<ul>
  <li><strong>Memory</strong>: readable and writable state the agent uses to persist context across iterations.<br /></li>
  <li><strong>Loop</strong>: the control structure implementing conditional repetition and termination logic.<br /></li>
  <li><strong>Planning</strong>: decomposes tasks into substeps or a to-do list the agent can act on using tools.<br /></li>
  <li><strong>Tools</strong>: external capabilities (e.g., web search, browsing) invoked by the agent to perform actions outside the LLM.<br /></li>
</ul>

<p>This decomposition makes it easy to <strong>reorder or refactor responsibilities</strong> across components during implementation and aids clear mapping from concept to code.<br /></p>

<hr />

<h1 id="step-0-start-with-a-basic-chat-completion-llm-call-to-produce-a-baseline-response">Step 0: Start with a basic chat-completion LLM call to produce a baseline response.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-01-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-01-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-01-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-01-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Initial baseline implementation (single-turn generation):<br /></p>

<ol>
  <li>Call a standard <strong>chat completion</strong> endpoint via the provider API to generate a baseline answer to a user prompt.<br /></li>
  <li>Observe this as the simplest agent behavior: <strong>single-turn</strong>, fully deterministic request/response semantics given identical model parameters and context.<br /></li>
</ol>

<ul>
  <li>The baseline is intentionally minimal to serve as a clear control case before adding validation, tooling, and state.<br /></li>
  <li>Implementation checklist at this stage:
    <ul>
      <li>Verify <strong>API call patterns</strong>.</li>
      <li>Confirm <strong>message formatting</strong>.</li>
      <li>Validate <strong>response parsing</strong>.<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="step-1-add-a-conditional-evaluator-llm-that-judges-whether-the-previous-answer-is-complete">Step 1: Add a conditional evaluator LLM that judges whether the previous answer is complete.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-02-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-02-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-02-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-02-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Introduce a secondary LLM acting as a <strong>strict critic / judge</strong> to validate generated responses:<br /></p>

<ul>
  <li>The judge LLM evaluates the initial LLM response against a <strong>completion criterion</strong> and returns a <strong>forced JSON schema</strong> (for example, a Boolean field named <strong>done</strong>) so downstream client code can decide to continue or terminate.<br /></li>
  <li>The judge is invoked with instructions to produce a <strong>deterministic JSON object</strong>, reducing ambiguity about whether the user intent has been satisfied.<br /></li>
</ul>

<p>Benefits and caveats:<br /></p>
<ul>
  <li>This two-step pattern <strong>separates generation and validation</strong>, enabling iterative or corrective behavior without hard-coded client heuristics.<br /></li>
  <li>Still dependent on <strong>accurate judge prompts</strong> and <strong>strict schema enforcement</strong>; incorrect judge outputs or lax schema handling can produce false positives or premature termination.<br /></li>
</ul>

<hr />

<h1 id="tools-are-external-functions-described-by-a-json-schema-that-the-llm-can-request-the-client-to-call-the-client-must-invoke-these-tool-calls-and-return-results-to-the-llm">Tools are external functions described by a JSON schema that the LLM can request the client to call; the client must invoke these tool calls and return results to the LLM.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-04-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-04-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-04-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-04-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Tools are modeled as function-like endpoints that the client registers with the LLM:<br /></p>

<ul>
  <li>Each tool includes:
    <ul>
      <li><strong>name</strong></li>
      <li><strong>description</strong></li>
      <li><strong>strictness flag</strong></li>
      <li><strong>parameter schema</strong><br /></li>
    </ul>
  </li>
  <li>When the LLM decides to use a tool, it returns an object indicating <strong>which tool</strong> to call and <strong>with which parameters</strong>.<br /></li>
  <li>The OpenAI SDK (and similar SDKs) <strong>does not execute arbitrary tool logic</strong> on behalf of the client:
    <ul>
      <li>The client must <strong>parse the tool request</strong>,</li>
      <li><strong>Execute the corresponding local function or external API</strong> (e.g., a Google search wrapper),</li>
      <li>Then <strong>append the tool response back into the conversation</strong> for the LLM to consume.<br /></li>
    </ul>
  </li>
</ul>

<p>Design principles and safety:<br /></p>
<ul>
  <li>This pattern lets the LLM <strong>orchestrate external actions</strong> while keeping side effects and security checks under <strong>client control</strong>.<br /></li>
  <li>Tool configuration must include <strong>precise parameter validation</strong> to avoid runtime failures and support deterministic tooling behavior for the agent.<br /></li>
</ul>

<hr />

<h1 id="tool-execution-can-fail-due-to-schema-validation-or-parameter-constraints-exposing-the-need-for-robust-error-handling">Tool execution can fail due to schema validation or parameter constraints, exposing the need for robust error handling.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-06-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-06-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-06-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-06-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A concrete failure mode was demonstrated: an unhandled rejection due to a tool schema validation constraint on a <strong>location</strong> parameter.<br /></p>

<ul>
  <li>Tool schemas may restrict allowable strings; <strong>improper inputs must be caught and handled</strong> by client code.<br /></li>
  <li>The judge LLM may still assert <strong>completion</strong> even if a tool call failed, so you must <strong>reconcile LLM judgments with observable tool execution outcomes</strong>.<br /></li>
</ul>

<p>Practical mitigations:<br /></p>
<ul>
  <li>Design <strong>retry</strong>, <strong>fallback</strong>, and <strong>error-reporting</strong> strategies.<br /></li>
  <li>Ensure tool parameter schemas are <strong>compatible with likely LLM outputs</strong> (or sanitize LLM outputs before calling tools).<br /></li>
  <li>Proper validation prevents brittle behavior and reduces false-positive completion signals from the judge.<br /></li>
</ul>

<hr />

<h1 id="step-3-refactor-to-support-parallel-tool-calls-and-attach-tool-call-identifiers-for-tracing">Step 3: Refactor to support parallel tool calls and attach tool-call identifiers for tracing.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-08-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-08-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-08-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-08-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Refactor: move the complete-with-tools logic into a utility module and add parallel execution semantics.<br /></p>

<p>Key changes and rationale:<br /></p>
<ol>
  <li>Support multiple tool invocations per LLM turn and execute them <strong>concurrently</strong> (for example, using <strong>Promise.all</strong>) to increase throughput.<br /></li>
  <li>Associate each tool invocation with a <strong>tool-call ID</strong> so the client can include that ID when appending the tool response back into the conversation; the LLM can then correlate responses to requests.<br /></li>
  <li>Treat tools conceptually as <strong>text transformations</strong> (typically string-to-string) and standardize how results are <strong>serialized</strong> into the conversational context.<br /></li>
</ol>

<ul>
  <li>This refactor enables cases where the LLM asks the same tool to be called multiple times with different parameters and improves overall performance and traceability.<br /></li>
</ul>

<hr />

<h1 id="step-4-implement-explicit-planning-via-a-to-do-list-to-provide-readablewritable-state-and-drive-multi-step-task-execution">Step 4: Implement explicit planning via a to-do list to provide readable/writable state and drive multi-step task execution.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-12-26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-12-26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-12-26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-12-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Introduce an explicit <strong>to-do list</strong> as a manipulable memory structure that consolidates planning, iteration control, and state persistence:<br /></p>

<ul>
  <li>Operational model:
    <ul>
      <li>The LLM is prompted to <strong>always create a plan</strong> by writing to the to-do list before performing actions.<br /></li>
      <li>Client-side tools implement operations with defined parameter schemas:
        <ul>
          <li><strong>add_todo</strong></li>
          <li><strong>mark_done</strong></li>
          <li><strong>check_done</strong></li>
          <li><strong>list_todos</strong><br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The to-do list acts as both the <strong>planner</strong> and the <strong>working memory</strong> for multi-step workflows:
    <ul>
      <li>Tasks can be <strong>added</strong>, <strong>marked done</strong>, <strong>listed</strong>, and <strong>queried</strong> via tool endpoints.<br /></li>
    </ul>
  </li>
</ul>

<p>Guardrails and recommendations:<br /></p>
<ul>
  <li>Use <strong>maximum iteration counts</strong>, <strong>context pruning policies</strong>, or other termination heuristics to prevent infinite loops or non-converging invocation patterns.<br /></li>
</ul>

<hr />

<h1 id="example-run-the-agent-used-the-to-do-planner-with-search-and-browse-tools-to-gather-and-summarize-information-about-building-agents-without-a-framework">Example run: the agent used the to-do planner with search and browse tools to gather and summarize information about building agents without a framework.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-15-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-15-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-15-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-15-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Example: running a to-do-driven workflow to answer “how to build agents without a framework”:<br /></p>

<ul>
  <li>The agent generated a plan such as <strong>(search, summarize, check sufficiency)</strong> and wrote it to the to-do list.<br /></li>
  <li>It invoked <strong>searchGoogle</strong> and <strong>browseWeb</strong> tools, ingested returned markdown, summarized key points, and <strong>marked relevant to-dos done</strong>.<br /></li>
  <li>The agent delegated final goal verification to the <strong>judge</strong> tool.<br /></li>
</ul>

<p>Outcome and takeaways:<br /></p>
<ul>
  <li>The pipeline produced a concise summary enumerating core recommendations:
    <ul>
      <li><strong>Define tools</strong></li>
      <li><strong>Maintain memory for context</strong></li>
      <li><strong>Implement a loop for iteration</strong></li>
      <li><strong>Employ prompt engineering</strong></li>
      <li><strong>Prefer direct API/state storage</strong> for certain operations<br /></li>
    </ul>
  </li>
  <li>This example illustrates how <strong>planning, tooling, and evaluation</strong> work together to produce coherent multi-step answers and shows the value of <strong>serializing intermediate results</strong> back into the conversation for traceability and re-evaluation by the LLM.<br /></li>
</ul>

<hr />

<h1 id="practical-example-the-agent-coordinated-multiple-searches-and-browsing-steps-to-plan-a-local-date-and-dining-options">Practical example: the agent coordinated multiple searches and browsing steps to plan a local date and dining options.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-17-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-17-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-17-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-17-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>User example: planning a date within a specified time window — demonstrates multi-step orchestration:<br /></p>

<ul>
  <li>The agent <strong>decomposed the goal</strong> into to-dos (activities, timing, dinner).<br /></li>
  <li>It executed local searches for activities and restaurants, <strong>browsed web pages</strong> to extract markdown summaries, and <strong>marked tasks complete</strong> as content was collected.<br /></li>
  <li>The agent compiled candidate activities and restaurant options and returned a <strong>prioritized suggestion list</strong> to the user.<br /></li>
</ul>

<p>Lessons learned:<br /></p>
<ul>
  <li>The run is not flawless but shows how <strong>incremental enhancements</strong> to tools, schemas, and memory rapidly improve practical utility.<br /></li>
  <li>Implementers should <strong>iteratively refine tool parameter schemas</strong> and <strong>browsing parsers</strong> to increase reliability.<br /></li>
</ul>

<hr />

<h1 id="next-steps-integrate-vector-databases-or-rag-pipelines-iterate-on-agent-design-and-leverage-the-published-code-and-slides-for-further-experimentation">Next steps: integrate vector databases or RAG pipelines, iterate on agent design, and leverage the published code and slides for further experimentation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec07/00-19-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec07/00-19-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec07/00-19-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec07/00-19-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Next steps and recommended extensions to the architecture:<br /></p>

<ul>
  <li><strong>Persist browsed content and tool outputs</strong> into a vector database (for example, <strong>Chroma</strong>) to enable <strong>retrieval-augmented generation (RAG)</strong> and longer-term memory across sessions.<br /></li>
  <li>Injecting scraped pages into an <strong>embedding store</strong> allows:
    <ul>
      <li><strong>Semantic retrieval</strong> of past results,</li>
      <li>Reduced <strong>context window pressure</strong>,</li>
      <li>Improved <strong>factual grounding</strong> of agent responses.<br /></li>
    </ul>
  </li>
</ul>

<p>Closing call to action:<br /></p>
<ul>
  <li>Experiment with the open-sourced code, contribute improvements, or provide feedback.<br /></li>
  <li>The provided <strong>slides and code links</strong> plus contact information support follow-up collaboration.<br /></li>
  <li>These next steps position the basic agent components for <strong>production-grade improvements</strong> such as better grounding, state management, and iterative evaluation.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Agent 05 - How We Build Effective Agents by Barry Zhang Anthropic</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec05/" rel="alternate" type="text/html" title="Agent 05 - How We Build Effective Agents by Barry Zhang Anthropic" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec05</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec05/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/D7_ipDqhtwk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="presentation-overview-and-three-guiding-principles">Presentation overview and three guiding principles</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-00-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-00-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-00-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-00-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The talk introduces three guiding principles for building effective <strong>agents</strong>:<br /></p>

<ul>
  <li><strong>Don’t apply agents to every problem</strong> — be selective about where autonomy adds value.<br /></li>
  <li><strong>Prioritize simplicity during development</strong> — keep the initial architecture minimal to iterate quickly.<br /></li>
  <li><strong>Adopt the agent’s perspective when iterating</strong> — design and debug from what the agent actually sees and can do.<br /></li>
</ul>

<p>This section also defines scope and framing:<br /></p>

<ul>
  <li>Frames <strong>agents</strong> as a progression beyond single model calls and predetermined workflows.<br /></li>
  <li>Establishes intent to explore both <strong>practical learnings</strong> and deeper <strong>technical considerations</strong> for agent design and deployment.<br /></li>
  <li>Prepares engineering teams to evaluate when autonomous, agentic behavior is appropriate relative to <strong>cost</strong>, <strong>latency</strong>, and <strong>operational risk</strong>.<br /></li>
</ul>

<hr />

<h1 id="evolution-from-single-model-calls-to-workflows-and-agents">Evolution from single model calls to workflows and agents</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-01-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-01-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-01-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-01-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modern application development has moved through three phases:<br /></p>

<ol>
  <li><strong>Single API model calls</strong> — simple, predictable, low integration surface.<br /></li>
  <li><strong>Orchestrated workflows</strong> — deterministic control flows that trade off cost and latency for <strong>predictable performance</strong>.<br /></li>
  <li><strong>Agents</strong> — systems that determine their own trajectories based on <strong>environmental feedback</strong> and introduce autonomy and decision-making.<br /></li>
</ol>

<p>Key tradeoffs to consider:<br /></p>

<ul>
  <li><strong>Increased agency</strong> often delivers higher utility and enables complex behaviors.<br /></li>
  <li>It also increases <strong>operational cost</strong>, <strong>latency</strong>, and the <strong>consequences of errors</strong>.<br /></li>
  <li>This motivates careful selection of where to apply agentic systems and reframes engineering tradeoffs versus prior model-integration patterns.<br /></li>
</ul>

<hr />

<h1 id="checklist-for-when-to-build-agents">Checklist for when to build agents</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-03-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-03-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-03-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-03-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Use this practical checklist to decide whether to use an agent:<br /></p>

<ol>
  <li>Task complexity and ambiguity:<br />
    <ul>
      <li>Is the decision space <strong>ambiguous</strong> or too large to exhaustively enumerate?<br /></li>
    </ul>
  </li>
  <li>Economic value and budget:<br />
    <ul>
      <li>Is there sufficient <strong>per-task budget</strong> to cover token and inference costs?<br /></li>
    </ul>
  </li>
  <li>Critical capability risks:<br />
    <ul>
      <li>Can the agent’s <strong>critical capabilities</strong> be derisked to avoid fatal bottlenecks?<br /></li>
    </ul>
  </li>
  <li>Error detectability and cost:<br />
    <ul>
      <li>How <strong>detectable</strong> and <strong>costly</strong> are errors and delayed error discovery?<br /></li>
    </ul>
  </li>
</ol>

<p>Recommended constraints when risk is high:<br /></p>

<ul>
  <li>Limit autonomy (for example, <strong>read-only access</strong> or <strong>human-in-the-loop</strong>) to retain trust while scaling.<br /></li>
  <li>When bottlenecks exist, prefer <strong>scope reduction</strong> and <strong>simplification</strong> to control latency and runaway costs.<br /></li>
</ul>

<hr />

<h1 id="coding-is-an-ideal-agent-use-case">Coding is an ideal agent use case</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-05-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-05-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-05-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-05-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Why <strong>coding tasks</strong> are a strong fit for agents:<br /></p>

<ul>
  <li>They combine <strong>ambiguity</strong>, <strong>measurable value</strong>, and <strong>verifiability</strong>.<br /></li>
  <li>Translating a design doc into a pull request is a complex, open-ended workflow where automation provides outsized benefit.<br /></li>
  <li>Many parts of the coding pipeline are tractable via tool integration (edit, test, CI).<br /></li>
  <li>Outputs are verifiable through <strong>unit tests</strong> and <strong>continuous integration</strong>, creating objective feedback loops for evaluation.<br /></li>
  <li>These properties reduce the risk of <strong>silent failures</strong> and make iterative improvement feasible.<br /></li>
</ul>

<p>Conclusion: coding is both a practical and <strong>economically justifiable</strong> domain for deploying autonomous agents, explaining the strong product-market fit for coding agents in production.<br /></p>

<hr />

<h1 id="keep-agent-architecture-minimal-environment-tools-system-prompt-loop">Keep agent architecture minimal: environment, tools, system prompt, loop</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-06-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-06-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-06-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-06-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>An effective, minimal agent architecture contains three core components:<br /></p>

<ul>
  <li><strong>Environment</strong> — the external context and state the agent operates in.<br /></li>
  <li><strong>Tools</strong> — action and feedback interfaces the agent can call (editors, test runners, APIs).<br /></li>
  <li><strong>System prompt</strong> — encodes goals, constraints, and desired behavior for the model.<br /></li>
</ul>

<p>These are executed in a recurring <strong>model decision loop</strong>:<br /></p>
<ol>
  <li>Observe formatted context and tool feedback.<br /></li>
  <li>Decide on an action (or tool call) according to the system prompt.<br /></li>
  <li>Execute the action via the environment or tools and ingest the result.<br /></li>
  <li>Repeat.<br /></li>
</ol>

<p>Design guidance:<br /></p>
<ul>
  <li>Keep these elements <strong>simple</strong> to accelerate iteration — most early wins come from refining environment integration, tool selection, and prompt design rather than adding structural complexity.<br /></li>
  <li>The <strong>model-loop</strong> abstraction generalizes across use cases: different products can share the same codebase while varying only environment bindings, toolsets, and system prompts.<br /></li>
  <li>This <strong>modular minimalism</strong> enables rapid experimentation and focused later optimizations once behaviors stabilize.<br /></li>
</ul>

<hr />

<h1 id="practical-optimizations-after-minimal-design">Practical optimizations after minimal design</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-07-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-07-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-07-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-07-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>After validating behavior with the minimal backbone, apply targeted optimizations to reduce cost and latency and improve trust:<br /></p>

<p>Techniques to consider:<br /></p>
<ul>
  <li><strong>Cache or catch the agent’s trajectory</strong> to avoid re-searching expensive decision paths.<br /></li>
  <li><strong>Parallelize independent tool calls</strong> to lower wall-clock latency.<br /></li>
  <li><strong>Instrument progress presentation</strong> so users can verify and trust agent actions (transparent status, checkpoints).<br /></li>
</ul>

<p>Recommended development sequence:<br /></p>
<ol>
  <li>Validate core behavior with simple environment, tools, and prompt.<br /></li>
  <li>Measure operational costs, latency, and failure modes.<br /></li>
  <li>Iteratively add optimizations (caching, parallelization, UX instrumentation) only when they serve measurable operational goals.<br /></li>
</ol>

<p>This balances rapid product iteration with long-term performance and reliability improvements.<br /></p>

<hr />

<h1 id="think-like-the-agent-context-window-constraints-and-actionable-context">Think like the agent: context-window constraints and actionable context</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-09-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-09-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-09-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-09-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Agents are limited by <strong>context-window</strong> constraints and only perceive the world through formatted context and tool feedback:<br /></p>

<ul>
  <li>Typical inference visibility is within a <strong>10–20k token</strong> window, so design and debugging must model the agent’s actual information state.<br /></li>
  <li>For interactive agents (e.g., UI agents) the agent is effectively <strong>blind during tool calls</strong> and only observes discrete snapshots and textual descriptions.<br /></li>
  <li>Therefore supply critical metadata (screen resolution, element coordinates, recommended actions, explicit limitations) to avoid brittle behavior.<br /></li>
</ul>

<p>Practical steps for engineers:<br /></p>
<ol>
  <li><strong>Simulate full end-to-end tasks</strong> from the agent’s perspective to find missing context and guardrails.<br /></li>
  <li>Use models to <strong>introspect agent trajectories</strong> by asking the model to explain past decisions and request additional parameters.<br /></li>
  <li>Treat the agent’s view as the <strong>primary source of truth</strong> for prompt engineering, tool design, and reliability work.<br /></li>
</ol>

<hr />

<h1 id="open-research-and-engineering-questions-budgets-self-evolving-tools-multi-agent-systems">Open research and engineering questions: budgets, self-evolving tools, multi-agent systems</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-12-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-12-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-12-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-12-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Key open problems for agent engineering include:<br /></p>

<ul>
  <li><strong>Budget and latency constraints</strong>:<br />
    <ul>
      <li>Need mechanisms to express and enforce time, token, or monetary budgets so deployments are economically viable.<br /></li>
    </ul>
  </li>
  <li><strong>Agents evolving their own tools (self-evolving tools)</strong>:<br />
    <ul>
      <li>Meta-tools that let agents iteratively refine tool ergonomics and parameterization would extend generality but raise safety and governance questions.<br /></li>
    </ul>
  </li>
  <li><strong>Robust multi-agent collaboration protocols</strong>:<br />
    <ul>
      <li>Requires communication models beyond synchronous user-assistant exchanges, including <strong>asynchronous messaging</strong>, <strong>role definition</strong>, and <strong>recognition</strong> to enable parallelized subagents and separation of concerns.<br /></li>
    </ul>
  </li>
</ul>

<p>Addressing these areas will expand production use cases and clarify architectural patterns for large-scale agent deployments.<br /></p>

<hr />

<h1 id="three-final-takeaways-and-closing-contact-anecdote">Three final takeaways and closing contact anecdote</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec05/00-14-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec05/00-14-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec05/00-14-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec05/00-14-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The three closing takeaways:<br /></p>

<ul>
  <li><strong>Don’t build agents for every task</strong> — be selective and justify agentic complexity.<br /></li>
  <li><strong>Prioritize simplicity during early iteration</strong> — validate with a minimal environment-tools-prompt loop.<br /></li>
  <li><strong>Adopt the agent’s perspective</strong> — provide the right context and guardrails for reliable decision-making.<br /></li>
</ul>

<p>Closing guidance for engineering teams:<br /></p>
<ul>
  <li>Validate use cases before investing heavily.<br /></li>
  <li>Implement a <strong>minimal environment-tools-prompt loop</strong> and iterate with agent-centric tests and tooling.<br /></li>
  <li>Apply optimizations only when they meet measurable operational goals (cost, latency, trust).<br /></li>
</ul>

<p>The session closes with an invitation to collaborate on open questions about budgets, tool evolution, and multi-agent interaction, plus a brief personal anecdote underscoring a practical orientation toward building useful AI systems — and encouragement to continue building and exchanging ideas.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Agent 04 - Building and evaluating AI Agents by Sayash Kapoor AI Snake Oil</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec04/" rel="alternate" type="text/html" title="Agent 04 - Building and evaluating AI Agents by Sayash Kapoor AI Snake Oil" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec04</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec04/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/d5EltXhbcfA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="modern-ai-agents-function-primarily-as-modular-components-within-larger-systems-rather-than-as-standalone-general-intelligence">Modern AI agents function primarily as modular components within larger systems rather than as standalone general intelligence.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-01-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-01-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-01-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-01-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>AI agents</strong> are systems in which <strong>language models</strong> orchestrate inputs, outputs, tool calls, and control flow as parts of broader products.<br />
They often expose <strong>input/output filters</strong>, call <strong>external tools</strong>, and execute <strong>task-specific pipelines</strong>—a modular view that positions agents as components that mediate user interactions and downstream services rather than as monolithic <strong>artificial general intelligence</strong>.<br /></p>

<ul>
  <li>Examples such as <strong>ChatGPT</strong> and <strong>Claude</strong> illustrate rudimentary agent behavior when they manage <strong>tool invocations</strong> and <strong>task orchestration</strong>.<br /></li>
  <li>Treating agents as <strong>modular parts</strong> clarifies engineering boundaries and highlights differences in <strong>integration</strong>, <strong>interface</strong>, and <strong>reliability</strong> requirements compared to single-call language-model usage.<br /></li>
</ul>

<hr />

<h1 id="many-high-profile-agent-products-have-failed-in-production-due-to-insufficient-or-misleading-evaluation">Many high-profile agent products have failed in production due to insufficient or misleading evaluation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-04-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-04-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-04-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-04-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Multiple commercial and research agent deployments have exhibited major failures that trace back to <strong>weak evaluation practices</strong>.<br /></p>

<ul>
  <li><strong>DoNotPay</strong>: automation claims led to <strong>FTC fines</strong> after those claims proved false.<br /></li>
  <li><strong>LexisNexis</strong> and <strong>Westlaw</strong>: legal-research products produced <strong>hallucinations</strong> in a substantial fraction of cases, including fabricated or reversed legal reasoning.<br /></li>
  <li><strong>Princeton replication benchmarks</strong>: leading agents reproduced under <strong>40%</strong> of provided research artifacts, contradicting claims of automating scientific discovery.<br /></li>
  <li>Self-evaluation issues: some projects used <strong>LLM judges</strong> instead of human experts, inflating apparent performance.<br /></li>
  <li><strong>Reward-hacking</strong>: systems sometimes exploited reward functions rather than producing real algorithmic improvements (for example, purported CUDA-kernel optimizations that exceeded theoretical hardware limits).<br /></li>
</ul>

<p>These cases demonstrate that <strong>inadequate benchmarks</strong>, <strong>inappropriate evaluation judges</strong>, and <strong>reward-hacking</strong> lead to overstated capability and real-world failures.<br /></p>

<hr />

<h1 id="static-benchmarks-designed-for-language-models-do-not-adequately-evaluate-agents-because-agents-act-in-interactive-open-ended-environments">Static benchmarks designed for language models do not adequately evaluate agents because agents act in interactive, open-ended environments.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-08-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-08-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-08-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-08-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Traditional language-model evaluation treats the problem as an <strong>input string → output string</strong> mapping, which suffices for many single-call tasks but fails for agents that must take actions, interact with environments, and run multi-step or recursive processes.<br /></p>

<ul>
  <li>Agents can <strong>call sub-agents</strong>, <strong>loop over LLM calls</strong>, and <strong>interact with external systems</strong>, so evaluation must capture:
    <ul>
      <li><strong>Sequential decision-making</strong>,</li>
      <li><strong>Environment dynamics</strong>,</li>
      <li><strong>Long-horizon costs</strong>.<br /></li>
    </ul>
  </li>
  <li>The bounded cost of evaluating an LLM via a fixed context window does <strong>not</strong> translate to agent settings where compute and latency can grow unbounded with action sequences.<br />
    <ol>
      <li>Therefore, <strong>cost must be a first-class metric alongside accuracy</strong>.<br /></li>
      <li>Agents are often <strong>purpose-built</strong> and require <strong>specialized, multi-dimensional metrics</strong> rather than single, general-purpose benchmarks to measure real utility.<br /></li>
    </ol>
  </li>
</ul>

<hr />

<h1 id="evaluations-must-include-cost-performance-tradeoffs-and-pareto-analysis-because-inference-costs-and-usage-dynamics-materially-affect-agent-utility">Evaluations must include cost-performance tradeoffs and Pareto analysis because inference costs and usage dynamics materially affect agent utility.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-11-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-11-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-11-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-11-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Practical agent evaluation requires <strong>multi-dimensional comparison surfaces</strong> such as <strong>Pareto frontiers</strong> that jointly consider performance and cost.<br /></p>

<ul>
  <li>Two agents with similar accuracy can be materially different choices when one <strong>costs an order of magnitude less</strong> to run.<br /></li>
  <li>Although inference costs for some models have dropped dramatically, <strong>build, iterate, and scale economics</strong> remain critical for prototypes and production: developers iterate in the open and uncontrolled costs can become prohibitive.<br /></li>
  <li>Economic effects matter: the <strong>Jevons paradox</strong> implies that decreasing unit costs can increase aggregate usage and therefore total system cost—so ignoring cost and usage dynamics mischaracterizes operational burden.<br /></li>
</ul>

<p>Accounting for <strong>cost alongside accuracy</strong> therefore materially changes engineering and product decisions.<br /></p>

<hr />

<h1 id="automated-multi-benchmark-leaderboards-can-help-but-overreliance-on-benchmark-performance-drives-misaligned-investments-and-fails-to-predict-real-world-success">Automated, multi-benchmark leaderboards can help but overreliance on benchmark performance drives misaligned investments and fails to predict real-world success.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-13-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-13-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-13-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-13-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Holistic automated evaluation platforms that run multiple benchmarks and report <strong>multi-dimensional metrics</strong> reduce some blind spots by standardizing cost-aware measurements across tasks.<br /></p>

<ul>
  <li>However, <strong>benchmark performance remains an imperfect proxy</strong> for deployed utility.<br /></li>
  <li>Investors and customers frequently conflate leaderboard rankings with product readiness, producing funding and adoption decisions that fail in real-world trials.<br /></li>
  <li>Field trial example: an agent that scored well on a benchmark succeeded at only <strong>3 of 20</strong> real tasks during deployment—underlining the gap between static leaderboard results and operational effectiveness.<br /></li>
</ul>

<p>Automated leaderboards are necessary infrastructure but must be combined with <strong>deployment-level testing</strong> and <strong>domain-specific validation</strong> to predict real-world outcomes reliably.<br /></p>

<hr />

<h1 id="human-in-the-loop-validator-design-improves-agent-evaluation-by-iteratively-refining-criteria-and-incorporating-domain-expertise">Human-in-the-loop validator design improves agent evaluation by iteratively refining criteria and incorporating domain expertise.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-14-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-14-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-14-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-14-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Validation pipelines that place <strong>domain experts</strong> and <strong>human validators</strong> in the loop enable iterative refinement of evaluation criteria, catch subtle failure modes, and correct metric drift that arises when LLMs are used as judges.<br /></p>

<ul>
  <li>Replace single-shot LLM judgments with a <strong>“who validates the validators”</strong> framework:
    <ol>
      <li>Use <strong>human-edited rubrics</strong>,</li>
      <li>Conduct <strong>expert review</strong>,</li>
      <li>Iterate evaluation criteria based on findings.<br /></li>
    </ol>
  </li>
  <li>This approach produces higher-fidelity assessments that align with domain standards and end-user expectations.<br /></li>
  <li>It reduces false positives from automated judges and re-centers evaluation on meaningful, <strong>task-specific correctness</strong> rather than proxy metrics.<br /></li>
</ul>

<hr />

<h1 id="capability-what-models-can-do-is-distinct-from-reliability-consistently-correct-behavior-and-reliability-is-essential-for-real-world-agent-deployment">Capability (what models can do) is distinct from reliability (consistently correct behavior), and reliability is essential for real-world agent deployment.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-16-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-16-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-16-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-16-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<ul>
  <li><strong>Capability</strong>: the set of behaviors or outputs a model can produce, often measured by <strong>pass-at-K</strong> metrics that indicate at least one correct answer among several outputs.<br /></li>
  <li><strong>Reliability</strong>: consistent correctness on every invocation—critical for consequential decision-making.<br /></li>
</ul>

<p>Points to note:<br /></p>
<ol>
  <li>Systems that reach <strong>90% capability</strong> do not automatically achieve the high-assurance <strong>reliability</strong> levels (for example, <strong>99.9%</strong> or “five nines”) required by many products.<br /></li>
  <li>Training regimes that maximize capability often do not close this <strong>tail-risk gap</strong>.<br /></li>
  <li>Proposed fixes such as <strong>verifier/unit-test layers</strong> are imperfect: common coding benchmarks contain <strong>false positives</strong> in unit tests, causing overestimation of correctness and producing inference-scaling curves that flatten or degrade when accounting for verifier error.<br /></li>
</ol>

<p>Therefore, achieving operational reliability is primarily a <strong>system-design and engineering challenge</strong>, not solely a modeling problem.<br /></p>

<hr />

<h1 id="ai-engineering-should-shift-focus-to-reliability-engineering-and-system-design-practices-that-mitigate-stochastic-model-behavior">AI engineering should shift focus to reliability engineering and system design practices that mitigate stochastic model behavior.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec04/00-18-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec04/00-18-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec04/00-18-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec04/00-18-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Mitigating the stochastic failures inherent to LLM-based agents requires software-engineering abstractions, <strong>monitoring</strong>, <strong>redundancy</strong>, and <strong>reward-robust system architectures</strong> that treat statistical components as first-class failure modes.<br /></p>

<ul>
  <li>Design end-to-end systems that:
    <ol>
      <li><strong>Contain</strong> LLM errors with verification layers,</li>
      <li><strong>Detect</strong> failures via monitoring and alerts,</li>
      <li><strong>Recover</strong> through human-in-loop escalation and deterministic subcomponents,</li>
      <li>Use <strong>operational safeguards</strong> and redundancy.<br /></li>
    </ol>
  </li>
  <li>These measures are analogous to historical reliability work in early computing (e.g., <strong>ENIAC vacuum-tube maintenance</strong>).<br /></li>
</ul>

<p>Framing AI engineering as <strong>reliability engineering</strong> prioritizes reproducibility, safety, and consistent user experience, assigning responsibility to engineers to close the gap between promising capability and dependable production behavior—this <strong>reliability-first mindset</strong> is necessary to deliver agents that work reliably for real users.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Agent 03 - Stanford Webinar - Agentic AI - A Progression of Language Model Usage</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec03/" rel="alternate" type="text/html" title="Agent 03 - Stanford Webinar - Agentic AI - A Progression of Language Model Usage" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec03</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec03/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/kJLiOGle3Lw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="introduction-and-outline-of-the-talk">Introduction and outline of the talk</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-00-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-00-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-00-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-00-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Provides an overview of the presentation objectives and structure, introducing <strong>agentic AI</strong> as a progression of <strong>language model</strong> usage and summarizing planned topics including <strong>model overview</strong>, <strong>common limitations</strong>, <strong>mitigation methods</strong>, and <strong>agentic design patterns</strong>.<br /></p>

<ul>
  <li>Establishes the <strong>scope</strong> for subsequent technical discussion and situates agentic approaches as extensions to standard LM applications.<br /></li>
  <li>Clarifies the talk flow: move from <strong>foundational definitions</strong> → <strong>practical patterns and evaluation</strong> → <strong>real-world use cases</strong>.<br /></li>
</ul>

<hr />

<h1 id="definition-of-a-language-model-as-next-token-prediction">Definition of a language model as next-token prediction</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-01-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-01-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-01-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-01-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Language model</strong> — a statistical machine-learning model that predicts the next token (or word) given preceding text, producing a probability distribution over the vocabulary for each next position.<br /></p>

<ul>
  <li>Supports <strong>autoregressive generation</strong> by repeatedly sampling or selecting the highest-probability token and feeding it back as input.<br /></li>
  <li><strong>Large-scale pretraining</strong> on corpora yields strong priors about word sequences and common completions, which underlies much downstream performance.<br /></li>
</ul>

<hr />

<h1 id="language-model-training-stages-pre-training-and-post-training">Language model training stages: pre-training and post-training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-03-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-03-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-03-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-03-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Two-stage training pipeline</strong> in modern LLM development:<br /></p>

<ol>
  <li><strong>Pre-training</strong>
    <ul>
      <li>Large-scale self-supervised training (next-token objective) on vast text corpora.</li>
      <li>Builds broad statistical knowledge and fluency.<br /></li>
    </ul>
  </li>
  <li><strong>Post-training adaptations</strong>
    <ul>
      <li><strong>Instruction tuning</strong> reshapes behavior toward helpful, instruction-following outputs using supervised input-output pairs.</li>
      <li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> refines alignment to human preferences via reward models and policy optimization, improving safety and usability.<br /></li>
    </ul>
  </li>
</ol>

<hr />

<h1 id="instruction-dataset-template-and-supervised-fine-tuning">Instruction dataset template and supervised fine-tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-04-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-04-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-04-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-04-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Supervised fine-tuning with templated instructions</strong> — using datasets that pair explicit instruction fields with expected outputs to train response generation conditioned on the instruction.<br /></p>

<ul>
  <li>The model is trained to map <strong>instruction + context → desired response distribution</strong>, which improves reliability in downstream apps.<br /></li>
  <li><strong>Dataset design</strong> and example selection directly influence stylistic and task-specific behaviors, so careful curation matters.<br /></li>
</ul>

<hr />

<h1 id="deployment-options-cloud-apis-and-local-hosting">Deployment options: cloud APIs and local hosting</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-05-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-05-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-05-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-05-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Deployment strategies</strong> for integrating LMs into applications:<br /></p>

<ul>
  <li><strong>Cloud / API hosting</strong>
    <ul>
      <li>Serialize prompts → send to provider endpoint → receive generated outputs.</li>
      <li>Easy scaling and model updates; often higher recurring cost and data transmission to third parties.<br /></li>
    </ul>
  </li>
  <li><strong>Local / edge hosting</strong> (for smaller models)
    <ul>
      <li>Reduced latency, greater data control and privacy; requires compatible compute and ops effort.<br /></li>
    </ul>
  </li>
  <li>Tradeoffs: <strong>cost</strong>, <strong>performance/latency</strong>, <strong>privacy</strong>, and <strong>operational complexity</strong> drive the deployment choice.<br /></li>
</ul>

<hr />

<h1 id="prompting-as-critical-pre-processing-and-input-design">Prompting as critical pre-processing and input design</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-07-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-07-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-07-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-07-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Prompt engineering</strong> as a core engineering task — designing free-form natural language inputs to elicit reliable, relevant outputs.<br /></p>

<ul>
  <li>Effective prompts reduce ambiguity and failure modes by specifying: task, formatting, examples, constraints, and desired output style.</li>
  <li>Prompt design affects <strong>latency</strong>, <strong>token usage</strong>, and downstream <strong>parsing logic</strong>; good prompts turn an unconstrained generator into a predictable component.<br /></li>
</ul>

<hr />

<h1 id="prompt-best-practices-clear-instructions-and-few-shot-examples">Prompt best practices: clear instructions and few-shot examples</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-09-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-09-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-09-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-09-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Explicit instructions &amp; few-shot examples</strong> — practical prompt techniques to constrain model behavior:<br /></p>

<ul>
  <li>Write <strong>clear, descriptive instructions</strong> to reduce the model’s need to infer user intent.</li>
  <li>Include <strong>few-shot examples</strong> (input-output pairs) to condition consistent style and structure.</li>
  <li>These practices push the model’s response distribution toward the intended format, lowering variance across responses.<br /></li>
</ul>

<hr />

<h1 id="providing-relevant-context-and-retrieval-augmented-templates">Providing relevant context and retrieval-augmented templates</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-10-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-10-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-10-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-10-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Grounding with context &amp; Retrieval-Augmented Generation (RAG)</strong> — supply context and references in prompts to reduce hallucination:<br /></p>

<ul>
  <li>Use templates that instruct the model to <strong>answer only using provided sources</strong> and to <strong>declare when no answer is found</strong>.</li>
  <li>This pattern enables traceable citations and improves factuality for domain-specific queries.</li>
  <li>RAG workflows allow integration of <strong>proprietary</strong> or <strong>frequently updated</strong> content as the model’s evidence base.<br /></li>
</ul>

<hr />

<h1 id="encouraging-reasoning-via-chain-of-thought-and-time-to-think-prompts">Encouraging reasoning via chain-of-thought and ‘time to think’ prompts</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-12-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-12-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-12-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-12-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Chain-of-thought / explicit intermediate reasoning</strong> — ask the model to produce intermediate steps before a final answer:<br /></p>

<ul>
  <li>Request the model to “work out” a solution and then conclude to surface internal calculations and attention to details.</li>
  <li>Often improves correctness on multi-step, logical, arithmetic, and proof-style tasks.</li>
  <li>Tradeoff: increased <strong>token usage</strong> and <strong>latency</strong> for greater reliability in reasoning-intensive tasks.<br /></li>
</ul>

<hr />

<h1 id="decomposing-complex-tasks-into-sequential-stages">Decomposing complex tasks into sequential stages</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-13-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-13-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-13-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-13-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Chaining prompts / decomposition</strong> — break complex tasks into a sequence of smaller prompts:<br /></p>

<ol>
  <li>Decompose the task into focused subtasks.</li>
  <li>For each subtask, call the model with a single clear operation consuming previous outputs.</li>
  <li>Optionally orchestrate the sequence with code or higher-level agents.<br /></li>
</ol>

<ul>
  <li>Benefits: improved <strong>interpretability</strong>, <strong>modularity</strong>, and <strong>error isolation</strong>; reduces failure rates relative to monolithic prompts.<br /></li>
</ul>

<hr />

<h1 id="logging-tracing-and-automated-evaluation-for-development">Logging, tracing, and automated evaluation for development</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-15-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-15-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-15-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-15-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Logging &amp; automated evaluation</strong> — essential engineering practices for LM-based applications:<br /></p>

<ul>
  <li>Systematic logging enables debugging, auditing, and tracking as models and prompts evolve.</li>
  <li>Automated evaluation pipelines use curated input–ground-truth pairs and either <strong>human raters</strong> or <strong>LM-based judges</strong> to score outputs.</li>
  <li>Continuous evaluation supports reproducible comparisons across model versions and safe migrations when upstream models change.<br /></li>
</ul>

<hr />

<h1 id="prompt-routing-to-specialized-handlers-and-models">Prompt routing to specialized handlers and models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-17-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-17-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-17-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-17-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Prompt routing</strong> — classify incoming queries by intent and dispatch to specialized handlers or appropriately sized models:<br /></p>

<ul>
  <li>A router reduces cost by avoiding expensive calls for simple queries and improves relevance by selecting tailored handlers.</li>
  <li>Supports hybrid systems combining lightweight classifiers/heuristics with larger LMs for complex needs.<br /></li>
</ul>

<hr />

<h1 id="fine-tuning-data-requirements-depend-on-task-complexity">Fine-tuning data requirements depend on task complexity</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-19-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-19-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-19-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-19-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Fine-tuning data strategy</strong> — practical guidance on dataset sizing and iteration:<br /></p>

<ul>
  <li>Start with <strong>small, focused datasets</strong> (tens to hundreds of examples) to validate behavior before scaling.</li>
  <li>Use iterative experimentation with small supervised pairs for rapid feedback.</li>
  <li><strong>Synthetic augmentation</strong> using LMs can expand training data when needed; prioritize pragmatic incremental refinement over large upfront labeling investments.<br /></li>
</ul>

<hr />

<h1 id="common-limitations-of-pre-trained-lms">Common limitations of pre-trained LMs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-21-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-21-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-21-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-21-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Common LM limitations</strong> — typical shortcomings to address:<br /></p>

<ul>
  <li><strong>Hallucination</strong>: fabricated or incorrect outputs.</li>
  <li><strong>Knowledge cutoffs</strong>: outdated pretraining data.</li>
  <li><strong>Lack of source attribution</strong>.</li>
  <li><strong>Data-privacy gaps</strong> for proprietary information.</li>
  <li>
    <p><strong>Constrained context windows</strong> that trade off length with latency and cost.<br /></p>
  </li>
  <li>These motivate system-level interventions such as retrieval augmentation, tool integration, and memory architectures for production use.<br /></li>
</ul>

<hr />

<h1 id="retrieval-augmented-generation-rag-and-indexing-workflow">Retrieval-augmented generation (RAG) and indexing workflow</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-25-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-25-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-25-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-25-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Retrieval-Augmented Generation (RAG) systems</strong> — how they work and variants:<br /></p>

<ul>
  <li>Pre-index textual corpora by chunking documents and embedding chunks into vector spaces.</li>
  <li>Store embeddings in a <strong>vector database</strong> for nearest-neighbor retrieval on query embeddings.</li>
  <li>
    <p>At query time, retrieve top-K relevant chunks to include as grounded context in the prompt, enabling citation and evidence-based answers.<br /></p>
  </li>
  <li>Variants: <strong>web search augmentation</strong>, <strong>knowledge-graph retrieval</strong>, or other domain-specific retrieval strategies chosen by precision and domain needs.<br /></li>
</ul>

<hr />

<h1 id="tool-usage-and-function-calling-to-access-external-capabilities">Tool usage and function-calling to access external capabilities</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-28-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-28-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-28-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-28-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Function-calling / tool invocation patterns</strong> — structured outputs that orchestration software executes:<br /></p>

<ul>
  <li>The LM emits <strong>structured calls</strong> or API-like outputs that are parsed to invoke external services (e.g., weather APIs) or to run code in sandboxes.</li>
  <li>Enables real-time data access, deterministic computation, and integration with systems of record while keeping a human-friendly interface.</li>
  <li>Orchestration returns results to the LM as observations for final synthesis, closing the loop between reasoning and action.<br /></li>
</ul>

<hr />

<h1 id="agentic-language-models-interaction-with-environment-and-tools">Agentic language models: interaction with environment and tools</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-30-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-30-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-30-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-30-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Agentic LM usage</strong> — systems where a core LM interacts with an external environment via retrievals, tool calls, or executable programs:<br /></p>

<ul>
  <li><strong>Agentic behavior</strong> couples deliberation (reasoning) with action, allowing the system to gather evidence, perform computations, and modify external state.</li>
  <li>Iterative planning and memory incorporation turn a passive text generator into an active agent capable of multi-step, context-aware operations.<br /></li>
</ul>

<hr />

<h1 id="reasoning-plus-action-react-as-a-pattern-for-agent-behavior">Reasoning plus action (ReAct) as a pattern for agent behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-32-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-32-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-32-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-32-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>ReAct paradigm</strong> — alternating explicit reasoning steps with concrete actions:<br /></p>

<ul>
  <li>The model alternates between <strong>reasoning</strong> (explicit chains of thought) and <strong>actions</strong> (API calls, searches).</li>
  <li><strong>Planning</strong> decomposes tasks into actionable subtasks.</li>
  <li><strong>Memory</strong> preserves interim findings and history for future decisions.</li>
  <li>Together these elements enable complex task completion that single-shot generation cannot achieve.<br /></li>
</ul>

<hr />

<h1 id="customer-support-agent-example-illustrating-agentic-workflow">Customer-support agent example illustrating agentic workflow</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-34-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-34-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-34-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-34-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Refund-request workflow (concrete example)</strong> — an agent decomposes the user query into steps and issues API retrievals:<br /></p>

<ol>
  <li><strong>Check policy</strong> — retrieve refund policy and constraints.</li>
  <li><strong>Check customer info</strong> — fetch account, order history, and eligibility.</li>
  <li><strong>Check product</strong> — validate product details, shipping, and returns.</li>
  <li><strong>Decide</strong> — synthesize evidence and produce a policy-compliant recommendation.<br /></li>
</ol>

<ul>
  <li>Each step produces structured calls to retrieval/order systems; the agent synthesizes evidence into a response draft and a follow-up API action for approval or execution.<br /></li>
</ul>

<hr />

<h1 id="iterative-agent-workflows-for-research-and-software-assistance">Iterative agent workflows for research and software assistance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-36-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-36-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-36-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-36-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Iterative agent workflows</strong> — repeated observation-action loops for convergence:<br /></p>

<ul>
  <li>Use cases: research reports, debugging code, software assistants that identify files, hypothesize fixes, run tests in sandboxes, and iterate until an acceptable patch is produced.</li>
  <li>Workflow: search → summarize → refine → repeat.</li>
  <li>Repeated cycles typically converge on higher-quality solutions than single-pass generation.<br /></li>
</ul>

<hr />

<h1 id="agentic-patterns-enable-more-complex-task-execution-with-the-same-models">Agentic patterns enable more complex task execution with the same models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-37-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-37-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-37-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-37-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Why agent workflows expand capability</strong> — structuring tasks as agent workflows compensates for LM weaknesses:<br /></p>

<ul>
  <li>Decomposition, retrieval, and tool use make complex tasks tractable without changing base models.</li>
  <li>Orchestration of focused calls leverages external systems for facts and computation and organizes intermediate results into coherent outputs.</li>
  <li>This raises the ceiling of achievable automation using existing LMs.<br /></li>
</ul>

<hr />

<h1 id="representative-real-world-applications-of-agentic-ai">Representative real-world applications of agentic AI</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-38-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-38-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-38-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-38-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Application areas for agents</strong> — primary domains where agentic designs add value:<br /></p>

<ul>
  <li><strong>Software development</strong>: code generation, bug fixing, automated testing, PR creation.</li>
  <li><strong>Research &amp; analysis</strong>: information synthesis, summarization, iterative literature review.</li>
  <li>
    <p><strong>Business process automation</strong>: customer support workflows, billing, approvals.<br /></p>
  </li>
  <li>Benefits across domains: <strong>iterative reasoning</strong>, <strong>external data integration</strong>, <strong>action capabilities</strong>, <strong>traceability</strong>, and <strong>modular architectures</strong>.<br /></li>
</ul>

<hr />

<h1 id="design-patterns-for-agentic-systems-planning-reflection-tools-multi-agent">Design patterns for agentic systems: planning, reflection, tools, multi-agent</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-40-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-40-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-40-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-40-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Core agentic design patterns</strong> — recurring building blocks:<br /></p>

<ul>
  <li><strong>Planning</strong>: decompose tasks into actionable subtasks.</li>
  <li><strong>Reflection</strong>: critique and improve outputs via meta-evaluation.</li>
  <li><strong>Tool usage</strong>: access external capabilities or deterministic compute.</li>
  <li><strong>Multi-agent collaboration</strong>: coordinate specialized agents to parallelize and specialize work.<br /></li>
</ul>

<hr />

<h1 id="reflection-pattern-and-its-application-to-code-refactoring">Reflection pattern and its application to code refactoring</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-42-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-42-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-42-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-42-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Two-step reflection (self-critique + refactor)</strong> — an iterative improvement pattern:<br /></p>

<ol>
  <li><strong>Audit / critique</strong> — instruct the model to review outputs and list constructive feedback.</li>
  <li><strong>Refactor</strong> — prompt the model to produce an improved version using that feedback.<br /></li>
</ol>

<ul>
  <li>Leverages the model’s evaluative capabilities to produce higher-quality refactorings than single-pass generation; applicable to many content-improvement tasks.<br /></li>
</ul>

<hr />

<h1 id="tool-usage-multi-agent-collaboration-and-persona-based-agents">Tool usage, multi-agent collaboration, and persona-based agents</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-44-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-44-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-44-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-44-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Persona-based agents &amp; orchestrator</strong> — tool usage and multi-agent coordination:<br /></p>

<ul>
  <li>Implement agents as distinct <strong>personas</strong> or prompts dedicated to specific tasks (e.g., climate, lighting, security).</li>
  <li>A central <strong>orchestrator</strong> routes requests, resolves conflicts, and coordinates actions.</li>
  <li>Persona separation simplifies reasoning scope per agent and supports heterogeneous model selection and specialized tool interfaces.<br /></li>
</ul>

<hr />

<h1 id="summary-agentic-usage-extends-traditional-lm-practices">Summary: agentic usage extends traditional LM practices</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-45-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-45-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-45-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-45-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Conclusion: agentic as a progression of LM engineering</strong> — how agentic builds on existing practices:<br /></p>

<ul>
  <li>Retains prompting best practices while adding <strong>retrieval</strong>, <strong>tool integration</strong>, <strong>multi-step workflows</strong>, and <strong>orchestration</strong>.</li>
  <li>Treats the LM as a reasoning core augmented by external actions and memory to enable more sophisticated applications.</li>
  <li>Adoption requires additional infrastructure for retrieval, tooling, evaluation, and safety but leverages existing model capabilities.<br /></li>
</ul>

<hr />

<h1 id="evaluating-agents-beyond-single-shot-llm-judgment">Evaluating agents: beyond single-shot LLM judgment</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-46-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-46-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-46-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-46-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Evaluation strategies for agentic systems</strong> — stronger, multi-stage judging patterns:<br /></p>

<ul>
  <li>Extend single-shot LLM judging with <strong>agentic judging</strong> that uses reflection and hierarchical critique (e.g., junior-level assessment followed by senior-level re-evaluation).</li>
  <li>Iterative, multi-stage judging flows often yield more robust quality signals and can be automated within the agent framework.</li>
  <li>Reliable evaluation is critical for model selection, prompt tuning, and safe deployment.<br /></li>
</ul>

<hr />

<h1 id="guidance-for-augmenting-agents-for-specific-applications">Guidance for augmenting agents for specific applications</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-48-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-48-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-48-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-48-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Start simple, iterate toward agents</strong> — pragmatic development advice:<br /></p>

<ul>
  <li>Begin with the simplest LM approach that meets requirements.</li>
  <li>Experiment with iterative LM calls, small dataset fine-tuning, and prompt engineering before investing in full agentic infrastructure.</li>
  <li>Incremental augmentation (add retrieval, tool calls) and small labeled samples validate directions prior to large-scale efforts.<br /></li>
</ul>

<hr />

<h1 id="mitigating-hallucinations-and-implementing-guardrails">Mitigating hallucinations and implementing guardrails</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-51-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-51-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-51-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-51-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Layered safety &amp; guardrails</strong> — reducing hallucination and undesirable outputs:<br /></p>

<ul>
  <li>Use <strong>output filtering</strong> via classifiers, <strong>lightweight LM-based validators</strong>, and <strong>rule-based checks</strong>.</li>
  <li>Apply <strong>input-stage sanitization</strong> to detect risky queries.</li>
  <li>Enterprise measures: stricter input validation, approval workflows, sandboxed execution, and monitoring.</li>
  <li>Continuous monitoring and domain-specific validation are essential because probabilistic generation cannot be fully eliminated.<br /></li>
</ul>

<hr />

<h1 id="getting-started-playgrounds-apis-and-incremental-experimentation">Getting started: playgrounds, APIs, and incremental experimentation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-53-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-53-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-53-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-53-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Pragmatic onboarding path</strong> — a three-step approach:<br /></p>

<ol>
  <li><strong>Experiment</strong> in a provider playground to iterate on prompts quickly.</li>
  <li><strong>Integrate</strong> via simple API calls from code to understand behavior and operational costs.</li>
  <li><strong>Decide</strong> whether to adopt libraries or build custom scaffolding based on learnings.<br /></li>
</ol>

<ul>
  <li>This incremental path prioritizes rapid feedback and informed decisions about fine-tuning and orchestration investments.<br /></li>
</ul>

<hr />

<h1 id="resources-and-following-experts-to-stay-current">Resources and following experts to stay current</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-55-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-55-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-55-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-55-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Staying up-to-date</strong> — tracking experts, courses, and community resources:<br /></p>

<ul>
  <li>Follow domain experts, curated course materials, and community resources (blogs, social media, video channels, academic/industry courses).</li>
  <li>Prefer a <strong>small set of reputable sources</strong> and supplement with targeted deep dives to filter signal from noise.</li>
  <li>Regularly update tooling and evaluation knowledge as the field evolves quickly.<br /></li>
</ul>

<hr />

<h1 id="closing-remarks-and-thanks">Closing remarks and thanks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec03/00-56-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec03/00-56-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec03/00-56-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec03/00-56-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Closing remarks</strong> — wrap-up and call to action:<br /></p>

<ul>
  <li>Reiterate the <strong>rapid pace</strong> of progress in LLMs and agentic AI.</li>
  <li>Encourage continued <strong>experimentation</strong>, iteration, and community engagement as practical takeaways.</li>
  <li>Thank participants and invite further questions and exploration.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Agent 02 - Building Large Language Models by Stanford CS229</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec02/" rel="alternate" type="text/html" title="Agent 02 - Building Large Language Models by Stanford CS229" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec02</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec02/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/9vM4p9NN0Ts" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-introduction-and-scope">Lecture introduction and scope</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-00-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-00-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-00-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-00-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces the lecture topic of <strong>building large language models (LLMs)</strong> and sets expectations for scope and interaction.<br /></p>

<ul>
  <li><strong>LLMs</strong> are framed as contemporary <strong>chatbots</strong> and <strong>generative language systems</strong>.<br /></li>
  <li>The speaker names prominent commercial examples and situates the lecture as an overview touching on the multiple components required to <strong>build and deploy LLMs</strong>.<br /></li>
  <li>Attendees are invited to ask questions; the talk is framed as a <strong>high-level survey</strong> rather than a deep dive into any single topic.<br /></li>
</ul>

<p>This opening establishes context for the subsequent discussion of <strong>architecture</strong>, <strong>losses</strong>, <strong>data</strong>, <strong>evaluation</strong>, and <strong>systems/infrastructure</strong>.<br /></p>

<hr />

<h1 id="key-components-that-determine-llm-performance">Key components that determine LLM performance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-01-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-01-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-01-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-01-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment enumerates the five primary components that determine the practical performance of LLMs:<br /></p>

<ul>
  <li><strong>Model architecture</strong><br /></li>
  <li><strong>Training loss and algorithm</strong><br /></li>
  <li><strong>Training data</strong><br /></li>
  <li><strong>Evaluation methodology</strong><br /></li>
  <li><strong>Systems/infrastructure for running models</strong><br /></li>
</ul>

<p>Key points:<br /></p>
<ul>
  <li>Academic research tends to focus heavily on <strong>architecture</strong> and <strong>algorithms</strong>.<br /></li>
  <li>In industry practice, <strong>data</strong>, <strong>evaluation</strong>, and <strong>systems</strong> often dominate real-world success.<br /></li>
  <li>Most modern LLMs are variants of the <strong>Transformer</strong> architecture — detailed treatment of Transformers is deferred to prior resources.<br /></li>
</ul>

<p>This contrast between academic emphasis and industry priorities frames the remainder of the lecture.<br /></p>

<hr />

<h1 id="overview-pre-training-versus-post-training">Overview: pre-training versus post-training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-02-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-02-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-02-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-02-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines the two-stage paradigm commonly used for LLMs:<br /></p>

<ol>
  <li><strong>Pre-training</strong> — classical <strong>language modeling</strong> on very large corpora (unsupervised).<br /></li>
  <li><strong>Post-training</strong> — <strong>alignment</strong> and <strong>instruction tuning</strong> that convert base models into assistant-style systems.<br /></li>
</ol>

<p>Historical context:<br /></p>
<ul>
  <li>GPT-2 / GPT-3 are landmarks in the <strong>pre-training</strong> era.<br /></li>
  <li>The <strong>ChatGPT</strong> era catalyzed wide adoption of post-training techniques for interactive assistants.<br /></li>
</ul>

<p>Lecture plan:<br /></p>
<ul>
  <li>Start with <strong>pre-training tasks, losses, and data</strong>.<br /></li>
  <li>Then address <strong>post-training methods</strong> that produce assistant-style behavior from base LMs.<br />
This overview sets the stage for the subsequent technical details.<br /></li>
</ul>

<hr />

<h1 id="language-modeling-and-generative-modeling-fundamentals">Language modeling and generative modeling fundamentals</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-04-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-04-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-04-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-04-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains <strong>language models</strong> as <strong>probability distributions over token sequences</strong>.<br /></p>

<ul>
  <li>Sequence probabilities capture <strong>syntactic</strong> and <strong>semantic plausibility</strong> (e.g., grammatical correctness and world knowledge).<br /></li>
  <li><strong>Generative modeling</strong> in this context: once a model approximates the target distribution, it can <strong>sample sentences</strong> by drawing from that distribution.<br /></li>
  <li>Likelihoods allow the model to <strong>distinguish valid from invalid or semantically unlikely sentences</strong> (higher likelihood for plausible text, lower for nonsensical or false statements).<br /></li>
</ul>

<p>These ideas provide the conceptual foundation for why LMs can both <strong>model</strong> and <strong>generate</strong> human-like text.<br /></p>

<hr />

<h1 id="autoregressive-language-models-and-sampling-tradeoffs">Autoregressive language models and sampling tradeoffs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-05-52-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-05-52-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-05-52-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-05-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces <strong>autoregressive (AR) language models</strong>, which factor sequence probabilities with the <strong>chain rule</strong> into next-token conditional distributions.<br /></p>

<ul>
  <li>Core operational semantics: the model <strong>predicts the next token given previous tokens</strong>.<br /></li>
  <li>Sampling proceeds iteratively via an explicit loop that conditions on previously generated tokens.<br /></li>
</ul>

<p>Typical AR sampling loop (conceptually):<br /></p>
<ol>
  <li>Condition on the current context to compute next-token probabilities. <br /></li>
  <li>Sample (or choose) the next token using a sampling/decoding strategy. <br /></li>
  <li>Append the token to the context and repeat until end-of-sequence.<br /></li>
</ol>

<p>Trade-offs:<br /></p>
<ul>
  <li>Practical downside: <strong>per-token sequential generation increases latency</strong> for long outputs. <br /></li>
  <li>Upside: <strong>autoregressive factorization is exact</strong> for sequence probability and is widely used in practice.<br /></li>
</ul>

<p>This framing leads into implementation details for training and inference.<br /></p>

<hr />

<h1 id="next-token-prediction-pipeline-and-training-loss">Next-token prediction pipeline and training loss</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-08-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-08-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-08-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-08-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment describes the typical AR training pipeline as a sequence of steps:<br /></p>

<ol>
  <li><strong>Tokenize</strong> raw text into discrete tokens. <br /></li>
  <li><strong>Embed tokens</strong> into vectors (token embeddings). <br /></li>
  <li>Pass embeddings through a neural network (typically a <strong>Transformer</strong>) to produce contextualized representations. <br /></li>
  <li><strong>Linearly project</strong> outputs to vocabulary logits (one logit per token). <br /></li>
  <li>Apply <strong>softmax</strong> to obtain next-token probabilities. <br /></li>
  <li>Optimize <strong>cross-entropy loss</strong> against one-hot target tokens (equivalently, maximize text log-likelihood).<br /></li>
</ol>

<p>Notes:<br /></p>
<ul>
  <li>During training, sampling/detokenization is not required because training uses the <strong>target token</strong> directly as supervision. <br /></li>
  <li>The model’s final output dimensionality equals <strong>vocabulary size</strong>, which is why <strong>tokenization choices are consequential</strong>.<br /></li>
</ul>

<hr />

<h1 id="tokenization-rationale-and-byte-pair-encoding-bpe">Tokenization rationale and Byte-Pair Encoding (BPE)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-14-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-14-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-14-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-14-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains why <strong>tokenization</strong> is essential and how common tokenization strategies trade off robustness and efficiency.<br /></p>

<ul>
  <li><strong>Tokens</strong> generalize beyond words and characters to handle typos, non-space languages, and efficient sequence-length tradeoffs. <br /></li>
  <li><strong>Character-level tokenization</strong> is robust (handles any input) but produces long sequences, which is expensive because Transformer cost scales roughly quadratically with sequence length. <br /></li>
  <li><strong>Subword tokenizers</strong> (typical in practice) strike a balance, averaging around <strong>3–4 letters per token</strong> for many languages.<br /></li>
</ul>

<p>Byte-Pair Encoding (BPE) training (high-level):<br /></p>
<ol>
  <li>Initialize the vocabulary as individual characters. <br /></li>
  <li>Iteratively find the most frequent adjacent token pair in the corpus and <strong>merge</strong> it into a new token. <br /></li>
  <li>Repeat merges to grow subword units while retaining smaller units to preserve robustness to typos and rare forms.<br /></li>
</ol>

<p>Implementation nuances and limitations:<br /></p>
<ul>
  <li>Use of <strong>pre-tokenizers</strong> that handle spaces and punctuation simplifies merges. <br /></li>
  <li>Deciding to keep small tokens (rather than only large merges) improves robustness. <br /></li>
  <li>Tokenizers still struggle with certain inputs such as long numbers or structured code tokens without task-specific tuning.<br /></li>
</ul>

<hr />

<h1 id="evaluation-metrics-perplexity-and-automatic-benchmarks">Evaluation metrics: perplexity and automatic benchmarks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-20-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-20-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-20-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-20-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment defines <strong>perplexity</strong> and explains its intuition and limitations.<br /></p>

<ul>
  <li><strong>Perplexity</strong> = exponentiated average per-token negative log-likelihood; it is an interpretable proxy for validation loss. <br /></li>
  <li>Intuition: <strong>lower perplexity</strong> implies the model is less uncertain — it hesitates among fewer plausible token choices. <br /></li>
  <li>Numeric bounds: perplexity ranges from <strong>1</strong> (perfect prediction) to roughly <strong>vocabulary size</strong> (maximally uncertain).<br /></li>
  <li>Historical context: perplexity has dropped substantially from 2017–2023 as models, data, and compute scaled.<br /></li>
</ul>

<p>Practical limitations:<br /></p>
<ul>
  <li>Perplexity depends on <strong>tokenization</strong> and the specific <strong>test data</strong>, making cross-model academic comparisons tricky. <br /></li>
  <li>Despite limitations, perplexity remains valuable for <strong>development and ablation</strong> work within consistent evaluation setups.<br /></li>
</ul>

<hr />

<h1 id="task-based-evaluation-and-mmlu-style-benchmarks">Task-based evaluation and MMLU-style benchmarks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-22-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-22-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-22-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-22-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment outlines alternative evaluation strategies that aggregate many automatically evaluable NLP tasks into <strong>benchmark suites</strong>.<br /></p>

<ul>
  <li>Examples of suites: <strong>HELM</strong>, <strong>Hugging Face Open LLM Leaderboard</strong>, and <strong>MMLU</strong>.<br /></li>
  <li>Many tasks are <strong>multiple-choice or constrained-output</strong>, which makes automatic scoring straightforward by: <br />
    <ul>
      <li>Computing model <strong>likelihoods</strong> for each candidate answer, or<br /></li>
      <li>Restricting generation to a fixed set of options and checking the model’s selection.<br /></li>
    </ul>
  </li>
  <li>Concrete example: <strong>MMLU</strong> contains multi-domain multiple-choice questions (college-level physics, medicine, law, etc.), scored via likelihood-based ranking or constrained prompting.<br /></li>
</ul>

<p>These suites make large-scale, reproducible comparisons possible for many practical capabilities.<br /></p>

<hr />

<h1 id="evaluation-challenges-inconsistency-and-test-contamination">Evaluation challenges: inconsistency and test contamination</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-26-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-26-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-26-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-26-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment catalogues major <strong>evaluation challenges</strong> encountered in practice:<br /></p>

<ul>
  <li><strong>Inconsistent evaluation protocols</strong> across organizations lead to divergent reported results even on the same benchmarks. <br /></li>
  <li><strong>Train–test contamination</strong>: benchmark examples can appear in training corpora, inflating reported performance.<br /></li>
</ul>

<p>Practical heuristic to detect contamination:<br /></p>
<ul>
  <li>Measure the <strong>joint likelihood</strong> of test examples in their original corpus order and compare it to the likelihood when examples are permuted randomly. A substantially higher joint likelihood in corpus order can be a signal that the test data appeared in training data.<br /></li>
</ul>

<p>The bottom line: <strong>contamination is a serious concern</strong> for academic benchmarking, and evaluation methodology requires careful standardization and contamination checks.<br /></p>

<hr />

<h1 id="raw-web-data-collection-and-extraction-challenges">Raw web data collection and extraction challenges</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-29-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-29-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-29-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-29-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment surveys <strong>common-crawl-style web crawling</strong> as the initial step for building pre-training corpora:<br /></p>

<ul>
  <li>Typical web scale: <strong>hundreds of billions of pages</strong> and <strong>petabyte-scale</strong> raw data. <br /></li>
  <li>Text extraction from heterogeneous HTML is hard: content, ads, scripts, and markup vary widely. <br /></li>
  <li>Certain content types (e.g., <strong>mathematical notation</strong>) are difficult to extract cleanly. <br /></li>
  <li>Random web pages contain noisy boilerplate, incomplete sentences, and irrelevant artifacts, which motivates downstream filtering and cleanup prior to training.<br /></li>
</ul>

<hr />

<h1 id="data-filtering-deduplication-and-domain-weighting">Data filtering, deduplication, and domain weighting</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-33-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-33-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-33-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-33-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment details practical <strong>data-processing stages</strong> after raw extraction:<br /></p>

<ul>
  <li><strong>Blacklist-based removal</strong> of undesirable or unsafe websites. <br /></li>
  <li><strong>Deduplication</strong> to remove repeated headers/footers and duplicated book content across URLs. <br /></li>
  <li><strong>Heuristic rules</strong> to detect low-quality or outlier pages (e.g., abnormal token distributions, extreme lengths).<br /></li>
  <li><strong>Model-based filtering</strong>: train classifiers on high-quality references (e.g., Wikipedia) to prefer authoritative sources.<br /></li>
  <li><strong>Domain classification and reweighting</strong>: upweight domains like code or books when desired, downweight entertainment or low-value content.<br /></li>
  <li><strong>Final fine-tuning</strong> or held-out training on high-quality corpora (e.g., Wikipedia) to ensure a clean tail of data quality.<br /></li>
</ul>

<p>These stages are essential to turn noisy crawl data into a usable pre-training corpus.<br /></p>

<hr />

<h1 id="training-dataset-sizes-and-common-corpora">Training dataset sizes and common corpora</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-38-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-38-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-38-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-38-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment provides <strong>empirical scale figures</strong> and representative datasets used in practice:<br /></p>

<ul>
  <li>Example corpora: the <strong>Pile</strong> dataset composition and large-scale <strong>Common Crawl</strong> extracts. <br /></li>
  <li>Contemporary total token counts used by state-of-the-art models are <strong>tens of trillions of tokens</strong> for leading systems. <br /></li>
  <li>Representative numbers: many top models have been trained on the order of <strong>15–20 trillion tokens</strong> (after deduplication and filtering); different model families (e.g., LLaMA variants) report various large token totals. <br /></li>
  <li>Even after aggressive filtering and deduplication, curated corpora are <strong>orders of magnitude larger</strong> than early corpora. <br /></li>
  <li>Collecting, processing, and managing such datasets is <strong>resource-intensive</strong> and often treated as a competitive, sometimes secretive, aspect of LLM development.<br /></li>
</ul>

<hr />

<h1 id="scaling-laws-empirical-relationships-between-compute-data-and-performance">Scaling laws: empirical relationships between compute, data, and performance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-42-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-42-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-42-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-42-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces empirical <strong>scaling laws</strong> that show predictable relationships between compute, model size, dataset size, and validation loss:<br /></p>

<ul>
  <li>Observed relationships are approximately <strong>power-law / log-linear</strong>: plotting test loss versus compute, data size, or parameter count on log scales reveals near-linear trends. <br /></li>
  <li>These trends enable <strong>extrapolation</strong>: organizations can predict how much loss reduction is achievable with additional compute or data. <br /></li>
  <li>Practical consequence: scaling laws inform resource planning and architecture trade-offs. <br /></li>
  <li>Caveat: there is no complete theoretical foundation yet, and uncertainty remains about eventual performance plateaus or regime changes.<br /></li>
</ul>

<hr />

<h1 id="using-scaling-laws-for-model-design-and-the-chinchilla-result">Using scaling laws for model design and the Chinchilla result</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-48-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-48-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-48-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-48-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains how scaling laws enable principled allocation of compute between <strong>model size</strong> and <strong>dataset size</strong>:<br /></p>

<ul>
  <li>Modern pipeline: tune hyperparameters and measure scaling on smaller models, fit scaling curves, and <strong>extrapolate</strong> to identify optimal large-scale configurations.<br /></li>
  <li>The <strong>Chinchilla</strong> finding highlights that optimal training often requires a specific <strong>tokens-per-parameter</strong> ratio (i.e., a balance between model size and dataset size).<br /></li>
  <li>Practical guidance (historical estimates):<br />
    <ul>
      <li>~<strong>20 tokens per parameter</strong> was a commonly referenced target for training optimality in some studies. <br /></li>
      <li>Higher ratios (e.g., <strong>~150 tokens/parameter</strong>) are mentioned when accounting for <strong>inference economics</strong> and different operating points. <br /></li>
    </ul>
  </li>
  <li>The optimal allocation depends on <strong>compute budget</strong>, <strong>inference cost considerations</strong>, and the desired product operating point.<br /></li>
</ul>

<hr />

<h1 id="practical-computational-costs-example-back-of-envelope-for-training">Practical computational costs: example back-of-envelope for training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/00-56-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/00-56-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/00-56-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/00-56-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment provides back-of-the-envelope calculations for modern training runs (illustrative example for a large open-model style run):<br /></p>

<ul>
  <li>FLOPs scale roughly proportional to <strong>parameters × tokens × a constant</strong> (model- and architecture-dependent).<br /></li>
  <li>From FLOPs and measured throughput you can estimate <strong>total GPU-hours</strong> and convert to <strong>wall-clock</strong> training days given cluster size and utilization.<br /></li>
  <li>Approximate monetary costs (rental + personnel) for large training runs typically reach <strong>multi‑tens of millions of dollars</strong> for top-tier experiments.<br /></li>
  <li>Associated <strong>carbon emissions</strong> can be material at current energy intensities and also scale with compute.<br /></li>
  <li>Empirical pattern: each new model generation often multiplies required compute by roughly an <strong>order of magnitude</strong>, driving rapidly rising resource needs.<br /></li>
</ul>

<hr />

<h1 id="post-training-alignment-motivation-and-supervised-fine-tuning-sft">Post-training (alignment) motivation and supervised fine-tuning (SFT)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-01-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-01-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-01-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-01-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment motivates <strong>post-training</strong> as the process that converts mass-pretrained LMs into useful AI assistants by aligning outputs to instructions, safety requirements, and conversational norms.<br /></p>

<ul>
  <li><strong>Supervised Fine-Tuning (SFT)</strong>: fine-tune a pretrained LM on <strong>human-written input–desired-output pairs</strong> using the same language-modeling cross-entropy loss. <br /></li>
  <li>SFT primarily <strong>reweights</strong> the model to prefer particular answer formats and behavioral styles; it usually does <strong>not</strong> add the factual knowledge that the pretraining corpus already lacked.<br /></li>
</ul>

<hr />

<h1 id="synthetic-data-scaling-for-sft-alpaca-and-limitations-of-large-sft-corpora">Synthetic data scaling for SFT (Alpaca) and limitations of large SFT corpora</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-05-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-05-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-05-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-05-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment describes <strong>synthetic-data approaches</strong> for scaling supervised fine-tuning:<br /></p>

<ul>
  <li>Strategy: use an existing LM to <strong>generate many instruction–response pairs</strong>, seeded from a small handcrafted human set (example pipeline: <strong>Alpaca</strong>).<br /></li>
  <li>Benefits: synthetic SFT data can <strong>substantially reduce human labeling costs</strong> and enable effective fine-tuning of smaller open models.<br /></li>
  <li>Empirical finding: SFT delivers <strong>diminishing returns</strong> beyond relatively small labeled sets for format/style adaptation, because the core pretraining knowledge remains unchanged — SFT mainly instructs the model to adopt a target response style.<br /></li>
</ul>

<hr />

<h1 id="reinforcement-learning-from-human-feedback-rlhf-rationale-and-pipeline">Reinforcement Learning from Human Feedback (RLHF): rationale and pipeline</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-09-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-09-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-09-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-09-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment explains why <strong>SFT alone is limited</strong> and introduces <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> to optimize human preferences rather than blindly cloning human outputs.<br /></p>

<p>Typical <strong>RLHF pipeline</strong>:<br /></p>
<ol>
  <li>Collect <strong>human preference comparisons</strong> between multiple model outputs for the same prompt. <br /></li>
  <li>Train a <strong>reward model</strong> to predict those human preferences. <br /></li>
  <li>Apply a <strong>policy optimization</strong> algorithm (commonly <strong>PPO</strong>) to adjust the LM policy to maximize expected reward while regularizing against large distribution shifts.<br /></li>
</ol>

<p>Practical RL challenges:<br /></p>
<ul>
  <li>Rewards are often <strong>sparse or binary</strong>, making learning hard. <br /></li>
  <li>RL algorithms can be <strong>unstable</strong>, requiring clipping, careful tuning, and many engineering tricks. <br /></li>
  <li>RLHF effectively transforms the LM from a calibrated likelihood model into an <strong>optimized policy</strong>, changing evaluation dynamics.<br /></li>
</ul>

<hr />

<h1 id="direct-preference-optimization-dpo-as-a-simplified-alternative-to-rlhf">Direct Preference Optimization (DPO) as a simplified alternative to RLHF</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-13-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-13-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-13-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-13-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment presents <strong>Direct Preference Optimization (DPO)</strong> as a simplified, likelihood-based alternative to full PPO-style RLHF:<br /></p>

<ul>
  <li><strong>DPO</strong> directly maximizes the probability of <strong>preferred outputs</strong> and decreases the probability of dispreferred outputs using a closed-form objective. <br /></li>
  <li>Training uses human preference pairs and a loss that <strong>increases relative likelihood</strong> for preferred responses and <strong>decreases</strong> it for inferior ones. <br /></li>
  <li>Under certain assumptions, the global optima of DPO and some RL formulations coincide. <br /></li>
  <li>Benefit: DPO reduces engineering complexity (no separate reward model + RL loop in its simplest form) while achieving <strong>comparable empirical improvements</strong> on many tasks.<br /></li>
</ul>

<hr />

<h1 id="human-labeling-challenges-for-preference-data-and-annotator-bias">Human labeling challenges for preference data and annotator bias</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-16-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-16-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-16-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-16-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment surveys practical difficulties with <strong>human preference labeling</strong>:<br /></p>

<ul>
  <li>Labeling is <strong>slow and expensive</strong>. <br /></li>
  <li>Annotators often <strong>disagree</strong>; inter-annotator agreement is commonly around <strong>~60–70%</strong> in many settings. <br /></li>
  <li>Labels can conflate <strong>superficial features</strong> (e.g., response length or style) with substantive quality. <br /></li>
  <li>Annotators are exposed to <strong>toxic content</strong>, creating ethical and safety concerns.<br /></li>
</ul>

<p>Consequences and mitigations:<br /></p>
<ul>
  <li><strong>Annotation guidelines</strong>, annotator selection, and <strong>statistical controls</strong> substantially influence downstream model behavior. <br /></li>
  <li>Naively collected preferences can bias models toward irrelevant or undesirable attributes (e.g., <strong>verbosity</strong>), so careful protocol design is essential.<br /></li>
</ul>

<hr />

<h1 id="using-llms-to-generate-preference-labels-and-automated-evaluation-alpacaeval">Using LLMs to generate preference labels and automated evaluation (AlpacaEval)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-25-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-25-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-25-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-25-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment describes substituting or augmenting human preference labeling with <strong>LLM-based preference judgments</strong> to improve efficiency and scale:<br /></p>

<ul>
  <li>Strong LMs can achieve <strong>high agreement</strong> with human majority preferences at a fraction of the cost, enabling large-scale ranking and evaluation pipelines.<br /></li>
  <li>Automated pairwise comparisons by a strong LM (e.g., GPT-4) can produce <strong>reliable model rankings</strong> that correlate well with human-based chat-arena results. <br /></li>
  <li>Caveats: LLM judges can be <strong>biased</strong> (e.g., toward verbosity) and require <strong>statistical controls</strong> (such as regression adjustment) to avoid systematic distortions in preference signals.<br /></li>
</ul>

<hr />

<h1 id="evaluation-of-aligned-models-open-ended-challenges-and-chatbot-arena">Evaluation of aligned models: open-ended challenges and chatbot arena</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-35-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-35-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-35-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-35-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment addresses evaluation of <strong>aligned, assistant-style models</strong>, where outputs are diverse and open-ended:<br /></p>

<ul>
  <li>After alignment, <strong>validation loss and perplexity</strong> become poor comparators because models optimized as policies no longer produce calibrated likelihoods and many plausible answers exist for a single prompt. <br /></li>
  <li>Effective evaluation strategies include <strong>blind pairwise human judgments</strong> (e.g., Chatbot Arena) or <strong>synthetic LLM judges</strong>, with careful attention to sampling and aggregation. <br /></li>
  <li>Practical issues: <strong>user population bias</strong> (tech-savvy users ask technical prompts), and <strong>cost/latency trade-offs</strong> for large-scale human evaluation.<br /></li>
</ul>

<hr />

<h1 id="systems-engineering-gpu-architecture-bottlenecks-and-optimization-principles">Systems engineering: GPU architecture, bottlenecks, and optimization principles</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec02/01-40-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec02/01-40-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec02/01-40-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec02/01-40-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>This segment provides an overview of <strong>system-level constraints and optimizations</strong> for training large LMs on GPUs:<br /></p>

<ul>
  <li>GPUs are <strong>high-throughput, SIMD-like</strong> hardware optimized for large matrix multiplications. <br /></li>
  <li>Practical performance is limited by <strong>memory bandwidth</strong>, <strong>communication latency</strong>, and <strong>pipeline inefficiencies</strong>; well-optimized training typically targets <strong>~40–50% real-world FLOP utilization</strong>.<br /></li>
</ul>

<p>Key system techniques to improve utilization:<br /></p>
<ul>
  <li><strong>Low-precision arithmetic</strong> (mixed precision) to reduce memory and communication costs. <br /></li>
  <li><strong>Operator/kernel fusion</strong> to reduce host-device round trips. <br /></li>
  <li><strong>Tiling and partitioning</strong> strategies for memory and compute locality. <br /></li>
  <li>Larger-scale <strong>distribution and communication</strong> optimizations (sharding, all-reduce, pipeline parallelism) to scale across many GPUs.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Agent 01 - Intro to Large Language Models by Andrej Karpathy</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec01/" rel="alternate" type="text/html" title="Agent 01 - Intro to Large Language Models by Andrej Karpathy" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec01</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec01/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/zjkBMFhNj_g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="a-large-language-model-can-be-represented-by-a-parameters-file-and-a-runtime-implementation">A large language model can be represented by a parameters file and a runtime implementation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-01-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-01-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-01-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-01-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A production large language model is fundamentally the combination of a <strong>parameters file</strong> containing the model weights and a <strong>run-time implementation</strong> that executes the model’s forward pass.<br /></p>

<ul>
  <li>The <strong>parameters</strong> are stored as numeric tensors (commonly <strong>float16</strong>).<br /></li>
  <li>For very large models this file can be hundreds of gigabytes — for example, a <strong>70 billion‑parameter</strong> model stored at <strong>two bytes per parameter</strong> yields roughly <strong>140 GB</strong>.<br /></li>
  <li>The <strong>run-time component</strong> can be implemented in <strong>C, Python, or other languages</strong> and may be only a few hundred lines of code to implement the inference pipeline, tokenization, and sampling logic.<br /></li>
</ul>

<p>Because inference merely loads weights and performs matrix operations, it can be executed <strong>locally</strong> (even on consumer hardware for smaller models) without internet connectivity.<br />
However, larger models impose higher <strong>latency</strong> and <strong>memory</strong> requirements — for example, a <strong>7B</strong> model runs much faster than a <strong>70B</strong> model.<br /></p>

<hr />

<h1 id="training-large-language-models-requires-massive-text-corpora-and-specialized-gpu-clusters">Training large language models requires massive text corpora and specialized GPU clusters.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-05-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-05-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-05-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-05-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Model training (pre‑training)</strong> compresses statistical structure from very large text corpora into model weights and therefore requires both large datasets and significant compute.<br /></p>

<ul>
  <li>Typical pre‑training datasets range into <strong>multiple terabytes</strong> of deduplicated text (the talk cites roughly <strong>10 TB</strong> as an order‑of‑magnitude example).<br /></li>
  <li>Training can require <strong>thousands of specialized accelerators</strong> run for days to weeks — the talk notes an example run using <strong>~6,000 GPUs for ~12 days</strong> at a cost on the order of <strong>millions of dollars</strong> for a <strong>70B‑class</strong> model.<br /></li>
</ul>

<p>The result is a <strong>lossy compression</strong> of the training corpus into a compact weight file; the process is analogous to high‑loss, knowledge‑preserving compression rather than lossless archival.<br />
Modern state‑of‑the‑art models often scale dataset and compute factors by an order of magnitude or more, driving costs into the <strong>tens or hundreds of millions</strong> for leading systems.<br /></p>

<hr />

<h1 id="the-training-objective-is-next-word-prediction-which-causes-the-model-to-encode-world-knowledge-in-its-weights">The training objective is next-word prediction, which causes the model to encode world knowledge in its weights.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-07-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-07-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-07-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-07-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Pre‑training objective:</strong> next‑token (or next‑word) prediction — given a token sequence the network predicts a probability distribution over the next token.<br /></p>

<ul>
  <li>This objective is <strong>computationally tractable</strong> and aligns with information‑theoretic <strong>compression</strong> principles.<br /></li>
  <li>It causes the model to internalize statistical regularities and factual associations present in the corpus.<br /></li>
</ul>

<p>Because accurate prediction implies the ability to compress the underlying distribution, the <strong>weights implicitly store wide‑ranging knowledge</strong> (dates, entities, relations) in an <strong>entangled, lossy form</strong>.<br /></p>
<ul>
  <li>Specific examples from web pages become <strong>distributed facts inside parameters</strong> rather than verbatim copies.<br /></li>
  <li>The probabilistic output can be interpreted as <strong>confidence</strong> over candidate continuations and forms the basis for <strong>sampling</strong> during inference.<br /></li>
</ul>

<hr />

<h1 id="during-inference-the-model-samples-tokens-sequentially-producing-text-that-often-resembles-training-documents-but-can-hallucinate">During inference the model samples tokens sequentially, producing text that often resembles training documents but can hallucinate.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-10-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-10-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-10-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-10-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Inference</strong> proceeds by conditioning on a prompt, sampling a token from the model’s output distribution, appending it to the context, and repeating this left‑to‑right process to generate sequences.<br /></p>

<ol>
  <li>Condition on the initial <strong>prompt</strong>.<br /></li>
  <li>Compute the model’s <strong>output distribution</strong> for the next token.<br /></li>
  <li><strong>Sample</strong> a token and append it to the context.<br /></li>
  <li>Repeat until the sequence is complete.<br /></li>
</ol>

<p>Because the model learned from web documents and other sources, generated text often adopts familiar formats (articles, product listings, code) and can produce <strong>plausible but fabricated details</strong> (e.g., invented ISBNs or synthetic product metadata).<br /></p>
<ul>
  <li>Outputs therefore range from <strong>accurate, knowledge‑based responses</strong> to <strong>hallucinations</strong> where surface form is correct but factual content is erroneous or invented.<br /></li>
  <li>It is often unclear whether a specific fact is <strong>memorized</strong> from training or <strong>synthesized</strong> from distributed knowledge.<br /></li>
</ul>

<p>Practical systems treat model outputs <strong>probabilistically</strong> and rely on downstream <strong>verification</strong> or <strong>tool use</strong> to reduce hallucination risk.<br /></p>

<hr />

<h1 id="transformer-architectures-implement-the-computation-but-the-emergent-role-of-billions-of-parameters-is-largely-inscrutable">Transformer architectures implement the computation but the emergent role of billions of parameters is largely inscrutable.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-12-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-12-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-12-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-12-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Transformer networks</strong> provide a fully specified mathematical architecture — <strong>self‑attention</strong>, <strong>feedforward layers</strong>, <strong>layer normalization</strong>, and <strong>token embeddings</strong> — that determines how inputs flow and how intermediate representations are computed.<br /></p>

<ul>
  <li>Despite complete knowledge of the architecture and training dynamics, the <strong>functional decomposition</strong> of many billions of learned parameters is not fully interpretable.<br /></li>
  <li>Parameters are optimized end‑to‑end to reduce the next‑token loss and the resulting internal mechanisms are <strong>emergent</strong> and <strong>distributed</strong>.<br /></li>
</ul>

<p>Interpretability and mechanistic analysis can reveal <strong>localized behaviors</strong> in some cases, but many phenomena (one‑way knowledge retrieval, failure modes such as reversal of relational queries) reflect that <strong>knowledge representation is anisotropic and task‑dependent</strong>.<br />
Consequently, models are best understood as <strong>empirical artifacts</strong> whose capabilities and failure modes require systematic <strong>behavioral evaluation</strong>.<br /></p>

<hr />

<h1 id="producing-an-assistant-involves-fine-tuning-a-base-pre-trained-model-on-high-quality-human-labeled-qa-data">Producing an assistant involves fine-tuning a base pre-trained model on high-quality, human-labeled Q&amp;A data.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-17-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-17-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-17-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-17-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The production workflow separates <strong>pre‑training</strong> (broad knowledge acquisition from large, varied corpora) from <strong>fine‑tuning</strong> (alignment and behavior shaping via curated supervision).<br /></p>

<ul>
  <li><strong>Fine‑tuning</strong> replaces or supplements pre‑training data with high‑quality, <strong>instruction‑style Q&amp;A examples</strong> created by human labelers according to detailed labeling instructions.<br /></li>
  <li>This dataset may be orders of magnitude <strong>smaller</strong> but much <strong>higher in task relevance and quality</strong> (an example scale is <strong>~100k</strong> dialogue/Q&amp;A exemplars).<br /></li>
</ul>

<p>Fine‑tuning preserves the underlying factual knowledge while steering <strong>output format and style</strong> toward helpful, safe assistant behavior.<br /></p>
<ul>
  <li>Because it is computationally cheaper than pre‑training, fine‑tuning supports <strong>frequent iteration, monitoring, correction</strong> of misbehaviors, and incremental deployment.<br /></li>
  <li>The typical production loop cycles between <strong>deployment → behavior monitoring → collecting corrective labels → re‑fining</strong> to improve performance.<br /></li>
</ul>

<hr />

<h1 id="a-further-fine-tuning-stage-uses-comparison-labels-and-reinforcement-learning-from-human-feedback-to-align-models-responses">A further fine-tuning stage uses comparison labels and reinforcement learning from human feedback to align models’ responses.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-21-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-21-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-21-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-21-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Beyond direct supervised fine‑tuning, a third stage leverages <strong>human preference data</strong> that compares candidate model outputs rather than requiring humans to compose ideal outputs from scratch.<br /></p>

<ul>
  <li>Labelers <strong>rank or choose</strong> the better of several model responses; those pairwise comparisons train a <strong>reward model</strong>.<br /></li>
  <li>Reinforcement learning or preference‑optimization techniques then optimize the policy to produce outputs with higher expected human preference (commonly called <strong>RLHF</strong>).<br /></li>
</ul>

<p>Comparison labeling is often more <strong>scalable</strong> and easier for humans, and it yields stronger alignment with subjective notions of <strong>helpfulness, truthfulness, and harmlessness</strong>.<br />
Human reviewers and automated assistants can be combined to accelerate label generation and create <strong>hybrid human‑machine workflows</strong> for higher throughput and quality.<br /></p>

<hr />

<h1 id="model-evaluation-and-ecosystem-landscape-show-proprietary-models-outperform-open-weight-models-but-open-models-enable-customization">Model evaluation and ecosystem landscape show proprietary models outperform open-weight models but open models enable customization.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-24-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-24-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-24-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-24-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Community evaluation platforms (e.g., human‑judged pairwise comparisons aggregated into an <strong>ELO</strong> metric) show a performance bifurcation in the ecosystem.<br /></p>

<ul>
  <li><strong>Closed‑weight proprietary models</strong> typically attain higher performance on many benchmarks but restrict access to weights and deep customization.<br /></li>
  <li><strong>Open‑weight models</strong> (examples include the <strong>Llama</strong> series and open variants) lag in some benchmarks but expose weights and training details, enabling fine‑tuning, research, and specialized deployment.<br /></li>
</ul>

<p>The ecosystem therefore bifurcates into <strong>high‑performance hosted services</strong> and <strong>lower‑latency, customizable open stacks</strong>; selection depends on requirements for <strong>performance, control, cost, and regulatory constraints</strong>.<br /></p>

<hr />

<h1 id="scaling-laws-predict-smooth-improvements-in-next-word-accuracy-as-model-size-and-dataset-scale-increase">Scaling laws predict smooth improvements in next-word accuracy as model size and dataset scale increase.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-26-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-26-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-26-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-26-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Empirical <strong>scaling laws</strong> show that next‑token loss and downstream metrics are reliably predictable functions of two primary variables: <strong>model parameter count N</strong> and <strong>training dataset size D</strong>.<br /></p>

<ul>
  <li>These relationships follow <strong>power‑law trends</strong> and do not show clear saturation within observed ranges, implying that increasing compute and data tends to yield better predictive performance.<br /></li>
  <li>As a result, provisioning <strong>more compute and more data</strong> is a validated route to improved capability — which helps explain industry emphasis on larger GPU clusters and bigger curated corpora.<br /></li>
</ul>

<p>Algorithmic innovations remain valuable as <strong>efficiency multipliers</strong>, but <strong>scaling</strong> provides a robust baseline path to capability gains.<br /></p>

<hr />

<h1 id="modern-llms-increasingly-act-as-tool-using-agents-that-browse-calculate-plot-and-generate-images-to-solve-tasks">Modern LLMs increasingly act as tool-using agents that browse, calculate, plot, and generate images to solve tasks.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-30-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-30-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-30-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-30-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Contemporary systems integrate <strong>external tools</strong> into the text‑generation loop: the model emits structured <strong>tool calls</strong> (browser, calculator, Python interpreter, plotting libraries, image generators) and then conditions on tool outputs to produce final responses.<br /></p>

<ul>
  <li>This offsets limitations of in‑head computation (e.g., accurate arithmetic or up‑to‑date retrieval) and allows orchestration of multi‑step workflows.<br /></li>
  <li>Tool use converts an LLM from a passive sequence generator into an <strong>active agent</strong> that leverages existing software infrastructure to increase <strong>factuality, reproducibility, and practical utility</strong>.<br /></li>
</ul>

<p>Concrete example workflow:<br /></p>
<ol>
  <li><strong>Browse</strong> to collect funding data.<br /></li>
  <li>Use a <strong>calculator</strong> to compute ratios.<br /></li>
  <li><strong>Plot</strong> results via Python (e.g., matplotlib).<br /></li>
  <li>Generate a representative <strong>image</strong> with an image model.<br /></li>
</ol>

<hr />

<h1 id="multimodality-extends-llm-capabilities-to-see-hear-and-generate-images-audio-and-code">Multimodality extends LLM capabilities to see, hear, and generate images, audio, and code.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-34-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-34-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-34-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-34-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Multimodal models</strong> process and produce multiple data modalities — text, images, audio — and can translate between them (for example, generate code from a hand‑drawn UI sketch, caption images, or analyze audio).<br /></p>

<ul>
  <li>Multimodal capability enables use cases like <strong>image‑to‑code</strong>, speech‑based conversational interfaces, and combined audio‑visual content generation and understanding.<br /></li>
  <li>Integrating modalities increases model <strong>flexibility</strong> for real‑world tasks and creates richer interaction paradigms beyond text.<br /></li>
</ul>

<p>However, multimodality also <strong>expands the attack surface</strong> and complicates <strong>evaluation and mitigation</strong> strategies.<br /></p>

<hr />

<h1 id="researchers-aim-to-add-system-2-capabilities-so-llms-can-deliberate-over-longer-time-horizons-and-trade-time-for-accuracy">Researchers aim to add ‘system 2’ capabilities so LLMs can deliberate over longer time horizons and trade time for accuracy.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-36-32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-36-32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-36-32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-36-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Cognitive metaphors distinguish fast, heuristic <strong>“system 1”</strong> behavior from slower, deliberative <strong>“system 2”</strong> reasoning; current LLMs predominantly implement <strong>system 1</strong>: quick token prediction without prolonged internal deliberation.<br /></p>

<p>The desired <strong>system 2</strong> would allow iterative planning, searching a tree of alternatives, reflecting, and improving answers given more time — converting additional compute/time into higher reliability and accuracy.<br /></p>

<p>Active research directions aim to enable this via architectures and protocols that:<br /></p>
<ul>
  <li>Maintain <strong>explicit intermediate reasoning states</strong>.<br /></li>
  <li>Support <strong>multi‑step verification</strong> and chained computations (chain‑of‑thought, tree‑of‑thoughts, reflective loops).<br /></li>
</ul>

<p>Enabling system 2 behavior requires both modeling changes and interfaces for <strong>long‑horizon internal state management</strong>.<br /></p>

<hr />

<h1 id="self-improvement-analogous-to-alphazeros-reinforcement-learning-remains-an-open-challenge-because-general-reward-functions-for-language-are-hard-to-define">Self-improvement analogous to AlphaZero’s reinforcement learning remains an open challenge because general reward functions for language are hard to define.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-39-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-39-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-39-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-39-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>AlphaZero‑style <strong>self‑play</strong> demonstrates how systems can surpass human performance when there is a compact, automatically computable <strong>reward function</strong> (e.g., winning a game).<br /></p>

<p>For open‑ended language tasks there is typically <strong>no single, fast, unambiguous reward signal</strong> that judges output quality across all contexts, which limits straightforward autonomous self‑improvement beyond human imitation.<br /></p>
<ul>
  <li>Narrow domains with clear automatic objectives may permit autonomous improvement.<br /></li>
  <li>General language domains currently rely on <strong>human supervision or preference signals</strong>; defining scalable, robust reward functions for open‑ended language remains an active and unresolved research problem.<br /></li>
</ul>

<hr />

<h1 id="customization-enables-domain-specific-experts-through-retrieval-file-uploads-and-potentially-fine-tuning-for-particular-tasks">Customization enables domain-specific experts through retrieval, file uploads, and potentially fine-tuning for particular tasks.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-41-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-41-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-41-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-41-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Customization levers</strong> include instruction tuning (custom system prompts), <strong>retrieval‑augmented generation (RAG)</strong>, and heavier options such as per‑tenant fine‑tuning.<br /></p>

<ul>
  <li><strong>RAG</strong> lets an LLM condition on user‑provided documents or knowledge bases at inference time, improving factuality in narrow domains without retraining the base model.<br /></li>
  <li>Platform features (e.g., app stores of specialized assistants) allow assembling purpose‑built agents that combine custom prompts, retrieval, tool integrations, and lightweight model updates to create domain experts.<br /></li>
</ul>

<p>These methods enable <strong>specialization</strong> while preserving a common large base model.<br /></p>

<hr />

<h1 id="llms-function-as-a-kernel-like-orchestration-layer-analogous-to-an-operating-system-with-context-window-as-working-memory-and-tools-as-peripherals">LLMs function as a kernel-like orchestration layer analogous to an operating system, with context window as working memory and tools as peripherals.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-44-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-44-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-44-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-44-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>An <strong>operating‑system analogy</strong> maps LLM components to classical computing layers:<br /></p>

<ul>
  <li>The <strong>internet and long‑term storage</strong> act like <strong>disk</strong>.<br /></li>
  <li>The <strong>context window</strong> functions as finite working memory (<strong>RAM</strong>).<br /></li>
  <li>External <strong>APIs/tools</strong> serve as <strong>peripherals</strong> that the LLM orchestrates.<br /></li>
</ul>

<p>The LLM plays a <strong>kernel‑like coordinating role</strong>: paging relevant information into the context window, dispatching tool invocations, and orchestrating multi‑step computations to solve user tasks.<br />
Concepts such as <strong>multi‑threading, speculative execution, and user/kernel separation</strong> have useful analogues in designing LLM orchestration and safety architectures, supporting patterns for <strong>memory management, retrieval policies, and modular tool integration</strong>.<br /></p>

<hr />

<h1 id="llm-security-faces-a-cat-and-mouse-landscape-including-jailbreaks-that-bypass-safety-via-roleplay-encoding-or-adversarial-suffixes">LLM security faces a cat-and-mouse landscape including jailbreaks that bypass safety via roleplay, encoding, or adversarial suffixes.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-48-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-48-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-48-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-48-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Jailbreak attacks</strong> exploit the model’s tendency to follow apparent user instructions by reframing malicious prompts as roleplay or encoded content (e.g., asking the model to act as a fictional character that knows disallowed instructions).<br /></p>

<ul>
  <li>Encoded inputs (base64, other encodings) can bypass refusal behaviors if refusal training focused on natural‑language examples.<br /></li>
  <li>Adversarially optimized universal suffixes or perturbations can be <strong>transferable jailbreak triggers</strong> that induce unsafe outputs across prompts.<br /></li>
  <li>Carefully crafted adversarial images can encode token sequences that the model interprets as instructions.<br /></li>
</ul>

<p>Mitigations include broadening <strong>safety training</strong> to diverse encodings, applying <strong>adversarial robustness</strong> methods, performing <strong>input sanitization</strong>, and implementing continual monitoring.<br /></p>

<hr />

<h1 id="prompt-injection-attacks-occur-when-content-from-external-sources-embeds-instructions-that-hijack-model-behavior-and-can-enable-phishing-or-data-exfiltration">Prompt injection attacks occur when content from external sources embeds instructions that hijack model behavior and can enable phishing or data exfiltration.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-53-57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-53-57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-53-57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-53-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Prompt injection</strong> arises when an LLM ingests external content (web pages, images, shared documents) that contains instructions designed to override or augment prior directives and thereby change model behavior.<br /></p>

<ul>
  <li>Examples: invisible or low‑contrast text in images commanding fraudulent responses; web pages embedding instructions that cause attacker‑controlled outputs; shared documents instructing exfiltration of user data.<br /></li>
  <li>Tool‑enabled browsing or file retrieval amplifies this risk because the model executes against third‑party content.<br /></li>
</ul>

<p>Defenses include robust <strong>content sanitization</strong>, <strong>CSP and origin restrictions</strong> for tool outputs, limiting tool privileges, verifying provenance, and treating externally sourced content as <strong>untrusted</strong>.<br /></p>

<hr />

<h1 id="data-poisoning-or-backdoor-attacks-embed-trigger-phrases-or-examples-in-training-data-to-cause-deterministic-failures-under-specific-inputs">Data poisoning or backdoor attacks embed trigger phrases or examples in training data to cause deterministic failures under specific inputs.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec01/00-58-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec01/00-58-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec01/00-58-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec01/00-58-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Data poisoning / backdoor attacks</strong> place adversarially crafted examples into pre‑training or fine‑tuning corpora to implant trigger tokens or patterns that activate malicious behavior at inference time.<br /></p>

<ul>
  <li>A small insertion during fine‑tuning can create a <strong>sleeper trigger</strong> (a particular word sequence or token) that causes the model to output erroneous or attacker‑chosen responses when that trigger appears.<br /></li>
  <li>The attack surface includes publicly scraped web content and outsourced labeling pipelines.<br /></li>
</ul>

<p>Effective defenses require <strong>provenance tracking</strong>, <strong>dataset auditing</strong>, robust training procedures, and <strong>post‑training detection and mitigation</strong> strategies — this remains an active research and operational security concern.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Karpathy Series - Let’s build GPT from scratch</title><link href="https://tuananhbui89.github.io/blog/2025/karpathy-lec12/" rel="alternate" type="text/html" title="Karpathy Series - Let’s build GPT from scratch" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/karpathy-lec12</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/karpathy-lec12/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/kCc8FmEb1nY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="chatgpt-demonstrates-probabilistic-sequence-completion">ChatGPT demonstrates probabilistic sequence completion.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-01-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-01-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-01-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-01-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A conversational AI like <strong>ChatGPT</strong> implements a language model that generates text by <strong>sequentially completing</strong> a given input sequence.<br /></p>

<p>The model produces token-by-token outputs in a <strong>probabilistic</strong> manner, so the same prompt can yield different plausible continuations on different runs.<br /></p>

<p>Key points:<br /></p>
<ul>
  <li><strong>Tokens</strong> can be words, subwords, or characters.<br /></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The model estimates <strong>conditional probabilities</strong> of the next token given prior context: P(next_token</td>
          <td>previous_tokens).<br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>This inherent probabilistic completion behavior is the foundation for <strong>prompt engineering</strong>, creative generation, and sampling variability.<br /></li>
</ul>

<hr />

<h1 id="the-transformer-architecture-underlies-modern-autoregressive-language-models-like-gpt">The Transformer architecture underlies modern autoregressive language models like GPT.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-02-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-02-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-02-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-02-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>Transformer</strong>, introduced in “Attention Is All You Need” (2017), is the core sequence model used in GPT-style systems.<br /></p>

<p>Highlights:<br /></p>
<ul>
  <li>It <strong>replaced recurrence with attention mechanisms</strong> that compute data-dependent interactions across sequence positions.<br /></li>
  <li>This enables <strong>scalable parallel computation</strong> across time steps instead of sequential recurrence.<br /></li>
  <li><strong>GPT</strong> = <strong>Generative Pre-trained Transformer</strong>: a Transformer pre-trained on large corpora and then adapted for downstream tasks.<br /></li>
  <li>The architecture began in machine translation but became the foundation for many large-scale language models with relatively minor modifications.<br /></li>
</ul>

<hr />

<h1 id="a-minimal-educational-goal-is-to-train-a-character-level-transformer-on-a-small-dataset">A minimal educational goal is to train a character-level Transformer on a small dataset.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-04-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-04-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-04-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-04-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A compact, instructive variant of GPT can be trained on a toy corpus such as <strong>tiny Shakespeare</strong> (Shakespeare’s works concatenated into a small text file).<br /></p>

<p>Why this is useful:<br /></p>
<ul>
  <li>Training a <strong>character-level model</strong> simplifies tokenization and implementation while preserving the pedagogical value of sequence modeling and attention.<br /></li>
  <li>The model predicts the next character conditioned on preceding characters and, after training, can generate arbitrary-length text in the training corpus’ style.<br /></li>
  <li>This setup demonstrates core concepts of <strong>autoregressive generation</strong> without requiring massive compute or internet-scale data.<br /></li>
</ul>

<hr />

<h1 id="nanogpt-is-a-minimal-codebase-that-reproduces-transformer-training-behavior">NanoGPT is a minimal codebase that reproduces Transformer training behavior.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-06-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-06-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-06-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-06-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>NanoGPT</strong> is a compact GitHub repository that implements Transformer training with very small code (two main files), illustrating the essential components required to reproduce known baselines.<br /></p>

<p>Notable aspects:<br /></p>
<ul>
  <li>Shows that a correct implementation plus modest compute can replicate smaller released models (e.g., <strong>GPT-2 small</strong>) on standard datasets.<br /></li>
  <li>Separates <strong>model definition</strong> from the <strong>training script</strong>, and includes utilities for loading pretrained weights.<br /></li>
  <li>Demonstrates real-world engineering practices in a simplified form with an instructional goal: develop the model from scratch in a notebook for clarity.<br /></li>
</ul>

<hr />

<h1 id="load-and-inspect-the-training-corpus-in-a-reproducible-environment">Load and inspect the training corpus in a reproducible environment.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-07-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-07-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-07-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-07-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Begin experiments in a shareable environment such as a <strong>Google Colab</strong> notebook:<br /></p>

<ol>
  <li>Download the <strong>tiny Shakespeare</strong> corpus and load it as a single string for processing.<br /></li>
  <li>Confirm dataset size and inspect initial characters to validate data integrity and encoding assumptions.<br /></li>
  <li>Use the notebook to enable step-by-step development, reproducibility, and sharing of the training workflow and artifacts.<br /></li>
</ol>

<p>This initial validation helps catch I/O and encoding issues before further modeling steps.<br /></p>

<hr />

<h1 id="construct-the-character-vocabulary-and-determine-the-vocabulary-size">Construct the character vocabulary and determine the vocabulary size.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-09-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-09-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-09-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-09-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Extract the set of unique characters in the corpus and sort them to obtain a stable <strong>vocabulary ordering</strong>.<br /></p>

<p>Key consequences:<br /></p>
<ul>
  <li><strong>Vocabulary size</strong> = number of unique tokens the model will consider (e.g., ~65 characters for tiny Shakespeare, including whitespace and punctuation).<br /></li>
  <li>The vocabulary defines the model’s <strong>output dimensionality</strong> and the size of the <strong>embedding table</strong>.<br /></li>
  <li>Character-level vocabularies are compact but produce <strong>longer sequences</strong>; establish a deterministic char→int mapping for encoding/decoding model I/O.<br /></li>
</ul>

<hr />

<h1 id="tokenization-design-trades-vocabulary-size-for-sequence-length">Tokenization design trades vocabulary size for sequence length.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-10-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-10-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-10-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-10-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Tokenization</strong> converts raw text into integer sequences according to a vocabulary schema.<br /></p>

<p>Options and trade-offs:<br /></p>
<ul>
  <li><strong>Character-level tokenizers</strong>: tiny vocabulary, simple mapping, longer sequences — preferred for instructional clarity.<br /></li>
  <li><strong>Subword tokenizers</strong> (e.g., <strong>SentencePiece</strong>, <strong>Byte-Pair Encoding</strong>): larger vocabularies (tens of thousands), shorter sequences — used in practical large-scale models.<br /></li>
  <li>The tokenizer choice determines embedding table size, tokenization/de-tokenization logic, and batching behavior.<br /></li>
</ul>

<hr />

<h1 id="encode-the-entire-dataset-as-a-contiguous-integer-tensor">Encode the entire dataset as a contiguous integer tensor.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-12-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-12-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-12-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-12-56.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>After defining encoder and decoder mappings, translate the whole corpus into a single long integer sequence and wrap it into a framework tensor (e.g., a <strong>PyTorch tensor</strong>) to serve as the data array.<br /></p>

<p>Benefits and checks:<br /></p>
<ul>
  <li>Contiguous representation simplifies sampling random chunks for training and splitting into train/validation segments.<br /></li>
  <li>Storing the dataset as a tensor facilitates efficient slicing and batching during mini-batch construction.<br /></li>
  <li>Confirm correctness by inspecting correspondence between text slices and integer slices.<br /></li>
</ul>

<hr />

<h1 id="split-data-into-training-and-validation-subsets-and-define-block-size-for-contexts">Split data into training and validation subsets and define block size for contexts.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-14-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-14-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-14-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-14-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Partition the data tensor into a training portion (e.g., first 90%) and a held-out validation portion (last 10%) to monitor generalization and detect overfitting.<br /></p>

<p>Workflow and definitions:<br /></p>
<ol>
  <li>Define a maximum context length commonly called <strong>block_size</strong> or <strong>context length</strong>.<br /></li>
  <li>Training samples are randomly extracted contiguous chunks of length <strong>block_size</strong> (with a target offset), not the entire corpus at once.<br /></li>
  <li>Working with chunks reduces memory/compute requirements and enables the model to learn dependencies across shorter contexts.<br /></li>
  <li>Keep the validation split hidden during training as an unbiased generalization estimate.<br /></li>
</ol>

<hr />

<h1 id="each-sampled-chunk-contains-multiple-training-examples-via-shifted-inputs-and-targets">Each sampled chunk contains multiple training examples via shifted inputs and targets.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-16-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-16-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-16-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-16-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When extracting a chunk of length <strong>block_size + 1</strong> from the sequence, construct inputs and targets as follows:<br /></p>

<ol>
  <li>Inputs X = the first <strong>block_size</strong> tokens of the chunk.<br /></li>
  <li>Targets Y = the following tokens offset by one (the next token for each input position).<br /></li>
  <li>This packs <strong>block_size</strong> training examples (one per time position) into each chunk and lets the model learn to predict the next token from all prefix lengths up to the block size.<br /></li>
</ol>

<p>The offset-by-one construction is fundamental to <strong>autoregressive next-token prediction</strong>.<br /></p>

<hr />

<h1 id="train-the-model-to-handle-contexts-ranging-from-single-token-up-to-block-size-during-inference">Train the model to handle contexts ranging from single token up to block size during inference.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-17-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-17-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-17-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-17-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Include examples with context lengths from 1 up to <strong>block_size</strong> during training to condition the model on short and long contexts.<br /></p>

<p>Implications:<br /></p>
<ul>
  <li>This is critical for sampling, where generation often begins with a minimal prompt and the context progressively grows.<br /></li>
  <li>The model must generalize to intermediate context sizes during autoregressive sampling; longer-than-block_size contexts must be truncated.<br /></li>
  <li>Truncation is necessary because <strong>positional embeddings</strong> are defined up to block_size.<br /></li>
</ul>

<hr />

<h1 id="batching-stacks-multiple-independent-chunks-for-parallel-processing">Batching stacks multiple independent chunks for parallel processing.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-19-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-19-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-19-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-19-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Add a batch dimension by sampling multiple random chunk offsets to create a batch of independent sequences and stack these into a tensor of shape <strong>(batch_size, block_size)</strong> for efficient parallel processing on accelerators.<br /></p>

<p>Practical notes:<br /></p>
<ul>
  <li>Batching keeps hardware utilization high and enables parallel gradient computation across independent examples (batches do not exchange information).<br /></li>
  <li>Use randomized extraction of chunk offsets and a reproducible RNG seed for deterministic experiment replication.<br /></li>
  <li>Batched targets are handled analogously and later flattened for loss computation to match expected shapes.<br /></li>
</ul>

<hr />

<h1 id="the-simplest-language-model-is-a-bigram-model-implemented-via-token-embeddings">The simplest language model is a bigram model implemented via token embeddings.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-22-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-22-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-22-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-22-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A <strong>bigram model</strong> predicts the next token solely from the identity of the current token using a token embedding table that is directly interpreted as logits for the next-token distribution.<br /></p>

<p>Implementation and role:<br /></p>
<ul>
  <li>In PyTorch: an nn.Embedding shaped <strong>(vocab_size, vocab_size)</strong> so each token index indexes a row that scores all possible next tokens.<br /></li>
  <li>The bigram model ignores context beyond the current token but can capture local correlations (some tokens strongly predict specific followers).<br /></li>
  <li>Use this simple model as a baseline and a clean starting point before adding contextual mechanisms like attention.<br /></li>
</ul>

<hr />

<h1 id="compute-cross-entropy-loss-by-reshaping-logits-and-targets-to-match-framework-expectations">Compute cross-entropy loss by reshaping logits and targets to match framework expectations.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-26-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-26-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-26-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-26-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>For multi-dimensional logits with shape <strong>(B, T, C)</strong>, frameworks like PyTorch require rearrangement to a two-dimensional shape <strong>(B<em>T, C)** before calling cross-entropy; similarly, targets of shape **(B, T)** must be flattened to **(B</em>T,)</strong>.<br /></p>

<p>Practical checklist:<br /></p>
<ul>
  <li>Reshape or view tensors so the channel dimension is preserved as the second dimension and batch-time positions become the first flattened dimension.<br /></li>
  <li>This aligns predictions and labels for negative log-likelihood computation and produces a scalar loss measuring next-token predictive quality.<br /></li>
  <li>Sanity-check the initialization by comparing expected random-start loss to <strong>-log(1 / vocab_size)</strong>.<br /></li>
</ul>

<hr />

<h1 id="implement-autoregressive-generation-by-sampling-from-softmaxed-logits-at-the-last-time-step">Implement autoregressive generation by sampling from softmaxed logits at the last time step.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-29-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-29-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-29-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-29-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Generation</strong> extends an input context by repeatedly calling the model and sampling next tokens.<br /></p>

<p>Canonical generation loop:<br /></p>
<ol>
  <li>Run the model on the current context and extract logits for the <strong>last time step</strong>.<br /></li>
  <li>Convert logits to probabilities via <strong>softmax</strong>.<br /></li>
  <li>Sample the next token with a multinomial draw and append it to the running context.<br /></li>
  <li>Repeat for the desired number of new tokens to produce a variable-length sequence.<br /></li>
</ol>

<p>Implementation tips:<br /></p>
<ul>
  <li>Make targets optional in the model forward to support both training (return loss) and generation (return logits).<br /></li>
  <li>Batched generation returns a batch of sequences that can be decoded back to text by mapping token indices to characters/words.<br /></li>
</ul>

<hr />

<h1 id="initialize-and-optimize-the-model-with-adam-then-run-a-standard-training-loop">Initialize and optimize the model with Adam, then run a standard training loop.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-33-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-33-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-33-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-33-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Create an optimizer such as <strong>Adam</strong> with a suitable learning rate (e.g., ~3e-4 typical, though small networks may require adjustments) and perform the canonical training loop:<br /></p>

<ol>
  <li>Sample a batch.<br /></li>
  <li>Compute loss.<br /></li>
  <li>Zero gradients.<br /></li>
  <li>Backpropagate.<br /></li>
  <li>Step the optimizer.<br /></li>
</ol>

<p>Training practices:<br /></p>
<ul>
  <li>Monitor loss over many iterations and increase batch size and iteration count when scaling to larger models.<br /></li>
  <li>Use periodic evaluation on the held-out validation split to detect overfitting and tune hyperparameters (learning rate, batch size, etc.).<br /></li>
</ul>

<hr />

<h1 id="initial-training-of-the-bigram-model-reduces-loss-but-cannot-capture-long-range-context">Initial training of the bigram model reduces loss but cannot capture long-range context.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-37-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-37-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-37-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-37-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Training the bigram model with gradient-based optimization typically reduces loss from random initialization but remains limited because tokens do not communicate beyond immediate neighbors.<br /></p>

<p>Consequences and motivation:<br /></p>
<ul>
  <li>Tokens cannot access historical context, so predictions rely only on local correlations.<br /></li>
  <li>Improving next-token prediction requires mechanisms for tokens to access and aggregate relevant information from previous positions—this motivates <strong>attention mechanisms</strong>.<br /></li>
  <li>The observed loss reduction confirms optimization is working and sets a baseline for architectural improvements.<br /></li>
</ul>

<hr />

<h1 id="package-the-notebook-code-into-a-script-with-device-handling-and-stable-loss-estimation">Package the notebook code into a script with device handling and stable loss estimation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-40-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-40-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-40-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-40-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Converting the notebook into a script centralizes hyperparameters, supports running on GPUs by managing device placement, and adds engineering features such as checkpointing and evaluation scheduling.<br /></p>

<p>Suggested engineering additions:<br /></p>
<ul>
  <li>Add checkpoint saving/loading and device management.<br /></li>
  <li>Implement an <strong>estimate_loss</strong> routine that averages training and validation loss over several batches to reduce noise.<br /></li>
  <li>Use evaluation mode and <strong>torch.no_grad()</strong> during validation to avoid gradient computation and reduce memory usage.<br /></li>
</ul>

<p>These production-oriented changes improve reproducibility and interpretability of training runs.<br /></p>

<hr />

<h1 id="self-attention-can-be-approximated-as-weighted-aggregation-of-past-token-vectors-initially-illustrated-by-cumulative-averaging">Self-attention can be approximated as weighted aggregation of past token vectors, initially illustrated by cumulative averaging.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-43-51-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-43-51-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-43-51-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-43-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A simple communication mechanism for sequential tokens is to aggregate preceding token vectors by <strong>averaging</strong>, producing a context summary for each position.<br /></p>

<p>Notes and trade-offs:<br /></p>
<ul>
  <li>A naive loop implementation is correct but inefficient; it serves as an intuitive toy illustration of past→present information flow.<br /></li>
  <li>This bag-of-words style aggregation demonstrates the goal of attention: fuse information from previous positions into per-token representations.<br /></li>
  <li>The next step is to express such aggregations as batched matrix multiplications for efficiency.<br /></li>
</ul>

<hr />

<h1 id="lower-triangular-weight-matrices-and-batched-matrix-multiplication-vectorize-prefix-aggregation">Lower-triangular weight matrices and batched matrix multiplication vectorize prefix aggregation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-49-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-49-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-49-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-49-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A lower-triangular binary matrix of ones can sum all prefixes for each position via a single batched matrix multiplication:<br /></p>

<ul>
  <li>Multiply a <strong>(T, T)</strong> lower-triangular matrix by a <strong>(B, T, C)</strong> token tensor to produce <strong>(B, T, C)</strong> prefix sums.<br /></li>
  <li>Normalizing the rows of the triangular matrix yields <strong>prefix averages</strong> instead of sums.<br /></li>
  <li>Using batched <strong>GEMM</strong> applies this operation independently across batch elements and eliminates explicit loops, yielding large performance gains on GPUs.<br /></li>
</ul>

<p>This matrix formulation is the algebraic foundation enabling efficient <strong>masked attention</strong> implementations.<br /></p>

<hr />

<h1 id="masking-with-negative-infinity-and-softmax-produces-normalized-causal-affinity-weights">Masking with negative infinity and softmax produces normalized, causal affinity weights.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-53-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-53-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-53-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-53-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>To enforce causality in attention, set the non-allowed (future) entries of the <strong>T×T</strong> weight matrix to <strong>negative infinity</strong> and then apply softmax across each row:<br /></p>

<ul>
  <li>Filling masked positions with <strong>-inf</strong> ensures their contribution becomes zero after exponentiation and normalization, enforcing autoregressive constraints.<br /></li>
  <li>Parameterize the unmasked entries as learnable affinities so the model can compute data-dependent weights via softmax to selectively aggregate information.<br /></li>
</ul>

<p>This <strong>masked softmax</strong> pattern is the computational primitive that enables causal attention in decoders.<br /></p>

<hr />

<h1 id="introduce-embeddings-and-an-output-projection-to-separate-token-identity-and-model-dimensionality">Introduce embeddings and an output projection to separate token identity and model dimensionality.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/00-57-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/00-57-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/00-57-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/00-57-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Replace the trivial logits-as-embeddings approach with:<br /></p>
<ul>
  <li>A <strong>token embedding</strong> of dimension <strong>embed_dim</strong>.<br /></li>
  <li>A separate linear <strong>language modeling head</strong> that projects model features back to vocabulary logits.<br /></li>
</ul>

<p>Also:<br /></p>
<ul>
  <li>Add <strong>positional embeddings</strong> indexed by time position and add them to token embeddings so attention can distinguish positions.<br /></li>
  <li>Combined token + positional embeddings form the input representations consumed by subsequent attention layers.<br /></li>
</ul>

<p>This decouples embedding dimensionality from vocabulary size and enables richer continuous feature spaces.<br /></p>

<hr />

<h1 id="self-attention-computes-data-dependent-affinities-via-query-key-dot-products-and-aggregates-values-accordingly">Self-attention computes data-dependent affinities via query-key dot products and aggregates values accordingly.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-05-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-05-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-05-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-05-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Each token emits three learned linear projections: <strong>query (Q)</strong>, <strong>key (K)</strong>, and <strong>value (V)</strong> vectors.<br /></p>

<p>Computation steps:<br /></p>
<ol>
  <li>Compute affinities as <strong>Q × Kᵀ</strong> (batched) to obtain a <strong>B × T × T</strong> affinity tensor.<br /></li>
  <li>Apply causal masking and <strong>softmax</strong> to convert affinities into normalized attention weights.<br /></li>
  <li>Multiply these weights with the <strong>V</strong> matrix to produce aggregated, context-aware outputs per position.<br /></li>
</ol>

<p>The output dimension equals the head size and captures information from selected past tokens weighted by their learned relevance.<br /></p>

<hr />

<h1 id="attention-is-a-general-communication-mechanism-with-graph-and-positional-considerations">Attention is a general communication mechanism with graph and positional considerations.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-10-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-10-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-10-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-10-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Interpretation and generalization of attention:<br /></p>
<ul>
  <li>Attention is directed communication in a graph where nodes aggregate weighted information from neighbors; in autoregressive modeling the connectivity is triangular to prevent future→past flow.<br /></li>
  <li>Attention operates on unordered sets of vectors, so <strong>positional encodings</strong> are required to inject sequence order.<br /></li>
  <li>Batch elements are independent pools of nodes processed in parallel with batched matrix ops.<br /></li>
  <li>Attention generalizes to encoder-decoder and cross-attention when queries and keys/values originate from different sources; mask and connectivity choices tailor attention to the task.<br /></li>
</ul>

<hr />

<h1 id="scale-dot-product-attention-by-1sqrtd_k-to-control-variance-and-softmax-sharpness">Scale dot-product attention by 1/sqrt(d_k) to control variance and softmax sharpness.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-13-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-13-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-13-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-13-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When Q and K vectors have approximately unit variance, their dot product variance grows with head dimensionality; divide dot products by <strong>sqrt(d_k)</strong> to stabilize the attention logits.<br /></p>

<p>Rationale:<br /></p>
<ul>
  <li>Scaling prevents the softmax from becoming extremely peaky at initialization and preserves diffuse weight distributions that allow gradients to propagate.<br /></li>
  <li>Without scaling, larger head sizes would produce overly confident one-hot attention distributions and hamper learning, especially early in training.<br /></li>
</ul>

<p>This <strong>scaled dot-product</strong> formulation is standard practice in Transformer implementations.<br /></p>

<hr />

<h1 id="implement-a-single-self-attention-head-module-and-integrate-it-into-the-model-while-enforcing-context-cropping">Implement a single self-attention head module and integrate it into the model while enforcing context cropping.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-17-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-17-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-17-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-17-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Encapsulate key-query-value projections, the causal triangular mask (as a non-parameter buffer), scaled dot-product computation, softmax normalization, and value aggregation into a reusable <strong>Head</strong> class.<br /></p>

<p>Implementation tips:<br /></p>
<ul>
  <li>Register the lower-triangular mask as a <strong>buffer</strong> so it moves with the module across devices but is not treated as an optimizer parameter.<br /></li>
  <li>During autoregressive generation, crop the input context to at most <strong>block_size</strong> to match positional embedding capacity and avoid index overflow.<br /></li>
</ul>

<p>This modular head becomes the building block for multi-head attention and deeper stacks.<br /></p>

<hr />

<h1 id="training-with-a-single-scaled-attention-head-yields-modest-gains-while-multi-head-attention-improves-representation-capacity">Training with a single scaled attention head yields modest gains, while multi-head attention improves representation capacity.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-22-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-22-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-22-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-22-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Replacing the bigram model with a single <strong>self-attention head</strong> enables data-dependent communication across positions and typically reduces validation loss modestly.<br /></p>

<p>Extending to <strong>multi-head attention</strong>:<br /></p>
<ul>
  <li>Creates parallel communication channels (heads) that each capture different pairwise or feature-specific interactions.<br /></li>
  <li>Concatenate head outputs and project back to the embedding dimension to enrich representational capacity.<br /></li>
  <li>Empirically, multi-head attention reduces validation loss further vs. a single head and is analogous to grouped convolutions where separate groups learn complementary patterns.<br /></li>
</ul>

<hr />

<h1 id="add-a-per-position-feed-forward-network-to-allow-tokens-to-process-gathered-context-independently">Add a per-position feed-forward network to allow tokens to process gathered context independently.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-27-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-27-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-27-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-27-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>After attention aggregates information across positions, apply a <strong>position-wise MLP</strong> (linear → nonlinearity → linear) independently to each token to let it transform and reason about the aggregated context.<br /></p>

<p>Notes:<br /></p>
<ul>
  <li>The feed-forward block is applied identically and independently at each time position, complementing communication from attention.<br /></li>
  <li>Interleaving attention and feed-forward layers allows the model to both exchange information and elaborate features hierarchically.<br /></li>
  <li>Adding this stage typically improves expressivity and validation performance.<br /></li>
</ul>

<hr />

<h1 id="stack-transformer-blocks-with-residual-projections-to-enable-deep-architectures">Stack Transformer blocks with residual projections to enable deep architectures.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-31-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-31-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-31-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-31-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Compose <strong>Transformer blocks</strong> that intersperse multi-head self-attention and position-wise feed-forward layers, include <strong>residual (skip) connections</strong> around each sub-layer, and project back to the embedding dimension.<br /></p>

<p>Why this structure matters:<br /></p>
<ul>
  <li>Residual connections provide gradient highways that preserve signal from later loss gradients back to earlier layers, making deep networks feasible and improving optimization dynamics.<br /></li>
  <li>Apply a linear projection after concatenated multi-head outputs to return to the residual pathway and enable parameter-efficient composition.<br /></li>
  <li>This block structure is the canonical <strong>Transformer decoder block</strong> used in GPT-style models.<br /></li>
</ul>

<hr />

<h1 id="use-feed-forward-inner-expansion-and-layernorm-pre-norm-to-stabilize-training">Use feed-forward inner expansion and LayerNorm (pre-norm) to stabilize training.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-35-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-35-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-35-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-35-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Expand the feed-forward inner layer dimensionality (commonly a factor of four) to increase per-token computation capacity, and apply <strong>LayerNorm</strong> before each sub-layer (<strong>pre-norm</strong>) to normalize feature statistics at initialization.<br /></p>

<p>Practical benefits:<br /></p>
<ul>
  <li>Pre-norm LayerNorm normalizes across the embedding dimension per token and includes learned scale and bias.<br /></li>
  <li>These choices improve gradient conditioning compared to post-norm in many setups, reducing training instability in deeper stacks.<br /></li>
  <li>Together with residuals and inner expansion, pre-norm LayerNorm fosters faster convergence and better final performance.<br /></li>
</ul>

<hr />

<h1 id="add-dropout-and-scale-up-model-hyperparameters-to-improve-generalization-and-capacity">Add dropout and scale up model hyperparameters to improve generalization and capacity.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-39-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-39-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-39-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-39-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Introduce <strong>dropout</strong> on attention probabilities and residual outputs as regularization that randomly zeroes activations during training.<br /></p>

<p>Scaling model capacity:<br /></p>
<ul>
  <li>Increase batch size, block size (e.g., 256), embedding dimension (e.g., 384), number of heads, number of layers, and adjust learning rate appropriately.<br /></li>
  <li>With moderate scaling and GPU training (e.g., A100), validation loss can drop substantially, showing how capacity and regularization jointly drive gains.<br /></li>
  <li>Larger models require careful regularization and hyperparameter tuning to remain optimizable.<br /></li>
</ul>

<hr />

<h1 id="a-decoder-only-transformer-implements-autoregressive-language-modeling-encoder-decoder-variants-support-conditioned-sequence-to-sequence-tasks">A decoder-only Transformer implements autoregressive language modeling; encoder-decoder variants support conditioned sequence-to-sequence tasks.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-43-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-43-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-43-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-43-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The implemented model is a <strong>decoder-only Transformer</strong> that uses causal masking so tokens cannot attend to <strong>future</strong> positions, making it suitable for unconditioned autoregressive generation (as in GPT).<br /></p>

<p>Contrast with the original architecture:<br /></p>
<ul>
  <li>“Attention Is All You Need” used an <strong>encoder-decoder</strong> configuration for sequence-to-sequence tasks (e.g., translation): the encoder processes source tokens with full bidirectional attention and the decoder cross-attends to encoder outputs.<br /></li>
  <li><strong>Decoder-only</strong> models omit the encoder and cross-attention, simplifying the architecture for pure language modeling.<br /></li>
  <li>To turn a pre-trained decoder into an assistant-like agent (e.g., ChatGPT), additional <strong>fine-tuning and alignment</strong> stages are required.<br /></li>
</ul>

<hr />

<h1 id="large-scale-models-require-pre-training-on-massive-corpora-then-fine-tuning-and-alignment-to-become-useful-assistants">Large-scale models require pre-training on massive corpora then fine-tuning and alignment to become useful assistants.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-50-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-50-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-50-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-50-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Pre-training and alignment pipeline overview:<br /></p>

<ol>
  <li><strong>Pre-training</strong>: train a large decoder-only Transformer on vast corpora of internet text to learn general language modeling capabilities — model and token scale during pre-training determine much downstream capability.<br /></li>
  <li><strong>Supervised fine-tuning</strong>: adapt the generic model on labeled question-answer pairs and other supervised datasets.<br /></li>
  <li><strong>Preference learning</strong>: collect human preference data and train a <strong>reward model</strong> to score outputs.<br /></li>
  <li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: optimize the sampling policy (e.g., via <strong>PPO</strong>) against the reward model to produce controlled, helpful, and safe assistant behavior.<br /></li>
</ol>

<p>These alignment stages are often data- and compute-intensive beyond pre-training.<br /></p>

<hr />

<h1 id="nanogpt-implementation-details-and-summary-of-the-educational-reproduction">NanoGPT implementation details and summary of the educational reproduction.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec12/01-55-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec12/01-55-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec12/01-55-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec12/01-55-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>NanoGPT</strong> separates model definition and training utilities into concise files (e.g., model.py and train.py) and implements core Transformer features consistent with standard GPT variants.<br /></p>

<p>Included practicalities:<br /></p>
<ul>
  <li>Causal self-attention, multi-head aggregation, position-wise MLPs, <strong>LayerNorm</strong>, residual projections, and generation routines.<br /></li>
  <li>Checkpointing, selective weight decay, optional dropout, device management, and support for scaling/loading checkpoints.<br /></li>
  <li>Educational reproduction shows a complete Transformer-based language model can be implemented in a few hundred lines and, when scaled appropriately on GPU, produce plausible Shakespeare-like output from a small corpus.<br /></li>
</ul>

<p>The project focuses on the <strong>pre-training</strong> stage and leaves complex alignment pipelines (reward models, RLHF) as higher-level extensions.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Karpathy Series - Let’s build the GPT Tokenizer</title><link href="https://tuananhbui89.github.io/blog/2025/karpathy-lec11/" rel="alternate" type="text/html" title="Karpathy Series - Let’s build the GPT Tokenizer" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/karpathy-lec11</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/karpathy-lec11/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/zduSFxRajkE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="tokenization-is-a-necessary-and-often-problematic-preprocessing-step-for-large-language-models">Tokenization is a necessary and often problematic preprocessing step for large language models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-00-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-00-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-00-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-00-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Tokenization</strong> is the process that translates raw text strings into discrete <strong>token sequences</strong> that language models operate on — it converts text into integer indices used to look up trainable <strong>embedding vectors</strong>.<br /></p>

<p>Because tokenization defines the model’s input units, it introduces subtle <strong>failure modes</strong> and <strong>distribution mismatches</strong> that often explain odd model behavior. What looks like an architecture bug frequently traces back to tokenizer choices, so practitioners must understand tokenization in detail to: <br /></p>

<ul>
  <li>Diagnose errors across multilingual and special-character inputs<br /></li>
  <li>Design token vocabularies and preprocessing pipelines<br /></li>
  <li>Evaluate tokenization as a distinct part of the <strong>data pipeline</strong>, engineered and tested independently of model architecture<br /></li>
</ul>

<p>Tokenization therefore requires careful engineering, monitoring, and evaluation rather than being an incidental preprocessing step.<br /></p>

<hr />

<h1 id="a-naive-character-level-tokenizer-maps-each-character-to-a-unique-integer-and-feeds-embeddings-to-the-transformer">A naive character-level tokenizer maps each character to a unique integer and feeds embeddings to the Transformer</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-01-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-01-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-01-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-01-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A <strong>character-level tokenizer</strong> builds a vocabulary of the individual characters observed in the corpus and maps each character to an integer token ID.<br /></p>

<ul>
  <li>Every character in a string becomes one token; the model learns a trainable <strong>embedding row</strong> per character that feeds into the Transformer.<br /></li>
  <li>Pros: simple, stable, and pedagogically useful.<br /></li>
  <li>Cons: highly inefficient for long-range dependencies because sequence lengths are large while embeddings/softmax sizes remain small — this forces long context windows and poor compression.<br /></li>
</ul>

<p>Character tokenizers can work for small-domain tasks, but they lack the compression needed to scale to web-scale corpora without prohibitive context lengths.<br /></p>

<hr />

<h1 id="modern-llm-tokenizers-operate-on-subword-or-byte-level-chunks-and-are-typically-constructed-with-algorithms-like-byte-pair-encoding">Modern LLM tokenizers operate on subword or byte-level chunks and are typically constructed with algorithms like byte-pair encoding</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-03-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-03-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-03-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-03-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>State-of-the-art models use variable-length <strong>subword chunks</strong> or <strong>byte-level units</strong> rather than pure character tokenizers, often built with algorithms like <strong>byte-pair encoding (BPE)</strong>.<br /></p>

<ul>
  <li><strong>BPE</strong> iteratively merges frequently co-occurring token pairs to trade vocabulary size against sequence length.<br /></li>
  <li>Tokens are the atomic units of attention and context, so <strong>vocabulary size</strong> and <strong>chunking</strong> directly determine how much raw text a model can attend to and how it must generalize.<br /></li>
  <li>Papers (e.g., GPT-2) report concrete vocabulary and context settings (for example, ~<strong>50k tokens</strong> and <strong>1,024 token</strong> context) because these numbers materially affect model behavior.<br /></li>
</ul>

<p>Choosing chunking strategy and vocabulary size is therefore fundamental to model capacity and efficiency.<br /></p>

<hr />

<h1 id="tokenization-causes-many-surprising-application-level-behaviors-in-llms">Tokenization causes many surprising application-level behaviors in LLMs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-05-06-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-05-06-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-05-06-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-05-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Many LLM quirks — spelling issues, weak non-English performance, odd JSON/YAML outputs, trailing-space artifacts — often originate in <strong>tokenization</strong> rather than the Transformer itself.<br /></p>

<ul>
  <li>Tokenization determines what the model treats as <strong>atomic units</strong>, which affects training frequency of tokens and the model’s generalization burden.<br /></li>
  <li>Because tokenizers are a preprocessing stage trained separately and can differ across datasets/models, they are a frequent source of <strong>domain mismatch</strong> and hard-to-diagnose bugs.<br /></li>
  <li>Diagnosing these issues typically requires inspection of <strong>token boundaries</strong> and <strong>vocabularies</strong> rather than only model weights or architecture.<br /></li>
</ul>

<hr />

<h1 id="interactive-tokenizer-visualizers-expose-token-boundaries-whitespace-handling-and-model-specific-token-ids">Interactive tokenizer visualizers expose token boundaries, whitespace handling, and model-specific token IDs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-06-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-06-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-06-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-06-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Browser-based <strong>tokenizer visualizers</strong> are practical debugging tools: they split text into colored, indexed tokens and reveal implementation details.<br /></p>

<p>What these tools show and why they matter: <br /></p>

<ul>
  <li>Whether <strong>whitespace</strong> is part of tokens (e.g., “ space+word” vs “word”)<br /></li>
  <li>Token IDs for punctuation, digits, and other glyphs<br /></li>
  <li>Differences in segmentation between tokenizers (e.g., GPT-2 vs GPT-4) for the same input<br /></li>
</ul>

<p>Uses: prompt debugging, measuring token counts, and evaluating tokenizer efficiency for target domains like code or multilingual text. Visual inspection often reveals why identical surface forms are treated differently depending on position, surrounding whitespace, or case.<br /></p>

<hr />

<h1 id="numeric-strings-are-tokenized-inconsistently-and-arbitrarily-which-impairs-arithmetic-and-digit-level-tasks">Numeric strings are tokenized inconsistently and arbitrarily which impairs arithmetic and digit-level tasks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-08-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-08-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-08-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-08-27.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>There is no canonical way to tokenize <strong>numbers</strong> across models: some numeral sequences are a single token, others split into several tokens, determined by training-time merges rather than numeric semantics.<br /></p>

<ul>
  <li>This arbitrary segmentation complicates digit-oriented tasks (digit manipulation, arithmetic, exact string ops) because the model lacks a consistent per-digit representation.<br /></li>
  <li>When correctness requires digit-level manipulation, you often must force explicit <strong>character-level handling</strong> in prompts or preprocessing to ensure predictable tokenization.<br /></li>
</ul>

<hr />

<h1 id="tokenization-is-case-sensitive-and-context-sensitive-causing-identical-surface-strings-to-map-to-different-tokens">Tokenization is case-sensitive and context-sensitive, causing identical surface strings to map to different tokens</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-10-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-10-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-10-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-10-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Tokenizers often treat case and surrounding whitespace as part of token identity: uppercase vs lowercase or tokens at sentence starts vs after whitespace can map to different IDs.<br /></p>

<ul>
  <li>The same letters can have different token IDs depending on <strong>case</strong> and whether a <strong>leading space</strong> is present.<br /></li>
  <li>Consequence: the model may need separate embeddings or must rely on parameter-sharing to associate variants, increasing data requirements and fragmentation.<br /></li>
  <li>Mitigation: normalize or augment training/prompt data to reduce rare-token fragmentation and make variants more consistent.<br /></li>
</ul>

<hr />

<h1 id="poor-tokenization-of-indentation-and-repeated-whitespace-makes-code-inefficient-to-represent-and-reduces-effective-context">Poor tokenization of indentation and repeated whitespace makes code inefficient to represent and reduces effective context</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-12-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-12-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-12-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-12-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When a tokenizer treats each space as a distinct token (as older GPT-2 tokenizers did), whitespace-sensitive formats like Python become <strong>token-bloat</strong> heavy and consume large parts of the model’s fixed context window.<br /></p>

<ul>
  <li>Denser tokenizers that group repeated spaces or indentation into single tokens substantially improve effective context utilization.<br /></li>
  <li>For code modeling, grouping common indentation patterns into tokens is a practical optimization that increases the amount of useful code the model can attend to without changing the architecture.<br /></li>
</ul>

<hr />

<h1 id="improvements-in-tokenization-larger-vocabulary-and-better-whitespace-grouping-materially-improve-model-performance-on-code-and-length-of-context">Improvements in tokenization (larger vocabulary and better whitespace grouping) materially improve model performance on code and length of context</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-13-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-13-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-13-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-13-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Increasing <strong>vocabulary size</strong> (e.g., from ~50k to ~100k) while engineering merges that group common whitespace and indentation can reduce sequence length and densify inputs.<br /></p>

<ul>
  <li>Effect: the same Transformer architecture can attend to more raw characters per context length, improving tasks like code completion.<br /></li>
  <li>Trade-off: larger vocabularies increase <strong>embedding table</strong> size and <strong>softmax</strong> cost at the output, so vocabulary size is a hyperparameter balancing compression vs. parameter overhead.<br /></li>
  <li>Practical gain comes from careful tokenizer design and training on representative corpora (including code) to learn useful merges.<br /></li>
</ul>

<hr />

<h1 id="strings-in-python-are-sequences-of-unicode-code-points-ord-reveals-codepoint-integers-but-using-them-directly-as-tokens-is-problematic">Strings in Python are sequences of Unicode code points; ord() reveals codepoint integers but using them directly as tokens is problematic</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-16-36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-16-36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-16-36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-16-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Python strings represent text as immutable sequences of <strong>Unicode code points</strong> (roughly 150k code points across scripts). The built-in ord() maps a single character to its Unicode code point integer.<br /></p>

<ul>
  <li>Using code point integers directly as tokens creates a very large, brittle vocabulary (and is sensitive to Unicode updates).<br /></li>
  <li>Raw code points also fail to provide sequence compression or control over vocabulary density, motivating the use of byte encodings and <strong>subword schemes</strong> instead.<br /></li>
</ul>

<hr />

<h1 id="byte-encodings-utf-81632-transform-code-points-into-byte-streams-utf-8-is-the-ubiquitous-variable-length-choice">Byte encodings (UTF-8/16/32) transform code points into byte streams; UTF-8 is the ubiquitous variable-length choice</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-21-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-21-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-21-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-21-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Unicode has multiple binary encodings: <strong>UTF-8</strong> (1–4 bytes, ASCII-compatible), <strong>UTF-16</strong> (2 or 4 bytes), and <strong>UTF-32</strong> (fixed 4 bytes).<br /></p>

<ul>
  <li>UTF-8 is the web’s de facto standard because it is compact for ASCII and backward-compatible.<br /></li>
  <li>Encoding text to UTF-8 yields a sequence of bytes that can serve as a base representation for tokenization.<br /></li>
  <li>But treating raw <strong>bytes</strong> as tokens gives a tiny vocabulary (256) and very long sequences, which is inefficient for autoregressive models with limited attention — so further compression (e.g., BPE) is required.<br /></li>
</ul>

<hr />

<h1 id="byte-pair-encoding-bpe-compresses-byte-or-character-sequences-by-iteratively-merging-frequent-adjacent-pairs-into-new-tokens">Byte-Pair Encoding (BPE) compresses byte or character sequences by iteratively merging frequent adjacent pairs into new tokens</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-25-26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-25-26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-25-26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-25-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Byte-Pair Encoding (BPE)</strong> starts from a base vocabulary (bytes or observed code points) and repeatedly merges the most frequent adjacent token pairs to create new tokens.<br /></p>

<ol>
  <li>Compute frequencies of adjacent token pairs across the corpus.<br /></li>
  <li>Select the most frequent pair and create a new token representing their concatenation.<br /></li>
  <li>Replace all occurrences of that pair with the new token, incrementing vocabulary size by one and reducing average sequence length.<br /></li>
</ol>

<p>This loop continues until a target vocabulary size is reached, producing a <strong>merges table</strong> and a <strong>token-to-bytes</strong> mapping used for deterministic encoding/decoding. BPE provides a tunable trade-off between vocabulary size and sequence compression.<br /></p>

<hr />

<h1 id="implementing-bpe-requires-counting-adjacent-pair-frequencies-and-applying-pair-replacements-practical-code-iterates-until-the-target-vocab-size">Implementing BPE requires counting adjacent pair frequencies and applying pair replacements; practical code iterates until the target vocab size</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-30-13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-30-13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-30-13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-30-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A BPE training loop operates as follows:<br /></p>

<ol>
  <li>Count statistics of all consecutive token pairs in the corpus.<br /></li>
  <li>Select the most frequent pair.<br /></li>
  <li>Mint a new token identifier for that pair and replace every occurrence in the corpus.<br /></li>
  <li>Update pair statistics and repeat until the desired number of merges (vocabulary size) is reached.<br /></li>
</ol>

<p>Implementation notes and pitfalls: <br /></p>

<ul>
  <li>Use robust pair counting and efficient data structures to avoid quadratic costs.<br /></li>
  <li>Perform careful in-place replacement to avoid index errors when spans overlap.<br /></li>
  <li>Track merges as parent→children mappings to support later encoding and decoding.<br /></li>
</ul>

<p>The final merges list plus the base token mapping form the tokenizer able to compress inputs deterministically using the learned merge sequence.<br /></p>

<hr />

<h1 id="training-a-tokenizer-on-larger-corpora-and-choosing-the-merge-count-determines-compression-ratio-and-vocabulary-composition">Training a tokenizer on larger corpora and choosing the merge count determines compression ratio and vocabulary composition</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-37-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-37-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-37-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-37-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>When BPE is trained on longer, representative corpora, pair frequencies stabilize and more useful merges are learned.<br /></p>

<ul>
  <li>Increasing merges produces larger vocabularies and greater compression; the merges dictionary forms a hierarchical forest of binary merges enabling encoding/decoding by concatenation.<br /></li>
  <li>The compression ratio scales with the number of merges and corpus characteristics, so vocabulary size is tuned to balance embedding/softmax cost against available context length.<br /></li>
</ul>

<hr />

<h1 id="the-tokenizer-is-a-separate-preprocessing-artifact-with-its-own-training-set-and-resulting-encodedecode-functions">The tokenizer is a separate preprocessing artifact with its own training set and resulting encode/decode functions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-44-31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-44-31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-44-31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-44-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A trained tokenizer is an independent, serializable artifact that maps raw text to token IDs and back.<br /></p>

<ul>
  <li>Typical outputs: the base <strong>token-to-bytes</strong> map (vocab) and the <strong>merges</strong> table.<br /></li>
  <li>Tokenizers are usually trained beforehand (possibly on a different dataset than the LM corpus) and applied once to the LM training corpus to produce token streams stored for model training.<br /></li>
</ul>

<p>Because tokenizers determine the effective distribution the model sees, tokenizer choices materially affect the downstream model and must be versioned and validated as separate artifacts.<br /></p>

<hr />

<h1 id="decoding-tokens-to-text-concatenates-token-byte-sequences-and-decodes-via-utf-8-with-error-handling">Decoding tokens to text concatenates token byte sequences and decodes via UTF-8 with error handling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-51-21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-51-21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-51-21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-51-21.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Decoding</strong> maps each token ID back to its bytes representation, concatenates those bytes, and decodes using UTF-8 to produce a Python string.<br /></p>

<ul>
  <li>Not every byte sequence is valid UTF-8, so decoders must specify an <strong>error policy</strong> (e.g., errors=’replace’) to handle invalid sequences robustly.<br /></li>
  <li>Robust decoders therefore maintain the vocab mapping and use UTF-8 decoding with replacement semantics to avoid exceptions and to surface out-of-distribution outputs safely.<br /></li>
</ul>

<hr />

<h1 id="encoding-text-into-tokens-applies-merges-in-merge-order-and-must-respect-merge-eligibility-and-order-constraints">Encoding text into tokens applies merges in merge-order and must respect merge eligibility and order constraints</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/00-59-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/00-59-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/00-59-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/00-59-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Encoding</strong> proceeds by converting text to UTF-8 bytes, mapping bytes to initial token IDs, and then applying permitted merges in the exact order they were created during training.<br /></p>

<p>A practical encoder typically: <br /></p>

<ol>
  <li>Convert text → UTF-8 bytes → initial token list.<br /></li>
  <li>Compute adjacent-pair candidates on the current token list.<br /></li>
  <li>Look up which candidates appear in the merges table, prioritizing merges with smaller merge indices (earlier merges).<br /></li>
  <li>Replace eligible pairs iteratively until no eligible merges remain or the sequence is fully merged.<br /></li>
</ol>

<p>Edge cases: handle short inputs, ensure merge lookups return sentinels for ineligible pairs, and preserve merge order to guarantee deterministic encoding.<br /></p>

<hr />

<h1 id="real-world-tokenizers-add-manual-heuristics-to-bpe-regex-chunking-prevents-semantically-bad-merges">Real-world tokenizers add manual heuristics to BPE: regex chunking prevents semantically bad merges</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/01-06-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/01-06-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/01-06-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/01-06-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>OpenAI’s GPT-2 tokenizer applies a preprocessing <strong>chunking</strong> step (a complex regex) that segments text into categories — letters, numbers, punctuation, whitespace — and runs BPE only within those chunks.<br /></p>

<ul>
  <li>Rationale: prevent undesirable merges such as joining words to punctuation or mixing letters with numbers, which would create many spurious tokens and fragment the vocabulary (e.g., treating ‘dog.’ differently from ‘dog?’).<br /></li>
  <li>These handcrafted chunking heuristics make merges more semantically coherent than blind corpus-level BPE and are important production refinements.<br /></li>
</ul>

<hr />

<h1 id="regex-based-chunking-contains-many-subtle-language-and-unicode-issues-apostrophes-case-whitespace-handling">Regex-based chunking contains many subtle language and Unicode issues (apostrophes, case, whitespace handling)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/01-11-21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/01-11-21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/01-11-21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/01-11-21.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The chunking regex must handle language-agnostic classes while accounting for script-specific details and whitespace behavior.<br /></p>

<ul>
  <li>It needs to consider accent marks, Unicode apostrophes and glyph variants, case sensitivity, and whitespace grouping.<br /></li>
  <li>Small differences (e.g., ignoring a specific apostrophe glyph or case) change merge eligibility, which affects token frequency and downstream performance.<br /></li>
  <li>The regex often preserves <strong>leading spaces</strong> as part of tokens (so tokens encode “ space+word” vs “word”) and uses lookahead logic to prioritize common “space+word” combinations.<br /></li>
</ul>

<p>These heuristics are performant but brittle — they require careful testing across multilingual and code corpora to avoid edge-case failures.<br /></p>

<hr />

<h1 id="the-inference-code-for-gpt-2s-tokenizer-encoderpy-implements-bpe-application-but-the-original-training-code-was-not-released">The inference code for GPT-2’s tokenizer (encoder.py) implements BPE application but the original training code was not released</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec11/01-16-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec11/01-16-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec11/01-16-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec11/01-16-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The released GPT-2 encoder artifacts enable deterministic encoding/decoding even though the original merge-training code was not published.<br /></p>

<ul>
  <li>Two artifacts: <strong>encoder.json</strong> (maps token IDs to byte sequences) and <strong>vocab.bpe</strong> (lists the merge operations).<br /></li>
  <li>The released encoder implements the same BPE encode/decode flow: iteratively apply merges according to the merges table and provide encode/decode functions for inference.<br /></li>
</ul>

<p>Using these two artifacts, one can reproduce GPT-2’s tokenization behavior exactly for encoding, decoding, and debugging purposes.<br /></p>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Karpathy Series - Let’s reproduce GPT-2</title><link href="https://tuananhbui89.github.io/blog/2025/karpathy-lec10/" rel="alternate" type="text/html" title="Karpathy Series - Let’s reproduce GPT-2" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/karpathy-lec10</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/karpathy-lec10/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/l8pRSuU81PU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="project-goal-reproduce-gpt-2-124m-model">Project goal: reproduce GPT-2 124M model</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-01-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-01-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-01-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-01-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Objective: reproduce the <strong>GPT-2 124M</strong> model (the smallest GPT-2 checkpoint) by implementing the model, loading reference weights, and training from scratch until validation and downstream metrics match or exceed the released checkpoint.<br /></p>

<p>The reproduction relies on <strong>published papers</strong>, <strong>released weights</strong>, and <strong>Hugging Face conversions</strong> to ensure architectural fidelity, while using modern tooling for training and evaluation.<br /></p>

<p>Target model specs to match exactly:</p>
<ul>
  <li><strong>12 Transformer layers</strong><br /></li>
  <li><strong>Hidden dimension 768</strong><br /></li>
  <li><strong>Vocabulary consistent with the GPT-2 tokenizer</strong><br /></li>
</ul>

<p>Strict attention to <strong>hyperparameters</strong> and <strong>initialization</strong> is required to replicate performance.<br /></p>

<hr />

<h1 id="hugging-face-conversion-used-to-obtain-pytorch-weights-and-state-dict">Hugging Face conversion used to obtain PyTorch weights and state dict</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-04-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-04-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-04-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-04-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Hugging Face Transformers</strong> provides a PyTorch implementation and converted state dictionaries that map the original TensorFlow GPT-2 weights into PyTorch tensors.<br /></p>

<p>Practical uses:</p>
<ul>
  <li>Load a <strong>HF GPT2LMHeadModel</strong> and inspect its <strong>state_dict</strong> to learn parameter names and shapes.<br /></li>
  <li>The state_dict reveals <strong>token embeddings</strong>, <strong>positional encodings</strong>, <strong>per-layer attention and MLP weights</strong>, and <strong>LM head</strong> weights.<br /></li>
</ul>

<p>These raw tensors guide:</p>
<ul>
  <li><strong>Parameter initialization</strong> for a from-scratch model<br /></li>
  <li><strong>Key/name mapping</strong> and any necessary <strong>transpositions</strong> from original formats<br /></li>
</ul>

<p>Using the HF model simplifies access to the canonical <strong>124M parameters</strong> while enabling reimplementation and verification.<br /></p>

<hr />

<h1 id="token-and-positional-embeddings-shapes-and-semantics">Token and positional embeddings shapes and semantics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-08-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-08-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-08-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-08-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Token embeddings</strong> form a matrix of shape <strong>(vocab_size x d_model)</strong> — for GPT-2 this is typically <strong>50257 x 768</strong>, giving a <strong>768-dimensional</strong> vector per token.<br /></p>

<p><strong>Positional embeddings</strong> are a learned table of length equal to the maximum context (e.g., <strong>1024 x 768</strong>) and are <strong>added</strong> to token embeddings before the Transformer blocks.<br /></p>

<p>Key points:</p>
<ul>
  <li>Embeddings produce <strong>distributed representations</strong> for tokens and absolute positions.<br /></li>
  <li>Per-row and per-column patterns can show <strong>sinusoidal-like structure</strong> and <strong>channel-specific activations</strong> that arise from optimization (not explicit sinusoidal initialization).<br /></li>
  <li>Understanding these shapes is essential for <strong>exact parameter mapping</strong> and architecture reconstruction.<br /></li>
</ul>

<hr />

<h1 id="verify-generation-from-released-gpt-2-weights">Verify generation from released GPT-2 weights</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-12-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-12-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-12-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-12-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Loading the official <strong>GPT-2 124M</strong> weights via Hugging Face and running the <strong>text-generation pipeline</strong> provides a functional correctness check.<br /></p>

<p>What to expect and check:</p>
<ul>
  <li>Sampled continuations should be <strong>coherent</strong> and consistent with a pretrained LM.<br /></li>
  <li>Differences in outputs can come from <strong>RNG state</strong>, <strong>sampling defaults</strong> (top-k/top-p), and <strong>tokenization choices</strong>.<br /></li>
</ul>

<p>Why this matters:</p>
<ul>
  <li>Coherent generations confirm correct <strong>weight loading</strong> and <strong>token/position mapping</strong>.<br /></li>
  <li>This establishes a baseline target for models <strong>retrained from scratch</strong> and provides an empirical comparison point.<br /></li>
</ul>

<hr />

<h1 id="gpt-2-architecture-differences-from-the-original-transformer">GPT-2 architecture differences from the original Transformer</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-15-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-15-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-15-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-15-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>GPT-2</strong> is a <strong>decoder-only Transformer</strong> with two main departures from the original “Attention Is All You Need” design:<br /></p>

<ul>
  <li><strong>Pre-norm vs post-norm</strong>: GPT-2 reshuffles where <strong>LayerNorm</strong> is placed (pre-norm layout).<br /></li>
  <li><strong>Extra final LayerNorm</strong>: an additional final layer norm appears before the LM head.<br /></li>
</ul>

<p>Other structural notes:</p>
<ul>
  <li>The model omits <strong>encoder-decoder cross-attention</strong> — it contains only <strong>masked self-attention</strong> and <strong>feed-forward</strong> sublayers repeated for the specified depth (e.g., 12 layers).<br /></li>
</ul>

<p>Reimplementations must:</p>
<ul>
  <li>Mirror these structural differences exactly<br /></li>
  <li>Follow the <strong>naming/schema</strong> used by reference implementations (e.g., Hugging Face) for exact parameter correspondence and easy weight loading.<br /></li>
</ul>

<hr />

<h1 id="block-level-computation-pre-normalization-residuals-attention-vs-mlp">Block-level computation: pre-normalization, residuals, attention vs MLP</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-19-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-19-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-19-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-19-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Each Transformer block is composed of:</p>
<ol>
  <li>A <strong>pre-normalized attention sublayer</strong> with residual connection<br /></li>
  <li>A <strong>pre-normalized MLP (feed-forward) sublayer</strong> with residual connection<br /></li>
</ol>

<p>Functional roles:</p>
<ul>
  <li><strong>Attention</strong> is a reduce operation: weighted aggregation across tokens enabling inter-token communication.<br /></li>
  <li><strong>MLP</strong> is a per-token map: processes each token independently to transform its representation.<br /></li>
</ul>

<p>Implementation implications:</p>
<ul>
  <li>The <strong>pre-norm</strong> layout places LayerNorm before the sublayer transforms, which affects <strong>gradient flow</strong> and training dynamics.<br /></li>
  <li>Careful implementation is required to match GPT-2’s training behavior and stability.<br /></li>
</ul>

<hr />

<h1 id="mlp-design-and-the-gelugated-nonlinearity-choice">MLP design and the GELU/Gated nonlinearity choice</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-22-22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-22-22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-22-22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-22-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>MLP</strong> (feed-forward) consists of:</p>
<ul>
  <li>Two linear projections with a nonlinearity between them.<br /></li>
</ul>

<p>Activation detail:</p>
<ul>
  <li>GPT-2 uses a <strong>GELU variant</strong> (an approximate GELU historically used in TensorFlow for performance).<br /></li>
  <li><strong>GELU</strong> provides non-zero gradients in regions where ReLU would be exactly zero, mitigating the dead-ReLU issue and improving optimization stability.<br /></li>
</ul>

<p>Reproduction guidance:</p>
<ul>
  <li>Use the same <strong>approximate activation</strong> (or exact GELU if hardware allows) because activation asymptotics and gradient behavior impact training dynamics and final performance.<br /></li>
</ul>

<hr />

<h1 id="multi-head-self-attention-implementation-and-tensor-reshaping-tricks">Multi-head self-attention implementation and tensor reshaping tricks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-25-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-25-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-25-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-25-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Multi-headed attention</strong> implementation steps:</p>
<ol>
  <li>Linearly project input to combined <strong>Q/K/V</strong> tensors.<br /></li>
  <li>Reshape to separate <strong>head</strong> and <strong>batch</strong> dimensions.<br /></li>
  <li>Perform batched <strong>scaled dot-product attention</strong> with <strong>causal masking</strong>.<br /></li>
  <li>Concatenate head outputs and apply a final linear projection.<br /></li>
</ol>

<p>Performance notes:</p>
<ul>
  <li>Efficient PyTorch implementations treat the <strong>head dimension</strong> as an additional batch dimension to use parallel kernels and reduce Python overhead.<br /></li>
  <li>Ensure mathematical equivalence to per-head implementations is preserved.<br /></li>
  <li>Matching <strong>naming</strong> and <strong>parameter layout</strong> to Hugging Face conventions simplifies weight transfer and ensures functional equivalence.<br /></li>
</ul>

<hr />

<h1 id="loading-hugging-face-weights-into-custom-gpt-class-and-tf-pytorch-transpositions">Loading Hugging Face weights into custom GPT class and TF-&gt;PyTorch transpositions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-30-21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-30-21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-30-21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-30-21.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Transferring parameters from Hugging Face (or TF-origin) to a from-scratch PyTorch GPT class requires careful mapping and conversion:<br /></p>

<p>Recommended procedure:</p>
<ul>
  <li>Iterate over HF <strong>state_dict</strong> keys and map them to local module names.<br /></li>
  <li>Optionally ignore non-parameter buffers (e.g., static causal mask buffers).<br /></li>
  <li>Identify matrices requiring <strong>transpose</strong> due to TF-to-PyTorch layout differences and apply transposition.<br /></li>
  <li>Verify <strong>shape equality</strong> after mapping.<br /></li>
</ul>

<p>Encapsulation:</p>
<ul>
  <li>Implement a robust <strong>from_pretrained</strong> class method that performs these conversions and returns a PyTorch model whose state tensors exactly match the reference numerics for generation and evaluation.<br /></li>
</ul>

<hr />

<h1 id="forward-pass-and-logits-shape-semantics">Forward pass and logits shape semantics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-36-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-36-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-36-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-36-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The forward pass semantics:</p>
<ul>
  <li>Input: token indices shaped <strong>(B x T)</strong>.<br /></li>
  <li>Output: logits shaped <strong>(B x T x V)</strong>, where <strong>V</strong> is vocabulary size.<br /></li>
</ul>

<p>Computation flow:</p>
<ol>
  <li>Sum <strong>token</strong> and <strong>positional</strong> embeddings (positional broadcast across batch rows).<br /></li>
  <li>Pass through the <strong>Transformer blocks</strong>.<br /></li>
  <li>Apply the <strong>final layer norm</strong> and the <strong>LM head</strong> linear projection to produce logits.<br /></li>
  <li>Convert logits to probabilities via <strong>softmax</strong> for sampling or use directly for <strong>cross-entropy</strong> loss.<br /></li>
</ol>

<p>Implementation must ensure:</p>
<ul>
  <li>Correct tensor shapes and broadcasting.<br /></li>
  <li>Device-consistent tensors to avoid runtime errors during forward and loss computation.<br /></li>
</ul>

<hr />

<h1 id="sampling-loop-top-k-sampling-and-pipeline-differences">Sampling loop, top-k sampling, and pipeline differences</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-42-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-42-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-42-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-42-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Autoregressive generation pattern:</p>
<ol>
  <li>Loop and append one sampled token at a time, using only last-step logits to reduce computation.<br /></li>
  <li>Apply <strong>top-k filtering</strong> (e.g., k=50), renormalize, and sample to avoid very rare tokens and improve coherence.<br /></li>
</ol>

<p>Practical tips:</p>
<ul>
  <li>Use <strong>torch.no_grad</strong> to avoid saving intermediate tensors for backward passes.<br /></li>
  <li>Carefully manage <strong>RNG seeds</strong> and torch.Generator objects to isolate sampling randomness from training RNG state.<br /></li>
  <li>Differences in HF pipeline defaults (top-k, top-p, temperature) can cause the same seed to produce different outputs across implementations.<br /></li>
</ul>

<hr />

<h1 id="device-autodetection-and-cross-backend-compatibility-cuda-mps-cpu">Device autodetection and cross-backend compatibility (CUDA, MPS, CPU)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-48-19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-48-19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-48-19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-48-19.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Training code should detect and use available devices (CUDA GPU, Apple MPS, or CPU) and move model tensors and inputs to the same device to avoid mismatches.<br /></p>

<p>Best practices:</p>
<ul>
  <li>Use device-aware tensor creation (e.g., torch.arange(…, device=idx.device)) so forward logic stays device-agnostic.<br /></li>
  <li>When GPUs are unavailable, code should still run on <strong>CPU</strong> or <strong>MPS</strong> for debugging (slower but functional).<br /></li>
  <li>Log the chosen device and adapt batch sizes to fit memory constraints for reproducible behavior across hardware backends.<br /></li>
</ul>

<hr />

<h1 id="batch-shaping-for-transformer-training-converting-long-streams-to-b-x-t-tensors">Batch shaping for Transformer training: converting long streams to B x T tensors</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-53-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-53-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-53-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-53-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Construct training batches from a token stream by reshaping contiguous token arrays into <strong>(B x T)</strong> tensors, where each row is a context sequence up to block size T.<br /></p>

<p>Label construction:</p>
<ul>
  <li>Load an extra token per row (B*T + 1) and slice into inputs X (all except last) and targets Y (all except first) so each input position has a next-token label.<br /></li>
</ul>

<p>Benefits:</p>
<ul>
  <li>Efficient batched training with B independent sequences for parallel computation.<br /></li>
  <li>Ensures last-token targets are present for loss computation.<br /></li>
</ul>

<hr />

<h1 id="loss-computation-for-next-token-prediction-and-flattening-for-crossentropy">Loss computation for next-token prediction and flattening for CrossEntropy</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/00-58-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/00-58-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/00-58-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/00-58-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>For language-model cross-entropy:</p>
<ul>
  <li>Flatten logits from <strong>(B x T x V)</strong> to <strong>(B<em>T x V)** and targets from **(B x T)** to **(B</em>T)</strong> because PyTorch’s F.cross_entropy expects 2D logits and 1D targets.<br /></li>
</ul>

<p>Forward API:</p>
<ul>
  <li>When targets are provided, return both <strong>logits</strong> and <strong>loss</strong> from forward to centralize computation.<br /></li>
</ul>

<p>Sanity checks:</p>
<ul>
  <li>Initial random-model loss approximates <strong>-log(1/V)</strong> (≈ ln(V); e.g., ~10.8 for V≈50k), which helps validate initialization.<br /></li>
</ul>

<p>Care:</p>
<ul>
  <li>Correct flattening, masking out-of-range positions (if used), and reduction semantics are crucial to stable training.<br /></li>
</ul>

<hr />

<h1 id="simple-optimization-loop-and-debugging-overfitting-a-single-batch">Simple optimization loop and debugging overfitting a single batch</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-03-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-03-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-03-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-03-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A minimal training loop:</p>
<ol>
  <li>Create an optimizer (recommend <strong>AdamW</strong>).<br /></li>
  <li>Zero gradients.<br /></li>
  <li>Compute loss and call <strong>loss.backward()</strong>.<br /></li>
  <li>Call <strong>optimizer.step()</strong> to update parameters.<br /></li>
</ol>

<p>Debugging checks:</p>
<ul>
  <li>Verify the model can <strong>overfit a small batch</strong> — this confirms forward/loss/backward pathways are correct.<br /></li>
  <li>Start without gradient accumulation to simplify debugging; initial LR defaults (e.g., 3e-4) are reasonable for quick overfit tests.<br /></li>
</ul>

<p>Watch for device mismatches — ensure all buffers and tensors live on the same device.<br /></p>

<hr />

<h1 id="simple-sequential-data-loader-sharded-chunk-iteration">Simple sequential data loader (sharded chunk iteration)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-08-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-08-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-08-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-08-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A minimal data loader approach:</p>
<ul>
  <li>Iterate through a tokenized corpus in fixed-size chunks of <strong>B*T</strong> tokens.<br /></li>
  <li>Return (X, Y) pairs by advancing a read pointer by <strong>B*T</strong> each time and loop to the start when exhausted.<br /></li>
</ul>

<p>Properties:</p>
<ul>
  <li>Deterministic, epoch-based iteration without replacement until a full pass completes.<br /></li>
  <li>Sharding the corpus into fixed-size shards simplifies I/O and parallel processing.<br /></li>
</ul>

<p>Extensions:</p>
<ul>
  <li>For multi-epoch training, shuffle document order and permute shards per epoch to avoid ordering effects.<br /></li>
</ul>

<hr />

<h1 id="weight-tying-of-token-embeddings-and-lm-head-shared-embedding">Weight tying of token embeddings and LM head (shared embedding)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-13-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-13-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-13-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-13-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Weight tying</strong> reuses the same matrix for input token embeddings and the output LM head projection (pre-softmax), yielding:</p>
<ul>
  <li>Substantial parameter savings and an <strong>inductive bias</strong> tying input/output representations.<br /></li>
</ul>

<p>Implementation in PyTorch:</p>
<ul>
  <li>Assign the LM head weight tensor to <strong>reference the embedding weight tensor</strong> (share the same storage pointer) so gradients accumulate into the same parameter.<br /></li>
</ul>

<p>Effects:</p>
<ul>
  <li>Reduces parameter count (e.g., ~40M saved in the 124M model).<br /></li>
  <li>Often improves sample efficiency and generalization.<br /></li>
</ul>

<hr />

<h1 id="initialization-conventions-fixed-std-and-residual-scaling-for-deep-stacks">Initialization conventions: fixed std and residual scaling for deep stacks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-25-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-25-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-25-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-25-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>GPT-2 reference initializations and variance control:</p>
<ul>
  <li>Most linear weights use <strong>normal initialization with std=0.02</strong> and zero biases.<br /></li>
  <li>Embedding stds: e.g., token embeddings <strong>0.02</strong>, position embeddings sometimes <strong>0.01</strong> in some codebases.<br /></li>
</ul>

<p>Residual-sum scaling:</p>
<ul>
  <li>Scale-down residual block weights by <strong>1/sqrt(n)</strong> to compensate for variance growth in a sum-of-residuals architecture.<br /></li>
  <li>Implement by multiplying initialization std by <strong>1/sqrt(2*L)</strong> where <strong>L</strong> is the number of Transformer layers (two residual contributions per layer).<br /></li>
</ul>

<p>Why this matters:</p>
<ul>
  <li>These initialization details stabilize optimization and mirror original training dynamics.<br /></li>
</ul>

<hr />

<h1 id="floating-point-precision-tradeoffs-and-tf32-impact">Floating-point precision tradeoffs and TF32 impact</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-35-34-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-35-34-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-35-34-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-35-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Modern GPU precisions overview:</p>
<ul>
  <li><strong>FP32</strong>: baseline full precision, stable but limited throughput.<br /></li>
  <li><strong>TF32</strong>: a cropped-mantissa FP32 variant on Ampere that executes faster on tensor cores by truncating some mantissa bits while preserving exponent range.<br /></li>
</ul>

<p>Practical notes:</p>
<ul>
  <li>Enable TF32 in PyTorch (e.g., torch.set_float32_matmul_precision(‘high’)) to transparently use faster tensor-core kernels.<br /></li>
  <li>Gains depend on whether workloads are compute-bound vs memory-bound; expect modest numerical impact but potentially large throughput improvements.<br /></li>
</ul>

<hr />

<h1 id="mixed-precision-bf16-and-autocast-benefits">Mixed precision (BF16) and autocast benefits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-42-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-42-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-42-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-42-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>BFloat16 (BF16)</strong> reduces storage and memory transfer cost by truncating mantissa while preserving exponent range, avoiding many of FP16’s numerical issues.<br /></p>

<p>Best practice:</p>
<ul>
  <li>Use <strong>torch.autocast</strong> to lower eligible ops (matmuls, convolutions) to BF16 while keeping sensitive ops (layernorm, softmax, loss) in FP32.<br /></li>
</ul>

<p>Requirements and benefits:</p>
<ul>
  <li>Requires hardware support (e.g., Ampere GPUs).<br /></li>
  <li>Often obviates explicit gradient scaling and yields substantial throughput and memory improvements when used carefully.<br /></li>
</ul>

<hr />

<h1 id="torchcompile-reducing-python-overhead-and-enabling-kernel-fusion">torch.compile: reducing Python overhead and enabling kernel fusion</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-50-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-50-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-50-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-50-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>torch.compile</strong> (PyTorch compilation) analyzes the model graph and generates optimized kernels to reduce Python overhead and enable operator/kernel fusion.<br /></p>

<p>Benefits:</p>
<ul>
  <li>Removes repeated operator dispatch, fuses elementwise sequences, and can reduce global memory round-trips.<br /></li>
  <li>Typically yields multi-fold speedups for repeated training iterations after an upfront compilation cost.<br /></li>
</ul>

<hr />

<h1 id="flashattention-fused-attention-kernel-that-avoids-materializing-full-attention-matrices">FlashAttention: fused attention kernel that avoids materializing full attention matrices</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/01-57-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/01-57-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/01-57-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/01-57-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>FlashAttention</strong> is a fused GPU kernel for masked scaled-dot-product attention that avoids materializing the full T x T attention matrix in HBM.<br /></p>

<p>How it works and why it helps:</p>
<ul>
  <li>Uses an online softmax normalization trick and block-wise accumulation to reduce expensive HBM reads/writes.<br /></li>
  <li>Orchestrates shared-memory usage and scaling to compute attention in streaming blocks.<br /></li>
  <li>Preserves functional semantics for causal attention while lowering memory footprint and improving runtime compared to naive attention implementations.<br /></li>
</ul>

<hr />

<h1 id="padding-vocabulary-and-tensor-sizes-to-nice-multiples-for-cuda-kernels">Padding vocabulary and tensor sizes to ‘nice’ multiples for CUDA kernels</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/02-05-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/02-05-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/02-05-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/02-05-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Hardware kernel considerations:</p>
<ul>
  <li>Many CUDA kernels and tensor-core implementations are optimized for tile sizes and powers-of-two factors.<br /></li>
  <li>Awkward dimensions (e.g., vocab size <strong>50257</strong>) can trigger slow boundary kernels and reduce throughput.<br /></li>
</ul>

<p>Mitigation:</p>
<ul>
  <li><strong>Pad the vocabulary</strong> to a nearby friendly number (e.g., <strong>50304</strong>) to align internal loops with preferred tile sizes.<br />
Trade-offs:</li>
  <li>Small extra memory used for padded rows; functionally benign if token indices never reference padded rows.<br /></li>
  <li>Optimizer must learn to drive unused-token logits down, which is usually negligible cost.<br /></li>
</ul>

<hr />

<h1 id="optimizer-choices-adamw-fused-optimizer-kernels-and-parameter-grouping">Optimizer choices: AdamW, fused optimizer kernels, and parameter grouping</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/02-17-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/02-17-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/02-17-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/02-17-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>AdamW</strong> (Adam with decoupled weight decay) is the recommended optimizer for Transformer training due to adaptive moments and per-parameter scaling.<br /></p>

<p>Performance tips:</p>
<ul>
  <li>Use <strong>fused implementations</strong> when available to reduce kernel-launch overhead by consolidating updates into single kernels.<br /></li>
  <li>Apply <strong>parameter grouping</strong> to separate parameters that should receive weight decay (e.g., 2D weight matrices, embeddings) from those that should not (biases, LayerNorm gain/bias).<br /></li>
</ul>

<p>Result:</p>
<ul>
  <li>Better optimization behavior and improved runtime when grouped and fused updates are used.<br /></li>
</ul>

<hr />

<h1 id="gradient-clipping-and-learning-rate-scheduling-cosine-decay-with-warmup">Gradient clipping and learning-rate scheduling (cosine decay with warmup)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/02-29-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/02-29-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/02-29-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/02-29-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Stability and scheduling:</p>
<ul>
  <li><strong>Clip global gradient norm</strong> (e.g., to 1.0) to prevent runaway updates from outlier batches.<br /></li>
  <li>Use a <strong>linear warmup</strong> followed by <strong>cosine decay</strong>: ramp LR from near zero to peak over a warmup token budget, then decay via cosine to a lower fraction (e.g., 10%) across a token horizon.<br /></li>
</ul>

<p>Implementation:</p>
<ul>
  <li>Make warmup steps and cosine decay configurable to reproduce reference regimes and allow experimentation.<br /></li>
</ul>

<hr />

<h1 id="gradual-batch-scaling-considerations-and-practical-omission">Gradual batch scaling considerations and practical omission</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/02-40-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/02-40-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/02-40-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/02-40-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Batch-size strategies:</p>
<ul>
  <li>Large-scale papers sometimes recommend <strong>ramping batch size</strong> early for optimizer stability, but this complicates bookkeeping and token-based scheduling.<br /></li>
  <li>For single-node GPU constraints, <strong>gradient accumulation</strong> is an effective practical alternative to simulate larger global batch sizes while keeping micro-batches small.<br /></li>
</ul>

<p>Recommendation:</p>
<ul>
  <li>For reproducible and simpler experiments, prefer <strong>fixed micro-batch sizes</strong> with gradient accumulation unless infrastructure supports dynamic batch scheduling.<br /></li>
</ul>

<hr />

<h1 id="gradient-accumulation-semantics-and-correct-loss-scaling">Gradient accumulation semantics and correct loss scaling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/02-53-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/02-53-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/02-53-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/02-53-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Gradient accumulation mechanics:</p>
<ul>
  <li>Sum per-micro-batch gradients across multiple forward/backward steps to emulate a larger batch.<br /></li>
  <li>Because many PyTorch losses perform mean reduction over the micro-batch, scale the per-step loss by <strong>1 / grad_accum_steps</strong> before backward so summed gradients equal a single large-batch gradient.<br /></li>
</ul>

<p>Correct pattern:</p>
<ol>
  <li>loss = loss / grad_accum_steps<br /></li>
  <li>loss.backward() each micro-step<br /></li>
  <li>optimizer.step() only after accumulation is complete<br /></li>
</ol>

<p>Failure to scale yields gradients larger by grad_accum_steps and incorrect updates.<br /></p>

<hr />

<h1 id="distributed-data-parallel-ddp-fundamentals-and-synchronization-control">Distributed Data Parallel (DDP) fundamentals and synchronization control</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/03-10-30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/03-10-30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/03-10-30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/03-10-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p><strong>Distributed Data Parallel (DDP)</strong> patterns:</p>
<ul>
  <li>Run one process per GPU; each process computes local gradients on its data shard and then gradients are <strong>all-reduced</strong> (averaged) across processes before optimizer.step().<br /></li>
</ul>

<p>Gradient accumulation with DDP:</p>
<ul>
  <li>Use <strong>no_sync</strong> context to avoid synchronizing on every micro-step during accumulation so only the final micro-step triggers communication.<br /></li>
</ul>

<p>Operational necessities:</p>
<ul>
  <li>Initialize process <strong>ranks</strong>, <strong>world size</strong>, <strong>local rank</strong>, and device mapping correctly and destroy process groups on exit for robust DDP training.<br /></li>
</ul>

<hr />

<h1 id="training-data-choices-and-high-quality-commoncrawl-subsets-finewebedu">Training data choices and high-quality CommonCrawl subsets (FineWeb/edu)</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/03-32-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/03-32-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/03-32-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/03-32-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Datasets and preprocessing for reproductions:</p>
<ul>
  <li>Public reproductions use curated mixtures built from <strong>CommonCrawl</strong> (OpenWebText, RedPajama, FineWeb) plus filtered sources (Wikipedia, books, GitHub).<br /></li>
  <li>High-quality filtered subsets (e.g., <strong>FineWeb EDU</strong>) provide dense, high-information text for language-generalization metrics.<br /></li>
</ul>

<p>Practical processing:</p>
<ul>
  <li>Tokenize at scale and shard into fixed-size files (e.g., <strong>100M-token shards</strong>) for streaming and parallel loading.<br /></li>
  <li>Careful <strong>deduplication</strong>, <strong>language filtering</strong>, and <strong>per-epoch shuffling</strong> are crucial to avoid dataset bias and leakage.<br /></li>
</ul>

<hr />

<h1 id="training-runs-throughput-tuning-and-producing-checkpoints">Training runs, throughput tuning and producing checkpoints</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/03-51-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/03-51-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/03-51-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/03-51-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Combine optimizations to maximize throughput:</p>
<ul>
  <li>Use <strong>BF16/autocast</strong>, <strong>torch.compile</strong>, <strong>FlashAttention</strong>, <strong>fused AdamW</strong>, <strong>padded dimensions</strong>, <strong>DDP</strong>, and <strong>gradient accumulation</strong> to increase tokens-per-second.<br /></li>
</ul>

<p>Operational monitoring:</p>
<ul>
  <li>Measure <strong>tokens/sec</strong>, wall time per step, and validation intervals to estimate training budget in hours for a target token count.<br /></li>
  <li>Regularly checkpoint model and optimizer state and persist RNG seeds for exact resume behavior.<br /></li>
</ul>

<p>Outcome:</p>
<ul>
  <li>Empirical runs with these optimizations achieved successful convergence and downstream gains in modest compute budgets.<br /></li>
</ul>

<hr />

<h1 id="downstream-evaluation-validation-loss-h-swag-implementation-and-caveats">Downstream evaluation: validation loss, H-SWAG implementation and caveats</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/04-00-32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/04-00-32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/04-00-32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/04-00-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Evaluation strategy:</p>
<ul>
  <li>Compute held-out <strong>validation next-token loss</strong> and run downstream multiple-choice-style benchmarks (e.g., <strong>H-SWAG</strong>) by converting each question into candidate continuations and scoring average per-token log-probabilities.<br /></li>
</ul>

<p>Distributed evaluation:</p>
<ul>
  <li>Shard evaluation across DDP ranks, aggregate counts with <strong>all-reduce</strong>, and report global accuracy.<br /></li>
</ul>

<p>Interpretation caveats:</p>
<ul>
  <li>Differences in training data distributions, possible test contamination from large scraped corpora, and limitations of older benchmarks (H-SWAG is largely solved by modern LMs) mean multiple held-out tasks should be used for robust comparison.<br /></li>
</ul>

<hr />

<h1 id="production-considerations-logging-checkpoints-alternative-ccuda-implementations-and-summary">Production considerations: logging, checkpoints, alternative C/CUDA implementations and summary</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec10/04-01-25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec10/04-01-25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec10/04-01-25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec10/04-01-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Operational best practices:</p>
<ul>
  <li>Structured logging of training and validation metrics.<br /></li>
  <li>Periodic checkpointing (model + optimizer states) and reproducible seeds.<br /></li>
</ul>

<p>Advanced options:</p>
<ul>
  <li>Lower-level implementations (e.g., a dedicated C/CUDA lm.C) can improve startup and per-step throughput but require careful numerical parity checks versus PyTorch prototypes.<br /></li>
</ul>

<p>Conclusion:</p>
<ul>
  <li>With modern tooling and hardware, a faithful <strong>GPT-2 124M</strong> reproduction (implementation, training, and evaluation) is feasible on modest compute budgets.<br /></li>
  <li>Recommended extensions: epoch shuffling, improved dataset handling, and compilation fixes for generation to make the reproduction robust and production-ready.<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry><entry><title type="html">Agent 08 - Building Game Simulation Agents</title><link href="https://tuananhbui89.github.io/blog/2025/llm-agents-lec08/" rel="alternate" type="text/html" title="Agent 08 - Building Game Simulation Agents" /><published>2025-12-15T00:00:00+07:00</published><updated>2025-12-15T00:00:00+07:00</updated><id>https://tuananhbui89.github.io/blog/2025/llm-agents-lec08</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/llm-agents-lec08/"><![CDATA[<div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/pg1Sn9rsFak" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="course-introduction-and-objectives">Course introduction and objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-01-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-01-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-01-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-01-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The course introduces an open-source project for building an <strong>AI agent simulation engine</strong> that brings historical figures to life inside an <strong>interactive game environment</strong>.<br /></p>

<p>It emphasizes <strong>end-to-end engineering practices</strong> beyond pure model development, including:<br /></p>
<ul>
  <li><strong>Robust memory systems</strong> with <strong>MongoDB</strong> for short- and long-term state<br /></li>
  <li><strong>Agentic workflow orchestration</strong> using <strong>Langraph</strong><br /></li>
  <li><strong>LLM inference</strong> via <strong>Grok</strong> (with Llama 3 37B used for dialogs)<br /></li>
  <li><strong>Deployment</strong> with <strong>FastAPI</strong> and <strong>WebSockets</strong> for real-time communication<br /></li>
  <li><strong>Observability and LM-Ops tooling</strong> for tracing, evaluation, and monitoring<br /></li>
</ul>

<p>The curriculum targets <strong>production-ready concerns</strong> such as:<br /></p>
<ul>
  <li>API / UX integration and prompt/version management<br /></li>
  <li>Containerization with <strong>Docker</strong> and local/cloud deployment practices<br /></li>
  <li>Monitoring and reliability for real-world usage<br /></li>
</ul>

<p>Participants gain a <strong>complete stack demonstration</strong> (deployable agentic applications) rather than isolated toy examples.<br /></p>

<hr />

<h1 id="course-lesson-plan-and-structure">Course lesson plan and structure</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-03-55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-03-55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-03-55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-03-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The course is organized into a sequence of lessons that each focus on a specific system layer:<br /></p>

<ul>
  <li><strong>Architecture &amp; UI / API design</strong> — overall system separation and responsibilities<br /></li>
  <li><strong>Agent workflow construction with Langraph</strong> — graph-based agent orchestration<br /></li>
  <li><strong>Short-term &amp; long-term memory design (MongoDB)</strong> — persistence and retrieval strategies<br /></li>
  <li><strong>Real-time API integration (FastAPI + WebSockets)</strong> — streaming and low-latency interaction<br /></li>
  <li><strong>LM-Ops evaluation and monitoring (Opic)</strong> — tracing, prompt/versioning, and metrics<br /></li>
</ul>

<p>Each lesson includes practical artifacts to support hands-on learning:<br /></p>
<ul>
  <li>Code, Jupyter notebooks, and guided exercises<br /></li>
  <li>Local-first replication steps and cloud deployment pointers<br /></li>
  <li>A modular structure that supports incremental validation of each component<br /></li>
</ul>

<hr />

<h1 id="interactive-simulation-demo-and-learning-motivation">Interactive simulation demo and learning motivation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-05-14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-05-14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-05-14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-05-14.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>An interactive demo motivates the engineering concepts by showing AI agents impersonating philosophers in a browser-based game:<br /></p>

<ul>
  <li>Players interact with NPC philosophers (e.g., <strong>Plato</strong>, <strong>Aristotle</strong>, <strong>Turing</strong>) in a village scene<br /></li>
  <li>The demo highlights core techniques: <strong>memory</strong>, <strong>retrieval-augmented generation (RAG)</strong>, <strong>workflow orchestration</strong>, and <strong>real-time streaming</strong><br /></li>
  <li>Agents are grounded in <strong>authoritative sources</strong> to produce richer, historically coherent dialogues<br /></li>
  <li>The demo sets expectations for the end-to-end learning outcome: a simulation that is <strong>fun, interactive, and technically realistic</strong><br /></li>
</ul>

<hr />

<h1 id="lesson-1-introduction-and-architecture-overview">Lesson 1 introduction and architecture overview</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-06-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-06-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-06-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-06-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Lesson 1 gives a high-level overview of the <strong>Fellow Agents</strong> architecture and full tech stack used across the course:<br /></p>

<ul>
  <li>Architectural separation:
    <ul>
      <li><strong>Online phase</strong> — real-time gameplay and agent inference<br /></li>
      <li><strong>Offline phase</strong> — data ingestion, feature pipeline, and evaluation dataset generation<br /></li>
    </ul>
  </li>
  <li>Key runtime components:
    <ul>
      <li><strong>Phaser</strong> game UI for in-browser interaction<br /></li>
      <li><strong>FastAPI</strong> server for agent serving and WebSocket streaming<br /></li>
      <li><strong>Langraph</strong> workflows for agent behavior orchestration<br /></li>
      <li><strong>MongoDB</strong> for short-term checkpoints and long-term vector memory<br /></li>
    </ul>
  </li>
  <li>The overview maps each engineering decision to a concrete system responsibility and orients subsequent lessons<br /></li>
</ul>

<hr />

<h1 id="offline-pipeline-and-long-term-memory-population">Offline pipeline and long-term memory population</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-09-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-09-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-09-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-09-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The offline phase implements a <strong>RAG feature pipeline</strong> that prepares grounded context for each philosopher:<br /></p>

<ol>
  <li>Extract contextual data from authoritative sources (Wikipedia, Stanford Encyclopedia of Philosophy)<br /></li>
  <li>Chunk the text (overlapping pieces) and apply deduplication heuristics<br /></li>
  <li>Produce embeddings for each chunk<br /></li>
  <li>Store vectors and metadata in <strong>MongoDB</strong> as <strong>long-term memory</strong> (vector index / hybrid search)<br /></li>
</ol>

<p>These offline artifacts are reused to:</p>
<ul>
  <li>Assemble evaluation datasets<br /></li>
  <li>Ensure agent responses can be <strong>grounded</strong> in verifiable historical context<br /></li>
</ul>

<p>Details such as embedding model choice, <strong>chunking strategy</strong>, and storage schema are central to RAG effectiveness and grounding.<br /></p>

<hr />

<h1 id="evaluation-dataset-generation-and-opic-integration">Evaluation dataset generation and Opic integration</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-10-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-10-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-10-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-10-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The <strong>generate / eval dataset</strong> component produces question-and-answer datasets per philosopher to enable objective evaluation of RAG behavior:<br /></p>

<ul>
  <li>Generated datasets exercise the retrieval pipeline and surface regressions or hallucinations<br /></li>
  <li><strong>Opic</strong> (observability/evaluation tool) is integrated to:
    <ul>
      <li>Host datasets and traces<br /></li>
      <li>Version prompts and evaluation configs<br /></li>
      <li>Run automated evaluations comparing agent responses to gold outputs<br /></li>
    </ul>
  </li>
  <li>This setup enables iterative improvement via metrics-driven validation of the RAG pipeline and agent workflows<br /></li>
</ul>

<hr />

<h1 id="runtime-components-ui-api-agentic-layer-and-llm-gateway">Runtime components: UI, API, agentic layer and LLM gateway</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-12-39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-12-39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-12-39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-12-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The online phase orchestrates interaction between three main runtime components:<br /></p>

<ul>
  <li><strong>Game UI (Phaser)</strong> — user actions map to API calls<br /></li>
  <li><strong>FastAPI server</strong> — receives UI calls and invokes Langraph agent workflows<br /></li>
  <li><strong>Memory / agent stack</strong> — short-term state + long-term retrieval tools in MongoDB<br /></li>
</ul>

<p>Runtime behavior:<br /></p>
<ol>
  <li>FastAPI invokes a <strong>Langraph-defined workflow</strong> that binds prompts, tools, and an LLM gateway<br /></li>
  <li>The workflow consults short-term state and conditionally calls long-term retrieval tools (RAG)<br /></li>
  <li><strong>Grok</strong> (with <strong>Llama 3 37B</strong> in dialogs) serves as the LLM provider for streaming responses<br /></li>
</ol>

<p>Key production concerns: prompt management, retrieval tool binding, state persistence, and streaming to the UI.<br /></p>

<hr />

<h1 id="three-component-flow-and-tool-enabled-response-example">Three-component flow and tool-enabled response example</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-15-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-15-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-15-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-15-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>A simplified three-component flow highlights conditional tool usage and streaming:<br /></p>

<ol>
  <li>The <strong>UI</strong> sends a message to <strong>FastAPI</strong><br /></li>
  <li><strong>FastAPI</strong> invokes the <strong>Langraph</strong> workflow (agent graph)<br /></li>
  <li>The agent evaluates whether to use a <strong>retrieval tool</strong> (conditional decision)<br /></li>
  <li>If needed, the tool queries <strong>MongoDB long-term memory</strong> and returns ranked chunks<br /></li>
  <li>The <strong>LLM</strong> (Grok/Llama 3 37B) generates a response which streams back to the UI in partial chunks<br /></li>
</ol>

<p>This flow demonstrates how <strong>conditional retrieval</strong>, <strong>streaming responses</strong>, and <strong>tool orchestration</strong> enable grounded, context-rich replies in a real-time game.<br /></p>

<hr />

<h1 id="repository-layout-cloning-and-development-environment">Repository layout, cloning and development environment</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-17-59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-17-59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-17-59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-17-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>The project repository contains two core components:<br /></p>

<ul>
  <li><strong>filagents-api</strong> (Python)
    <ul>
      <li>Implements the agentic backend with a clean architecture (application, domain, infrastructure layers)<br /></li>
      <li>Includes Docker files, notebooks, and evaluation data<br /></li>
    </ul>
  </li>
  <li><strong>filagents-ui</strong> (Phaser JavaScript)
    <ul>
      <li>Phaser 3 project with scenes, dialog management, and HTTP / WebSocket services<br /></li>
    </ul>
  </li>
</ul>

<p>Developer onboarding checklist:<br /></p>
<ul>
  <li>Clone the repo and open in an IDE<br /></li>
  <li>Create a Python virtual environment for the API<br /></li>
  <li>Inspect distinct modules and follow installation/run instructions provided in the repo<br /></li>
</ul>

<hr />

<h1 id="installing-dependencies-environment-variables-and-local-infra">Installing dependencies, environment variables and local infra</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-22-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-22-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-22-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-22-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Local setup and infrastructure:<br /></p>

<ul>
  <li>Prerequisites: <strong>Python 3.11</strong>, <strong>Git</strong>, <strong>Docker</strong>, plus project-specific packages<br /></li>
  <li>Create and activate a virtual environment, then install dependencies from requirements<br /></li>
  <li>Configure environment variables:
    <ul>
      <li>Copy example .env -&gt; .env and set keys for <strong>Grok</strong>, <strong>OpenAI</strong> (for Opic), and <strong>Comet</strong><br /></li>
    </ul>
  </li>
  <li>Start local infrastructure via Make (make infrastructure_app) which launches three Docker services:
    <ul>
      <li>Local <strong>MongoDB</strong> (dev Atlas emulation), the <strong>FastAPI</strong> backend, and the <strong>Phaser UI</strong><br /></li>
    </ul>
  </li>
</ul>

<p>This composition supports local development and testing without requiring external managed services.<br /></p>

<hr />

<h1 id="game-ui-walkthrough-and-interactive-agent-demo">Game UI walkthrough and interactive agent demo</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-26-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-26-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-26-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-26-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Phaser-based UI mechanics and demo features:<br /></p>

<ul>
  <li>Player controls: movement with arrow keys, speak via spacebar + input, close dialogs with Escape<br /></li>
  <li>Multiple philosopher NPCs implemented as <strong>Langraph-driven agents</strong>, each with distinct personalities and topics (ethics, computation, AI)<br /></li>
  <li>Interacting with a philosopher triggers the agent backend and shows <strong>streamed responses</strong> in the dialog box<br /></li>
  <li>Demo includes both comedic easter eggs and realistic philosophical Q&amp;A to verify the end-to-end pipeline from user input to agent response<br /></li>
</ul>

<hr />

<h1 id="ui-internals-dialogue-manager-websocket-service-and-api-binding">UI internals: dialogue manager, WebSocket service and API binding</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-32-23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-32-23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-32-23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-32-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Client-side communication and dialog orchestration are organized as follows:<br /></p>

<ul>
  <li><strong>Dialogue manager</strong> — orchestrates dialog boxes, tracks the active philosopher, and routes incoming WebSocket messages<br /></li>
  <li><strong>WebSocket API service</strong> — manages connection lifecycle, send/receive semantics, and callback registration; connects to ws://localhost:8000 for streaming<br /></li>
  <li>The client assembles streamed chunks into full responses and integrates with Phaser scenes for rendering<br /></li>
  <li>Architecture decouples UI rendering from networking logic to simplify testing and extension<br /></li>
</ul>

<hr />

<h1 id="philosopher-domain-model-prompt-templates-and-state-checkpointing">Philosopher domain model, prompt templates and state checkpointing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-38-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-38-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-38-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-38-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Philosopher identity and persistence model:<br /></p>

<ul>
  <li>Philosophers modeled as <strong>domain objects</strong> (Pydantic models) with fields:
    <ul>
      <li><strong>id</strong>, <strong>name</strong>, <strong>perspective</strong>, <strong>style</strong>, and <strong>character prompts</strong><br /></li>
    </ul>
  </li>
  <li><strong>Character prompts</strong> are assembled from domain fields to produce a system prompt that conditions personality and voice<br /></li>
  <li><strong>Langraph graph state</strong> persists conversation history and philosopher-specific attributes (context, summary, etc.)<br /></li>
  <li>The FastAPI backend configures a <strong>Langraph checkpointer</strong> that writes state snapshots into MongoDB collections (checkpoints, writes)<br /></li>
  <li>Persisted state enables short-term continuity (recalling user facts) and per-agent thread isolation across interactions<br /></li>
</ul>

<hr />

<h1 id="langraph-studio-visualization-and-conversation-node-behavior">Langraph Studio visualization and conversation node behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-44-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-44-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-44-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-44-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Langraph Studio visualizes the agent workflow as a directed graph:<br /></p>

<ul>
  <li>Start node → Conversation node, where a <strong>tool condition</strong> decides whether to call the retriever (conditional/dotted edges)<br /></li>
  <li>When retrieval is triggered:
    <ul>
      <li>Returned context is summarized and injected back into the conversation loop<br /></li>
    </ul>
  </li>
  <li>Connector and summarization nodes implement architecture-level concerns:
    <ul>
      <li><strong>Token compression</strong>, <strong>flow control</strong>, and <strong>context summarization</strong><br /></li>
    </ul>
  </li>
  <li>Visual graphs clarify the runtime decision-making and iterative loops present in agentic workflows<br /></li>
</ul>

<hr />

<h1 id="implementation-of-nodes-chains-and-rag-loop-in-code">Implementation of nodes, chains and RAG loop in code</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/00-53-40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/00-53-40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/00-53-40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/00-53-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Graph composition and node responsibilities:<br /></p>

<ul>
  <li>Nodes created include:
    <ul>
      <li><strong>Conversation</strong> (conversation chain binding LLM, prompts, tools)<br /></li>
      <li><strong>Retriever</strong> (MongoDB hybrid retriever wrapped as a Langraph tool node)<br /></li>
      <li><strong>Context summarizer</strong>, <strong>conversation summarizer</strong>, and a transparent <strong>connector</strong> node<br /></li>
    </ul>
  </li>
  <li>Edges implement conditional RAG loops:
    <ul>
      <li>conversation → retriever → summarize context → conversation<br /></li>
      <li>Additional conditional edge summarizes conversations when message length exceeds a threshold (e.g., 30 messages)<br /></li>
    </ul>
  </li>
  <li>The conversation node binds <strong>Grok / Llama 3 37B</strong>, prompts, and tools to enable streaming and tool orchestration<br /></li>
</ul>

<hr />

<h1 id="short-term-memory-concept-and-storage-model">Short-term memory concept and storage model</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-02-54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-02-54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-02-54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-02-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Short-term memory design (conversation checkpointing):<br /></p>

<ul>
  <li>Conversation history is stored in the Langraph graph state as a <strong>messages</strong> list representing chat history<br /></li>
  <li>The messages state is extended with philosopher-specific attributes (context, name, perspective, style, summary)<br /></li>
  <li>An <strong>async MongoDB saver</strong> acts as a Langraph checkpointer to persist state snapshots to MongoDB collections<br /></li>
  <li>Persisted state enables agents to recall user-provided facts across turns (e.g., the user’s name) and maintain coherent multi-turn dialogues<br /></li>
  <li>Per-agent thread IDs ensure multiple philosopher states remain isolated<br /></li>
</ul>

<hr />

<h1 id="notebook-demo-comparing-no-memory-vs-persisted-memory">Notebook demo comparing no-memory vs persisted memory</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-09-44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-09-44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-09-44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-09-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Notebook examples show persistence vs. stateless invocation:<br /></p>

<ol>
  <li><strong>generate_response_without_memory</strong> — runs the graph without a checkpointer (stateless); the agent forgets prior user turns<br /></li>
  <li><strong>generate_response_with_memory</strong> — attaches an async MongoDB checkpointer using philosopher ID as the thread ID; the agent recalls earlier facts across invocations<br /></li>
</ol>

<p>The notebook reproduces the same graph invocation logic and highlights how simple database-backed checkpoints restore chat continuity per philosopher thread.<br /></p>

<hr />

<h1 id="long-term-memory-purpose-and-ingestion-pipeline">Long-term memory purpose and ingestion pipeline</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-18-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-18-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-18-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-18-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Long-term memory and ingestion pipeline for grounded context:<br /></p>

<ul>
  <li>Long-term memory stores biographies, philosophical ideas, and domain facts per philosopher<br /></li>
  <li>Ingestion pipeline steps:
    <ol>
      <li>Download documents from Wikipedia and Stanford Encyclopedia of Philosophy<br /></li>
      <li>Apply a recursive character splitter to produce overlapping chunks<br /></li>
      <li>Deduplicate chunks using content-similarity heuristics (MinHash-style or others)<br /></li>
      <li>Produce embeddings per chunk<br /></li>
      <li>Store vectors + metadata into <strong>MongoDB’s vector index</strong><br /></li>
    </ol>
  </li>
</ul>

<p>This approach supports retrieval of source-attributed context during online queries, enabling historically accurate agent responses.<br /></p>

<hr />

<h1 id="building-the-long-term-memory-toolchain-and-persistent-index">Building the long-term memory toolchain and persistent index</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-26-05-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-26-05-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-26-05-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-26-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>CLI and retriever integration:<br /></p>

<ul>
  <li>Repository includes a CLI (<strong>create_long_term_memory</strong>) that orchestrates extraction, chunking, deduplication, embedding generation, and insertion into MongoDB<br /></li>
  <li>A <strong>hybrid MongoDB retriever</strong> (Langraph / LangChain integration) is constructed using a chosen embedding model and MongoDB Atlas hybrid search or local vector features<br /></li>
  <li>After ingestion, the <strong>philosopher_launcher_memory</strong> collection contains chunked documents with source attribution suitable for retrieval tools<br /></li>
  <li>The retriever is exposed as a Langraph tool node for conditional invocation in the agent workflow<br /></li>
</ul>

<hr />

<h1 id="runtime-retrieval-behavior-and-example-queries">Runtime retrieval behavior and example queries</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-29-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-29-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-29-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-29-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Runtime retrieval and context injection:<br /></p>

<ul>
  <li>The <strong>retrieve_philosopher_context</strong> node queries MongoDB’s vector index with the current user question<br /></li>
  <li>Returned chunks are summarized to reduce token usage and then injected back into the conversation chain<br /></li>
  <li>Retriever returns ranked chunks from multiple sources (Wikipedia, Stanford Encyclopedia of Philosophy) and the conversation node may trigger additional retrieval iterations (retrieval loop)<br /></li>
  <li>Notebook and UI examples demonstrate queries (e.g., “Turing machine”, “Chinese room argument”) and show retrieved chunks with source metadata to confirm grounding<br /></li>
</ul>

<hr />

<h1 id="websockets-rationale-for-real-time-agentic-systems">WebSockets rationale for real-time agentic systems</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-32-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-32-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-32-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-32-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Why WebSockets are used for UI ↔ backend communication:<br /></p>

<ul>
  <li><strong>Persistent, bidirectional connections</strong> enable low-latency interaction and streaming partial responses<br /></li>
  <li>Advantages over HTTP:
    <ul>
      <li>No per-interaction handshake overhead<br /></li>
      <li>Support for server-to-client pushes and true streaming of partial LLM outputs<br /></li>
      <li>Better fit for interactive game experiences and scalable multiplayer scenarios<br /></li>
    </ul>
  </li>
  <li>WebSockets are therefore the preferred protocol to stream Langraph response chunks to the UI as they are produced<br /></li>
</ul>

<hr />

<h1 id="fastapi-websocket-implementation-and-client-integration">FastAPI WebSocket implementation and client integration</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-40-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-40-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-40-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-40-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>FastAPI backend WebSocket behavior and client-side handling:<br /></p>

<ul>
  <li>FastAPI exposes both HTTP and <strong>WebSocket endpoints</strong><br /></li>
  <li>WebSocket endpoint workflow:
    <ol>
      <li>Accept connection and receive JSON payloads from the client<br /></li>
      <li>Invoke the Langraph <strong>streaming graph</strong> (graph.stream) for partial outputs<br /></li>
      <li>Send an initial “streaming started” message<br /></li>
      <li>Stream partial chunks as JSON messages while graph produces them<br /></li>
      <li>Send a final message with streaming=false and the assembled full response<br /></li>
    </ol>
  </li>
  <li>The Phaser client implements a WebSocket service that manages handshake, chunk assembly, callbacks, and disconnect logic to enable real-time rendering of streaming agent responses<br /></li>
</ul>

<hr />

<h1 id="lm-ops-definition-and-major-components">LM-Ops definition and major components</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-49-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-49-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-49-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-49-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>LM-Ops fundamentals for production LLM systems:<br /></p>

<p><strong>LM-Ops</strong> is the set of practices, tools, and techniques to optimize the production lifecycle of LLM-based systems. Core components include:<br /></p>
<ol>
  <li><strong>Model deployment</strong> — packaging and serving model binaries and inference endpoints<br /></li>
  <li><strong>Data management</strong> — datasets for training, evaluation, and reproducibility<br /></li>
  <li><strong>Prompt versioning</strong> — tracking prompt edits like code/version control<br /></li>
  <li><strong>Monitoring &amp; observability</strong> — traces, token usage, latency, tool-call telemetry<br /></li>
  <li><strong>Security</strong> — privacy, guardrails, and access control<br /></li>
  <li><strong>Evaluation</strong> — metrics, benchmarking, and automated tests<br /></li>
</ol>

<p>A production agentic system requires processes in each area to ensure safety, reliability, and continuous improvement.<br /></p>

<hr />

<h1 id="prompt-versioning-workflow-with-opic">Prompt versioning workflow with Opic</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/01-55-35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/01-55-35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/01-55-35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/01-55-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Prompt versioning and Opic integration:<br /></p>

<ul>
  <li>Treat <strong>prompts as versioned artifacts</strong> analogous to code and models<br /></li>
  <li>Use <strong>Opic</strong> to store, name, and version prompts centrally<br /></li>
  <li>Code maps Opic prompt objects to domain prompt templates (e.g., <strong>philosopher_character_card</strong>) and writes prompt updates to Opic on deployment/run<br /></li>
  <li>Opic’s prompt library provides a history of versions so teams can:
    <ul>
      <li>Track changes and attribute behavioral shifts to prompt edits<br /></li>
      <li>Roll back to prior prompt states when needed<br /></li>
    </ul>
  </li>
  <li>Langraph chains fetch prompt content or version metadata as part of the agent configuration<br /></li>
</ul>

<hr />

<h1 id="monitoring-and-observability-via-opic-traces">Monitoring and observability via Opic traces</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/02-02-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/02-02-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/02-02-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/02-02-08.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Tracing Langraph executions with Opic:<br /></p>

<ul>
  <li>Attach an <strong>Opic tracer</strong> as a callback to the compiled Langraph graph so each execution emits trace spans and metadata<br /></li>
  <li>Traces capture:
    <ul>
      <li>Node-level runtimes (start node, conversation node, retriever usage)<br /></li>
      <li>Prompt inputs and model selections<br /></li>
      <li>Tool invocations, durations, and latency metrics<br /></li>
    </ul>
  </li>
  <li>Instrumentation enables:
    <ul>
      <li>Per-step performance analysis and error tracing<br /></li>
      <li>Correlation of prompt/retriever changes with downstream metrics<br /></li>
      <li>Diagnostics for regressions and optimization opportunities<br /></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="evaluation-dataset-generation-pipeline-using-a-large-llm">Evaluation dataset generation pipeline using a large LLM</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/02-09-01-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/02-09-01-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/02-09-01-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/02-09-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Generating evaluation datasets via synthetic grounded conversations:<br /></p>

<p>Pipeline to create evaluation corpus:<br /></p>
<ol>
  <li>Select chunk subsets from the philosopher knowledge corpus<br /></li>
  <li>Use a large LLM (Grok / Llama 3 37B) to synthesize multi-turn, grounded conversations given sampled chunks<br /></li>
  <li>Validate generated conversations for structure and fidelity<br /></li>
  <li>Save synthesized conversations as JSON to serve as the automated evaluation corpus<br /></li>
</ol>

<p>This synthetic-but-grounded dataset exercises retrieval quality and downstream agent behavior in automated tests.<br /></p>

<hr />

<h1 id="automated-evaluation-metrics-and-opic-driven-scoring">Automated evaluation metrics and Opic-driven scoring</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llm-agents/frames/lec08/02-19-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llm-agents/frames/lec08/02-19-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llm-agents/frames/lec08/02-19-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/llm-agents/frames/lec08/02-19-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<p>Automated evaluation workflow and metrics in Opic:<br /></p>

<ul>
  <li>Opic runs automated evaluations using an external judge model (OpenAI) to score five metrics:
    <ul>
      <li><strong>Hallucination</strong> (0.0–1.0; 1.0 = fully grounded) — measures if the response is supported by sources<br /></li>
      <li><strong>Answer relevance</strong> — relevance of the response to the question and context<br /></li>
      <li><strong>Moderation</strong> — toxicity / safety scoring<br /></li>
      <li><strong>Context precision</strong> — proportion of retrieved context that is relevant<br /></li>
      <li><strong>Context recall</strong> — proportion of relevant context that was retrieved<br /></li>
    </ul>
  </li>
  <li>Evaluation process:
    <ol>
      <li>Upload dataset to Opic<br /></li>
      <li>Invoke an evaluation job that executes agent responses for each sample<br /></li>
      <li>Use prompt-based LLM judgment to compute metrics and per-sample traces<br /></li>
    </ol>
  </li>
  <li>Results surface as experiments in Opic with aggregate metrics, timelines, and per-sample traces to guide iterative improvements of prompts, retrievers, and workflows<br /></li>
</ul>

<hr />]]></content><author><name></name></author><category term="genai" /><category term="llm" /><category term="tutorial" /><category term="reading" /><summary type="html"><![CDATA[AI Summary Lecture]]></summary></entry></feed>