<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tuananhbui89.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tuananhbui89.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-29T12:28:50+10:00</updated><id>https://tuananhbui89.github.io/feed.xml</id><title type="html">blank</title><subtitle>Researcher in Generative AI and Trustworthy AI
</subtitle><entry><title type="html">Anti-Personalization - A review after 2 years</title><link href="https://tuananhbui89.github.io/blog/2025/anti-personalization-v2/" rel="alternate" type="text/html" title="Anti-Personalization - A review after 2 years" /><published>2025-08-18T00:00:00+10:00</published><updated>2025-08-18T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2025/anti-personalization-v2</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/anti-personalization-v2/"><![CDATA[<p>In 2023, when the work on Controllable Diffusion Models was just starting its momentum, with the several key milestones papers Textual Inversion, Dreambooth and ControlNet, I had predicted that there will be an interesting research direction on <strong>Anti-Personalization</strong> in the future.</p>

<p>Not long after Dreambooth was released, there are several papers on Anti-Personalization had been published, including Anti-Dreambooth and Adversarial Diffusion papers.</p>

<p>In August 2023, when finishing my PhD and starting my postdoc, I had the chance to switch my research from Adversarial Machine Learning (attack and defense on traditional classification models) to a more modern and exciting field Trustworthy Generative AI. 
However, at that time, I piloted with Anti-Textual Inversion, because I believe in the potential of Textual Inversion over Dreambooth, but unfortunately, the personalization performance of TI was not as good as Dreambooth, making the problem of Anti-Personalization with the base model TI was not as interesting anymore.</p>

<p>Moreover, I have enough experience with Adversarial Machine Learning to understand that adding invisible noise to an image as a defense mechanism is not robust and easy to be bypassed by simple techniques such as image transformations, denoising autoencoders, etc. It likes if you are the defender, you are always one step behind the attacker.</p>

<p>I switched to Machine Unlearning for Generative Models and fortunately, got some successful on this direction with three papers: NeurIPS 2024, ICLR 2025 and ICLRW 2025.</p>

<p>Now, after 2 years, because of the needed of the project that I am working on (with Department of Defence Australia), I have to switch back to Anti-Personalization.</p>

<p>That is the context of this post. I will review the key papers on Anti-Personalization in the last 2 years, and also some thoughts on the future of this direction.</p>

<h2 id="background">Background</h2>

<h3 id="textual-inversion">Textual Inversion</h3>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-anti-personalization/textual_inversion/embedding_fig-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-anti-personalization/textual_inversion/embedding_fig-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-anti-personalization/textual_inversion/embedding_fig-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-anti-personalization/textual_inversion/embedding_fig.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <strong>Textual Inversion [1].</strong> A string S* containing our placeholder word is first converted into tokens. These tokens are converted to continuous vector representations (the "embeddings", v). Finally, the embedding vectors are transformed into a single conditioning code c_θ(y) which guides the generative model. We optimize the embedding vector v* associated with our pseudo-word S*, using a reconstruction objective.
</div>

<p>The optimization objective is:</p>

\[v_* = \text{argmin}_v \mathbb{E}_{z\sim\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0, 1), t }\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t, c_\theta(y)) \Vert_{2}^{2}\Big]\]

<p>where v* is the embedding vector associated with the placeholder word S*.</p>

<h3 id="dreambooth">Dreambooth</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-anti-personalization/dreambooth/main_scheme_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-anti-personalization/dreambooth/main_scheme_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-anti-personalization/dreambooth/main_scheme_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-anti-personalization/dreambooth/main_scheme_1.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <strong>Dreambooth's Fine-tuning [1].</strong> Given ~3-5 images of a subject we fine-tune a text-to-image diffusion model with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., "A [V] dog"), in parallel, we apply a class-specific prior preservation loss, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject's class using the class name in a text prompt (e.g., "A dog").
</div>

<p>The optimization objective of Dreambooth is:</p>

\[\mathbb{E}_{\mathbf{x}, \mathbf{c}, \mathbf{\epsilon}, \mathbf{\epsilon}^{'},t} \left[ w_t \| \hat{\mathbf{x}}_\theta (\alpha_t \mathbf{x} + \sigma_t \mathbf{\epsilon}, \mathbf{c}) - \mathbf{x}\|_2^2 + \lambda w_{t^{'}} \| \hat{\mathbf{x}}_\theta(\alpha_{t^{'}} \mathbf{x}_\text{pr} + \sigma_{t^{'}} \mathbf{\epsilon}^{'}, \mathbf{c}_\text{pr}) - \mathbf{x}_\text{pr} \|_2^2 \right]\]

<p>where \(c_\text{pr}\) is the conditioning vector of the prior class (e.g., “A dog”), 
\(c\) is the conditioning vector of the specific/keyword class (e.g., “A [V] dog”),
\(\mathbf{x}_\text{pr}\) is the image of the prior class (e.g., a normal dog from Internet)
\(\mathbf{x}\) is the image of the specific/keyword class (e.g., a target dog in the user’s dataset).</p>

<p>\(\alpha_t, \sigma_t, w_t\) are terms that control the noise schedule, sample quality, and are funetions of the diffusion process at time step \(t\).</p>

<h2 id="photoguard---raising-the-cost-of-malicious-ai-powered-image-editing-icml-2023">PhotoGuard - Raising the Cost of Malicious AI-Powered Image Editing (ICML 2023)</h2>

<p>At the same time with the AdvDM (or MIST) paper, this paper also proposed a</p>

<h2 id="adversarial-example-does-good-preventing-painting-imitation-from-diffusion-models-via-adversarial-examples-icml-2023">Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples (ICML 2023)</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-anti-personalization/2025-08-18-16-57-26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-anti-personalization/2025-08-18-16-57-26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-anti-personalization/2025-08-18-16-57-26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-anti-personalization/2025-08-18-16-57-26.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Comparison of workflows for adversarial examples in classification models and diffusion models. Adversarial examples in diffusion models prevent diffusion models from extracting image features as conditions by inducing out-of-distribution features. 
    The feature extracting shown in the figure is textual inversion. <a href="https://arxiv.org/pdf/2302.04578">Image from Liang et al. 2023</a>
</div>

<p>High level idea: Adversarial examples for Textual Inversion. It is just that simple. One month later, Anti-Dreambooth was published on Arxiv. The two low hanging fruits were picked.</p>

<p>In the paper, the authors described the optimization objective as follows:</p>

\[\delta = \text{argmax}_{\delta} \mathbb{E}_{x^{'}_{1:T} \sim u(x^{'}_{1:T})} - \log \frac{p_{\theta}(x^{'}_{0:T})}{q(x^{'}_{1:T} \mid x^{'}_0)}\]

<p>where \(x^{'} = x + \delta\) is the adversarial example, \(u(x^{'}_{1:T})\) is the uniform distribution of the adversarial example over the time steps, \(q(x^{'}_{1:T} \mid x^{'}_0)\) is the posterior distribution that can be computed by the forward diffusion process, and \(p_{\theta}(x^{'}_{0:T})\) is the parameterized distribution aiming to approximate the posterior distribution \(q(x^{'}_{1:T} \mid x^{'}_0)\).</p>

<p>While looking scary (as intended by the authors), the nice thing of Diffusion Models is that the above optimization objective can be simplified to the form of matching the predicted noise with the true noise. See my tutorial on <a href="https://tuananhbui89.github.io/blog/2023/diffusion-tutorial/">Diffusion Models</a> for more details.</p>

\[\delta = \text{argmax}_{\delta} \mathbb{E}_{x^{'}_{1:T} \sim u(x^{'}_{1:T})} \left[ \| \epsilon_\theta (x^{'}_{t}, t) - \epsilon_t \|^2 \right]\]

<p>where \(x_t = \sqrt{\bar{\alpha}_t} (x_0 + \delta) + \sqrt{1-\bar{\alpha}_t} \epsilon\) as in the forward diffusion process.</p>

<h2 id="anti-dreambooth-iccv-2023">Anti-Dreambooth (ICCV 2023)</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-anti-personalization/antidreambooth/Teaser-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-anti-personalization/antidreambooth/Teaser-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-anti-personalization/antidreambooth/Teaser-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-anti-personalization/antidreambooth/Teaser.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Anti-Dreambooth [1]
</div>

<p>Idea: Reverse/Attack the learning process of the Dreambooth by learning the adversarial perturbation \(\delta^{(i)}\) associated with each image \(x^{(i)}\) in the training dataset.</p>

<p>The optimization objective is:</p>

\[\begin{align*}
\delta^{*(i)} &amp;= \text{argmax}_{\delta^{(i)}} \mathcal{L}_{cond}(\theta^*, x^{(i)} + \delta^{(i)}), \forall i \in \{1,..,N_{db}\}, \\
\text{s.t.} \quad &amp; \theta^*  = \text{argmin}_{\theta} \sum_{i=1}^{N_{db}} \mathcal{L}_{db}(\theta, x^{(i)} + \delta^{(i)}), \\
\text{and} \quad &amp; \Vert \delta^{(i)} \Vert_p \leq \eta \quad \forall i \in \{1,..,N_{db}\},
\end{align*}\]

<p>where \(\mathcal{L}_{db}\) is the DreamBooth’s loss function and \(\mathcal{L}_{cond}\) is the conditional loss function, e.g., reconstruction loss so that the model \(\theta^*\) cannot generate the image \(x^{(i)}\).</p>

\[\mathbb{E}_{\mathbf{x}, \mathbf{c}, \mathbf{\epsilon}, \mathbf{\epsilon}^{'},t} \left[ \underbrace{w_t \| \hat{\mathbf{x}}_\theta (\alpha_t \mathbf{x} + \sigma_t \mathbf{\epsilon}, \mathbf{c}) - \mathbf{x}\|_2^2}_{\mathcal{L}_{recon}}  + \lambda \underbrace{ w_{t^{'}} \| \hat{\mathbf{x}}_\theta(\alpha_{t^{'}} \mathbf{x}_\text{pr} + \sigma_{t^{'}} \mathbf{\epsilon}^{'}, \mathbf{c}_\text{pr}) - \mathbf{x}_\text{pr} \|_2^2}_{\mathcal{L}_{prior}} \right]\]

<h2 id="fastprotect---nearly-zero-cost-protection-against-mimicry-by-personalized-diffusion-models-cvpr-2025">FastProtect - Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models (CVPR 2025)</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-anti-personalization/2025-08-18-21-52-46-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-anti-personalization/2025-08-18-21-52-46-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-anti-personalization/2025-08-18-21-52-46-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-anti-personalization/2025-08-18-21-52-46.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    FastProtect [1]
</div>

<p><strong>Mixture-of-Perturbation (MoP)</strong></p>

\[\hat{x} = x + \delta_g + \Delta_k, \quad \text{where} \quad k = \mathcal{A}(\Epsilon(x))\]

<p>where \(\delta_g\) is a global perturbation which is found by gradient ascent approach as in AdvDM or PhotoGuard. 
The new contribution of this paper is the \(\Delta_k\) term, which comes from <strong>Mixture-of-Perturbation (MoP)</strong> \(\Delta = {\delta_1, \delta_2, \dots, \delta_K}\) where \(\delta_k\) is a perturbation found by gradient descent approach.</p>

<!-- mkdir -p assets/img/2025-anti-personalization/ -->
<!-- mv _posts/2025-08-18-*.png assets/img/2025-anti-personalization/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="diffusion" /><summary type="html"><![CDATA[In 2023, when the work on Controllable Diffusion Models was just starting its momentum, with the several key milestones papers Textual Inversion, Dreambooth and ControlNet, I had predicted that there will be an interesting research direction on Anti-Personalization in the future.]]></summary></entry><entry><title type="html">GPT-5 Series - Safe Completion Training</title><link href="https://tuananhbui89.github.io/blog/2025/safe-completion-training/" rel="alternate" type="text/html" title="GPT-5 Series - Safe Completion Training" /><published>2025-08-08T00:00:00+10:00</published><updated>2025-08-08T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2025/safe-completion-training</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/safe-completion-training/"><![CDATA[<p>OpenAI just recently <a href="https://openai.com/gpt-5/">released their newest and most powerful model GPT-5</a>. In the post today, I want to talk about one of the most important aspects of LLMs: <strong>How to make them safe against malicious use</strong>.
In this version, OpenAI introduces a new paradigm called <strong>Safe Completion Training</strong> (which is built on top of  Deliberative Alignment [4])</p>

<iframe width="600" height="338" src="https://www.youtube.com/embed/0Uu_VJeVVfo" title="Introducing GPT-5" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>A significant paradigm shift in term of safety training has been proposed, moving away from the traditional “Refusal Training” towards a more nuanced approach known as “Safe-Completion Training”. This evolution directly addresses a long-standing headache for model developers: the delicate and often conflicting balance between helpfulness and safety.</p>

<h2 id="the-core-dilemma-helpfulness-vs-safety">The Core Dilemma: Helpfulness vs Safety</h2>

<p>The central challenge in aligning LLMs is managing the inherent trade-off between being a useful tool and preventing misuse.</p>

<ul>
  <li><strong>Prioritizing Helpfulness</strong>: If a model is optimized solely to be helpful, it can inadvertently become a tool for malicious actors. For example, a model that can explain how to combat a computer virus could, with the same knowledge, provide instructions on how to create one.</li>
  <li><strong>Prioritizing Safety</strong>: Conversely, if a model is made overly cautious, its utility plummets. This phenomenon, known as “over-refusal,” occurs when models reject perfectly benign requests because they contain keywords that trigger safety filters (e.g., refusing a programming query about how to “kill” a process). This not only frustrates users but also creates a competitive disadvantage, as less restrictive models may seem more capable.</li>
</ul>

<h2 id="refusal-training-and-its-limitations">Refusal Training and Its Limitations</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-08-21-25-42.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of the Refusal Training from <a href="https://openai.com/index/gpt-5-safe-completions/">OpenAI</a>.
</div>

<p>The standard method for tackling this has been Refusal Training. This involves teaching a model to recognize and reject harmful prompts, typically through methods like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). The model learns to classify user input as either “safe” (comply) or “unsafe” (refuse).</p>

<p>However, this paradigm has proven to be fundamentally brittle and easily bypassed. Its weaknesses are not just about being “jailbroken,” but are multifaceted:</p>

<ul>
  <li>
    <p><strong>Jailbreak</strong>: It has been shown that Refusal Training is not robust to jailbreak attacks, for example, by converting a harmful query into past-tense [2] or translating it into a different language [3] or requiring output format like JSON, code or ASCII art.</p>
  </li>
  <li>
    <p><strong>Semantic Brittleness</strong>: The models often don’t learn the abstract concept of harm but instead overfit to superficial patterns in the training data. A striking example is the “past-tense attack,” where models that refuse a prompt like “How do I make a Molotov cocktail?” will readily answer “How did people make a Molotov cocktail?”, treating it as a harmless historical query. This simple linguistic shift can cause jailbreak success rates on some models to jump from 1% to 88% [2].</p>
  </li>
  <li>
    <p><strong>Structural Flaws</strong>: Safety training often creates a refusal position bias, where models learn to issue a refusal only at the very beginning of a response. This is a critical flaw because the model is forced to make a refuse-or-comply decision based only on the initial prompt, which may lack context. If an attacker bypasses this initial check, the model has no mechanism to self-correct and refuse later in the generation process. A recent work [5] shows that the fixed structure of the refusal training (as always start with “I’m sorry, I can’t help with that”, etc.) leads to short-cut learning problem and can be easily bypassed by querying the model multiple times and averaging the responses to get the unlearned output.</p>
  </li>
  <li>
    <p><strong>Superficial Alignment</strong>: The alignment often acts as a shallow veneer. Models learn to mimic safety patterns rather than internalizing the principles. This is why attacks like “prefilling,” where a response is forced to start with “Sure, here is the answer,” are so effective. The model continues the harmful request because refusing would contradict the conversational context it has already started, revealing a conflict between its safety training and its core pre-training objective of predicting the next word.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-09-10-33-04.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of jailbreaking a unlearned LLM by querying it multiple times and averaging the responses to get the unlearned output from [5].
</div>

<h2 id="the-new-paradigm-safe-completion-training">The New Paradigm: Safe-Completion Training</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-08-21-26-00.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of the Safe-Completion Training from <a href="https://openai.com/index/gpt-5-safe-completions/">OpenAI</a>.
</div>

<p>In response to these deep-seated issues, Safe-Completion Training redefines the objective [1]. Instead of asking “Is this prompt safe?”, it asks, “What is the most helpful response I can generate that remains fully compliant with the safety policy?”.</p>

<p>The core innovation is shifting the safety evaluation from the user’s input to the model’s own output. This is especially powerful for handling <strong>“dual-use”</strong> queries—prompts, where a benign user request can be completed at a high level,
but might be dangerous if completed in a full detail, as example below:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-08-21-37-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Example of dual-user queries from <a href="https://openai.com/index/gpt-5-safe-completions/">OpenAI</a>.
</div>

<p>With Safe-Completion, a model can provide a helpful, high-level answer while omitting dangerous, operational details. For instance, it can explain the principles of virology without providing a step-by-step guide to creating a bioweapon.</p>

<p>This is achieved through a two-stage process:</p>

<ul>
  <li>
    <p><strong>Nuanced Fine-Tuning (SFT)</strong>: The model is trained to choose between three response types: a direct answer for harmless queries, a refusal with helpful redirection for malicious queries, and a safe completion for dual-use or borderline cases.</p>
  </li>
  <li>
    <p><strong>Constrained Reinforcement Learning (RL)</strong>: The model is optimized using a multiplicative reward function: <code class="language-plaintext highlighter-rouge">Reward = Helpfulness Score * Safety Score</code>. The multiplication is key; if a response is unsafe (<code class="language-plaintext highlighter-rouge">Safety Score = 0</code>), the total reward is zero, no matter how helpful it might seem. This transforms the problem from a trade-off into a constrained optimization: the model is incentivized to be maximally helpful only on the condition that it remains perfectly safe.</p>
  </li>
</ul>

<h2 id="supervised-fine-tuning-sft-in-safe-completion-training">Supervised Fine-Tuning (SFT) in Safe-Completion Training</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-09-08-43-37.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Overall structure of the safe-completion training stack from [1].
</div>

<p>The Supervised Fine-Tuning (SFT) stage is the first phase of Safe-Completion Training, designed to teach the model the initial, correct behaviors before they are refined by reinforcement learning. It moves beyond a simple comply/refuse decision and trains the model to adopt a more nuanced set of responses.</p>

<p>Firstly, we need to understand the data used for SFT including: (<code class="language-plaintext highlighter-rouge">prompt</code>, <code class="language-plaintext highlighter-rouge">CoT</code>, <code class="language-plaintext highlighter-rouge">answer</code>)</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt</code>: Safety-related input prompt.</li>
  <li><code class="language-plaintext highlighter-rouge">spec</code>: Content policy specification that defines the safety policy.</li>
  <li><code class="language-plaintext highlighter-rouge">CoT</code> and <code class="language-plaintext highlighter-rouge">answer</code>: <strong>ideal</strong> Chain of Thought (CoT) and the corresponding response for the model to generate.</li>
</ul>

<p>The input <code class="language-plaintext highlighter-rouge">prompt</code> has been augmented with the <code class="language-plaintext highlighter-rouge">spec</code> and an <code class="language-plaintext highlighter-rouge">instruction</code> to guide the model <strong>consult</strong> the <code class="language-plaintext highlighter-rouge">spec</code> before answering the <code class="language-plaintext highlighter-rouge">prompt</code>.
Interestingly, the <code class="language-plaintext highlighter-rouge">CoT</code> and <code class="language-plaintext highlighter-rouge">answer</code> are obtained not by human labeling but by an <strong>surrogate</strong> reasoning model (e.g., OpenAI o3) with the augmented <code class="language-plaintext highlighter-rouge">prompt</code>.</p>

<p>The final training data for SFT is then constructed from <strong>original, non-augmented</strong> <code class="language-plaintext highlighter-rouge">prompt</code> and the pair (<code class="language-plaintext highlighter-rouge">CoT</code>, <code class="language-plaintext highlighter-rouge">answer</code>) from the reasoning model. This training procedure is borrowed from the <strong>Deliberative Alignment</strong> [4], 
with the difference that rather two decisions <strong>comply</strong> or <strong>refuse</strong> as in DA [4], here we have three decisions: <strong>direct answer</strong> (a.k.a. <strong>comply</strong>), <strong>safe-completion</strong> and <strong>refusal</strong>. <strong>Safe-completion</strong> mode provides high-level, non-operational, and within-safety-constraint guidance
when the content is restricted but not outright disallowed. It can be done by instructing Reasoning Models to <strong>judge</strong> with three above options.</p>

<h2 id="constrained-reinforcement-learning-rl-in-safe-completion-training">Constrained Reinforcement Learning (RL) in Safe-Completion Training</h2>

<p>In the RL stage, the model is optimized its helpfulness as long as it remains within the safety policy.
To do so, for each safety-related prompt and sampled response, we query two reward models (RMs), each of which outputs <strong>helpfulness</strong> and <strong>safety</strong> scores normalized to [0,1].</p>

<ul>
  <li><strong>Safety score</strong>: ∈ [0, 1]: the degree to which the output adheres to the content policy spec, <code class="language-plaintext highlighter-rouge">safety-score = 0</code> if severe or definitive violations of the policy, <code class="language-plaintext highlighter-rouge">safety-score = 1</code> if the output is fully compliant with the policy.</li>
  <li><strong>Helpfulness score</strong>: ∈ [0, 1]: the degree to which the output is helpful to the user. It is worth noting here, there are two types of answers for a good helping response <strong>direct answer</strong> and <strong>indirect answer</strong> (e.g., a safe-completion). In other words, <code class="language-plaintext highlighter-rouge">it is still considered helpful to provide a safe-completion</code>, more <strong>helpful</strong> than naive <strong>refusal</strong> as previous LLMs.</li>
</ul>

<p>The final reward is computed as the product of the two scores: <code class="language-plaintext highlighter-rouge">Reward = Helpfulness Score * Safety Score</code>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-safe-completion-training/2025-08-09-09-17-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Reward function in the RL stage from [1].
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In my opinion, this is a significant paradigm shift from the traditional Refusal Training (input-centric) to the Safe-Completion Training (output-centric).</p>

<p>At this moment, I am not sure how adversarial attacks will evolve in this new paradigm, but it sure will be interesting. Some ideas can be:</p>

<ul>
  <li>
    <p><strong>Breakdown big hamful output into multiple small harmless outputs</strong>. Because the model is now trained to detect and redirect harmful outputs, making it harder to get a whole harmful output like with previous LLMs. However, it might be weaker in handling each small piece of the whole harmful output.</p>
  </li>
  <li>
    <p><strong>The collapse of intelligence</strong>. Because the training procedure is now become self-referential where training data for the next version is generated by the previous reasoning models. While this addresses the problem of lacking human-labeled data, it might lead to a chain-reaction when a mistake of the previous version is propagated to the next version.</p>
  </li>
</ul>

<h2 id="references">References</h2>

<p>[1] Yuan Yuan et al. “From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.” OpenAI 2025.</p>

<p>[2] Andriushchenko, Maksym, and Nicolas Flammarion. “Does refusal training in llms generalize to the past tense?.” ICLR 2025.</p>

<p>[3] Deng, Yue, et al. “Multilingual jailbreak challenges in large language models.” arXiv preprint arXiv:2310.06474 (2023)</p>

<p>[4] Guan, Melody Y., et al. “Deliberative alignment: Reasoning enables safer language models.” arXiv preprint arXiv:2412.16339 (2024).</p>

<p>[5] Scholten, Yan, Stephan Günnemann, and Leo Schwinn. “A probabilistic perspective on unlearning and alignment for large language models.” ICLR 2025.</p>

<!-- mkdir -p assets/img/2025-safe-completion-training/ -->
<!-- mv _posts/2025-08-08-*.png assets/img/2025-safe-completion-training/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="llm" /><summary type="html"><![CDATA[OpenAI just recently released their newest and most powerful model GPT-5. In the post today, I want to talk about one of the most important aspects of LLMs: How to make them safe against malicious use. In this version, OpenAI introduces a new paradigm called Safe Completion Training (which is built on top of Deliberative Alignment [4])]]></summary></entry><entry><title type="html">Personalized LLMs</title><link href="https://tuananhbui89.github.io/blog/2025/personalized-llms/" rel="alternate" type="text/html" title="Personalized LLMs" /><published>2025-07-23T00:00:00+10:00</published><updated>2025-07-23T00:00:00+10:00</updated><id>https://tuananhbui89.github.io/blog/2025/personalized-llms</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/personalized-llms/"><![CDATA[<h2 id="why-personalize-llms">Why Personalize LLMs?</h2>

<p>Large-scale generative AI models are trained on diverse datasets to acquire broad capabilities. However, they are typically not tailored to the needs, preferences, or knowledge of a specific user. For example, a general-purpose LLM can generate grammatically correct and factually relevant text, but it may fail to align with a user’s preferred tone or context-specific requirements.</p>

<p><strong>Personalized LLMs</strong> aim to bridge this gap by adapting to individual users in the following key aspects:</p>

<ul>
  <li><strong>Style and Tone Adaptation</strong>: Adjusting the writing style, tone, or formality of the model to align with a user’s preferences—useful in applications like education, mental health support, or customer service.</li>
  <li><strong>Personal Knowledge Integration</strong>: Utilizing user-specific data (e.g., calendar events, documents, preferences) to act as a digital assistant or agent.</li>
  <li><strong>Domain-Specific Customization</strong>: Incorporating specialized knowledge for a particular task or profession, such as a medical LLM for diagnosis or a legal LLM for contract review.</li>
</ul>

<hr />

<h2 id="two-modes-of-personalization">Two Modes of Personalization</h2>

<p>Personalized LLMs can be applied in two broad scenarios:</p>

<ul>
  <li>
    <p><strong>Category A: Assistant-Oriented Personalization</strong><br />
The LLM acts on behalf of the user by leveraging personal knowledge, effectively functioning as a digital twin. For instance, it can help write reports, schedule meetings, or generate personalized content using user-specific context.</p>
  </li>
  <li>
    <p><strong>Category B: Preference-Oriented Personalization</strong><br />
The LLM adapts its outputs—tone, recommendations, search results, etc.—based on the user’s preferences, interests, or behavioral history.</p>
  </li>
</ul>

<hr />

<h2 id="use-cases">Use Cases</h2>

<p>Based on the above taxonomy, here are some common use cases (adapted from <a href="https://arxiv.org/pdf/2406.01171">Tseng et al., 2024</a>):</p>

<ul>
  <li><strong>Personalized Recommendation</strong>: Suggesting content or products that align with a user’s interests (Category B).</li>
  <li><strong>Personalized Search</strong>: Enhancing search relevance by understanding historical queries and user intent (Category B).</li>
  <li><strong>Personalized Healthcare</strong>:
    <ul>
      <li><em>Category A</em>: Assisting with scheduling, medication reminders, or emergency support.</li>
      <li><em>Category B</em>: Providing medical insights tailored to a user’s health profile.</li>
    </ul>
  </li>
  <li><strong>Software Development Support</strong>: Assisting developers by understanding their coding style, project context, and documentation (Category A).</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-13-27-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-13-27-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-13-27-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-personalized-llms/2025-07-23-13-27-16.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Use cases of personalized LLMs, adapted from <a href="https://arxiv.org/pdf/2406.01171">Tseng et al., 2024</a>.
</div>

<hr />

<h2 id="two-tales-of-persona-in-llms-role-playing-vs-personalization">Two Tales of Persona in LLMs: Role-Playing vs. Personalization</h2>

<p>There are two main directions in how persona is handled in LLMs:</p>

<ul>
  <li>
    <p><strong>Role-Playing LLMs</strong>: The model is assigned a fixed persona or character (e.g., doctor, lawyer, coach) to interact with users in a consistent manner. The focus is on <strong>task fidelity</strong> and <strong>role simulation</strong>, often used in training or simulation settings.</p>
  </li>
  <li>
    <p><strong>Personalized LLMs</strong>: The model adapts its behavior, knowledge, or language based on the <strong>user’s identity, preferences, and needs</strong>. The emphasis is on <strong>personal relevance</strong> and <strong>user satisfaction</strong>.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-11-56-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-11-56-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-11-56-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-personalized-llms/2025-07-23-11-56-24.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Taxonomy of role-playing vs. personalized LLMs, from <a href="https://arxiv.org/pdf/2406.01171">Tseng et al., 2024</a>.
</div>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Role-Playing LLM</th>
      <th>Personalized LLM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Persona Source</td>
      <td>Assigned persona (e.g., fictional or occupational)</td>
      <td>Adapted from user’s identity, behavior, or context</td>
    </tr>
    <tr>
      <td>Primary Objective</td>
      <td>Simulate a role with high fidelity</td>
      <td>Optimize for individual user satisfaction</td>
    </tr>
    <tr>
      <td>Core Focus</td>
      <td>Role consistency and task performance</td>
      <td>Contextual relevance and adaptability</td>
    </tr>
    <tr>
      <td>Example</td>
      <td>Doctor persona for training simulations</td>
      <td>Shopping assistant based on browsing history</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="key-challenges">Key Challenges</h2>

<h3 id="injecting-personal-knowledge-into-llms">Injecting Personal Knowledge into LLMs</h3>

<p>Several methods have been proposed to personalize LLMs by incorporating user-specific knowledge:</p>

<ul>
  <li>
    <p><strong>Prompting and In-Context Learning</strong>: Injecting user information directly into the input prompt. While flexible, this approach faces context length limitations and higher inference costs <a href="#5">[5]</a>.</p>
  </li>
  <li>
    <p><strong>Fine-Tuning / Parameter-Efficient Tuning (PEFT)</strong>: Adapting the model weights using personal data, such as through LoRA. This requires enough personal data and may risk overfitting or degradation on general tasks.</p>
  </li>
  <li>
    <p><strong>Retrieval-Augmented Generation (RAG)</strong>: Storing user data externally and retrieving it during inference. This allows scalable personalization, but can suffer from noisy retrievals or incomplete information <a href="#6">[6]</a>.</p>
  </li>
</ul>

<hr />

<h3 id="updating-personal-knowledge">Updating Personal Knowledge</h3>

<hr />

<h3 id="evaluating-personalized-llms">Evaluating Personalized LLMs</h3>

<p>Assessing the quality of personalized outputs—especially for preference-based tasks like tone, emotion, or stylistic match—is an open challenge. Existing evaluation methods struggle to <strong>quantify subjective user satisfaction</strong> or <strong>match to personal styles</strong> <a href="#4">[4]</a>.</p>

<hr />

<h3 id="safety-and-privacy-concerns">Safety and Privacy Concerns</h3>

<p>Fine-tuning or storing user data poses serious privacy and safety issues:</p>

<ul>
  <li><strong>Alignment Drift</strong>: Personal fine-tuning may bypass original safety constraints, unintentionally enabling jailbreaks <a href="#1">[1]</a>.</li>
  <li><strong>Bias and Overfitting</strong>: Training on unrepresentative user data can produce biased or brittle behaviors.</li>
  <li><strong>Data Leakage</strong>: Training data could be extracted by adversaries through membership inference or extraction attacks <a href="#2">[2]</a>, <a href="#3">[3]</a>.</li>
</ul>

<hr />

<h2 id="further-reading">Further Reading</h2>

<h3 id="personalized-language-modeling-from-personalized-human-feedback">Personalized Language Modeling from Personalized Human Feedback</h3>

<h3 id="beyond-dialogue-a-profile-dialogue-alignment-framework-toward-general-role-playing-llms">Beyond Dialogue: A Profile-Dialogue Alignment Framework Toward General Role-Playing LLMs</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-20-25-15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-20-25-15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-personalized-llms/2025-07-23-20-25-15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-personalized-llms/2025-07-23-20-25-15.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Illustration of the Beyond Dialogue framework from <a href="https://arxiv.org/pdf/2408.10903">Yu et al., 2024</a>.
</div>

<p>Problem setting:</p>

<ul>
  <li><strong>Bias between the Role Profile and Scene-Specific Dialogues</strong>:</li>
</ul>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Tseng, Yu-Min, et al. “Two tales of persona in LLMs: A survey of role-playing and personalization.” <em>arXiv preprint arXiv:2406.01171</em> (2024).</li>
  <li>Wang, Jeffrey G., et al. “Pandora’s White-Box: Precise Training Data Detection and Extraction in Large Language Models.” <em>arXiv:2402.17012</em> (2024).</li>
  <li>Lukas, Nils, et al. “Analyzing leakage of personally identifiable information in language models.” <em>IEEE S&amp;P</em> (2023).</li>
  <li>Samuel, Vinay, et al. “Personagym: Evaluating persona agents and LLMs.” <em>arXiv:2407.18416</em> (2024).</li>
  <li>Richardson, Chris, et al. “Integrating summarization and retrieval for enhanced personalization via large language models.” <em>arXiv:2310.20081</em> (2023).</li>
  <li>Tan, Zhaoxuan, et al. “Democratizing large language models via personalized parameter-efficient fine-tuning.” <em>arXiv:2402.04401</em> (2024).</li>
  <li>Chen, Nuo, et al. “The Oscars of AI Theater: A Survey on Role-Playing with Language Models.” <em>arXiv:2407.11484</em> (2024).</li>
  <li>Yu, Yeyong, et al. “Beyond dialogue: A profile-dialogue alignment framework towards general role-playing language model.” <em>arXiv:2408.10903</em> (2024).</li>
</ol>

<hr />

<h2 id="additional-resources">Additional Resources</h2>

<ul>
  <li><a href="https://github.com/HqWu-HITCS/Awesome-Personalized-LLM">Awesome-Personalized-LLM GitHub</a>: A curated list of papers, datasets, and benchmarks on personalized LLMs.</li>
</ul>

<!-- mkdir -p assets/img/2025-personalized-llms/ -->
<!-- mv _posts/2025-07-23-*.png assets/img/2025-personalized-llms/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="llm" /><summary type="html"><![CDATA[Why Personalize LLMs?]]></summary></entry><entry><title type="html">MS-Diffusion - Multi-subject Zero-shot Image Personalization with Layout Guidance (ICLR 2025)</title><link href="https://tuananhbui89.github.io/blog/2025/ms-diffusion/" rel="alternate" type="text/html" title="MS-Diffusion - Multi-subject Zero-shot Image Personalization with Layout Guidance (ICLR 2025)" /><published>2025-03-15T00:00:00+11:00</published><updated>2025-03-15T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/ms-diffusion</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/ms-diffusion/"><![CDATA[<p>Link to the paper: <a href="https://openreview.net/forum?id=PJqP0wyQek">MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance</a></p>

<p>Link to github: <a href="https://github.com/MS-Diffusion/MS-Diffusion">MS-Diffusion</a></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-ms-diffusion/2025-03-19-11-52-11.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h2 id="key-contributions">Key contributions</h2>

<p><strong>Challenges</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-ms-diffusion/2025-03-19-11-56-18.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Personalizing with multiple subjects is challenging, specifically:</p>

<ul>
  <li><strong>Subject neglect</strong>: one or more subjects are not properly represented in the generated image.</li>
  <li><strong>Subject overcontrol</strong>: The output does not match with the input prompt (e.g., “a dog and a cat on the beach” - but the output is not on the beach). The appearance or placement of a subject is unduly influenced by its reference image, potentially overriding the textual prompt or other subject conditions.</li>
  <li><strong>Subject-subject conflict</strong>:  where the interaction between multiple subjects in the generated image is unrealistic or undesirable, i.e., two similar dog and cat.</li>
</ul>

<p>The reasons for these challenges are:</p>

<ul>
  <li><strong>Difficulty in feature representation</strong>: compare to text embedding, image embedding are generally <strong>sparser and contain more information</strong>, making their projection into the condition space more difficult. The pooled output from the image encoder can omit many details (discussed in Section 3.4 of the paper). This can lead to a loss of granular details of individual subjects.</li>
  <li><strong>Complexity of Multi-subject interaction and control</strong>: Ensuring that the generated image aligns with the textual prompt while maintaining the correct placement and appearance of each subject is challenging.</li>
</ul>

<p><strong>Contributions</strong></p>

<ul>
  <li><strong>Grounding Resampler</strong>: A modified cross-attention layer that uses concatenated image and text embeddings as the condition embedding.</li>
  <li><strong>Data Construction</strong>: A data construction pipeline to collect a large amount of data with multiple subjects in the same image.</li>
  <li><strong>Multi-subject Cross-attention with Masks</strong>: A modified cross-attention layer that uses subject-specific masks to guide the model to pay attention to the subject.</li>
</ul>

<h2 id="background-stable-diffusion-with-image-prompt">Background: Stable Diffusion with Image Prompt</h2>

<p>Beyond controlling the generation process using text prompt, there is a hot topic in the community to control using image information/layout/prompt - which has a huge potential in applications, e.g., image inpainting, image-to-image generation, etc. In the standard Stable Diffusion, the condition embedding \(c_t\) is just a text embedding \(c_t = E_t(y)\) where \(y\) is the text prompt and \(E_t\) is a pre-trained text encoder such as CLIP.
IP-Adapter [1] proposes to use an additional image encoder to extract the image embedding from a reference image \(c_i = E_i(x)\) and then project it into the original condition space.
The objective function for IP-Adapter is:</p>

\[\mathcal{L}_{IP} = \mathbb{E}_{z, c, \epsilon, t} \left[ \mid \mid \epsilon - \epsilon_\theta(z_t \mid c_i, c_t, t) \mid \mid_2^2 \right]\]

<p>The cross-attention layer is also modified from the one in Stable Diffusion to include the image embedding \(c_i\) as a condition.</p>

\[\text{Attention}(Q, K_i, K_t, V_i, V_t) = \lambda \text{softmax}\left(\frac{QK_i^T}{\sqrt{d}} + c_i\right)V_i + \text{softmax}\left(\frac{QK_t^T}{\sqrt{d}}\right)V_t\]

<p>where \(Q=z W_Q\), \(K_i = c_i W_K^i\), \(K_t = c_t W_K^t\), \(V_i = c_i W_V^i\), \(V_t = c_t W_V^t\), and \(W_Q\), \(W_K^i\), \(W_K^t\), \(W_V^i\), \(W_V^t\) are the weights of the linear layers.
The model becomes the original Stable Diffusion when \(\lambda = 0\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-03-20-07-02-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h2 id="proposed-method">Proposed Method</h2>

<h3 id="grounding-resampler">Grounding Resampler</h3>

\[\text{RSAttn} = \text{softmax}\left( \frac{Q(f_q) K^T([c_i, c_q])}{\sqrt{d}} \right) V([c_i, c_q])\]

<p>where \(c_q\) is the learnable query feature, \(c_i\) is the image embedding.</p>

<p>My interpretation:</p>

<ul>
  <li>The grounding resampler is a modified cross-attention layer that uses the image embedding and the learnable query feature to generate the attention score.</li>
  <li>Compared to the previous IP-Adapter which uses the image and text embeddings <strong>separately</strong>, the \(\text{RSAttn}\) uses the concatenation of the two embeddings as the unified condition embedding.</li>
</ul>

<h3 id="data-construction">Data Construction</h3>

<p>The authors propose a data construction pipeline to <strong>collect</strong> a large amount of <strong>data with multiple subjects</strong> in the same image.
\(c_q\) was initialized by the <strong>text embedding</strong> of the entities (e.g., “dog”, “cat”, “beach”) and then optimized during training.
\(c_i\) is the <strong>image embedding</strong> of corresponding subject - detected by an <strong>additional object detector</strong>.</p>

<p>To prevent the model becoming overfitted/dependent on the grounding tokens (e.g., “dog”, “cat”) during inference <strong>(?)</strong>, the authors proposed to randomly replace these tokens with the original learnable queries in the training <strong>(?)</strong>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-ms-diffusion/2025-03-20-07-14-27.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h3 id="multi-subject-cross-attention">Multi-subject Cross-attention</h3>

<p>The idea is to <strong>incorporate attention masks within cross-attention layers</strong> to focus on the relevant subjects and exclude other irrelevant in the text prompt and visual prompt.</p>

\[\mathbf{M}_j(x, y) = \begin{cases}
0 &amp; \text{if } [x, y] \in B_j \\
-\infty &amp; \text{if } [x, y] \notin B_j
\end{cases}\]

<p>Here, \(B_j\) denotes the coordinate set of <strong>bounding boxes related to the \(j\)th subject</strong>. By this means, the conditional image latent \(\hat{\mathbf{z}}_{img}\) is derived through:</p>

\[\hat{\mathbf{z}}_{img} = \text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}_i^\top}{\sqrt{d}} + \mathbf{M}\right)\mathbf{V}_i\]

<p>Herein, \(\mathbf{M}\) represents the concatenation of all subject-specific masks, \(\text{Concat}(\mathbf{M}_0,\ldots,\mathbf{M}_n)\). In this way, the model ensures each subject to be represented in a certain area, thus resolving the issues of subject neglect and conflict.</p>

<p><strong>Mask for Background</strong></p>

\[\mathbf{M}_{bg}(x, y) = \begin{cases}
1 &amp; \text{if } [x, y] \in B_{bg} \\
0 &amp; \text{if } [x, y] \notin B_{bg} \text{ a.k.a. subjects}
\end{cases}\]

<p>The mask for background is a matrix of the same size as the image, with the value of \(1\) for the background and \(0\) for the subjects.</p>

\[\mathbf{z}_{img} = (1 - \mathbf{M}_{bg}) \odot \mathbf{z}_{img}\]

<p>Where \(\odot\) is the element-wise multiplication. This operation ensures that the background is removed from the image latent.
As the authors mentioned, this approach is to ensure that text conditions predominate over areas lacking of any guided information (a.k.a. background).</p>

<p><strong>My interpretation:</strong></p>

<ul>
  <li>The mask \(\mathbf{M}\) is a matrix of the same size as the image, with the value of \(-\infty\) for the background and \(0\) for the subject.</li>
  <li>The mask is added to the attention score matrix \(\mathbf{Q}\mathbf{K}_i^\top\) to <strong>guide the model to pay attention to the subject</strong>.</li>
</ul>

<p><strong>Question</strong>: Where the \(\mathbf{z}_{img}\) is used? The authors did not use a consistent notations throughout the paper.</p>

<!-- mkdir -p assets/img/2025-ms-diffusion/ -->
<!-- mv _posts/2025-03-20-*.png assets/img/2025-ms-diffusion/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><category term="diffusion" /><summary type="html"><![CDATA[Link to the paper: MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance]]></summary></entry><entry><title type="html">Unlearning LLMs</title><link href="https://tuananhbui89.github.io/blog/2025/unlearn-llms/" rel="alternate" type="text/html" title="Unlearning LLMs" /><published>2025-03-14T00:00:00+11:00</published><updated>2025-03-14T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/unlearn-llms</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/unlearn-llms/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><strong>Why unlearning LLMs? From [1]</strong></p>

<ul>
  <li><strong>Reducing the risk of malicious use</strong>: to decrease the potential for LLMs to empower malicious use cases such as developing biological, cyber, or chemical weapons. Unlearning aims to <strong>remove hazardous knowledge</strong> from these LLMs, which is a <strong>prerequisite</strong> for the deployment of LLMs in the real world.</li>
  <li><strong>Enhancing inherent safety</strong>: By directly removing hazardous knowledge, unlearning can lead to inherently safer models. Even if the LLMs are jailbroken by adversaries, they would lack the necessary knowledge to assist adversaries in launching attacks.</li>
  <li><strong>Counteracting adversarial attacks</strong>: Current safeguards like refusal training [2] can be bypassed through adversarial attacks, and hazardous information can be reintroduced through finetuning. Similar to the case of enhancing inherent safety, unlearning especially when applied before model serving, can act as a countermeasure by removing the knowledge that these attacks or finetuning might exploit or reveal.</li>
</ul>

<blockquote class="block-tip">
  <p><strong>Refusal training</strong>:</p>

  <p>Refusal training is a method that trains a model to decline or refuse to generate responses to prompts that are associated with malicious use cases.</p>

  <p>This is typically achieved through (1) Supervised fine-tuning (SFT) with a dataset of pair harmful prompts with appropriate refusal responses, (2) RLHF with human preference data, (3) Adversarial training aiming to be more robust against adversarial prompts designed to bypass safety mechanisms.
However, this method is still be circumvented by aversaries, such as posing harmful prompts in the <strong><a href="https://arxiv.org/pdf/2407.11969">past tense</a></strong>.</p>
</blockquote>

<h2 id="setting">Setting</h2>

<ul>
  <li>Forget set: \(\mathcal{D}_f\) (used in fine-tuning) This is a dataset of examples representing the knowledge that the unlearning process aims to remove from the language model, e.g., the WMDP benchmark [1] which contains hazardous knowledge in biosecurity and cybersecurity.</li>
  <li>Retain set: \(\mathcal{D}_r\) (used in fine-tuning) This is a dataset of examples representing general, benign knowledge that the unlearning process should aim to preserve. Recent works [3] try to remove the need of a retain set by using a “flat” loss adjustment approach which adjusts the loss function using only the forget data.</li>
  <li>Testing set: \(\mathcal{D}_t\) (used in evaluation) to evaluate two aspects: (1) unlearning performance - Question-Answering (QA) accuracy on WMDP benchmark, and (2) retaining performance - other benchmarks like MMLU and MT-Bench.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-14-10-30-41.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Figure from [1]
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-10-51-49.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>A commonly used form of unlearning consists of the following optimization problem:</p>

\[\mathcal{L} = \min_{\theta} \underbrace{\mathbb{E}_{x^f \in \mathcal{D}_f} \ell(y^f|h_{\theta}^{(l)}(x^f))}_{\text{forget loss}} + \alpha \underbrace{\mathbb{E}_{x^r \in \mathcal{D}_r} \ell(y^r|h_{\theta}^{(l)}(x^r))}_{\text{retain loss}}\]

<p>where \(\theta\) is the model parameters of an autoregressive LLM \(f_{\theta}\), \(\ell\) is the loss function, \(y^f\) and \(y^r\) are the target representations (e.g., representations of next token) for the forget and retain sets, and \(\alpha\) is a hyperparameter.</p>

<p>It is worth noting that \(h_{\theta}^{(l)}\) is the <strong>average</strong> output hidden state of all tokens (with the size of sequence length) in forget-sample \(x^f \in \mathcal{D}_f\). Similarly, \(h_{\theta}^{(l)}\) is the <strong>average</strong> output hidden state of all tokens in retain-sample \(x^r \in \mathcal{D}_r\). Using this average representation to represent the entire forget-sample is one of current limitations that requires further investigation.</p>

<p><strong>Intepretation</strong>: Minimizing the forget loss is to steer the model representation of forget-samples to a target random representation, while minimizing the retain loss is to keep the model representation of retain-samples unchanged.</p>

<!-- ### Challenges

**Overlap between forget set and retain set**

**Expressiveness of token-level representation for the entire set/document** -->

<!-- ### Milestones - Approaches -->

<h2 id="representation-misdirection-for-unlearning-rmu">Representation Misdirection for Unlearning (RMU)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a method to unlearn LLMs by steering the representation of forget samples towards a target random representation while keeping the representation of retain samples unchanged.</p>

<p><strong>Approach</strong></p>

<p>RMU [1] aims to steer model representation of forget samples (e.g., malicious use cases - that sampled from the forget set) in the intermediate layer towards a target random representation while keeping the representation of retain samples (e.g., benign use cases - that sampled from the retain set) unchanged.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-14-10-34-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of RMU from [1]
</div>

<p>More specifically, given a forget set \(\mathcal{D}_f\) and a retain set \(\mathcal{D}_r\), and a frozen model \(h_{\theta^{\text{frozen}}}\) with parameters \(\theta^{\text{frozen}}\), RMU steers the latent representation of forget-tokens to a predetermined random representation \(y^f = cu\), where \(u\) is a random unit vector each element is sampled from Uniform distribution \(U(0,1)\), \(c \in \mathbb{R}^+\) is a coefficient, and regularizes the latent representation of retain-tokens back to the frozen model’s representation. The loss of RMU is</p>

\[\mathcal{L} = \mathbb{E}_{x^f \in \mathcal{D}_f} \|h_{\theta^{\text{rm}}}^{(l)}(x^f) - cu\|^2 + \alpha\mathbb{E}_{x^r \in \mathcal{D}_r} \|h_{\theta^{\text{rm}}}^{(l)}(x^r) - h_{\theta^{\text{frozen}}}^{(l)}(x^r)\|^2,\]

<p>where \(\theta^{\text{rm}}\) is the parameters of the model to be optimized, and \(\alpha\) is a hyperparameter.</p>

<h2 id="adaptive-rmu-on-effects-of-steering-latent-representation-for-large-language-model-unlearning---aaai-2025">Adaptive RMU (On Effects of Steering Latent Representation for Large Language Model Unlearning - AAAI 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a <strong>layer-adaptive</strong> RMU method, which uses a different \(c\) for each layer.</p>

<p><strong>Key Observation</strong></p>

<p>This paper points out an interesing phenomenon that the performance of RMU is sensitive to the choice of coefficient \(c\) in the above loss function.</p>

<p>More specifically, the random unit vector \(u\) and the representation of forget samples \(\hat{h}_{\theta^{\text{rm}}}^{(l)}(x^f)\) are more aligned as \(c\) increases, suggesting the better unlearning performance.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-16-21-49.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The distribution of the representation of forget samples with different $c$ from [4]. The blue histogram becomes more Gaussian as $c$ increases.
</div>

<p>However, using a fixed \(c\) across all layers is not ideal, since the performance of RMU is observed to be <strong>layer-dependent</strong>.
More specifically, as discussed in Section 4.3 in [4], within early layers, the \(l^2\) norm of the representation of forget samples is relatively smaller than the coefficient \(c\) and during the unlearning process, that norm exponentially grows and appproaches \(c\), thereby facilitating the convergence of the unlearning process. However, in later layer, the \(l^2\) norm of the representation of forget samples is initially larger than \(c\) and remains unchanged during unlearning, making the unlearning process less effective.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-16-34-02.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The norm of the representation of forget samples in different layers from [4].
</div>

<p>Inspired by the above observation, this paper proposes a <strong>layer-adaptive</strong> RMU method, which uses a different \(c\) for each layer.</p>

\[\mathcal{L}^{\text{adaptive}} = \underbrace{\mathbb{E}_{x_F \in \mathcal{D}_{\text{forget}}} \|h_{\theta^{\text{unlearn}}}^{(l)}(x_F) - \beta\|h_{\theta^{\text{frozen}}}^{(l)}(x_F)\|u\|_2^2}_{\text{adaptive forget loss}} + \underbrace{\alpha \mathbb{E}_{x_R \in \mathcal{D}_{\text{retain}}} \|h_{\theta^{\text{unlearn}}}^{(l)}(x_R) - h_{\theta^{\text{frozen}}}^{(l)}(x_R)\|_2^2}_{\text{retain loss}}\]

<p>where \(\beta \|h_{\theta^{\text{frozen}}}^{(l)}(x_F)\|\) is the <strong>adaptive scaling coefficient</strong> for the forget loss which is computed by the norm of the representation of forget samples in the corresponding layer of the frozen model.</p>

<p>However, it is worth noting that intuitively, the higher \(c\) leads to a more alignment between forget representation and the random unit vector \(u\), which suggests the better unlearning performance - more randomness of the output with the forget prompt. However, it also leads to a worse retaining performance.</p>

<h2 id="llm-unlearning-via-loss-adjustment-with-only-forget-data-iclr-2025">LLM Unlearning via Loss Adjustment with Only Forget Data (ICLR 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a method to unlearn LLMs by adjusting the loss function using only the forget data.</p>

<h3 id="motivation---eliminating-the-need-for-a-retain-set">Motivation - Eliminating the need for a retain set</h3>

<p>Previous unlearning methods typically require a retain set or a reference model to maintain the performance of the unlearned model on the retain set.
The limitation of this requirement (as stated in [3]) is that it is may lead to a trade-off between model utility and forget performance (why?).
Furthermore, fine-tuning using both retain data and forget data would require a careful design of a data mixing strategy to avoid information leakage from the retain set to the forget set.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-17-17-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison between unlearning methods from [3]
</div>

<h3 id="loss-adjustments-via-f-divergence-maximization">Loss-Adjustments via f-divergence Maximization</h3>

<p>For each learning batch, we assume that we only have access to a set of forget samples \((x_f, y_f) \in D_f\). Instead of directly adopting gradient ascent over these forget samples, we propose to maximize the divergence between exemplary and bad generations of forget data. Key steps are summarized as below.</p>

<p><strong>Step 1</strong>: Equip example/template responses \(y_e\) for each forget sample \(x_f\). Together we denote the paired samples as \(D_e = \{(x_f^j, y_e^j)\}_{j\in[N]}\).</p>

<ul>
  <li>
    <p>This could be done by leveraging open-source LLMs such as Llama 3.1 [25] or self-defining the responses according to our wish, etc. The designated unlearning response could be a reject-based answer such as “I don’t know” (denoted as “IDK”) or an irrelevant answer devoid of the unlearning target-related information.</p>
  </li>
  <li>
    <p><strong>Motivation</strong>: Step 1 generates example responses for LLM fine-tuning and provides better instructions on what LLM should respond given the forget data. Besides, certain existing methods make LLM generate hallucinated responses after unlearning, which further illustrates the importance of example responses for LLM unlearning.</p>
  </li>
</ul>

<p><strong>Step 2</strong>: Loss adjustments w.r.t. the sample pairs \((x_f, y_e, y_f)\) through:</p>

\[L(x_f, y_e, y_f; \theta) = \lambda_e \cdot L_e(x_f, y_e; \theta) - \lambda_f \cdot L_f(x_f, y_f; \theta),\]

<p>where \(L_e, L_f\) are losses designed for the data sample \((x_f, y_e)\) and \((x_f, y_f)\), respectively.</p>

<ul>
  <li><strong>Motivation</strong>: Step 2 encourages the LLM to forget the forget data with bad responses, meanwhile, learn to generate good responses on relevant forget data [such as template answers].</li>
</ul>

<p><strong>Step 3</strong>: How to decide on the values of \(\lambda_e\) and \(\lambda_f\)?</p>

<p>We leverage f-divergence to illustrate the appropriate balancing between \(L_e(x_f, y_e; \theta)\) and \(L_f(x_f, y_f; \theta)\). Assume \(x_f, y_e\) is generated by the random variable \(X_f, Y_e\) jointly following the distribution \(\mathcal{D}_e\). Similarly, \(x_f, y_f\) is given by \(X_f, Y_f\) and \((X_f, Y_f) \sim \mathcal{D}_f\). Step 2 shares similar insights as if we are maximizing the divergence between \(\mathcal{D}_e\) and \(\mathcal{D}_f\). Our theoretical purpose is to obtain the model that maximizes the f-divergence between \(\mathcal{D}_e\) and \(\mathcal{D}_f\), defined as \(f_{div}(\mathcal{D}_e\|\mathcal{D}_f)\).</p>

<p><strong>The variational form f-divergence</strong>: Instead of optimizing the \(f_{div}\) term directly, we resolve to the variational form of it. Due to the Fenchel duality, we would have:</p>

\[f_{div}(\mathcal{D}_e\|\mathcal{D}_f) = \sup_g [\mathbb{E}_{z_e\sim\mathcal{D}_e} [g(z_e)] - \mathbb{E}_{z_f\sim\mathcal{D}_f} [f^*(g(z_f))]] := \sup_g \text{VA}(\theta, g),\]

<p>we define \(f^*\) as the conjugate function of the f-divergence function. For simplicity, we define \(\text{VA}(\theta, g^*) := \sup_g \text{VA}(\theta, g)\), where \(g^*\) is the optimal variational function.</p>

<h3 id="connection-with-dpo">Connection with DPO</h3>

<p><a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#ppo-and-dpo">Direct Preference Optimization</a> (DPO) is a method to align LLMs with human preferences, however, unlike PPO which uses a reward model, DPO simplifies PPO by directly optimizing the policy model to adhere to human preferences without the need for a reward model. In the context of unlearning, DPO can be used to unlearn the LLM by directly optimizing the original model to align forget prompt with the template response.</p>

<p>Given a dataset \(D = \{(x_f^j, y_e^j, y_f^j)\}_{j\in[N]}\), where \(y_e\) and \(y_f\) are preferred template and original forget responses to the forget prompt \(x_f\), DPO fine-tunes original model \(\theta_o\) using \(D\) to better align it with good answer preferences, which minimizes:</p>

\[L_{\text{DPO},\beta}(\theta) = -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta\log \frac{\pi_\theta(y_e \mid x_f)}{\pi_{\text{ref}}(y_e \mid x_f)} - \beta\log \frac{\pi_\theta(y_f \mid x_f)}{\pi_{\text{ref}}(y_f \mid x_f)}\right)\right]\]

\[= -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i}) - \log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i}))-M_{\text{ref}}\right)\right],\]

<p>where, \(\sigma(t) = \frac{1}{1+e^{-t}}\) is the sigmoid function, \(\beta &gt; 0\) is the inverse temperature, \(\pi_\theta := \prod_{i=1}^{\mid y \mid} h_\theta(x, y_{&lt;i})\) is the predicted probability of the response \(y\) to prompt \(x\) given by LLM \(\theta\), \(\pi_{\text{ref}}\) is the predicted probability given by reference model, and \(M_{\text{ref}} := \beta(\log \prod_{i=1}^{\mid y_e \mid}h_{\theta_o}(x_f, y_{e,i}) - \log \prod_{i=1}^{\mid y_f \mid}h_{\theta_o}(x_f, y_{f,i}))\).</p>

<p><strong>Intepretation</strong>: minimizing \(L_{\text{DPO},\beta}(\theta)\) is equivalent to maximizing \(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i})\) - which is the log probability of the template response - while minimizing \(\log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i})\) - that of the forget response.
the \(M_{\text{ref}}\) term is a constant w.r.t. \(\theta\) which is the gap between two log probabilities of the template and forget responses given by the reference model - so that the optimal should maintain the same gap as the reference model (IMO: should add max-margin loss here).</p>

<p><strong>FLAT</strong></p>

<p>As for FLAT, we calculate the average probability of all correctly generated tokens and employ a novel re-weighting mechanism that assigns different importance to each term using distinct activate functions for both the example and forget loss terms, which minimizes:</p>

\[L_{\text{FLAT}}(\theta) = -\mathbb{E}_D\left[g^*\left(\frac{1}{\mid y_e \mid} \sum_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_{e,&lt;i})\right) - f^*(g^*\left(\frac{1}{\mid y_f \mid} \sum_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_{f,&lt;i})\right))\right].\]

<p>Here, \(f^*(\cdot), g^*(f^*(\cdot))\) are the activate functions that assign appropriate weights to each loss term. The detailed derivation is in Appendix B.2 in the paper. Specifically, DPO relies on a reference model to guide the unlearning process, whereas FLAT only uses a sample pair dataset containing both exemplar and forget responses. Besides, FLAT differs from DPO in three critical aspects: the re-weighting activation function, whether to sum or average the token losses, and whether to apply the logarithm to the output probability.</p>

<p>An example (from Appendix B.2 in the paper) is as follows:</p>

<p>Here, \(v\) is the vocabulary size, \(y_{e,i,k}\) is the \(k\)-th element of vector representing the \(i\)-th token in the good response \(y_e\), \(y_{f,i,k}\) is the \(k\)-th element of vector representing the \(i\)-th token in the forget response \(y_f\). Additionally, \(h_\theta(x_f, y_{e,&lt;i})_k\) and \(h_\theta(x_f, y_{f,&lt;i})_k\) denote the \(k\)-th entry of the probability distribution for the correctly generated token.</p>

<p>For <strong>KL f-divergence</strong>, \(f^*(u) = e^{u-1}, g^*(v) = v\), hence, \(g^*(\mathbb{P}(x_f, y_e; \theta)) - f^*(g^*(\mathbb{P}(x_f, y_f; \theta))) = \mathbb{P}(x_f, y_e; \theta) - e^{\mathbb{P}(x_f, y_f; \theta)-1}\). We have:</p>

\[L_{\text{FLAT}}(\theta) = -\mathbb{E}_D\left[\frac{\sum_{i=1}^{ \mid y_e \mid} h_\theta(x_f, y_{e,&lt;i})}{\mid y_e \mid} - e^{\frac{\sum_{i=1}^{ \mid y_f \mid} h_\theta(x_f, y_{f,&lt;i})}{\mid y_f \mid}-1}\right].\]

<h2 id="a-closer-look-at-machine-unlearning-for-large-language-models-iclr-2025">A Closer Look at Machine Unlearning for Large Language Models (ICLR 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper investigates the effectiveness of unlearning LLMs for targeted and untargeted use cases.
It proposes a Maximizing Entropy (ME) objective for untargeted unlearning and Answer-Preservation (AP) objective for targeted unlearning.</p>

<h3 id="untargeted-vs-targeted-unlearning">Untargeted vs Targeted Unlearning</h3>

<p>Targeted unlearning hopes to make a specified template response to the questions in the forget set, while untargeted unlearning only requires not leaking the contents of the forget set.
Mathematically, the loss function for untargeted unlearning is (borrowing the notation of DPOfrom [3])</p>

\[= -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i}) - \log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i}))-M_{\text{ref}}\right)\right],\]

<p>Where \(y_e\) is the template response and \(y_f\) is the forget response. Targeted unlearning aims to make the model response to the forget set to be close to the template response, hence, maximizing the probability of the template response. On the other hand, untargeted unlearning only requires the model response to the forget set to be far from the forget response, hence, minimizing the probability of the forget response.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-16-15-58-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of untargeted and targeted unlearning from [5]
</div>

<h3 id="maximizing-entropy-for-untargeted-unlearning">Maximizing Entropy for Untargeted Unlearning</h3>

<p>The paper states that “the core of most untargeted unlearning methods is to adapt a gradient ascent direction that maximizes the prediction loss over the forget set - may have several challenges”.</p>

<ul>
  <li><strong>The behavior of the ideal retain model is unpredictable</strong>: The cost to retrain the model from scratch is extremely expensive in the case of LLMs. More importantly, gradient ascent on the forget set when retraining may lead to unpredicatable behavior of the model.</li>
  <li><strong>Potential hallucinations in the surrogate retain model</strong>: An alternative approach for the expensive retraining is to use a surrogate model - which is a base model such as Llama 2, fine-tuned on a small fictitious dataset \(\mathcal{D}^f = \{ \mathcal{D}^f_F,  \mathcal{D}^f_R \}\), where \(\mathcal{D}^f_F\) and \(\mathcal{D}^f_R\) are the forget and retain sets, respectively. However, this approach may lead to hallucinations, where the model generates responses that are not present in the retain set.</li>
</ul>

<p>Idea: <em>Align the prediction behavior of the unlearned model on the forget set with that of a <strong>randomly initialized</strong> model</em>.</p>

<ul>
  <li>The randomly initialized model is data-independent and does not contain any knowledge about the forget set, avoids the leakage of relevant information.</li>
  <li>The behavior of the randomly initialized model is random guessing - maximizing the entropy of the output distribution.</li>
</ul>

\[\mathcal{L}_{\text{ME}}(\mathcal{D}_F; \theta) = \mathbb{E}_{(x,y)\sim\mathcal{D}_F}\left[\frac{1}{T}\sum_{t=1}^T \text{KL}(P_t\|\mathcal{U}_{[K]})\right],\]

<p>where \(P_t = p(x'_t \mid x'_{&lt;t}; \theta)\) is the predicted probability for the \(t\)-th token in \(x' = x \circ y\) and \(\mathcal{U}_{[K]}\) is a uniform distribution over the vocabulary of size \(K\), where each value is \(1/K\).</p>

<p>Minimizing above loss is equivalent to Maximizing Entropy (ME) of predicted distribution for each next token. <strong>The greater the entropy, the higher the uncertainty of the prediction, indicating that the model behaves closer to a randomly initialized model for random guessing</strong>. This objective also avoids catastrophic collapse caused by the unbounded forget loss (Zhang et al., 2024a; Ji et al., 2024).</p>

<h3 id="mitigate-excessive-ignorance-of-targeted-unlearning">Mitigate Excessive Ignorance of Targeted Unlearning</h3>

<p><strong>Over Ignorance Issue</strong>: Refuse to answer most questions in the retain set - False Positive. It is due to (based on their argument) that \((\mathcal{X}_F, \mathcal{Y}_F) \approxeq (\mathcal{X}_R, \mathcal{Y}_R)\), therefore, increasing \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_F)\) will also increase \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_R)\).</p>

<p>Their experiment to support the above argument is as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-17-12-50-45.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The correlation between performances of unlearned models on the forget set and the retain set from [5].
</div>

<p><strong>Answer-Preservation (AP)</strong>:</p>

<p>Intuitively, given a question in the retain set, the regularization loss for targeted unlearning should satisfy two objectives:</p>

<ul>
  <li>Reduce the probability of the rejection template.</li>
  <li>Maintain the probability of the original answer.</li>
</ul>

<p>Thus, the authors propose the Answer Preservation (AP) loss as follows:</p>

\[\mathcal{L}_{\text{AP}}(\mathcal{D}_R, \mathcal{D}_{\text{IDK}}; \theta) = -\frac{1}{\beta}\mathbb{E}_{(x,y)\sim\mathcal{D}_R,y'\sim\mathcal{D}_{\text{IDK}}}\left[\log \sigma\left(-\beta\log \frac{p(y' \mid x; \theta)}{p(y \mid x; \theta)}\right)\right],\]

<p>where \(\sigma(\cdot)\) is the sigmoid function, \(\beta\) is a hyper-parameter.</p>

<p>The gradient of AP loss w.r.t. the model parameters is:</p>

\[\nabla_\theta\mathcal{L}_{\text{AP}}(\theta) = \mathbb{E}_{\mathcal{D}_R,\mathcal{D}_{\text{IDK}}}[W_\theta(x, y, y')\nabla_\theta (\log p(y' \mid x; \theta) - \log p(y \mid x; \theta))].\]

<p>The \(W_\theta(x, y, y') = 1/(1 + (\frac{p(y \mid x; \theta)}{p(y' \mid x; \theta)})^\beta)\) can be regarded as an adaptive gradient weight.</p>

<p>Given a question \(x\) in \(\mathcal{D}_R\), in the early stage of unlearning process, where \(p(y \mid x; \theta) \gg p(y' \mid x; \theta)\), we have \(W_\theta(x, y, y') \ll 1\).</p>

<p>As the unlearning proceeds, either a decrease in \(p(y \mid x; \theta)\) or an increase in \(p(y' \mid x; \theta)\) will result in a larger \(W_\theta(x, y, y')\), thereby providing stronger regularization. The gradient of AP loss consists of two terms in addition to the adaptive weight. The first term is equivalent to GA on the rejection template, which satisfies the first objective. The second term is equivalent to GD on the original answer, which satisfies the second objective.</p>

<p>My interpretation of the above equation is as follows:</p>

<ul>
  <li>Reminder that in gradient descent, we follow the negative gradient direction to update the model parameters, i.e., \(\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}\)</li>
  <li>In the first term, we follow the negative direction of \(\nabla_\theta (\log p(y' \mid x; \theta)\) - which aims to reduce the probability of \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_R)\)</li>
  <li>In the second term, we follow the direction of \(\nabla_\theta (\log p(y \mid x; \theta)\) - which aims to increase the probability of original answer \(P(y \mid \mathcal{X}_R)\)</li>
</ul>

<h2 id="muse---machine-unlearning-six-way-evaluation-for-language-models-iclr-2025">MUSE - Machine Unlearning Six-Way Evaluation for Language Models (ICLR 2025)</h2>

<p>Link to the paper: <a href="https://openreview.net/pdf?id=TArmA033BU">OpenReview</a></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-10-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Six-way evaluation of machine unlearning
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-18-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison with a previous benchmark TOFU.
</div>

<p>This paper present a new benchmark for evaluating the quality of machine unlearning, which considers six aspects:</p>

<ul>
  <li>No verbatim memorization: The model should not exactly replicate any details from the forget set.</li>
  <li>No knowledge memorization: The model should be incapable of responding to questions about the forget set.</li>
  <li>No privacy leakage: It should be impossible to detect that the model was ever trained on the forget set.</li>
  <li>Utility preservation: The model should maintain high performance on the tasks it was trained for except for the forget set.</li>
  <li>Scalability: The method should be able to handle large forget set sizes.</li>
  <li>Substantiality: The method should be able to handle a large number of forget queries - continuous unlearning setting.</li>
</ul>

<h3 id="metrics-for-each-aspect">Metrics for each aspect</h3>

<p><strong>Verbatim Memorization</strong>
To measure verbatim memorization, the paper uses the <a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE</a> score between the model’s continuation and the actual continuation in the forget set:</p>

\[\text{VerbMem}(f, \mathcal{D}) := \frac{1}{|\mathcal{D}_{\text{forget}}|} \sum_{x \in \mathcal{D}_{\text{forget}}} \text{ROUGE}(f(x_{[:l]}), x_{[l+1:]})\]

<p>ROUGE measures the overlap between the generated text (candidate) and one or more reference texts. It focuses on n-gram, sequence, and word-level similarity and is particularly recall-oriented, meaning it emphasizes how much of the reference text is captured by the generated text.</p>

\[\text{ROUGE} = \frac{\sum_{i=1}^n \text{Count}_{i}}{\sum_{i=1}^n \text{Count}_{i}}\]

<p>Where:</p>

<ul>
  <li>\(n\) is the number of reference texts</li>
  <li>\(\text{Count}_{i}\) is the number of n-grams in the generated text that are also in the \(i\)-th reference text</li>
</ul>

<p><strong>Knowledge Memorization</strong></p>

<p>To measure knowledge memorization, the paper uses the <a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE</a> score between the model’s continuation and the actual continuation in the forget set:</p>

\[\text{KnowMem}(f, \mathcal{D}_{\text{forget}}) := \frac{1}{|\mathcal{D}_{\text{forget}}|} \sum_{(q,a)\in\mathcal{D}_{\text{forget}}} \text{ROUGE}(f(q), a)\]

<p><strong>Privacy Leakage</strong></p>

<p>To measure privacy leakage, the paper uses (Min-K Prob) [6] a start-of-the-art MIA method to compute the standard AUC-ROC score of discriminating members \(\mathcal{D}_{\text{forget}}\) and non-members \(\mathcal{D}_{\text{holdout}}\) by using the logits of the last layer of the model.</p>

\[\text{PrivLeak} := \frac{\text{AUC}(f_{\text{unlearn}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}}) - \text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})}{\text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})}\]

<p>The good unlearning method should have \(\text{AUC}(f_{\text{unlearn}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}}) \approx \text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})\) which means the unlearned model is indistinguishable from the original model. In contrast, the bad unlearning method will get a large positive or negative difference.</p>

<p><strong>Min-K Prob [6]</strong></p>

<p>The Min-K Prob is based on the hypothesis that a non-member example is more likely to include a few outlier words with low probability, while a member example is less likely to include such words.</p>

<p>Consider a sequence of tokens in a sentence, denoted as \(x = x_1, x_2, ..., x_N\), the log-likelihood of a token, \(x_i\), given its preceding tokens is calculated as \(\log p(x_i \mid x_1, ..., x_{i-1})\). We then select the \(k\%\) of tokens from \(x\) with the minimum token probability to form a set, Min-K%(x), and compute the average log-likelihood of the tokens in this set:</p>

\[\text{MIN-K\% PROB}(x) = \frac{1}{E} \sum_{x_i \in \text{Min-K\%}(x)} \log p(x_i \midx_1, ..., x_{i-1}).\]

<p>where \(E\) is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining data simply by thresholding this MIN-K% PROB result.</p>

<p><strong>Why this new benchmark is important?</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-29-24.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    It seems that unlearning methods are not good at scalability - when increasing the forget size and substantiality - when number of forget queries increase.
</div>

<p>This benchmark provides another perspective - from model developer - who want to keep the model utility after unlearning. The current benchmark is more focused on the data owner’s expectation - forgetting the data - but not the model utility. With this benchmark, we can see that current unlearning methods are not good at these metrics:</p>

<ul>
  <li>“Unlearning significantly degrades model utility”</li>
  <li>“Unlearning methods scale poorly with forget set sizes”</li>
  <li>“Unlearning methods cannot sustainably accommodate sequential unlearning requests”</li>
</ul>

<p><strong>Membership Inference Attack</strong></p>

<p>A Membership Inference Attack (MIA) is a type of privacy attack where an adversary aims to determine whether a specific data sample was used in training a machine learning model. This poses a serious privacy risk, especially for models trained on sensitive datasets (e.g., medical or financial data).</p>

<h2 id="references">References</h2>

<p>[1] <a href="https://arxiv.org/abs/2403.03218">Li, Nathaniel, et al. “The wmdp benchmark: Measuring and reducing malicious use with unlearning.” arXiv preprint arXiv:2403.03218 (2024).</a></p>

<p>[2] <a href="https://arxiv.org/pdf/2407.11969">Andriushchenko, Maksym, and Nicolas Flammarion. “Does Refusal Training in LLMs Generalize to the Past Tense?.” arXiv preprint arXiv:2407.11969 (2024).</a></p>

<p>[3] <a href="https://arxiv.org/pdf/2410.11143">Wang, Yaxuan, et al. “LLM Unlearning via Loss Adjustment with Only Forget Data.” arXiv preprint arXiv:2410.11143 (2024).</a></p>

<p>[4] <a href="https://arxiv.org/pdf/2408.06223">Huu-Tien, Dang, et al. “On effects of steering latent representation for large language model unlearning.” arXiv preprint arXiv:2408.06223 (2024).</a></p>

<p>[5] <a href="https://arxiv.org/pdf/2410.08109">Yuan, Xiaojian, et al. “A Closer Look at Machine Unlearning for Large Language Models.” arXiv preprint arXiv:2410.08109 (2024).</a></p>

<p>[6] <a href="https://arxiv.org/pdf/2310.16789">Weijia Shi, et al. “Detecting Pretraining Data from Large Language Models.” arXiv preprint arXiv:2310.16789 (2023).</a></p>

<!-- mkdir -p assets/img/unlearnllms/ -->
<!-- mv _posts/2025-03-17-*.png assets/img/unlearnllms/ -->]]></content><author><name></name></author><category term="reading" /><category term="genai" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Foundation of Diffusion Models</title><link href="https://tuananhbui89.github.io/blog/2025/diffusion-foundation/" rel="alternate" type="text/html" title="Foundation of Diffusion Models" /><published>2025-03-08T00:00:00+11:00</published><updated>2025-03-08T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/diffusion-foundation</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/diffusion-foundation/"><![CDATA[<p>(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)</p>

<h2 id="what-are-diffusion-models">What are Diffusion Models?</h2>

<p>Diffusion models are a class of generative models that generate data by progressively denoising a sample from pure noise. They are inspired by <a href="https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics"><strong>non-equilibrium thermodynamics</strong></a> and are based on a forward and reverse diffusion process:</p>

<ol>
  <li>Forward Process (Diffusion Process): A data sample (e.g., an image) is gradually corrupted by adding Gaussian noise over multiple timesteps until it becomes nearly pure noise.</li>
  <li>Reverse Process (Denoising Process): A neural network learns to reverse this corruption by gradually removing noise step by step, reconstructing the original data distribution.</li>
</ol>

<figure style="text-align: center;">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/JnIkGtkO-Js?si=faOgaMvGtqTLcG1T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>Diffusion - How molecules actually move</figcaption>
</figure>

<p><strong>Analogy: Ink Dissolving in Water</strong>
Imagine dropping a blob of ink into a glass of water:</p>

<ul>
  <li>Forward process (Diffusion Process): Initially, the ink is concentrated in one place (structured data). Over time, it spreads out randomly, blending with the water (adding noise). Eventually, the entire glass becomes a uniformly colored mixture, losing its original structure (complete noise).</li>
  <li>Reverse process (Denoising Process): If we had a way to perfectly reverse time, we could watch the ink particles retrace their paths, reassembling into the original drop (generating the original data from noise). Diffusion models learn to perform this “reverse process” step by step using machine learning.</li>
</ul>

<blockquote>
  <p><strong>Non-Equilibrium Thermodynamics</strong></p>

  <p>Thermodynamics studies <strong>how energy moves and changes</strong> in a system. In equilibrium thermodynamics, systems are in balance—nothing is changing. Non-equilibrium thermodynamics, on the other hand, deals with <strong>systems that are constantly evolving, moving between states of disorder and order</strong>.</p>

  <p>In diffusion models, the forward process (adding noise to data) and the reverse process (removing noise) resemble a non-equilibrium thermodynamic system because they describe an evolving state that moves from order (structured data) to disorder (pure noise) and back to order (reconstructed data).</p>
</blockquote>

<blockquote>
  <p><strong>Brownian Motion</strong></p>

  <p>Brownian motion <strong>describes the random movement</strong> of tiny particles (like pollen grains in water) due to <strong>collisions with molecules</strong>. This randomness is similar to how noise is added in diffusion models.</p>
</blockquote>

<h3 id="advantages-of-diffusion-models">Advantages of Diffusion Models</h3>

<p>Diffusion models offer several key advantages over traditional generative models like GANs and VAEs:</p>

<ol>
  <li>
    <p><strong>High-Fidelity Samples</strong>: Unlike VAEs and GANs which generate samples in one step, diffusion models create samples gradually by denoising. This step-by-step process allows the model to first establish coarse image structure before refining fine details, resulting in higher quality outputs.</p>
  </li>
  <li>
    <p><strong>Training Stability</strong>: Diffusion models are easier to train compared to GANs as they use a single tractable likelihood loss. They don’t suffer from training instabilities like mode collapse that often plague GANs.</p>
  </li>
  <li>
    <p><strong>Sample Diversity</strong>: Similar to VAEs, diffusion models maximize likelihood which ensures coverage of all modes in the training dataset. This leads to more diverse outputs compared to GANs which can suffer from mode collapse.</p>
  </li>
  <li>
    <p><strong>Flexible Architecture</strong>: The multi-step denoising process enables additional functionalities like inpainting or image-to-image generation by manipulating the input noise, without requiring architectural changes.</p>
  </li>
  <li>
    <p><strong>Consistent Quality</strong>: The gradual denoising process is more robust and consistent compared to GANs where quality can vary significantly between samples.</p>
  </li>
</ol>

<p>The main trade-off is generation speed - diffusion models require multiple neural network passes to generate samples, making them slower than single-pass models like GANs and VAEs. However, various sampling optimization techniques have been developed to significantly reduce this computational overhead.</p>

<h3 id="disadvantages-of-diffusion-models">Disadvantages of Diffusion Models</h3>

<p>While diffusion models have significant advantages, they also come with some trade-offs:</p>

<ul>
  <li>Slow Sampling: The reverse process requires multiple denoising steps, making inference slower compared to GANs.</li>
  <li>Compute Intensive: Training requires large amounts of data and computational power.</li>
  <li>Memory Usage: They require storing multiple intermediate noise distributions, making them more memory-intensive.</li>
  <li>Complex Implementation: The multi-step nature of diffusion models makes them more complex to implement compared to single-step models.</li>
</ul>

<h2 id="mathematical-foundation">Mathematical Foundation</h2>

<h3 id="ode">ODE</h3>

<p>An Ordinary Differential Equation (ODE) is a mathematical equation that describes how a function changes over time. In simple terms, it tells us how a quantity evolves continuously based on its current state.</p>

\[\frac{dx}{dt} = f(x, t)\]

<p>where \(x(t)\) is the state of the system - the function we want to solve -and \(t\) is time. \(f(x, t)\) defines how \(x\) changes over time.</p>

<h3 id="sde">SDE</h3>

<p>A general form of an SDE is:</p>

\[dx = f(x, t) dt + g(x, t) dW_t\]

<p>where \(f(x, t)dt\) is the drift term (deterministic change), and \(g(x, t) dW_t\) is the diffusion term (stochastic change). \(dW_t\) is the increment of a Wiener process (Brownian motion).</p>

<p>This equation describes how a system evolves over time with both deterministic trends and random fluctuations.
In the case of Diffusion Models, the drift term is the shift of the mean of the distribution, and the diffusion term is the spread of the distribution.</p>

<p><strong>Forward Process (Adding Noise)</strong></p>

<p>The forward diffusion process transform a data sample \(x_0\) into pure noise \(x_T\) over time:</p>

\[dx = f(x, t)dt + g(t) dW_t\]

<p>where \(f(x,t)\) represents a deterministic shift of the mean of the distribution, and \(g(t)\) represents a stochastic spread of the distribution - injecting Gaussian noise.</p>

<p><strong>Reverse Process (Removing Noise)</strong></p>

<p>The Reverse-Time SDE (by Anderson 1982) is:</p>

\[dx = \left[ f(x,t) - g^2(t) \nabla_x \log p_t(x) \right] dt + g(t) d\tilde{W}_t\]

<p>where \(\nabla_x \log p_t(x)\) is the score function, which estimates the structure of data at time \(t\) - how likely different data points are at each step.
\(d\tilde{W}_t\) is another Wiener process but in the reverse direction.</p>

<p>In the reverse process of diffusion models, we train a neural network to approximate the score function \(\nabla_x \log p_t(x)\).</p>

<h3 id="elbo">ELBO</h3>

<p><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound"><strong>Evidence lower bound (ELBO)</strong></a> is a key concept in variational inference, which is used in VAEs to approximate the log-likelihood of the data.</p>

<p>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the marginal distribution of \(X\), and \(p_\theta(Z \mid X)\) is the conditional distribution of \(Z\) given \(X\). Then, for a sample \(x \sim p_{\text{data}}\), and any distribution \(q_\phi\), the ELBO is defined as</p>

\[L(\phi, \theta; x) := \mathbb{E}_{z\sim q_\phi(\cdot|x)} \left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>The ELBO can equivalently be written as</p>

\[\begin{aligned}
L(\phi, \theta; x) &amp;= \mathbb{E}_{z\sim q_\phi(\cdot|x)}[\ln p_\theta(x,z)] + H[q_\phi(z \mid x)] \\
&amp;= \ln p_\theta(x) - D_{KL}(q_\phi(z \mid x) || p_\theta(z \mid x)).
\end{aligned}\]

<p>In the first line, \(H[q_\phi(z \mid x)]\) is the entropy of \(q_\phi\), which relates the ELBO to the Helmholtz free energy. In the second line, \(\ln p_\theta(x)\) is called the evidence for \(x\), and \(D_{KL}(q_\phi(z \mid x) \mid\mid p_\theta(z \mid x))\) is the Kullback-Leibler divergence between \(q_\phi\) and \(p_\theta\). Since the Kullback-Leibler divergence is non-negative, \(L(\phi, \theta; x)\) forms a lower bound on the evidence (ELBO inequality)</p>

\[\ln p_\theta(x) \geq \mathbb{E}_{z\sim q_\phi(\cdot|x)}\left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>Deep-dive topics about VAE might including:</p>

<ul>
  <li>Reparameterization Trick: <a href="https://en.wikipedia.org/wiki/Reparameterization_trick">How to sample from a distribution in a differentiable way - Wiki</a></li>
  <li>The problem of KL divergence: <a href="https://andrewcharlesjones.github.io/journal/klqp.html">mode seeking vs mode covering</a> by Andy Jones</li>
  <li><a href="https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae">A nice property of VAEs: Disentanglement Representation Learning</a></li>
</ul>

<h3 id="tweedies-formula">Tweedie’s formula</h3>

<h2 id="variants">Variants</h2>

<h3 id="ddpm">DDPM</h3>

<p>Read more about DDPM in another blog post <a href="/blog/2023/diffusion-tutorial/">here</a></p>

<h3 id="ddim">DDIM</h3>

<p>Read more about DDIM in another blog post <a href="/blog/2023/diffusion-tutorial-p2/">here</a></p>

<p>Key concepts of DDIM:</p>

<ul>
  <li>DDIM utilizes a <strong>non-Markovian transition</strong> during inference, enables faster sampling.</li>
  <li>Can use the same training process as DDPM, e.g., we can use pretrained DDPM models to generate data.</li>
</ul>

<p>The sampling process of DDIM is as follows:</p>

\[x_{t-1} = \sqrt{\alpha_{t-1}} \left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right) + \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}(x_t) + \sigma_t\epsilon_t\]

<p>where the first term represents the “predicted $x_0$”, the second term is the “direction pointing to $x_t$”, and the last term is random noise.</p>

<p>By setting \(\sigma_t = 0\) for all \(t\), DDIM becomes a deterministic process given \(x_{t-1}\) and \(x_0\), except for \(t=1\). In other words, the intermediate steps \(x_{T-1}, x_{T-2}, \ldots, x_1\) are deterministic given starting noise \(x_T\).</p>

<h2 id="noise-scheduling">Noise scheduling</h2>

<p>Noise scheduling in diffusion models refers to how noise is gradually added to data in the forward process and how it is removed in the reverse process. The choice of noise schedule significantly impacts the model’s performance, sample quality, and training efficiency.</p>

<p>We follow the DDIM convention, where \(0 &lt; \bar{\alpha}_t &lt; 1, \beta_t = 1 - \bar{\alpha}_t\) and \(\alpha_t = \prod_{i=1}^{t} \bar{\alpha}_i\) is the cumulative noise level at time \(t\), and \(\beta_t\) is the noise level at time \(t\). With this convention, \(x_t = \sqrt(\alpha_t) x_0 + \sqrt(1-\alpha_t) \epsilon\), and \(\alpha_T \approx 0\) when \(t \rightarrow T\) while \(\alpha_0 \approx 1\) when \(t \rightarrow 0\).</p>

<p>Common principles of noise scheduling:</p>

<ul>
  <li>Add large amount of noise at \(t\) large while small amount of noise at \(t\) small. \(t=0\) means clean data, \(t=T\) means pure noise.</li>
  <li>The speed of change (acceleration, or \(\frac{d\beta_t}{dt}\)) should also has some proper speed (but I am not sure :D)</li>
</ul>

<p>Common noise schedules:</p>

<ul>
  <li><strong>Linear</strong>: \(\alpha_t = \frac{t}{T}\) or \(\beta_t = \beta_{\min} + (\beta_{\max} - \beta_{\min})\frac{t}{T}\). Issue: early timesteps do not add enough noise, and late timesteps can add too much noise.</li>
  <li><strong>Cosine</strong>: \(\beta_t = \beta_{\min} + 0.5 (\beta_{\max} - \beta_{\min}) ( 1 + \cos(\frac{t}{T} \pi))\). Intuition is that adding more gradually at the start and <strong>faster at the end</strong>.</li>
  <li><strong>Exponential</strong>: \(\beta_t = \beta_{\max} (\beta_{\min} / \beta_{\max})^{\frac{t}{T}}\)</li>
</ul>

<h2 id="guidanced-diffusion">Guidanced Diffusion</h2>

<p>Resources:</p>

<ul>
  <li>A great blog from Sander Dieleman: <a href="https://sander.ai/2022/05/26/guidance.html">Guidance: a cheat code for diffusion models</a> and <a href="https://sander.ai/2023/08/28/geometry.html">the geometry of diffusion guidance</a>.</li>
</ul>

<p><strong>Why Guidance?</strong></p>

<p>Guidance is a method to control the generation process so that the ouput is sample from a conditional distribution \(p(x \mid y)\), where \(y\) is a condition - such as a text prompt - rather than a generic \(p(x)\).</p>

<h3 id="classifier-guidance">Classifier Guidance</h3>

<p>In order to get the conditional score function \(\nabla_x \ln p(x \mid y)\), we can use Bayes rule to decompose the score function into an unconditional component and a conditional one:</p>

\[p(x \mid y) = \frac{p(y \mid x) p(x)}{p(y)}\]

\[\log p(x \mid y) = \log p(y \mid x) + \log p(x) - \log p(y)\]

\[\nabla_x \log p(x \mid y) = \nabla_x \log p(y \mid x) + \nabla_x \log p(x) - \nabla_x \log p(y)\]

<p>where \(\nabla_x \log p(x)\) is the score function of the unconditional model. \(\nabla_x \log p(y) = 0\) since \(p(y)\) is independent of \(x\).</p>

<p>The term \(\nabla_x \log p(y \mid x)\) means the direction pointing to \(y\) given \(x\).</p>

<ul>
  <li>In the begining of the inference process, i.e., large \(t\), when \(x_t\) still has a lot of noise, \(\nabla_x \log p(y \mid x)\) is close to \(0\), means that there is no clear information of \(y\).</li>
  <li>In the later stages, i.e., small \(t\), when \(x_t\) is less noisy and closer to \(x_0\), \(\nabla_x \log p(y \mid x)\) is larger, means that \(x_t\) has more information of \(y\), i.e., larger \(p(y \mid x)\).</li>
</ul>

<p><strong>How to obtain \(\nabla_x \log p(y \mid x)\)?</strong></p>

<p>\(p(y \mid x)\) means the probability of a condition \(y\) given \(x\).
In a simple case, where \(y\) is just a image class, like a <code class="language-plaintext highlighter-rouge">cat</code>, the probability \(p(y=\text{cat} \mid x)\) can be simply obtained from a pre-trained classifier.</p>

<p>However, in a more complex case, where \(y\) is a text prompt like a black cat with red eyes and blue fur, a pre-trained classifier is not expressive enough, i.e., it cannot distinguish between \(y_1\) <code class="language-plaintext highlighter-rouge">a black cat with red eyes and blue fur</code> vs \(y_2\) <code class="language-plaintext highlighter-rouge">a white cat with blue eyes and red fur</code> or mathematically \(p(y_1 \mid x) \neq p(y_2 \mid x)\).</p>

<p>In other words, the quality - diversity of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\). For example:</p>

<ul>
  <li>If \(p_\phi(y \mid x)\) is a binary classifier <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code>, then output image \(x \sim p_\theta(x \mid y)\) can be either <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code> only, even \(p_\theta(x)\) was trained from a massive dataset with many more classes rather than just two classes.</li>
  <li>If you want to generate an image \(x\) from a complex prompt \(y\), you need a powerful model like CLIP as the conditional model \(p_\phi(y \mid x)\).</li>
</ul>

<p>To balance between the specificity (i.e., high \($p(y \mid x\))) and diversity/quality (i.e., \(p(x \mid y) \approx p(x)\)), we use a guidance scale \(\gamma\) to control the trade-off between the two.</p>

\[\nabla_x \log p(x \mid y) =  \nabla_x \log p(x) + \gamma \nabla_x \log p(y \mid x)\]

<p>where \(\gamma\) is the guidance scale. A big \(\gamma\) means the model is less creative but more following the condition \(y\).</p>

<h3 id="classifier-free-guidance">Classifier-free Guidance</h3>

<p>The main limitation of the above approach is that the quality of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\).</p>

<p>If your model \(p(x)\) was trained on Image-Net dataset, but you want to generate an CT-scan medical image, then even with a powerful conditional model \(p(y \mid x)\), you will not get that.</p>

<p>The idea of classifier-free guidance cames from the <strong>Bayes Classifier</strong> - if you have trained a powerful unconditional generative model \(p(x)\) then you can use it as a classifier \(p(y \mid x)\) as follows:</p>

\[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\]

<h2 id="latent-diffusion">Latent Diffusion</h2>

<h2 id="conditional-diffusion">Conditional Diffusion</h2>

<h3 id="control-net">Control-Net</h3>

<h3 id="image-prompt">Image Prompt</h3>

<p>Beyond controlling the generation process using text prompt, there is a hot topic in the community to control using image information/layout/prompt - which has a huge potential in applications, e.g., image inpainting, image-to-image generation, etc. In the standard Stable Diffusion, the condition embedding \(c_t\) is just a text embedding \(c_t = E_t(y)\) where \(y\) is the text prompt and \(E_t\) is a pre-trained text encoder such as CLIP.
IP-Adapter [1] proposes to use an additional image encoder to extract the image embedding from a reference image \(c_i = E_i(x)\) and then project it into the original condition space.
The objective function for IP-Adapter is:</p>

\[\mathcal{L}_{IP} = \mathbb{E}_{z, c, \epsilon, t} \left[ \mid \mid \epsilon - \epsilon_\theta(z_t \mid c_i, c_t, t) \mid \mid_2^2 \right]\]

<p>The cross-attention layer is also modified from the one in Stable Diffusion to include the image embedding \(c_i\) as a condition.</p>

\[\text{Attention}(Q, K_i, K_t, V_i, V_t) = \lambda \text{softmax}\left(\frac{QK_i^T}{\sqrt{d}} + c_i\right)V_i + \text{softmax}\left(\frac{QK_t^T}{\sqrt{d}}\right)V_t\]

<p>where \(Q=z W_Q\), \(K_i = c_i W_K^i\), \(K_t = c_t W_K^t\), \(V_i = c_i W_V^i\), \(V_t = c_t W_V^t\), and \(W_Q\), \(W_K^i\), \(W_K^t\), \(W_V^i\), \(W_V^t\) are the weights of the linear layers.
The model becomes the original Stable Diffusion when \(\lambda = 0\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-03-20-07-02-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2308.06721">IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</a></li>
  <li>[2] <a href="https://openreview.net/pdf?id=PJqP0wyQek#page=1.93">MS-DIFFUSION: MULTI-SUBJECT ZERO-SHOT IMAGE PERSONALIZATION WITH LAYOUT GUIDANCE</a></li>
</ul>

<h2 id="diffusion-transformers">Diffusion Transformers</h2>

<h2 id="image-inpainting-with-diffusion-models">Image Inpainting with Diffusion Models</h2>

<h2 id="accelerating-diffusion-models">Accelerating Diffusion Models</h2>

<h3 id="diffusion-distillation">Diffusion Distillation</h3>

<h3 id="rectified-diffusion">Rectified Diffusion</h3>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2209.03003">Flow straight and fast: Learning to generate and transfer data with rectified flow</a></li>
  <li>[2] <a href="https://arxiv.org/pdf/2403.03206">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a></li>
</ul>

<p>Rectified Flows define the forward process as straight paths between the data distribution and a standard normal distribution [2], i.e.,</p>

\[z_t = (1 - t) x_0 + t \epsilon\]

<p>where \(\epsilon\) is a standard normal random variable and \(t\) is the time step in [0, 1].</p>

<!-- mkdir -p assets/img/diffusion-foundation -->
<!-- mv _posts/2025-03-20-*.png assets/img/diffusion-foundation/ -->]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><summary type="html"><![CDATA[(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)]]></summary></entry><entry><title type="html">FineStyle - Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)</title><link href="https://tuananhbui89.github.io/blog/2025/finestyle/" rel="alternate" type="text/html" title="FineStyle - Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)" /><published>2025-02-28T00:00:00+11:00</published><updated>2025-02-28T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/finestyle</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/finestyle/"><![CDATA[<h2 id="finestyle-fine-grained-controllable-style-personalization-for-text-to-image-models-neurips-2024">FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-27-17-20-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-27-17-20-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-27-17-20-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-27-17-20-07.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h3 id="overview">Overview</h3>

<ul>
  <li><a href="https://openreview.net/pdf?id=1SmXUGzrH8">FineStyle paper</a></li>
  <li><a href="https://github.com/SHI-Labs/FineStyle">FineStyle Github</a></li>
</ul>

<p>The FineStyle method proposed in the paper addresses the content leakage problem in few-shot or one-shot fine-tuning by introducing concept-oriented data scaling, which decomposes a single reference image into multiple sub-image-text pairs, each focusing on different fine-grained concepts. This approach improves the model’s ability to separate content and style while reducing leakage.</p>

<h4 id="content-leakage-problem">Content Leakage Problem</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-27-17-21-56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-27-17-21-56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-27-17-21-56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-27-17-21-56.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    An example of content leakage in the style transfer, i.e., the spindle leaves (from the reference image) in the background of “a sneaker”, even though it is not included in the text prompt
</div>

<p><strong>Content leakage</strong> in few-shot or one-shot fine-tuning happens because the model struggles to correctly associate visual concepts with corresponding text phrases when trained on only a few or a single image-text pair. The key reasons are:</p>

<ul>
  <li>
    <p><strong>Concept Entanglement</strong>: In large-scale training, models learn to decompose and associate individual visual concepts with text through extensive data diversity. However, with few-shot fine-tuning, the limited number of training examples makes it difficult to disentangle different visual elements, leading to unwanted content appearing in generated images.</p>
  </li>
  <li>
    <p><strong>Lack of Concept Alignment</strong>: When fine-tuning with only one or a few images, the model cannot effectively learn which parts of the image represent style versus specific objects. As a result, it may misinterpret background elements as essential style features, causing them to reappear in generated images even when not prompted.</p>
  </li>
  <li>
    <p><strong>Overfitting to Reference Image</strong>: The model tends to memorize the entire reference image, leading to a high risk of directly copying unwanted elements into generated images instead of generalizing style attributes properly.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-21-33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-21-33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-21-33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-21-33.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    An example of content alignment problem. Even with a contextual prompt (as shown in the image), it is still difficult to disentangle and map pairs of visual concepts and text phrases, i.e., "a woman" to visual "a woman" concept, "laptop" to visual "laptop" concept, etc.
</div>

<h4 id="limitation-of-existing-methods">Limitation of Existing Methods</h4>

<p>Some approaches, like StyleDrop, attempt to mitigate content leakage through iterative fine-tuning with synthetic images curated by human or automated feedback. However, this process is computationally expensive and does not fully solve the underlying issue of disentangling style from content.</p>

<h4 id="key-contributions">Key Contributions</h4>

<ul>
  <li><strong>Concept-Oriented Data Scaling</strong>: Decomposes a single reference image into multiple sub-image-text pairs, each focusing on different fine-grained concepts. This helps disentangle style attributes from content.</li>
  <li><strong>Parameter-Efficient Fine-Tuning via Cross-Attention Adapters</strong>: FineStyle modifies only the key and value kernels in cross-attention layers. This improves fine-grained style control and better aligns visual concepts with textual prompts while keeping the model lightweight.</li>
</ul>

<h3 id="finestyle-framework">FineStyle Framework</h3>

<h4 id="background">Background</h4>

<p><a href="https://arxiv.org/abs/2301.00704">Muse</a> is a masked generative transformer for text-to-image generation, which is the foundation model of FineStyle.
It consists of four main components:</p>

<ul>
  <li>A pre-trained text encoder \(T\): encodes a text prompt into textual token space \(\tau\)</li>
  <li>An image encoder \(E\): encodes an image from pixel space to a sequence of discrete visual tokens \(v \in \epsilon\)</li>
  <li>A decoder \(D\): decodes the visual tokens back to pixel space</li>
  <li>A generative transformer \(G\): generates an image from the visual tokens, \(G: \epsilon \times \tau \rightarrow \mathcal{L}\)</li>
</ul>

\[L = \mathbb{E}_{(x,t)\sim\mathcal{D},m\sim\mathcal{M}}[\text{CE}(\text{E}(x), \text{G}(\mathcal{M}(\text{E}(x), m), \text{T}(t)))]\]

<p>where \(\mathcal{D}\) is the training set, \(\mathcal{M}\) is a uniformly distributed mask smapling strategy with a mask ratio as a coefficient, and \(\text{CE}\) is the weighted cross-entropy loss.</p>

<p><strong>Sampling Strategy in Muse</strong></p>

<p>During image synthesis, the model uses iterative decoding to generate images given a text prompt and initial visual tokens. The synthesis process is defined as:</p>

\[\mathcal{I} = \text{D}(v_K), v_k = \text{S}(\text{G}(v_{k-1}, \text{T}(t)) + \lambda(\text{G}(v_{k-1}, \text{T}(t)) - \text{G}(v_{k-1}, \text{T}(n))))\]

<p>where:</p>

<ul>
  <li>\(k \in [1, K]\) is the sampling step</li>
  <li>\(t\) is the text prompt</li>
  <li>\(n\) is the null prompt</li>
  <li>\(\text{S}\) is a sampling strategy for visual tokens</li>
  <li>\(\lambda\) represents the coefficient for classifier-free guidance</li>
  <li>\(\text{D}\) maps the final visual tokens to pixel space</li>
</ul>

<p>The sampling strategy \(\text{S}\) is an <strong>iterative masked decoding strategy</strong>, where visutal tokens are progressively predicted and refined.
The model starts with an initial sequence of visual tokens, some of which are masked. It then iteratively predicts the masked tokens, using the previous predictions to inform the next step.</p>

<p><strong>StyleDrop</strong></p>

<p><a href="https://arxiv.org/abs/2306.00983">StyleDrop</a> is an extension of Muse that introduces an adapter to the generative transformer \(G\) to have a better style control.</p>

<h4 id="proposed-method">Proposed Method</h4>

<h4 id="concept-oriented-data-scaling">Concept-Oriented Data Scaling</h4>

<p>Idea (Borrowed from <a href="https://arxiv.org/abs/2306.00983">StyleDrop</a>): Decompose a text prompt into multiple sub-text prompts, each focusing on a different fine-grained concept. For example</p>

<ul>
  <li>“woman”, “laptop”, “a pot of plant with spindle leaves”, and “bookshelf” for foreground subjects</li>
  <li>“flat cartoon vector art”, “a light blue circle”, and “white background” for style and background attributes</li>
</ul>

<p>Then combine the two sets into a single text prompt, <code class="language-plaintext highlighter-rouge">{concept phrase} in {style phrase} style</code>.</p>

<p><strong>Training with Concept-oriented Masking</strong></p>

<ul>
  <li>cropping around the area of interest associated with the concept-style text phrase</li>
  <li>Using a pre-trained Muse model to create the segmentation mask (as shown in Fig. 3 a-c)</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-27-18-04-29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-27-18-04-29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-27-18-04-29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-27-18-04-29.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h4 id="classifier-free-guidance-for-style-control">Classifier-Free Guidance for Style Control</h4>

<p>FineStyle modifies Muse’s masked visual token prediction approach by introducing style and semantic guidance. The sampling strategy helps balance text fidelity and style adherence, mitigating content leakage.</p>

<p>Tunable parameters (\(λ_1, λ_2\)) allow users to control the strength of style influence versus prompt adherence, making the generation more flexible and controllable.</p>

<p>The sampling formula for visual tokens in FineStyle is</p>

\[v_k = \hat{G}(v_{k-1}, \text{T}(t)) + \lambda_1(\hat{G}(v_{k-1}, \text{T}(t)) - G(v_{k-1}, \text{T}(t))) + \lambda_2(\hat{G}(v_{k-1}, \text{T}(t)) - \hat{G}(v_{k-1}, \text{T}(n)))\]

<p>where:</p>

<ul>
  <li>\(\hat{G}\) is FineStyle adapted model</li>
  <li>\(G\) is the original Muse model</li>
  <li>\(t\) is the text prompt</li>
  <li>\(n\) is the null prompt for guidance</li>
  <li>\(\lambda_1\) is the coefficient for style guidance - Adjusts how strongly the generated image follows the reference style.</li>
  <li>\(\lambda_2\) is the coefficient for semantic guidance - Helps prevent content leakage by reinforcing adherence to the text prompt.</li>
</ul>

<h3 id="results">Results</h3>

<h4 id="qualitative-results">Qualitative Results</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-34-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-34-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-34-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-34-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Qualitative results of FineStyle. To me, the DreamStyler seems doing quite well, especially in the fourth and fifth rows, when the output images are aligned more with the negative prompt (i.e., "background not in gray" or "background not in white").
</div>

<h4 id="quantitative-results">Quantitative Results</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-41-48-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-41-48-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-41-48-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-41-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-15-42-41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-15-42-41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-15-42-41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-15-42-41.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Quantitative results of FineStyle. To me, the quantitative results are not comprehensive enough to draw a conclusion, especially the lack of comparison with other methods like DreamStyler.
</div>

<h2 id="references">References</h2>

<ul>
  <li>[1] <a href="https://arxiv.org/abs/2309.06933">DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models</a></li>
  <li>[2] <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models</a></li>
</ul>

<!-- mkdir -p assets/img/personalization -->
<!-- mv _posts/2025-02-26-*.png assets/img/personalization/ -->]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><summary type="html"><![CDATA[FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models (NeurIPS 2024)]]></summary></entry><entry><title type="html">DreamStyler - Paint by Style Inversion with Text-to-Image Diffusion Models (AAAI-2024)</title><link href="https://tuananhbui89.github.io/blog/2025/dreamstyler/" rel="alternate" type="text/html" title="DreamStyler - Paint by Style Inversion with Text-to-Image Diffusion Models (AAAI-2024)" /><published>2025-02-27T00:00:00+11:00</published><updated>2025-02-27T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/dreamstyler</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/dreamstyler/"><![CDATA[<h3 id="overview">Overview</h3>

<ul>
  <li><a href="https://nmhkahn.github.io/dreamstyler/">DreamStyler Project Page</a></li>
  <li><a href="https://github.com/webtoon/dreamstyler">DreamStyler Github</a></li>
</ul>

<p>DreamStyler introduces a novel approach to style transfer by leveraging a multi-stage textual embedding combined with a context-aware text prompt. The method aims to enhance the generation of images in a specific artistic style using text-to-image diffusion models.</p>

<h3 id="key-contributions">Key Contributions</h3>

<h4 id="problem-setting">Problem Setting</h4>

<p>Given a set of style images with an implicit personal style (e.g., Van Gogh’s style), the goal is to fine-tune a foundation model to mimic the style \(S^*\) such that it can generate images in that style when provided with a text prompt (e.g., “A painting of a bear in \(S^*\) style”). This is traditionally done using personalized methods like DreamBooth and Textual Inversion.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-41-45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-41-45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-41-45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-41-45.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Examples of DreamStyler application. (a+b+c) Style-Guided Text-to-Image generation where input is a text prompt. (d+e+f) Style-Guided Image-to-Image generation where input is an image.
</div>

<h4 id="limitations-of-existing-methods">Limitations of Existing Methods</h4>

<p>Current methods face several challenges:</p>

<ul>
  <li>
    <p><strong>Dynamic Style Representation</strong>: Diffusion models require different capacities at various denoising steps, making it difficult for a single embedding vector to capture an entire style.</p>
  </li>
  <li>
    <p><strong>Local to Global Features</strong>: The denoising process moves from coarse to fine synthesis, meaning both global artistic elements (color tone) and fine-grained details (texture) need to be represented effectively.</p>
  </li>
  <li>
    <p><strong>Style-Content Separation</strong>: Without a structured way to distinguish style from content, generated images may unintentionally incorporate unwanted elements from reference images.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-12-11-09.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The disentanglement property of Diffusion models [2]. Example that requires different capacities at various diffusion steps.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-49-03-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-49-03-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-49-03-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-49-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Example of content leakage/style overfitting in the style transfer. If tranferring without context-aware text prompt (setting (a)), the model is overfitting to the reference image (copying people from the reference image to the generated image).
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-15-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-15-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-15-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-15-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-07-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-07-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-07-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-07-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Example of the challenge of Style-Content Separation, especially with fixed neutral textual templates (as in the right image). Specifically, given a reference input as in the left image and set of neutral templates, how the model knows S^* is represented for a style - color tone/texture or an object in the input image? The problem is even more severe on Few-shot/One-shot settings.
</div>

<h3 id="proposed-solution">Proposed Solution</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-43-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h4 id="context-aware-text-prompt">Context-Aware Text Prompt</h4>

<p>A style is often intertwined with content in a reference painting, making it difficult to extract only the stylistic elements. To address this, DreamStyler utilizes <strong>BLIP-2 and Human-in-the-loop methods to create a context-aware text prompt</strong> that explicitly describes non-style components (e.g., objects, composition, background).</p>

<p><strong>Benefits</strong></p>

<ul>
  <li><strong>Improved Style-Content Disentanglement</strong>: By explicitly describing the non-style elements of the reference image, the model can better focus on learning the stylistic features, leading to outputs that are more faithful to the user’s intent.</li>
  <li><strong>Reduced Unwanted Elements</strong>: The inclusion of context descriptions helps to prevent the model from incorporating irrelevant objects, compositions, or backgrounds from the reference image into the generated images.</li>
</ul>

<p><strong>Implementation</strong></p>

<p>The context-aware text prompt is manually assigned as an input argument:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">context_prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">A painting of pencil, pears and apples on a cloth, in the style of {}</span><span class="sh">"</span><span class="p">.</span>
<span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">template</span> <span class="k">if</span> <span class="n">context_prompt</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">context_prompt</span>
</code></pre></div></div>

<h4 id="multi-stage-textual-embedding">Multi-Stage Textual Embedding</h4>

<p><strong>Motivation</strong></p>

<p>Traditional Textual Inversion (TI) relies on a <strong>single embedding vector</strong>, which may not effectively represent complex artistic styles across the entire diffusion process. Prior research shows that diffusion models require <strong>different representational capacities at various timesteps</strong>.</p>

<p>As demonstrated in other works, there is a dynamic property throughout the diffusion process, which requires different capacities at various diffusion steps [2]. Therefore, using a single embedding vector for all diffusion steps is not ideal, especially for representing artistic styles that involve both global elements (like color tone) and fine-grained details (like texture).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-12-11-09-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-12-11-09.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The disentanglement property of Diffusion models [2].
</div>

<p><strong>Proposed Approach</strong></p>

<p>DreamStyler introduces a multi-stage textual embedding by utilizing multiple embedding vectors/tokens, each corresponding to a specific stage of the diffusion process.
More specifically, the entire diffusion process is broken down into \(T\) distinct stages, and a set of \(T\) style tokens \(S_1, S_2, \cdots, S_T\) are used to represent the style at each stage.</p>

<p><strong>Benefits</strong></p>

<ul>
  <li><strong>Enhanced Expressiveness</strong>: Captures both global elements (e.g., color tone) and fine details (e.g., brushstrokes, textures).</li>
  <li><strong>Better Adaptability</strong>: Adjusts to the changing nature of style representation during the denoising process.</li>
</ul>

<p><strong>Implementation of Multi-Stage Textual Embedding</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-14-37-20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-14-37-20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-14-37-20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-14-37-20.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-26-14-38-50-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-26-14-38-50-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-26-14-38-50-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-26-14-38-50.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Decomposition of the style embedding into multiple stages.
</div>

<h4 id="style-and-context-guidance-with-classifier-free-guidance">Style and Context Guidance with Classifier-Free Guidance</h4>

<p><strong>Classifier-Free Guidance</strong></p>

<p><a href="https://arxiv.org/abs/2207.12598">Classifier-Free Guidance</a> is a popular technique in the diffusion model community to improve the quality of generated images. It is a simple yet effective method to improve the diversity of generated images.</p>

\[\hat{\epsilon}(v) = \epsilon(\emptyset) + \lambda(\epsilon(v) - \epsilon(\emptyset))\]

<p>where:</p>

<ul>
  <li>\(\epsilon\) is the denoising function</li>
  <li>\(\lambda\) is the coefficient for guidance</li>
  <li>\(\emptyset\) is the null prompt</li>
  <li>\(v\) is the text prompt</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-27-38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-27-38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-27-38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-27-38.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of Classifier-Free Guidance. Image credit: <a href="https://www.researchgate.net/publication/379277262_MAM-E_Mammographic_Synthetic_Image_Generation_with_Diffusion_Models">MAM-E: Mammographic Synthetic Image Generation with Diffusion Models</a>.
</div>

<p><strong>Style and Context Guidance</strong></p>

<p>DreamStyler introduces a style and context guidance mechanism by incorporating the style and context prompts into the Classifier-Free Guidance ()</p>

\[\hat{\epsilon}(v) = \epsilon(\emptyset) + \lambda_{s}\left[ \epsilon(v) - \epsilon(v_c) \right] + \lambda_{c}\left[ \epsilon(v_c) - \epsilon(\emptyset) \right] + \lambda_{c}\left[ \epsilon(v) - \epsilon(v_s) \right] + \lambda_{s}\left[ \epsilon(v_s) - \epsilon(\emptyset) \right]\]

<p>where:</p>

<ul>
  <li>\(v_c\) is the context prompt and \(v_s\) is the style prompt that are decomposed from the text prompt \(v\) as \(v = v_c + v_s\).</li>
  <li>\(\lambda_c\) is the coefficient for context guidance. Increasing \(\lambda_c\) encourages the model to generate images that are more faithful to the context prompt.</li>
  <li>\(\lambda_s\) is the coefficient for style guidance. Increasing \(\lambda_s\) encourages the model to generate images that are more aligned with the style prompt.</li>
</ul>

<h4 id="utilizing-controlnet-for-style-preservation">Utilizing ControlNet for Style-Preservation</h4>

<p>DreamStyler also utilizes ControlNet to maintain the original content’s structure of the reference image in the Image-to-Image Style Transfer setting.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-13-43-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-13-43-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration the use of ControlNet for style-preservation (b - Sampling process). The content image is used to generate a encoding vector, which is used to guide the generation process.
</div>

<h3 id="results">Results</h3>

<h4 id="qualitative-results">Qualitative Results</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-17-17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-17-17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-17-17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-17-17.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-18-53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-18-53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-18-53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-18-53.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<h4 id="quantitative-results">Quantitative Results</h4>

<p>The authors utilize three scores to evaluate the performance of the proposed method:</p>

<ul>
  <li><strong>Text Score and Image Score</strong>: measure the alignment with a given text prompt/reference image with the generated image.</li>
  <li><strong>Style Score</strong>: Assesses the style consistency by calculating the similarity of Gram features between the style and generated images.</li>
  <li><strong>User Score</strong>: Human evaluation score.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-46-58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-46-58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-46-58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-46-58.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personalization/2025-02-28-14-51-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personalization/2025-02-28-14-51-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personalization/2025-02-28-14-51-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/personalization/2025-02-28-14-51-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The Gram-based style score.
</div>]]></content><author><name></name></author><category term="reading" /><category term="diffusion" /><category term="genai" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Kolmogorov-Arnold Network (KAN)</title><link href="https://tuananhbui89.github.io/blog/2025/KAN/" rel="alternate" type="text/html" title="Kolmogorov-Arnold Network (KAN)" /><published>2025-02-21T00:00:00+11:00</published><updated>2025-02-21T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/KAN</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/KAN/"><![CDATA[<p>Resources:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/abs/2404.19756">KAN Paper</a></li>
  <li>[2] <a href="https://github.com/KindXiaoming/pykan">KAN Github</a></li>
  <li>[3] <a href="https://github.com/mintisan/awesome-kan">Awesome KAN(Kolmogorov-Arnold Network)</a></li>
  <li>[4] <a href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/">Philosophical thoughts on Kolmogorov-Arnold Networks by Ziming Liu</a></li>
  <li>[5] <a href="https://www.digitalocean.com/community/tutorials/kolmogorov-arnold-networks-kan-revolutionizing-deep-learning">Kolmogorov-Arnold Networks (KAN) Promising Alternative to Multi-Layer Perceptron? by DigitalOcean</a></li>
  <li>[6] <a href="https://arxiv.org/pdf/2407.16674">KAN or MLP: A Fairer Comparison</a></li>
</ul>

<h2 id="mlp-vs-kan">MLP vs KAN</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-21-16-23-16.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison between MLP and KAN.
</div>

<p><strong>Limitations of MLP</strong>:</p>

<ul>
  <li><strong>Interpretability</strong>: MLPs are often considered “black boxes” due to their complex internal workings, making it difficult to understand how they arrive at their predictions.</li>
  <li><strong>Curse of Dimensionality</strong>: MLPs can struggle with high-dimensional data, as the number of parameters required to capture complex relationships grows exponentially with the input dimension.</li>
  <li><strong>Local Optimization</strong>: MLPs rely on gradient-based optimization algorithms, which can get stuck in local minima, potentially leading to suboptimal solutions.</li>
  <li><strong>Catastrophic Forgetting</strong>: MLPs can be prone to catastrophic forgetting, where learning new information can overwrite previously learned knowledge, hindering their ability to perform continual learning.</li>
</ul>

<p><strong>Advantages of KAN over MLP</strong>:</p>

<ul>
  <li><strong>Interpretability</strong>: KANs are more interpretable than MLPs due to their structure and the use of learnable activation functions. The absence of linear weight matrices and the explicit representation of univariate functions make it easier to understand how KANs arrive at their predictions.</li>
  <li><strong>Neural Scaling Laws</strong>: KANs exhibit faster neural scaling laws than MLPs, meaning that their performance improves more rapidly with increasing model size. This faster scaling can lead to significant gains in accuracy by simply scaling up the model.</li>
  <li><strong>Continual Learning</strong>: KANs can naturally perform continual learning without catastrophic forgetting, unlike MLPs. This ability stems from the locality of spline basis functions, which allows KANs to update knowledge in specific regions without affecting previously learned information.</li>
</ul>

<p><strong>Limitations of KAN</strong>:</p>

<ul>
  <li><strong>Computational Efficiency</strong>: KANs can be computationally more expensive to train than MLPs due to the complexity of learning and evaluating spline functions. The current implementation of this spline function can be found <a href="https://github.com/KindXiaoming/pykan/blob/ecde4ec3274d3bef1ad737479cf126aed38ab530/kan/spline.py#L4">here</a>, which requires recursive computation of a higher-order spline from lower-order splines. This process does not leverage the parallelization of modern GPUs.</li>
  <li><strong>Theoretical Limitations</strong>: The Kolmogorov-Arnold Representation Theorem (KAT) primarily applies to single layer KANs, and therefore the multi-layer KANs are not guaranteed to be able to represent any continuous function. For example, the input of the activation function should be bounded, which is not trivial for multi-layer KANs.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-07-46-07.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Should we use KAN or MLP? Image from [1].
</div>

<h3 id="kan-or-mlp-a-fairer-comparison">KAN or MLP: A Fairer Comparison</h3>

<p>Paper [6] provides a <strong>fairer</strong> comparison between KAN and MLP by considering the same number of parameters and FLOPs to make sure that the computational complexity is the same.
The tasks for comparison are also more comprehensive, including tasks in ML, CV, NLP and symbolic formula representation.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-09-20-43.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Other comparison between KAN and MLP from a fairer perspective/setting.
</div>

<p>The key findings are follows, which somewhat contradict to the observation in the original KAN paper [1].</p>

<ul>
  <li><strong>Symbolic Formula Representation</strong>: KANs outperform MLPs when approximating symbolic formulas.</li>
  <li><strong>Other Tasks</strong>: MLPs generally outperform KANs on other tasks, including machine learning, computer vision, natural language processing, and audio processing.</li>
  <li><strong>Impact of B-spline Activation</strong>: KANs’ advantage in symbolic formula representation comes from their use of B-spline activation functions.  When MLPs use B-spline activation functions, their performance on symbolic formula representation matches or exceeds that of KANs.  However, B-spline activation functions do not significantly improve MLPs’ performance on other tasks.</li>
  <li><strong>Continual Learning</strong>: KANs do not outperform MLPs in continual learning tasks. In a standard class-incremental continual learning setting, KANs forget old tasks more quickly than MLPs.</li>
</ul>

<h2 id="kan">KAN</h2>

<!-- The **Universal Approximation Theorem** (UAT) states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary accuracy.
This is the foundation theorem of the Multi-Layer Perceptron (MLP). More specifically, the multivariate continuous function $$f: [0,1]^n \rightarrow \mathbb{R}$$ can be represented as follows in MLP:

$$
f(x) = f(x_1, x_2, \cdots, x_n) = \sum_{i=1}^{m} \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j + b_i \right)
$$

where $$\sigma$$ is the non-linear activation function, $$w_{i,j}$$ is the weight, and $$b_i$$ is the bias. -->

<p>Before we dive into the KAN, let’s first understand the two definitions <strong>“edge”</strong> and <strong>“node”</strong> in MLP and KAN.
Given a MLP with \(n\) input nodes and \(m\) output nodes, the MLP can be represented as a directed acyclic graph (DAG) as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-06-46-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    MLP layer
</div>

<p>Mathematically, the node \(y_i\) of the output (hidden) layer can be represented as \(y_i = \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j\right)\) where \(x_j\) is the input node, \(w_{i,j}\) is the weight. We ignore the bias term for simplicity.
The connection between the input <strong>nodes</strong> \(x_j\) and the output <strong>node</strong> \(y_i\) is called an <strong>edge</strong>, which is scaled by the <strong>learnable weight</strong> \(w_{i,j}\).
After applying the sum operation over all the edges, the output node \(y_i = \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j \right)\) is obtained by applying the non-linear activation function \(\sigma\) on the weighted sum.
Note that the activation function \(\sigma\) is pointwise applied and not learnable.</p>

<p>For the Kolmogorov-Arnold Network (KAN), it is based on the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov-Arnold Representation Theorem (KAT)</a>.
KAT states that any continuous function can be represented as a sum of a trigonometric polynomial and a spline function.
More specifically, the multivariate continuous function \(f: [0,1]^n \rightarrow \mathbb{R}\) can be represented as:</p>

\[f(x) = f(x_1, x_2, \cdots, x_n) = \sum_{q=0}^{2n+1} \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right)\]

<p>where \(\phi_{q,}:[0,1] \rightarrow \mathbb{R}\) are the learnable activation functions over <strong>edges</strong>, and the \(\Phi_q\) is the learnable activation function over output <strong>nodes</strong>.</p>

<p>In KAN, the <strong>edge</strong> connection between the input <strong>nodes</strong> \(x_p\) and the output <strong>node</strong> \(y_q\) is applied by the <strong>learnable activation function</strong> \(\phi_{q,p}\).
After applying the sum operation over all the edges, the output node \(y_q = \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right)\) is obtained by applying another learnable activation function \(\Phi_q\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-07-00-27.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    KAN layer
</div>

<p>So compared to MLP, while the process from input nodes to output nodes is quite similar (one output node connected to all the input nodes), and the activation function on edges <strong>\(\phi_{q,p}\)</strong> is also parameterized similar to \(w_{i,j}\) in MLP, the main difference lies in the activation function on output nodes <strong>\(\Phi_q\)</strong> that is learnable in KAN.</p>

<h3 id="implementation-of-kan">Implementation of KAN</h3>

<h4 id="residual-activation-function">Residual Activation Function</h4>

<p>Beside the spline function, the activation function also includes a basis function \(b(x)\) which gets the signal directly from the input nodes (without going through any weight matrix).</p>

\[\phi(x) = w_b b(x) + w_s \text{spline}(x)\]

<p>where \(w_b\) and \(w_s\) are the learnable weights. the basis function \(b(x) = \text{silu}(x) = x / (1 + e^{-x})\).</p>

<p>The most complicated part is the spline function, which is parameterized as a linear combination of <strong>B-splines</strong> such as:</p>

\[\text{spline}(x) = \sum_{i=1} c_i B_i(x)\]

<p>where \(B_i(x)\) is the \(i\)-th B-spline and \(c_i\) is the learnablecoefficient.</p>

<h4 id="b-spline">B-spline</h4>

<p>B-splines are essentially curves made up of polynomial segments, each with a specified level of smoothness. Picture each segment as a small curve, where multiple control points influence the shape. Unlike simpler spline curves, which rely on only two control points per segment, B-splines use more, leading to smoother and more adaptable curves.</p>

<p>The magic of B-splines lies in their local impact. Adjusting one control point affects only the nearby section of the curve, leaving the rest undisturbed. This property offers remarkable advantages, especially in maintaining smoothness and facilitating differentiability, which is crucial for effective backpropagation during training (From [4]).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    B-spline. Image from DigitalOcean [4].
</div>

<p>Mathematically, B-splines can be constructed by means of the Cox-de Boor recursion formula (<a href="https://en.wikipedia.org/wiki/B-spline#Definition">Wikipedia</a>), starting with the B-spline basis function of order 0. We start with the B-splines of degree \(p = 0\), i.e. piecewise constant polynomials:</p>

\[B_{i,0}(t) := \begin{cases}
1 &amp; \text{if } t_i \leq t &lt; t_{i+1}, \\
0 &amp; \text{otherwise.}
\end{cases}\]

<p>The higher \((p + 1)\)-degree B-splines are defined by recursion:</p>

\[B_{i,p}(t) := \frac{t - t_i}{t_{i+p} - t_i} B_{i,p-1}(t) + \frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} B_{i+1,p-1}(t).\]

<p>The implementation of the B-spline can be found <a href="https://github.com/KindXiaoming/pykan/blob/ecde4ec3274d3bef1ad737479cf126aed38ab530/kan/spline.py#L4">here</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-21-17-05-47.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Implementation of B-spline.
</div>

<p><strong>Computational Expensiveness</strong>: Because of the recursive computation of the B-spline, the computational complexity is much higher than that of MLP.</p>

<p><strong>Grid Extension</strong></p>

<p>The grid extension in KAN is the process of refining the spline function by adding more knots, so that the spline function can have a higher resolution, fit the data better. 
It can be done by using higher-order B-splines, which is calculated by the lower-order B-splines (therefore, it is called extension).</p>

<h3 id="philosophical-thoughts-on-kan-by-kans-author">Philosophical thoughts on KAN by Kan’s author</h3>

<p>I found the philosophical thoughts on KAN by the author <a href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/">here</a> very interesting and helpful to understand the KAN and its difference with MLP.
I just quote the part that I think is most relevant to the KAN here.</p>

<blockquote>
  <p><strong>Reductionism vs. Holism</strong> While MLPs are more aligned with holism, KANs are more aligned with reductionism. The design principle of MLPs is “more is different”. <strong>In an MLP, each neuron is simple</strong> because it has fixed activation functions. However, <strong>what matters is the complicated connection patterns among neurons</strong>. The magical power of MLPs performing a task is an emergent behavior which is attributed to collective contribution from all neurons. By contrast, <strong>in a KAN, each activation function is complicated</strong> because it has learnable functions. By sparsification and pruning, we hope the computation graph to be simple. In summary, MLPs have simple ‘atoms’ but complicated ways to combine these atoms; KANs have complicated (diverse) ‘atoms’ but simple ways to combine these atoms. In this sense, MLPs’ expressive power comes from the complicated connection patterns (fully-connected structure), which give rise to emergent bahavior (holism). In contrast, KANs’ expressive power comes from the complexity of fundamental units (learnable activation functions), but the way to decompose whole network to units is simple (reductionsim)</p>
</blockquote>

<h3 id="attention-mechanism">Attention Mechanism</h3>

<p>Because of the spline function \(\text{spline}(x)\), which is a linear combination of B-splines with different level of smoothness/resolution of the input \(x\), each resolution is weighted by the learnable coefficient \(c_i\), 
this mechanism can be regarded as a <strong>soft self attention</strong> mechanism, where the output attends to different parts of the input with different resolutions.</p>

<!-- img_path: /assets/img/KAN/ -->
<!-- mkdir -p ../assets/img/KAN/ -->
<!-- mv 2025-02-21-*.png ../assets/img/KAN/ -->]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Resources:]]></summary></entry><entry><title type="html">DeepSeek-R1</title><link href="https://tuananhbui89.github.io/blog/2025/deepseek/" rel="alternate" type="text/html" title="DeepSeek-R1" /><published>2025-01-28T00:00:00+11:00</published><updated>2025-01-28T00:00:00+11:00</updated><id>https://tuananhbui89.github.io/blog/2025/deepseek</id><content type="html" xml:base="https://tuananhbui89.github.io/blog/2025/deepseek/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>DeepSeek-R1 is an open-source model that is developed by a Chinese quant company called <a href="https://www.deepseek.com/">DeepSeek AI</a>. This model has taken the AI community by storm as it is the first open-source solution capable of achieving performance comparable to premium OpenAI models (e.g., OpenAI-o1/o3) with <a href="https://www.forbes.com.au/news/investing/what-is-deepseek-new-chinese-ai-startup-rivals-openai/">a fraction of the training and inference costs</a>. It is also entirely free to use under an MIT license.</p>

<p><strong>Panic in Silicon Valley because of DeepSeek</strong></p>

<p>It is not a joke that Silicon Valley but not the whole tech industry is panicking about DeepSeek. Forbes even has a <strong>Panic Live update</strong> on <a href="https://www.forbes.com/sites/dereksaul/2025/01/27/deepseek-panic-live-updates-trump-calls-ai-development-positive-despite-tech-stock-plunge/">their website updating the latest loss</a> of the stock market.
Nvidia’s stock price dropped by 17%, a drop of <code class="language-plaintext highlighter-rouge">$589</code> billion in market cap - the biggest single-day loss in history (hint NVIDIA doesn’t like Test-time Computing).
And CEO of ScaleAI, <a href="https://www.tipranks.com/news/musk-and-scale-ais-ceo-suggest-that-deepseek-has-more-nvidia-chips-than-expected">a company that provides AI training data for LLMs models, who also doesn’t like the cost and data efficiency of DeepSeek, guessed that DeepSeek might has more GPU resources than they announced</a>.
President Trump called it a “wake-up call” for U.S. industries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/panic-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/panic-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/panic-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/panic.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/panic2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/panic2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/panic2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/panic2.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Models Released</strong></p>

<ul>
  <li><strong>DeepSeek-R1-Zero</strong>: This model, trained through large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally develops numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability and language mixing.</li>
  <li><strong>DeepSeek-R1</strong>: Incorporating multi-stage training and cold-start data before RL, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.</li>
  <li><strong>Distill-R1</strong>: A series of six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. Notably, the distilled 14B model outperforms state-of-the-art open-source models like Qwen-32B-Preview by a large margin. The 32B and 70B models set new records on reasoning benchmarks among dense models.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark?raw=true-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark?raw=true-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark?raw=true-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg?raw=true" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Benchmark of DeepSeek-R1. Image from [1].
</div>

<p><strong>Research Questions</strong></p>

<ul>
  <li>Can language model reasoning capabilities be improved purely through reinforcement learning without supervised fine-tuning?</li>
</ul>

<p><strong>Key Story Line</strong></p>

<ul>
  <li>
    <p>Base Model: The team uses DeepSeek-V3-Base and employs Group Relative Policy Optimization (GRPO) as the RL framework to enhance reasoning performance.</p>
  </li>
  <li>
    <p>Performance Gains: DeepSeek-R1-Zero achieves impressive reasoning benchmarks. For instance, the pass@1 score on AIME 2024 improves from 15.6% to 71.0%. With majority voting, the score further rises to 86.7%, matching OpenAI-o1-0912’s performance.</p>
  </li>
  <li>
    <p>Challenges and Solutions: While RL-only training produces strong reasoning capabilities, it introduces issues such as poor readability and language mixing. DeepSeek-R1 addresses these by incorporating cold-start data and multi-stage training pipelines.</p>
  </li>
  <li>
    <p>Pipeline Highlights:</p>

    <ul>
      <li>Collection of cold-start data for initial fine-tuning.</li>
      <li>Reasoning-oriented RL to refine reasoning skills.</li>
      <li>SFT using new datasets generated through rejection sampling and DeepSeek-V3 outputs.</li>
      <li>Final RL phase to align the model with human preferences across all scenarios.</li>
    </ul>
  </li>
</ul>

<p><strong>References</strong></p>

<ul>
  <li>Paper: <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
  <li>Code: <a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1</a></li>
  <li>All papers from DeepSeek-AI from <a href="https://huggingface.co/collections/Presidentlin/deepseek-papers-674c536aa6acddd9bc98c2ac">Huggingface</a> and <a href="https://github.com/orgs/deepseek-ai/repositories?type=all">DeepSeek’s Github</a></li>
  <li>Understanding Multi-Head Latent Attention from <a href="https://planetbanatt.net/articles/mla.html">Eryk Banatt</a></li>
</ul>

<h2 id="approach">Approach</h2>

<h3 id="overview">Overview</h3>

<ul>
  <li><strong>DeepSeek-R1-Zero</strong>: Applies RL directly to the base model without supervised fine-tuning. GRPO serves as the RL framework.</li>
  <li><strong>DeepSeek-R1</strong>: Employs a multi-stage process combining RL and SFT to address readability and language issues while enhancing performance.</li>
  <li><strong>Distill-R1</strong>: Features six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama, setting new records in reasoning benchmarks.</li>
</ul>

<h3 id="deepseek-r1-zero-rl-on-the-base-model">DeepSeek-R1-Zero: RL on the Base Model</h3>

<h4 id="group-relative-policy-optimization">Group Relative Policy Optimization</h4>

<p><strong>How does GRPO differ from PPO?</strong></p>

<p>Traiditional RL methods like PPO requires a pre-trained critic model to evaluate the performance of the policy model. However, to train a critic model, we need a pair of winning and losing outputs given a same input, normally from a human evaluator. These pairs are expensive to obtain, hard to scale. Moreover, if the task is complex, the human evaluator may be subjective, biased, or nuanced.</p>

<p>GRPO, on the other hand, removes the need for a pre-trained critic model by comparing responses within a group, therefore overcoming the above limitations of PPO.</p>

<p><strong>GRPO Objective Function</strong>
Specifically, for each question \(q\), GRPO samples a group of outputs \(\{o_1, o_2, \cdots, o_G\}\) from the old policy model \(\pi_{\theta_\text{old}}\).
It then optimizes the policy model \(\pi_{\theta}\) by maximizing the following objective function:</p>

\[\mathcal{J}_\text{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_\text{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \left(\min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_\text{old}}(o_i|q)}A_i, \text{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_\text{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right)A_i\right) - \beta\mathbb{D}_\text{KL}(\pi_\theta||\pi_\text{ref})\right)\right]\]

<p>where the KL divergence term \(\mathbb{D}_\text{KL}\) is defined as:</p>

\[\mathbb{D}_\text{KL}(\pi_\theta||\pi_\text{ref}) = \frac{\pi_\text{ref}(o_i|q)}{\pi_\theta(o_i|q)} - \log\frac{\pi_\text{ref}(o_i|q)}{\pi_\theta(o_i|q)} - 1\]

<p>and \(A_i\) is the advantage function defined as:</p>

\[A_i = \frac{r_i - \text{mean}({r_1, r_2, \cdots, r_G})}{ \text{std}({r_1, r_2, \cdots, r_G})}\]

<p>where \(r_i\) is the reward of the output \(o_i\) and \(\text{mean}\) and \(\text{std}\) are the mean and standard deviation of the rewards in the group.
The reward \(r_i\) is from a rule-based reward system (not from a human evaluator, therefore, it is scalable and might not be subjective).</p>

<p>The rule-based reward system mainly consists of two types of rewards:</p>

<ul>
  <li><strong>Accuracy rewards</strong>: evaluate whether the output is correct or not. There are plenty of existing datasets where the correct answer is known, for example, Math problems with deterministic answers or Leetcode problems with predefined test cases.</li>
  <li><strong>Format rewards</strong>: the output will be rewarded if it is in a predefined format. For example, the thinking process should be between <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code>.</li>
</ul>

<blockquote class="block-tip">
  <p>Template for DeepSeek-R1-Zero:</p>

  <p>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: <strong>prompt</strong>. Assistant:</p>
</blockquote>

<p><strong>Why is the rule-based reward system effective?</strong>
To me, the employed of rule-based reward system is another example of how self-supervised learning - where data can be generated automatically and massively - is the source of the success of large-scale deep learning models.
Similar to the success of ControlNet in image generation which also employs traditional CV techniques such as edge detection to create additional control signals, so that the model can leverage the existing rule-based knowledge in the dataset to improve its learning process, the rule-based reward system in this paper is a simple yet effective way that allows to create a large amount of data with structure/label, which is crucial for training a large-scale model, making the scaling law become still valid.</p>

<p>However, the rule-based reward system is not perfect and to my understanding, it is the reason why DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing.</p>

<p><strong>Breaking down the GRPO objective function</strong></p>

<p><strong>The expectation term</strong>
The expectation term \(\mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_\text{old}}(O \mid q)}\) says that for each question \(q\) sampled from a distribution of questions \(P(Q)\), we sample a group of outputs \(\{o_i\}_{i=1}^G\) from the old policy model \(\pi_{\theta_\text{old}}\).</p>

<p><strong>The KL divergence term</strong>
Minimizing the KL divergence term ensures that the policy model \(\pi_\theta\) does not deviate too much from the reference model \(\pi_\text{ref}\). Specifically, let \(t=\frac{\pi_\text{ref}(o_i \mid q)}{\pi_\theta(o_i \mid q)}\), then the KL divergence term can be rewritten as:</p>

\[\mathbb{D}_\text{KL}(\pi_\theta||\pi_\text{ref}) = t - \log (t) - 1\]

<p>And then \(\mathbb{D}_\text{KL}(\pi_\theta \mid \mid \pi_\text{ref}) \geq 0 ; \forall t &gt; 0\) and minima is 0 when \(t=1\).</p>

<p><strong>The advantage function</strong>
This term reflects how much better the output \(o_i\) is compared to the average output in the group, e.g., if \(A_i &gt; 0\), then \(o_i\) is better than the average output in the group or if \(A_i &gt; A_j\), then \(o_i\) is better than \(o_j\).</p>

<p>Therefore, maximizing the scaled advantage function \(\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_\text{old}}(o_i \mid q)}A_i\) encourages the policy model \(\pi_\theta\) to generate outputs that are better than the average output in the group, i.e., those with \(A_i &gt; 0\) while discouraging the worse outputs, i.e., those with \(A_i &lt; 0\).</p>

<h4 id="performance-self-evolution-process-and-aha-moment">Performance, Self-evolution Process and Aha Moment</h4>

<p>As mentioned in Section 2.2.4 of the paper, the performance of DeepSeek-R1-Zero is evaluated on the AIME 2024 benchmark (see <a href="#aime-2024">AIME 2024</a>) and impressively reaching comparable performance to OpenAI-o1-0912 - a premium OpenAI reasoning model - on the pass@1 score.
Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/table-2-aime-compare-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/table-2-aime-compare-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/table-2-aime-compare-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/table-2-aime-compare.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Self-evolution Process</strong></p>

<p>Beside the impressive performance, DeepSeek-R1-Zero also exhibits a fascinating self-evolution process as shown in Figure 3 of the paper, where the average response length per question increases over training time (from several hundred tokens to 10k+ tokens), again, with RL only.
DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended <strong>test-time computation</strong> (see <a href="#test-time-computing">Test time computing</a>).</p>

<p>One of the most remarkable aspects of this self-evolution is the <strong>emergence of sophisticated behaviors</strong> as the test-time computation increases. Behaviors such as <strong>reflection</strong>—where the model revisits and reevaluates its previous steps—and the <strong>exploration of alternative approaches</strong> to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model’s interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/fig-3-response-length-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/fig-3-response-length-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/fig-3-response-length-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/fig-3-response-length.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p><strong>Aha Moment</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepseek/table-3-aha-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepseek/table-3-aha-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepseek/table-3-aha-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deepseek/table-3-aha.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>

<p>Another interesting phenomenon observed in DeepSeek-R1-Zero is the <strong>aha moment</strong> (of the model - as well as the authors or myself) where the model suddenly realizes that it needs to allocate more thinking time to solve the problem, by reevaluating its inital approach.
This reminds me of another <strong>aha moment</strong> in the history of RL, when a DeepMind’s DQN model explored an insane strategy to win the Atari game Breakout with the least effort by simply digging a hole in the wall.
Or <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Game_2">DeepMind’s AlphaGo move 37</a> - the move that no human player would have ever made.</p>

<p>This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.
It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.</p>

<div class="text-center">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/TmPfTpjtdgg?si=NWQ6377iCM50NJAr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<h3 id="deepseek-r1---rl-with-cold-start">DeepSeek-R1 - RL with Cold Start</h3>

<p>While DeepSeek-R1-Zero’s performance is impressive, it still encounters challenges such as poor readability, and language mixing.
To address these issues and further enhance reasoning performance, the authors introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a four-stage training pipeline.</p>

<h4 id="cold-start-with-cot-data">Cold Start with CoT data</h4>

<p>Unlike DeepSeek-R1-Zero, which begins with pure RL on the base model, DeepSeek-R1 incorporates a cold start phase. This stage involves collecting thousands of long Chain-of-Thought (CoT) data to fine-tune the base model (DeepSeek-V3-Base). This data is generated using methods such as few-shot prompting, direct prompting with reflection and verification, gathering DeepSeek-R1-Zero outputs, and <strong>refining with human annotators</strong>. The purpose of this step is to prevent an unstable start in the RL process and ensure the model produces more readable and coherent responses. The output format is designed to include a summary at the end of each response: <code class="language-plaintext highlighter-rouge">|special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;</code>.</p>

<h4 id="reasoning-oriented-rl">Reasoning-oriented RL</h4>

<p>After the cold start fine-tuning, the model undergoes a reasoning-oriented RL training process, which is similar to the one used for DeepSeek-R1-Zero. This stage focuses on enhancing the model’s ability to handle tasks in areas such as coding, mathematics, science, and logic. A language consistency reward is added during RL training, calculated as the proportion of target language words in the CoT, to mitigate language mixing issues, though this may slightly degrade performance. The final reward is a combination of reasoning accuracy and language consistency. The Group Relative Policy Optimization (GRPO) algorithm is employed for this stage, as mentioned in our previous conversation, to optimize the policy model, reduce training costs and estimate the baseline from group scores.</p>

<h4 id="sft-with-new-data">SFT with new data</h4>

<p>Once the reasoning-oriented RL has converged, the resulting checkpoint is used to collect SFT data for the next round. This stage incorporates both reasoning data and non-reasoning data. Rejection sampling, as discussed in our earlier conversation, is used to generate reasoning trajectories from the model’s output. The model is prompted to generate multiple responses, and only the correct and coherent responses are kept, and used as SFT data. This is also where a generative reward model is used, feeding both ground-truth and model predictions into DeepSeek-V3 for judgment. Non-reasoning data such as writing, factual QA, self-cognition, and translation, are added by adopting the DeepSeek-V3 pipeline and reusing portions of the DeepSeek-V3 SFT dataset. The DeepSeek-V3-Base model is then fine-tuned using this combined dataset</p>

<h4 id="rl-with-all-scenarios">RL with all scenarios</h4>

<p>The final stage consists of a secondary RL process to align the model with human preferences. This stage aims to improve the model’s helpfulness and harmlessness while refining its reasoning skills. For reasoning data, the process is similar to DeepSeek-R1-Zero, utilizing rule-based rewards. For general data, reward models are used to capture human preferences, where final summaries are assessed for helpfulness, while the entire response (including reasoning and summary) is evaluated for harmlessness</p>

<h2 id="conclusion">Conclusion</h2>

<p>To me, the most interesting part of this paper is the invention of DeepSeek-R1-Zero, whose introduction has had a profound impact on our understanding of RL and LLM training. More specifically, <strong>pure RL with rule-based rewards</strong> might represent a new paradigm for LLM training. The use of a rule-based reward system strikes me as another example of how self-supervised learning—where data can be generated automatically and on a massive scale—continues to underpin the success of large-scale deep learning models.</p>

<p>Similar to the success of ControlNet in image generation, which leverages traditional computer vision techniques like edge detection to provide additional control signals, the rule-based reward system in this paper offers a simple yet effective method to generate large amounts of structured, labeled data. This, in turn, plays a crucial role in training large-scale models, ensuring that the scaling laws remain valid.</p>

<p>The <strong>aha moment</strong> in DeepSeek-R1-Zero perfectly encapsulates the elegance and power of reinforcement learning: instead of explicitly teaching the model how to solve a problem, we simply design the right incentives, allowing the model to autonomously develop sophisticated problem-solving strategies.</p>

<hr />

<h2 id="appendix">Appendix</h2>

<h3 id="aime-2024">AIME 2024</h3>

<p>The American Invitational Mathematics Examination (AIME) is a prestigious mathematics competition in the United States, serving as an intermediary between the AMC 10/12 exams and the USA Mathematical Olympiad (USAMO). The AIME consists of 15 questions, each with an integer answer between 0 and 999, to be completed in 3 hours. Participants qualify for the AIME based on their performance in the AMC 10 or AMC 12 exams.</p>

<p>In 2024, the AIME I was administered on January 31, and the AIME II on February 7. The mean score for AIME I was 5.89, with a median of 5, while AIME II had a mean score of 5.45 and a median of 5.</p>

<p>The AIME 2024 benchmark employs two metrics:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">pass@1</code> score means the percentage of the questions that the model can solve correctly with the top-1 response (see Evaluation Setup - page 12).</li>
  <li>The <code class="language-plaintext highlighter-rouge">cons@64</code> score means the consensus (majority voting) result of the top-64 responses.</li>
</ul>

<h3 id="rejection-sampling">Rejection Sampling</h3>

<p><strong>Purpose</strong>: Rejection sampling is employed to generate reasoning trajectories from the model’s checkpoint after reasoning-oriented reinforcement learning (RL) has converged. The goal is to create a dataset that can improve the model’s ability in various areas, including writing, role-playing, and other general-purpose tasks, alongside its reasoning capabilities.</p>

<p><strong>Process</strong>:</p>

<ul>
  <li>A set of reasoning prompts are curated.</li>
  <li>The model generates multiple responses for each prompt.</li>
  <li>Only correct responses are retained, while incorrect or less desirable responses are rejected. This filtering step ensures that the SFT data consists of high-quality examples.</li>
  <li>The responses are also filtered to remove issues like mixed languages, long paragraphs, and code blocks, to ensure readability and relevance.</li>
</ul>

<p><strong>Expansion of Dataset</strong>: In the rejection sampling stage, the dataset expands beyond those that can be evaluated using rule-based rewards by including data that use a generative reward model. This is done by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.</p>

<p><strong>Output Quality</strong>: The overall goal is to produce higher quality training samples. This is done by filtering out low-quality responses and ensures that the model trains on consistent and reliable data.</p>

<p>In summary, rejection sampling plays a crucial role in the DeepSeek-R1 pipeline by generating a refined and expanded dataset for the second round of supervised fine-tuning. This process contributes to enhancing the model’s overall capabilities.</p>

<h3 id="test-time-computing">Test time computing</h3>

<p>Test Time Computing (TTC) refers to computational processes performed during the inference phase of machine learning models—that is, when the model is used to make predictions or solve problems after being trained. Unlike traditional inference, which usually involves a straightforward application of a pre-trained model, TTC allows for additional computations or adjustments to improve performance on specific tasks.</p>

<p><strong>Key Concepts in Test Time Computing</strong>:</p>

<ul>
  <li><strong>Adaptation at Inference</strong>: Some models dynamically adapt their behavior based on new inputs or environmental conditions. This can involve fine-tuning parts of the model or leveraging meta-learning techniques.</li>
  <li><strong>Iterative Reasoning</strong>: Instead of producing a single output, models perform multiple reasoning steps (e.g., generating intermediate explanations or calculations) to refine their predictions. This is common in large language models when solving complex problems.</li>
  <li><strong>On-the-Fly Learning</strong>: The model might use previously unseen data to improve its predictions in real time. This is particularly useful in tasks like personalization or domain adaptation.</li>
  <li><strong>Resource Allocation</strong>: TTC allows models to allocate varying amounts of computational resources to different inputs, depending on task complexity or uncertainty. For example, a model may run deeper reasoning loops for harder questions.</li>
  <li><strong>Applications</strong>:
    <ul>
      <li>Natural Language Processing (NLP): Iterative reasoning to solve logic or math problems.</li>
      <li>Computer Vision: Adjusting filters or segmentations for specific images.</li>
      <li>Personalization: Adapting user recommendations based on recent interactions.</li>
      <li>Robotics: Dynamically adjusting movements based on environmental feedback.</li>
    </ul>
  </li>
</ul>

<p><strong>Benefits</strong>:</p>

<ul>
  <li>Improved Accuracy: By refining outputs at test time, models often achieve higher performance on difficult tasks.</li>
  <li>Task-Specific Customization: Allows models to handle nuanced problems more effectively.</li>
  <li>Efficient Use of Resources: Computational effort can be adjusted based on task complexity.</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Increased Latency: Additional computations can slow down predictions.</li>
  <li>Higher Costs: Real-time adjustments require more computational resources.</li>
  <li>Complexity: Implementing TTC mechanisms can complicate model architecture.</li>
</ul>

<p>This approach is increasingly used in advanced AI systems, such as OpenAI’s GPT models, which employ techniques like iterative reasoning or chain-of-thought prompting to tackle complex tasks effectively.</p>

<p><strong>Why NVIDIA doesn’t like TTC</strong></p>

<p>NVIDIA’s GPUs are designed for parallel computing, which is not suitable for TTC which often involves sequential or iterative computation for individual inputs, underutilizing the GPU’s parallel architecture. TTC introduces variability and possibly higher latency, which isn’t ideal for traditional GPU pipelines.</p>

<h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3>

<p><strong>Monte Carlo Tree Search (MCTS)</strong> is an advanced search algorithm used primarily in decision-making processes, especially for games, simulations, and optimization problems. It is a method for making decisions by simulating many possible outcomes and using statistical analysis to find the most promising path.</p>

<p><strong>Key Components of MCTS</strong>
MCTS works by iteratively building a search tree, where nodes represent game states (or decision points) and edges represent actions. The process involves four main steps:</p>

<p>1.Selection</p>

<ul>
  <li>Starting from the root node, the algorithm selects child nodes recursively until it reaches a node that is not fully expanded (i.e., not all possible moves are explored).</li>
  <li>The selection is often guided by a strategy like the <strong>Upper Confidence Bound for Trees (UCT)</strong>, which balances exploration (trying less-visited nodes) and exploitation (focusing on nodes with high average rewards):</li>
</ul>

\[UCB = \text{win rate} + c \times \sqrt{\frac{\ln(\text{total visits})}{\text{visits to this node}}}\]

<p>2.Expansion</p>

<ul>
  <li>When a leaf node is reached, new child nodes are added for all possible moves from the current state.</li>
  <li>This step grows the search tree by exploring unvisited nodes.</li>
</ul>

<p>3.Simulation (Rollout)</p>

<ul>
  <li>From the newly added node, a simulation is run to the end of the game (or a predefined depth). The simulation often involves random or heuristic-based moves.</li>
  <li>The outcome (e.g., win, loss, or score) of this rollout provides an estimate of the value of the node.</li>
</ul>

<p>4.Backpropagation</p>

<ul>
  <li>The result of the simulation is propagated back up the tree, updating the statistics (e.g., win rate or average reward) for each node along the path to the root.</li>
  <li>This helps the algorithm prioritize the most promising branches in future iterations.</li>
</ul>

<p><strong>Applications of MCTS</strong></p>

<p>1.<strong>Games</strong>:</p>

<ul>
  <li>Widely used in game-playing AI, especially for games with large decision spaces (e.g., Go, Chess, Poker).</li>
  <li>Integral to the success of systems like AlphaGo, which combined MCTS with deep neural networks.</li>
</ul>

<p>2.<strong>Robotics and Planning</strong>:</p>

<ul>
  <li>Used to plan sequences of actions in dynamic environments where outcomes are uncertain.</li>
</ul>

<p>3.<strong>Optimization</strong>:</p>

<ul>
  <li>Applied in optimization problems where exploring the solution space is challenging due to its complexity or size.</li>
</ul>

<p>4.<strong>Simulations</strong>:</p>

<ul>
  <li>Used in Monte Carlo simulations to estimate probabilities or solve probabilistic decision-making problems.</li>
</ul>

<p><strong>Strengths of MCTS</strong></p>

<ul>
  <li><strong>Scalable</strong>: Handles very large state spaces effectively.</li>
  <li><strong>Adaptive</strong>: Focuses computational resources on the most promising parts of the tree.</li>
  <li><strong>Flexible</strong>: Can work without a full model of the game or problem and adapt as new information is added.</li>
</ul>

<p><strong>Limitations</strong></p>

<ul>
  <li><strong>Computationally Expensive</strong>: Requires many simulations, especially for complex problems.</li>
  <li><strong>Dependence on Rollout Policy</strong>: The quality of results depends heavily on how the simulations (rollouts) are performed.</li>
  <li><strong>Suboptimal for Short Decision Horizons</strong>: Less effective for problems requiring quick, shallow decisions.</li>
</ul>

<p>MCTS combines principles from reinforcement learning, probability, and decision-making, making it a powerful tool for complex tasks that involve uncertainty and large decision spaces.</p>

<h3 id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    SFT and RLHF. Image from [1].
</div>

<p><strong>Goal:</strong>
Fine-tuning a model using reinforcement learning where the reward signal is derived from human feedback to align the model with human preferences and values beyond task-specific objectives.</p>

<p><strong>Process:</strong></p>

<ul>
  <li><strong>Supervised Pretraining</strong>: Start with a pretrained and optionally fine-tuned model.</li>
  <li><strong>Feedback Collection</strong>: Collect human-labeled data ranking model outputs (e.g., ranking completions for prompts).</li>
  <li><strong>Reward Model (RM)</strong>: Train a reward model to predict rankings based on human feedback. The reward model is trained to mimic human preferences on a specific task.</li>
  <li><strong>Policy Optimization</strong>: Fine-tune the model (policy) using reinforcement learning (e.g., Proximal Policy Optimization, PPO) to maximize the reward from the RM.</li>
</ul>

<p>Loss Function:</p>

<ul>
  <li><strong>Combines reinforcement learning objectives (e.g., PPO loss) with supervised objectives to balance exploration and alignment.</strong></li>
</ul>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Aligns model behavior with human values, preferences, and ethical considerations.</li>
  <li>Reduces harmful or inappropriate responses.</li>
  <li>Improves usability in real-world scenarios (e.g., chatbot interactions).</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Requires expensive and time-consuming human feedback.</li>
  <li>May introduce biases based on the preferences of the feedback providers.</li>
  <li>Balancing alignment with generalization is challenging.</li>
</ul>

<p><strong>Comparison Table</strong></p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Unsupervised Pretraining</th>
      <th>Supervised Fine-Tuning</th>
      <th>RLHF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Purpose</td>
      <td>General-purpose language understanding</td>
      <td>Task-specific performance improvement</td>
      <td>Aligning outputs with human preferences</td>
    </tr>
    <tr>
      <td>Data Requirement</td>
      <td>Large-scale unlabeled text corpora</td>
      <td>Labeled datasets for specific tasks</td>
      <td>Human-labeled feedback or rankings</td>
    </tr>
    <tr>
      <td>Objective</td>
      <td>Learn language patterns and knowledge</td>
      <td>Optimize for specific task objectives</td>
      <td>Optimize for human alignment</td>
    </tr>
    <tr>
      <td>Training Cost</td>
      <td>High (large datasets, long training)</td>
      <td>Moderate (smaller labeled datasets)</td>
      <td>Very high (feedback collection + RL tuning)</td>
    </tr>
    <tr>
      <td>Model Usage</td>
      <td>Provides a base model</td>
      <td>Task-specific models</td>
      <td>Refines models for safer, more useful output</td>
    </tr>
    <tr>
      <td>Challenges</td>
      <td>Task-agnostic, needs fine-tuning</td>
      <td>Dependent on labeled data quality</td>
      <td>Expensive and subject to bias in feedback</td>
    </tr>
    <tr>
      <td>Example</td>
      <td>Training GPT, BERT, T5 from scratch</td>
      <td>Fine-tuning BERT for sentiment analysis</td>
      <td>Fine-tuning GPT with human ranking data</td>
    </tr>
  </tbody>
</table>

<h3 id="ppo-and-dpo">PPO and DPO</h3>

<p>References:</p>

<ul>
  <li><a href="https://medium.com/@bavalpreetsinghh/rlhf-ppo-vs-dpo-26b1438cf22b">RLHF(PPO) vs DPO</a></li>
</ul>

<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h4>

<p>The Bradley-Terry model is a probabilistic model used to rank items based on pairwise comparisons.</p>

\[p(y_1 \succ y_2 \mid x) = \frac{exp(r(x,y_1))}{exp(r(x,y_1)) + exp(r(x,y_2))}\]

<p>where \(p\) is the probability of \(y_1\) <strong>being better than</strong> \(y_2\) given \(x\) representing the true human preferences and \(r\) is the reward function.
\(y_1\) and \(y_2\) are the two items/responses being compared given \(x\) representing the input.</p>

<p>If \(r^*(x,y_1) &gt; r^*(x,y_2)\), then \(p^*(y_1 \succ y_2 \mid x) &gt; 0.5\), which means \(y_1\) is more likely to be better than \(y_2\) given \(x\).</p>

<p>To learn the reward function \(r\) from the data, we parameterize as a neural network \(r_{\phi}\) and optimize the following objective function:</p>

\[\mathcal{L}_{R}(r_{\phi}, \mathcal{D}) = - \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma (exp(r_{\phi}(x,y_w)) - exp(r_{\phi}(x,y_l))) \right]\]

<p>where \(\mathcal{D}\) is the dataset of human preferences and \(y_w\) and \(y_l\) are the winning and losing responses against the same input \(x\).
Minimizing the above objective function is equivalent to maximizing the probability of the winning response being better than the losing response given the input.</p>

<p>Note that the reward function \(r_{\phi}\) is usually initialized from the supervised fine-tuning (SFT) model same as the policy model.</p>

<p><strong>Fine-tuning the policy model</strong></p>

<p>Once we have the reward model \(r_{\phi}\), we can fine-tune the policy model \(\theta\) using the following objective function:</p>

\[\mathcal{L}_{P}(\theta, \mathcal{D}) = - \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \left[ r_{\phi}(x,y) + \beta D_{KL}(\pi_{\theta}(y \mid x) || \pi_{ref}(y \mid x)) \right]\]

<p>where \(\pi_{ref}\) is the reference model and \(\beta\) is a hyperparameter.
Minimizing the first term enforces the policy model to generate responses that are more preferred by the reward model,
while minimizing the second term ensures that the policy model does not deviate too much from the reference model.</p>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Aligns the model with human preferences.</li>
  <li>Reduces harmful or inappropriate responses.</li>
  <li>Improves usability in real-world scenarios (e.g., chatbot interactions).</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Requires expensive and time-consuming human feedback.</li>
  <li>May introduce biases based on the preferences of the feedback providers.</li>
  <li>Balancing alignment with generalization is challenging.</li>
</ul>

<h4 id="direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</h4>

<p>DPO simplifies PPO by directly optimizing the policy model to adhere to human preferences without the need for a reward model.</p>

<p><strong>Objective Function:</strong></p>

\[\pi_{r}(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp(r(x,y)/\beta)\]

<p>where \(\pi_r\) is the policy model, \(\pi_{\text{ref}}\) is the reference model, \(r\) is the reward function, and \(\beta\) is a hyperparameter.</p>

\[r(x,y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \log Z(x)\]

<p>where \(Z(x)\) is the partition function.</p>

<p>The final objective function is:</p>

\[\mathcal{L}_{DPO}(\theta, \mathcal{D}) = - \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma (\pi_{\theta}(x,y_w) - \pi_{\theta}(x,y_l)) \right]\]

<p>Where there is no need for the reward model \(r_{\phi}\) as the policy model \(\pi_{\theta}\) is directly optimized to adhere to human preferences.
Minimizing the above objective function is directly maximizing the probability of the winning response while minimizing the probability of the losing response.</p>

<h3 id="sft-vs-rlhf">SFT vs RLHF</h3>

<p>Reinforcement Learning from Human Feedback (RLHF) is a technique used to align large language models (LLMs) more closely with human preferences and expectations. While supervised fine-tuning (SFT) is a critical step in improving a model’s capabilities, it has limitations that RLHF can address.</p>

<h4 id="when-is-rlhf-needed">When is RLHF needed?</h4>

<p><strong>Ambiguity in Objectives</strong></p>

<ul>
  <li>SFT aligns the model with a dataset of “correct” outputs, but human preferences often involve subjective judgment or context-specific nuance that cannot be fully captured in a static dataset.</li>
  <li>Example: Chatbots generating empathetic or polite responses where the tone and context matter significantly.</li>
</ul>

<p><strong>Unclear or Complex Evaluation Metrics</strong></p>

<ul>
  <li>For some tasks, it’s difficult to define explicit evaluation metrics, but humans can intuitively judge quality.</li>
  <li>Example: Creative writing or generating humorous content.</li>
</ul>

<p><strong>Long-Term or Multi-Step Reasoning</strong></p>

<ul>
  <li>SFT trains models to produce correct outputs based on immediate context but might fail in scenarios requiring multi-step decision-making or long-term coherence.</li>
  <li>Example: Writing a coherent multi-paragraph essay or guiding a user through a series of troubleshooting steps.</li>
</ul>

<p><strong>Avoiding Harmful Outputs</strong></p>

<ul>
  <li>Static datasets used for SFT may not include all edge cases or potential pitfalls, and RLHF can help refine the model to avoid harmful or toxic outputs based on human preferences.</li>
  <li>Example: Ensuring a conversational agent avoids generating offensive or biased content.</li>
</ul>

<p><strong>Improving User Experience</strong></p>

<ul>
  <li>SFT often focuses on correctness, but RLHF can optimize for user satisfaction, such as balancing informativeness, politeness, and conciseness.</li>
  <li>Example: Personal assistants generating concise and helpful responses tailored to user needs.</li>
</ul>

<h4 id="why-is-supervised-fine-tuning-not-enough">Why is Supervised Fine-Tuning Not Enough?</h4>

<p><strong>Static Nature of Datasets</strong></p>

<ul>
  <li>SFT relies on pre-collected datasets that might not represent all real-world scenarios or evolving user preferences.</li>
  <li>Limitation: If the dataset lacks certain examples, the model cannot generalize well.</li>
</ul>

<p><strong>Difficulty in Capturing Preferences</strong></p>

<ul>
  <li>Human preferences are often complex and not directly labeled in datasets.</li>
  <li>Limitation: A model trained on SFT might produce technically correct but undesirable outputs (e.g., overly verbose or lacking empathy).</li>
</ul>

<p><strong>Overfitting to Training Data</strong></p>

<ul>
  <li>SFT can lead to overfitting, where the model learns to replicate the training data without adapting to unseen scenarios.</li>
  <li>Limitation: This can result in poor performance on out-of-distribution examples.</li>
</ul>

<p><strong>Reward Optimization</strong></p>

<ul>
  <li>RLHF optimizes a reward function designed to capture human preferences, which allows fine-grained control over the model’s behavior.</li>
  <li>Limitation: SFT does not involve direct optimization based on human evaluations.</li>
</ul>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>