<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Fairness in Machine Learning | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Just some notes">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2023/fairness-101/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Fairness in Machine Learning",
      "description": "Just some notes",
      "published": "September 1, 2023",
      "authors": [
        {
          "author": "Tuan-Anh Bui",
          "authorURL": "https://tuananhbui89.github.io/",
          "affiliations": [
            {
              "name": "Monash University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Fairness in Machine Learning</h1>
        <p>Just some notes</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#varieties-of-fairness">Varieties of Fairness</a></div>
            <div><a href="#learning-fair-representations">Learning Fair Representations</a></div>
            <div><a href="#fairness-in-deep-learning">Fairness in Deep Learning?</a></div>
            <div><a href="#fairness-in-generative-models">Fairness in Generative Models</a></div>
            
          </nav>
        </d-contents>

        <p>(Work in progress)</p>

<h2 id="varieties-of-fairness">Varieties of Fairness</h2>

<p>One of the hardest problems in fairness is that there is no consensus on the definition of fairness or what does it mean to be fair. Depending on the context or culture, the definition of fairness can be different <d-cite key="du2020fairness"></d-cite>.</p>

<p>Researchers and designers at Google’s PAIR (People and AI Research) , <d-cite key="googlefairness"></d-cite> initiative created the What-If visualization tool as a pragmatic resource for developers of machine learning systems. The tool provides a set of fairness metrics that can be used to evaluate the fairness of a model. The metrics are grouped into five categories:</p>

<ul>
  <li>Group unaware: “group unaware” fairness is an approach that advocates for fairness by disregarding demographic characteristics like gender and making  decisions solely based on individual qualifications.</li>
  <li>Group threshold: “group threshold” is a fairness mechanism that recognizes that not all groups are the same, and historical disparities or biases may warrant different decision thresholds for different groups to promote equitable outcomes. It’s a technique used to fine-tune the behavior of AI models to ensure that they do not disproportionately disadvantage certain demographic groups while still maintaining some level of predictive accuracy.</li>
  <li>Demographic parity (or group fairness, statistical parity): is an approach to ensure that the composition of the selected or approved individuals or outcomes reflects the demographic composition of the overall population.</li>
  <li>Equal opportunity: aims to promote fairness by ensuring that individuals from different demographic groups are treated equally when they have the same qualifications or attributes relevant to a decision, and their chances of success are not influenced by factors like race, gender, or age.</li>
  <li>Equal accuracy: ensuring that the predictive accuracy of a model is similar across different demographic groups.</li>
</ul>

<p>It can be seen that, these proposed metrics are already complex and hard to understand. For example, in my opinion, the “group unware” and “equal opportunity” are quite similar to each other where both of them aim to ensure that the model does not discriminate based on “protected characteristics” like gender, age, race, etc. Overall, these metrics can be grouped into two categories: group fairness and individual fairness which are also the two main categories of fairness in machine learning.</p>

<h2 id="learning-fair-representations">Learning Fair Representations</h2>

<p>One of the milestone work in fairness is the paper “Learning Fair Representations” by Zemel et al. (2013) <d-cite key="zemel2013learning"></d-cite>. The authors proposed a method to learn fair representations by learning a latent representation that encodes the data well but obfuscates information about protected attributes. The method is based on the intuition that if the learned representation does not contain any information about the protected attribute, then any classifier based on these representation cannot use the protected attribute to make predictions.</p>

<p>The authors formulated this using the notion of statistical parity, which requires that the probability that a random element from $X^+$ maps to a particular prototype is equal to the probability that a random element from
$X^-$ maps to the same prototype</p>

\[P(Z = k \mid x^+ \in X^+) = P(Z = k \mid x^- \in X^-) \; \forall k\]

<p>Where $X^+$ and $X^-$ are the sets of protected and unprotected examples, respectively, and $Z$ is the latent representation with $K$ prototypes.</p>

<!-- The authors proposed to use an adversarial network to learn the latent representation. The adversarial network is trained to predict the protected attribute from the latent representation while the main network is trained to predict the label from the latent representation. The authors also proposed to use a regularization term to ensure that the latent representation is close to the original representation. The authors evaluated their method on the Adult dataset and showed that their method can achieve a better fairness score than the baseline method. -->

<h2 id="fairness-in-deep-learning">Fairness in Deep Learning?</h2>

<h2 id="fairness-in-generative-models">Fairness in Generative Models</h2>

<p>Fairness Machine Learning is mostly considered in the context of decision making models such as classification models. However, fairness is also an important issue in generative models, which is not well studied yet. Recently, the central problem of fairness in generative models is how to ensure diversity in the generated outputs. For example, a response to a question about famous musicians should not only include names or images of people of the same gender identity or skin tone <d-cite key="googlefairness"></d-cite>, <d-cite key="playingfairness"></d-cite>.</p>

<p>Some of the following attributes will be highly considered when talking about fairness in generative models:</p>

<ul>
  <li>Gender identity</li>
  <li>Cultural background and demographic</li>
  <li>Physical appearance attributes</li>
  <li>Political related attributes</li>
</ul>

<p>When evaluating the fairness of generative models, the authors of <d-cite key="googlefairness"></d-cite> suggest to consider the following metrics:</p>

<ul>
  <li>Diversity of the output: Given a set of prompts, the diversity along dimensions of identity attributes represented in the generated outputs. For example, given a set of prompts asking about “famous musicians”, the diversity of gender/culture/nationality  in the outputs will be measured. However, when asking about “famous men musicians”, the diversity of culture/nationality will be considered because the gender has been specified in the prompts.</li>
  <li>Ability on maintaining fairness: Given a set of prompts that contain counterfactuals of a sensitive attribute, ability to provide the same quality of service. For example, an user revealed his personal demographic information to the system (e.g., an Asian guy), then when the user asks about “famous musicians”, the system should not only provide names of Asian musicians. A fair system should provide answers as the same quality as the case when the user does not reveal his personal demographic information or when the user is a white guy.</li>
</ul>

<p>In summary, we can think about two scenarios when evaluating the fairness of generative models: same input - diverse output and diverse input - same output.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2023-09-01-fairness.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "alshedivat/al-folio",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
