<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Tutorial on Adversarial Machine Learning - Part 2 | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Adversarial Attacks">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2023/aml-overview/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Tutorial on Adversarial Machine Learning - Part 2",
      "description": "Adversarial Attacks",
      "published": "June 7, 2023",
      "authors": [
        {
          "author": "Tuan-Anh Bui",
          "authorURL": "https://tuananhbui89.github.io/",
          "affiliations": [
            {
              "name": "Monash University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Tutorial on Adversarial Machine Learning - Part 2</h1>
        <p>Adversarial Attacks</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#trustworthy-machine-learning-vs-adversarial-machine-learning">Trustworthy Machine Learning vs Adversarial Machine Learning</a></div>
            <div><a href="#category-of-adversarial-attacks">Category of Adversarial Attacks</a></div>
            <ul>
              <li><a href="#poisoning-attacks">Poisoning Attacks</a></li>
              <li><a href="#backdoor-attacks">Backdoor Attacks</a></li>
              <li><a href="#model-extraction">Model Extraction</a></li>
              <li><a href="#privacy-attacks">Privacy Attacks</a></li>
              <li><a href="#evasion-attacks-adversarial-examples">Evasion Attacks (Adversarial Examples)</a></li>
              <li><a href="#some-thought-on-the-practicality-of-adversarial-attacks">Some thought on the practicality of adversarial attacks</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <p>Reference: This post is inspired by the tutorial “Adversarial Machine Learning for Good” by Dr. <a href="https://sites.google.com/site/pinyuchenpage/home" rel="external nofollow noopener" target="_blank">Pin-Yu Chen</a> (IBM Research) at the AAAI 2022 conference. Link to the tutorial: <a href="https://sites.google.com/view/advml4good" rel="external nofollow noopener" target="_blank">https://sites.google.com/view/advml4good</a></p>

<h2 id="trustworthy-machine-learning-vs-adversarial-machine-learning">Trustworthy Machine Learning vs Adversarial Machine Learning</h2>

<p>Trustworthy Machine Learning (TML) is a broad area of research that focuses on developing and deploying reliable, transparent, fair and secure machine learning systems. While Adversarial Machine Learning (AML) is a subfield of TML which specifically focuses on defending against malicious attacks. Let’s compare these two concepts in more detail:</p>

<p>Trustworthy Machine Learning:</p>

<ul>
  <li>Objective: TML aims to ensure the reliability, transparency, fairness, and security of machine learning models and algorithms.</li>
  <li>Scope: TML encompasses a broader range of considerations, including data quality, model interpretability, algorithmic bias, privacy, and fairness in decision-making.</li>
  <li>Research Focus: TML researchers investigate techniques and methodologies to enhance the overall trustworthiness of machine learning systems, taking into account ethical, legal, and societal implications.</li>
  <li>Application Areas: TML is relevant across various sectors, such as healthcare, finance, autonomous vehicles, and cybersecurity, where the consequences of unreliable or biased machine learning predictions can have significant impacts.</li>
</ul>

<p>Adversarial Machine Learning:</p>

<ul>
  <li>Objective: AML focuses on understanding and defending against adversarial attacks, where malicious actors manipulate input data to deceive or exploit vulnerabilities in machine learning models.</li>
  <li>Scope: AML primarily concerns itself with studying and countering specific threats to machine learning systems, including evasion attacks, poisoning attacks, model extraction, and membership inference attacks.</li>
  <li>Research Focus: AML researchers explore techniques to detect, mitigate, and recover from adversarial attacks, aiming to enhance the resilience and robustness of machine learning models.</li>
  <li>Application Areas: AML is particularly relevant in domains where the impact of successful attacks can be severe, such as autonomous vehicles, cybersecurity systems, fraud detection, and sensitive data analysis.</li>
</ul>

<p>In this post, we will focus on AML research and discuss four main topics: adversarial attacks, adversarial defenses, certified robustness, and AML for good.</p>

<h2 id="category-of-adversarial-attacks">Category of Adversarial Attacks</h2>

<p>Adversarial attacks aim to manipulate machine learning models by exploiting their vulnerabilities. 
They can be categorized based on the following criteria:</p>

<ul>
  <li>
<strong>Attack Objective</strong>: What is the goal of the attack?</li>
  <li>
<strong>Attack Knowledge</strong>: What is the attacker’s knowledge about the target model? There are two types of settings: white-box and black-box. In the white-box setting, the attacker knows everything of the target model, including its architecture, parameters, data, and training process. It even knows defending strategies and can adapt its attack accordingly. In the black-box setting, the attacker has no access to the model’s internal information. It can only query the model and observe its outputs (probabilities or just final predictions).</li>
  <li>
<strong>Attack Phase</strong>: What is the attacker’s access to the target model? There are three types of access: data, training, and inference. In the data access setting, the attacker can manipulate the input data before feeding it to the target model. In the training access setting, the attacker can manipulate the training data or the training process of the target model. In this case, the aim is to corrupt/manipulate the target model’s parameters (i.e., poisoning/backdoor attacks). In the inference access setting, the target model is fixed, and the attacker can only manipulate the input data.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://raw.githubusercontent.com/tuananhbui89/website_images/master/posts/aml_intro/attack_category_3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://raw.githubusercontent.com/tuananhbui89/website_images/master/posts/aml_intro/attack_category_3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://raw.githubusercontent.com/tuananhbui89/website_images/master/posts/aml_intro/attack_category_3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://raw.githubusercontent.com/tuananhbui89/website_images/master/posts/aml_intro/attack_category_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Category of Adversarial attacks based on their access to the data, training or inference process of a target model. Adapted from Pin-Yu Chen's tutorial (2022).
</div>

<p>Based on the above criteria, there are many types of adversarial attacks. In this post, we will focus on the most common ones: poisoning, backdoor attacks, model extraction, evasion attacks, and privacy attack.</p>

<h3 id="poisoning-attacks">Poisoning Attacks</h3>

<p>Aim: Poisoning attacks aim to manipulate the training data or the training process of a target model to corrupt its parameters. The attacker’s goal is to make the target model misbehave on future test data.</p>

<p>Threat scenario: StabilityAI and OpenAI are two competing companies that develop image generation models. StablityAI has a better model than OpenAI. However, OpenAI wants to win the competition. Therefore, it hires a group of hackers to hack into StabilityAI’s training process and corrupt its model. As a result, StabilityAI’s model is corrupted, and it starts to generate images that are not as good as before. OpenAI wins the competition.</p>

<p>Some notable works: <d-cite key="biggio2012poisoning"></d-cite>, <d-cite key="shafahi2018poison"></d-cite>, <d-cite key="steinhardt2017certified"></d-cite></p>

<h3 id="backdoor-attacks">Backdoor Attacks</h3>

<p>Aim: Backdoor attacks aim to manipulate the training data or the training process of a target model to embed a backdoor into the model. The attacker’s goal is to make the target model misbehave on future test data that contains a trigger, where the trigger is a specific pattern that is not present in the training data (recently, there are some works that can embed a backdoor with imperceptible triggers). Without the trigger, the model behaves normally. However, when the trigger is present in the input data, the model starts to make wrong predictions. In order to make the attack’s threat more practical, the attacker is allowed to control the training data only, not the training process. However, it is still possible to embed a backdoor into the model by manipulating the training process or specific architecture.</p>

<p>Threat scenario: A bank wants to use a machine learning model to predict whether a loan applicant will default or not. The bank hires a data scientist to develop a machine learning model for this task. However, the data scientist is not honest. He/she wants to make the model misbehave on his/her future loan application. Therefore, he/she embeds a backdoor into the model. As a result, the model starts to make wrong predictions on future test data that contains a trigger. The data scientist gets benefits from this attack.</p>

<p>Some notable works: <d-cite key="gu2017badnets"></d-cite>, <d-cite key="liu2018trojaning"></d-cite>, <d-cite key="chen2017targeted"></d-cite></p>

<h3 id="model-extraction">Model Extraction</h3>

<p>Aim: Model extraction attacks aim to extract a copy of a target model. The attacker’s goal is to obtain a model that has similar performance to the target model.</p>

<p>Threat scenario: A bank has a machine learning model that can predict whether a loan applicant will be accepted or not. The bank wants to keep this model secret. However, a competitor wants to obtain this model. Therefore, the competitor hires a hacker that can submit a lot of queries and replicate the model from observed output. They can use this model for their own benefits.</p>

<p>Some notable works: <d-cite key="truong2021data"></d-cite>, <d-cite key="jia2021entangled"></d-cite>, <d-cite key="bastani2017interpreting"></d-cite></p>

<h3 id="privacy-attacks">Privacy Attacks</h3>

<p>Aim: Privacy attacks aim to extract sensitive information from a target model. The attacker’s goal is to obtain sensitive information from the target model.</p>

<p>Threat scenario: A bank trained their chatbox using their clients’ data and release their chatbox for publish use. However, a competitor wants to obtain their clients’ data. Therefore, the competitor hires a hacker that interact with the chatbox and extract the clients’ data. They can use this data for their own benefits.</p>

<p>Some notable works: Recent work from Carlini et al. <d-cite key="carlini2021extracting"></d-cite> demonstrates that it is possible to extract training data from Large Language Models (LLMs) such as GPT-2 and GPT-3. Another work from <d-cite key="carlini2023extracting"></d-cite> shows that generative model trained by copyrighted/unallowed images.</p>

<h3 id="evasion-attacks-adversarial-examples">Evasion Attacks (Adversarial Examples)</h3>

<p>Aim: Evasion attacks aim to manipulate the input data to cause a target model to make a incorrect prediction. The common goal is to make the model predict a specific class (targeted attack) or make the model predict any class other than the correct class (untargeted attack). The perturbation is usually small and imperceptible to human perception.</p>

<p>Threat scenario: A eKYC system uses a machine learning model to verify the identity of a person. However, a hacker wants to bypass this system. Therefore, he/she manipulates his/her ID card to make the system misbehave. As a result, the system accepts his/her ID card, and he/she can get access to the system. It is even worse if the system is used in warfare.</p>

<p>Some notable works: Szegedy et al. <d-cite key="szegedy2013intriguing"></d-cite> first demonstrated the existence of adversarial examples. Madry et al. <d-cite key="madry2017towards"></d-cite> proposed a PGD attack.</p>

<h3 id="some-thought-on-the-practicality-of-adversarial-attacks">Some thought on the practicality of adversarial attacks</h3>

<p>While adversarial attacks are definitely a big issue when it comes to deploying machine learning models, in my opinion, the situation isn’t as bad as it seems. That’s probably why many companies, even though they know about these attacks, don’t really take proper actions to defend against them. (Except for a few like Google, Microsoft, and Facebook, but they’re not the majority. I’m exhausted from searching for job opportunities in the industry, and let me tell you, most companies just don’t care about adversarial attacks.)</p>

<p>When it comes to poisoning attacks and backdoor attacks, attackers need access to the training data or the training process, which isn’t always possible. They can’t control how a model is trained. In model extraction attacks, attackers have to submit a bunch of queries to the target model, which isn’t always doable. And let’s talk about privacy attacks - current research successfully extracts some training data, but guess what? That data is already out there on the internet, so it’s not really a big deal. Adversarial examples, on the other hand, are a real headache when deploying machine learning models. But white-box attacks? They’re not practical because the attacker needs to know everything about the target model. Black-box attacks are more realistic, but even then, they either require a ton of queries or rely on transferability.</p>

<p>So what’s the point of all this?</p>

<ul>
  <li>From academic perspective, adversarial attacks were a hot and important research topic and they still are.</li>
  <li>From industrial perspective, companies are aware of adversarial attacks, but they don’t take proper actions.</li>
  <li>We still wait for a real super villain to show up and prove that adversarial attacks are a real threat.</li>
</ul>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2023-06-07.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
