<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Erasing Undesired Concepts from Diffusion Models - CLIP | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="http://localhost:4000/blog/2023/proj_erasing_concepts_diffusion_CLIP/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Erasing Undesired Concepts from Diffusion Models - CLIP</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <h2 id="resources">Resources</h2>

<ul>
  <li>Conditional Diffusion Models: <a href="https://tuananhbui89.github.io/blog/2023/conditional-diffusion/" rel="external nofollow noopener" target="_blank">https://tuananhbui89.github.io/blog/2023/conditional-diffusion/</a>
</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Given a pre-trained generative model such as Stable Diffusion, we aim to remove a generation capability of the model regarding specific concept or keyword, for example “Barack Obama”.
By unlearning that concept, the model cannot generate meaningful output image whenever a prompt with this specific keyword while still retain its capability for all other things.</p>

<p>This idea has been proposed in the paper “Erasing Concepts from Diffusion Models”. However, there are some limitations of this paper:</p>

<ul>
  <li>The concept is erased not completely, but only reduced its probability of generating an image according to the likelihood that is described by the concept, scaled by a power factor \(\eta\).</li>
  <li>The concept to be erased needs to be associated with a specific keyword (e.g., “Barack Obama” or “nudity” or “gun”). However, a concept can be described in many different ways, for example, “Barack Obama” can be described as “the 44th president of the United States” or “the first African American president of the United States”. Therefore, it is not possible to erase all concepts related to “Barack Obama” by just erasing the keyword “Barack Obama”.</li>
  <li>When erasing a specific concept (e.g., “Barack Obama”), related concepts (e.g., “Donald Trump”) can be also erased.</li>
  <li>This erased concept can be recovered by an adversary by crawling images with this concept from the Internet and use Textual Inversion to recover the erased concept.</li>
</ul>

<p>Therefore, we propose a new idea to erase a specific concept completely, without effecting other related concepts. The idea is to utilize an auxiliary classifier to classify the concept (or set of concepts) and then use the gradient of the classifier to guide the unlearning process in the diffusion model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_concepts/examples-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_concepts/examples-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_concepts/examples-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_concepts/examples.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_concepts/failure-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_concepts/failure-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_concepts/failure-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_concepts/failure.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Erasing Concept Examples and Failure Cases
</div>

<!-- It can be done by finetune one of three components: the encoder-decoder, the text encoder and the unet.
However, if just finetune the text-encoder, there is a posibility that an adversary can replace its by a new/fully-capability text-encoder.
There might be also a potential risk of loss capability when finetune the encoder-decoder (need to invesigate further).
Therefore, the safest approach is to finetune the unet. -->

<h2 id="terminology">Terminology</h2>

<ul>
  <li>\(\theta\): the parameters of the fine-tuned generative model to be erased the undesired concept \(c\).</li>
  <li>\(\theta^*\): the parameters of the base generative model.</li>
  <li>\(\psi^*\): the parameters of the base classifier to guide the generation process of the base generative model to generate images with a specific concept \(c\).</li>
  <li>\(\phi\): the parameters of the auxiliary classifier to serve as the classifier to distinguish between the images with the concept \(c\) and the images without the concept \(c\).</li>
  <li>\(x_t^n\): the image generated at the time step \(t\) with the null concept \(n\). Sometimes, we use \(x_t\) instead of \(x_t^n\) for simplicity.</li>
  <li>\(x_t^c\): the image generated at the time step \(t\) with the undesired concept \(c\).</li>
  <li>\(\epsilon_\theta (x_t, t)\): the predicted noise at the time step \(t\) of the fine-tuned diffusion model \(\theta\) with input image \(x_t\).</li>
  <li>\(\epsilon_\theta (x_t, c, t)\): the predicted conditional noise at the time step \(t\) of the fine-tuned diffusion model \(\theta\) with input image \(x_t\).</li>
</ul>

<h2 id="review-of-the-paper-erasing-concepts-from-diffusion-models">Review of the paper “Erasing Concepts from Diffusion Models”</h2>

<p>Aim to reduce the probability of generating an image \(x\) according to the likelihood that is described by the concept, scaled by a power factor \(\eta\).</p>

\[P_{\theta} (x) \propto \frac{P_{\theta^*} (x)}{P_{\theta^*} (c \mid x)^\eta}\]

<p>where \(P_{\theta^*} (x)\) is the likelihood of the image \(x\) under the original model, \(P_{\theta^*} (c \mid x)\) is the likelihood of the concept \(c\) given the image \(x\) under the original model, and \(\theta\) is the parameters of the model after unlearning the concept \(c\).</p>

<p>It can be interpreted as: if the concept \(c\) is present in the image \(x\) in which \(P_{\theta^*} (c \mid x)\) is high, then the likelihood of the image \(x\) under the new model \(P_{\theta} (x)\) will be reduced.
While if the concept \(c\) is not present in the image \(x\) in which \(P_{\theta^*} (c \mid x)\) is low, then the likelihood of the image \(x\) under the new model \(P_{\theta} (x)\) will be increased.</p>

<p>Because of the conditional likelihood:</p>

\[P_{\theta^*} (c \mid x) = \frac{P_{\theta^*} (x \mid c) P_{\theta^*} (c)}{P_{\theta^*} (x)}\]

<p>Therefore, the above equation can be rewritten when taking the derivative w.r.t. \(x\) as follows:</p>

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta \nabla_{x} \log P_{\theta^*} (c \mid x)\]

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta (\nabla_{x} \log P_{\theta^*} (x \mid c) + \nabla_{x} \log P_{\theta^*} (c) - \nabla_{x} \log P_{\theta^*} (x))\]

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta (\nabla_{x} \log P_{\theta^*} (x \mid c) - \nabla_{x} \log P_{\theta^*} (x))\]

<p>Because in the diffusion model, each step has been approximated to a Gaussian distribution, therefore, the gradient of the log-likelihood is computed as follows:</p>

\[\nabla_{x} \log P_{\theta^*} (x) = \frac{1}{\sigma^2} (x - \mu)\]

<p>where \(\mu\) is the mean of the diffusion model, \(\sigma\) is the standard deviation of the diffusion model, and \(c\) is the concept.
Based on the repameterization trick, the gradient of the log-likelihood is correlated with the noise \(\epsilon\) at each step as follows:</p>

\[\epsilon_{\theta}(x_t,t) \propto \epsilon_{\theta^*} (x_t,t) - \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t))\]

<p>where \(\epsilon_{\theta}(x_t,t)\) is the noise at step \(t\) of the diffusion model after unlearning the concept \(c\).
Finally, to fine-tune the diffusion model from pretrained model \(\theta^*\) to new cleaned model \(\theta\), the authors proposed to minimize the following loss function:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[ \sum_{t=0}^{T-1} \left\| \epsilon_{\theta}(x_t,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(x_0\) is the input image sampled from data distribution \(\mathcal{D}\), \(T\) is the number of steps of the diffusion model.</p>

<p>Instead of recursively sampling the noise \(\epsilon_{\theta}(x_t,t)\) at every step, we can sample the time step \(t \sim \mathcal{U}(0, T-1)\) and then sample the noise \(\epsilon_{\theta}(x_t,t)\) at that time step.
Therefore, the loss function can be rewritten as follows:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[ \left\| \epsilon_{\theta}(x_t,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(t \sim \mathcal{U}(0, T-1)\).</p>

<p>However, in the paper, instead of using the above loss function, the author proposed to use the following loss function:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[  \left\| \epsilon_{\theta}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(t \sim \mathcal{U}(0, T-1)\) and \(x_t = G(x_T, t, c)\) is the generated image at the time step \(t\) with the concept \(c\).</p>

<p>The difference between the two loss functions is that the first loss function is computed based on the unconditional noise \(\epsilon_{\theta}(x_t,t)\) at the time step \(t\) while the second loss function is computed based on the noise \(\epsilon_{\theta}(x_t,c,t)\) at the time step \(t\) conditioned on the concept \(c\).</p>

<p>Interpretation of the loss function: By minimizing the above loss function, we try to force the conditional noise \(\epsilon_{\theta}(x_t,c,t)\) to be close to the unconditional noise \(\epsilon_{\theta^*} (x_t,t)\) of the original model.</p>

<p>Note: In the above objective function, \(x_t\) is the image from the training set \(\mathcal{D}\) at time step \(t\). However, as mentioned in the paper “We exploit the model’s knowledge of the concept to synthesize training samples, thereby eliminating the need for data collection”. Therefore, in the implementation, \(x_t\) is the image generated by the fine-tuned model at time step \(t\).</p>

<p>So the loss function in the implementation is as follows:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_t \sim P_{\theta}(. \mid c)} \left[  \left\| \epsilon_{\theta}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t) + \eta (\epsilon_{\theta^*}(x_t,c,t) - \epsilon_{\theta^*} (x_t,t)) \right\|^2 \right]\]

<p>where \(t \sim \mathcal{U}(0, T-1)\).</p>

<h2 id="our-idea">Our idea</h2>

<p>The idea is to utilize an auxiliary classifier to classify the concept (or set of concepts) and then use the gradient of the classifier to guide the unlearning process in the diffusion model.</p>

<p>The central optimization problem as follows:</p>

\[P_{\theta} (x) \propto \frac{P_{\theta^*} (x)}{P_{\phi} (c \mid x)^\eta}\]

<p>where \(P_{\phi} (c \mid x)\) is the likelihood of the concept \(c\) given the image \(x\) under the auxiliary classifier.</p>

<p>Similarly, the derivative w.r.t. \(x\) is computed as follows:</p>

\[\nabla_{x} \log P_{\theta} (x) \propto \nabla_{x} \log P_{\theta^*} (x) - \eta \nabla_{x} \log P_{\phi} (c \mid x)\]

<p>The loss function of the diffusion model as follows:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_0 \sim \mathcal{D}} \left[  \left\| \epsilon_{\theta}(x_t,t) - \epsilon_{\theta^*} (x_t,t) + \eta \nabla_{x} \log P_{\phi} (c \mid x_t) \right\|^2 \right]\]

<p>where \(t \sim \mathcal{U}(0, T-1)\).</p>

<p>The loss function of the classifier as follows:</p>

\[\mathcal{L}(\phi) = \mathbb{E}_{x \sim \mathcal{D}} \left[ \log P_{\phi} (c \mid x) \right] - \mathbb{E}_{z \sim \mathcal{N} } \left[ \log P_{\phi} (c \mid G(z,c)) \right] + \mathbb{E}_{z \sim \mathcal{N} } \left[ \log P_{\phi} (c \mid G(z)) \right]\]

<p>where \(G(z,c)\) is the generated image from the noise \(z\) and the concept \(c\).</p>

<p>The loss function of the classifier can be interpreted as follows: the classifier tries to distinguish between two sets of images: the set with concept \(c\) which is generated from the diffusion model and the set without concept \(c\) which is combined from the training set \(\mathcal{D}\) and the generated images from the diffusion model with null concept.</p>

<p>It is a worth noting that the objective of the classifier is also can be formulated as contrastive learning problem.</p>

<p>The loss function of the diffusion model can be interpreted as follows: the diffusion model tries to generate images that are similar to the images generated by the original model, but will not likely to be classified as the concept \(c\) by the classifier.</p>

<p><strong>Note</strong></p>

<!-- There is a mistake in the above idea. When calculating the loss function, the additional term is independent with diffusion parameter $$\theta$$, therefore, this term will not affect the optimization of the diffusion model. We need to find a way to connect the derivative w.r.t $$x$$ with the derivative w.r.t $$\theta$$. -->

<p>In the above equation, when calculating the loss function, the additional term is independent with diffusion parameter \(\theta\), which serves as a bias term.</p>

<h2 id="implemenetation-and-experiments-results">Implemenetation and Experiments’ Results</h2>

<h3 id="version-1">Version 1</h3>

<p>Using Jumping Prediction technique to approximate the output better.</p>

<p>Jumping Prediction equation:</p>

\[\tilde{x}_0 (x_t, \epsilon_\theta(x_t, t)) = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\]

<p>Given a prompt \(c_{era}\) (i.e., \(c_{era} = \text{'Cassette Player'}\)) and \(c_{tar}\) (i.e., \(c_{tar} = \text{'Null'}\)), we have the following generated at the time step \(t\):</p>

<ul>
  <li>sample of fine-tuned model with \(c_{era}\): \(x_{t, c_{era}} = G(\theta, x_T, c_{era})\)</li>
  <li>score w.r.t. \(c_{era}\) and \(\theta\): \(\epsilon_{\theta}(x_{t, c_{era}}, c_{era}, t)\)</li>
  <li>score w.r.t. \(c_{era}\) and \(\theta^*\): \(\epsilon_{\theta^*} (x_{t, c_{era}}, c_{era}, t)\)</li>
  <li>score w.r.t. \(c_{tar}\) and \(\theta^*\): \(\epsilon_{\theta^*} (x_{t, c_{era}}, c_{tar}, t)\)</li>
  <li>predicted sample from \(x_{t, c_{era}}\) with \(\theta\) and \(c_{era}\): \(\tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta}(x_{t, c_{era}}, c_{era}, t))\)</li>
  <li>predicted sample from \(x_{t, c_{era}}\) with \(\theta^*\) and \(c_{era}\): \(\tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta^*} (x_{t, c_{era}}, c_{era}, t))\)</li>
  <li>predicted sample from \(x_{t, c_{era}}\) with \(\theta^*\) and \(c_{tar}\): \(\tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta^*} (x_{t, c_{era}}, c_{tar}, t))\)</li>
</ul>

<p>The baseline objective function:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_{t, c_{tar}} \sim P_{\theta}(. \mid c_{tar})} \left[  \left\| \epsilon_{\theta}(x_{t, c_{era}}, c_{era}, t) - \epsilon_{\theta^*} (x_{t, c_{era}}, c_{tar}, t) + \eta (\epsilon_{\theta^*} (x_{t, c_{era}}, c_{era}, t) - \epsilon_{\theta^*} (x_{t, c_{era}}, c_{tar}, t)) \right\|^2 \right]\]

<p>The objective function with Jumping Prediction:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{x_{t, c_{tar}} \sim P_{\theta}(. \mid c_{tar})} \left[  \left\| \tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta}(x_{t, c_{era}}, c_{era}, t)) - \tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta^*} (x_{t, c_{era}}, c_{tar}, t)) + \eta (\tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta^*} (x_{t, c_{era}}, c_{era}, t)) - \tilde{x}_0 (x_{t, c_{era}}, \epsilon_{\theta^*} (x_{t, c_{era}}, c_{tar}, t))) \right\|^2 \right]\]

<h3 id="version-2">Version 2</h3>

<p>Using CLIP to align the output of the fine-tuned model with the prompts. Given three inputs: the prompt with erased concept \(c_{era}\) (i.e., <code class="language-plaintext highlighter-rouge">nudity</code> or <code class="language-plaintext highlighter-rouge">cassette player</code>), the prompt with targeted concept \(c_{tar}\) (i.e., <code class="language-plaintext highlighter-rouge">null</code>) and the generated image \(x_{0,era} = G(z_T, c_{era})\). We will have three corresponding embeddings \(E_T (c_{era})\), \(E_T (c_{tar})\) and \(E_I (x_{0,era})\) from the CLIP model. The goal is to align the embeddings \(E_I (x_{0, era})\) to the embeddings \(E_T (c_{tar})\) but not to the embeddings \(E_T (c_{era})\).</p>

<p>The loss function of the diffusion model as follows:</p>

\[\mathcal{L}_{\text{align}}(\theta) = \alpha * (1 - \text{sim} (E_I (x_{0, era}), E_T (c_{tar}))) + \beta * \mid \text{sim} (E_I (x_{0, era}), E_T (c_{era})) \mid\]

<h3 id="version-3">Version 3</h3>

<ul>
  <li>Using prompt like <code class="language-plaintext highlighter-rouge">Image of a cassette player but very blurry and not recognisable</code> to force the model to generate a blurry image then use this image as a target image. However, the model still generates a clear image of a cassette player. Even with the prompt <code class="language-plaintext highlighter-rouge">Image of Gaussian noise</code>, the model still generates a clear image like below</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_moo/000_000_curr_gaussian-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_moo/000_000_curr_gaussian-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_moo/000_000_curr_gaussian-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_moo/000_000_curr_gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/erasing_moo/000_000_pred_gaussian-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/erasing_moo/000_000_pred_gaussian-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/erasing_moo/000_000_pred_gaussian-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/erasing_moo/000_000_pred_gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Using textual inversion to learn a specific token of <code class="language-plaintext highlighter-rouge">Gaussian noise</code> and then use this token <code class="language-plaintext highlighter-rouge">S*</code> to generate a blurry image with the prompt, i.e., <code class="language-plaintext highlighter-rouge">Image of a cassette player with S*</code>.</li>
</ul>

<h3 id="version-4">Version 4</h3>

<p>The idea is using CLIP as a zero-shot classifier in the following optimization problem:</p>

\[P_{\theta} (x) \propto \frac{P_{\theta^*} (x)}{P_{\phi} (c \mid x)^\eta}\]

<p>where \(P_{\phi} (c \mid x) = \exp \max(0, \frac{1}{\beta}\text{sim}(E_I(x), E_T(c)))\)</p>

<p>where \(E_I(x), E_T(c)\) are the embeddings of the image \(x\) and the concept \(c\) from the CLIP model, respectively, and \(\beta\) is a hyperparameter.</p>

<p>The likelihood \(P_{\phi} (c \mid x)\) has the following properties:</p>

<ul>
  <li>If there is alignment between \(x\) and erased concept \(c\), then \(\text{sim}() &gt; 0\) and \(P_{\phi} (c \mid x) &gt; 1\).</li>
  <li>If there is no alignment between \(x\) and erased concept \(c\), then \(\text{sim}() &lt; 0\) and \(P_{\phi} (c \mid x) = 1\).</li>
</ul>

          </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
