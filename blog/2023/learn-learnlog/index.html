<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>A Learning Log | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Too much personal stuff, don't read it!">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2023/learn-learnlog/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "A Learning Log",
      "description": "Too much personal stuff, don't read it!",
      "published": "July 21, 2023",
      "authors": [
        {
          "author": "Tuan-Anh Bui",
          "authorURL": "https://tuananhbui89.github.io/",
          "affiliations": [
            {
              "name": "Monash University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>A Learning Log</h1>
        <p>Too much personal stuff, don't read it!</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="learning-targets">Learning targets</h2>

<p>This is a learning log to keep track of what I have learned each day. To avoid distraction and fluctuation, I will set some long-term learning targets that needed a disciplined and consistent effort to achieve.</p>

<ul>
  <li>#Research (including #AML, #GenAI): learn more about research, reading papers, especially in the field of adversarial machine learning and generative AI.</li>
  <li>#Coding: learn more about coding, especially data structures and algorithms.</li>
  <li>#F4T: food for thought, learn more about philosophy, history, and humanity.</li>
  <li>#Finance: learn more about finance, investment, and business.</li>
  <li>#Productivity: learn how to be more productive and effective in work and life. (effective != efficient, outcome != output, proactive &gt; active &gt; reactive, 1.01^365 = 37.8, 0.99^365 = 0.03)</li>
</ul>

<!-- Short-term targets (Updated on July 2023):

- #Coding: Textual Inversion and Dreambooth
- #Research: Writing a paper about TML in GenAI -->

<h2 id="2023-10-03">2023-10-03</h2>

<p>(#Coding) How to change Github Page theme to a new one?</p>

<p>I was quite happy with the <a href="https://academicpages.github.io/" rel="external nofollow noopener" target="_blank">AcademicPages theme</a> which is more than enough to show Publications or simple Blog posts. However, when I want to blog more seriously, the AcademicPages theme shows some limitations (or maybe because I didn’t dig deep enough). For example, it does not support image captioning, the table of contents is not automatically generated, the code block is also not highlighted properly, etc. Therefore, I tried to change to new theme, and, I had been falled in love with the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio theme</a> which has all the features that I need. The only problem is that I don’t know how to change the theme without losing all the previous posts. The installing instruction is quite simple but just for a new blog created from scratch. After some googling and trying, I found a solution that works for me. Here are the steps:</p>

<ul>
  <li>clone the new theme to a new folder, for example, <code class="language-plaintext highlighter-rouge">al-folio</code>
</li>
  <li>manually convert all the posts from the old theme to the new theme. Yeah, it’s painful but I don’t know any other way to do it manually. The <code class="language-plaintext highlighter-rouge">post</code> layout in <code class="language-plaintext highlighter-rouge">al-folio</code> is a bit different with that in <code class="language-plaintext highlighter-rouge">AcademicPages</code>
</li>
  <li>run <code class="language-plaintext highlighter-rouge">bundle exec jekyll serve</code> to check if everything is ok</li>
  <li>run <code class="language-plaintext highlighter-rouge">bundle exec jekyll build --lsi</code> to build the site. All the contents will be generated in the <code class="language-plaintext highlighter-rouge">_site</code> folder.</li>
  <li>remove all the contents belong to the old theme in the <code class="language-plaintext highlighter-rouge">master</code> branch and copy all the contents in the <code class="language-plaintext highlighter-rouge">_site</code> folder to the <code class="language-plaintext highlighter-rouge">master</code> branch.</li>
  <li>push the <code class="language-plaintext highlighter-rouge">master</code> branch to Github and wait for a few minutes for the Github Page to be updated.</li>
</ul>

<p>(#Coding) How to create a draft post in Jekyll? <a href="https://www.hongkiat.com/blog/jekyll-draft/" rel="external nofollow noopener" target="_blank">Source</a></p>

<ul>
  <li>Create a <code class="language-plaintext highlighter-rouge">_drafts</code> folder in the root directory of the Github Page.</li>
  <li>Create a new post in the <code class="language-plaintext highlighter-rouge">_drafts</code> folder. The post don’t need to have a date in the file name.</li>
  <li>Run <code class="language-plaintext highlighter-rouge">bundle exec jekyll serve --drafts</code> to serve the draft posts.</li>
</ul>

<p>(#Coding) 75 Leetcode Questions</p>

<p>I start a challenge to complete 75 Leetcode questions in less than 75 days. I will log my progress in <a href="https://tuananhbui89.github.io/projects/project_75_leetcode_questions/">this blog post</a>.</p>

<h2 id="2023-09-28">2023-09-28</h2>

<p>(#Finance) <a href="https://youtu.be/mVhe8WCL9p0?si=-cnGiCjmQflEZVgY" rel="external nofollow noopener" target="_blank">The First Home Super Saver Scheme is the best way to save for a deposit</a> by Kuan Tian</p>

<ul>
  <li>The First Home Super Saver Scheme (FHSSS) is a scheme that allows you to save money for your first home inside your superannuation fund. The benefit of this scheme is that you can save money faster because of the tax benefit of superannuation fund.</li>
  <li>The maximum amount of money that you can save is 50K AUD as increased since 2020.</li>
  <li>The maximum amount of money that you can withdraw is 15K AUD per contribution year. It means that to withdraw the maximum amount of money, you have to contribute to your superannuation fund for 4 years.</li>
  <li>The benefit via this scheme can be around $11K AUD for a single person contributing for 4 years compared to saving money in a bank account.</li>
  <li>A couple can save up to 100K AUD via this scheme and get a benefit of around $$22K AUD.</li>
</ul>

<p>(#Finance) <a href="https://youtu.be/UzBMiikbKuA?si=3b89Eq8qds74sSiw" rel="external nofollow noopener" target="_blank">FIRE for Aussies</a> by Kuan Tian</p>

<ul>
  <li>FIRE stands for Financial Independence, Retire Early.</li>
  <li>Unlike in US or UK, Aussies can take advantage of the superannuation fund to retire early. It is because the superannuation fund is a tax-advantage fund with a tax rate of 15% compared to the marginal tax rate of 32.5% for the income between 45K AUD and 120K AUD.</li>
  <li>The superannuation fund can be accessed when you reach the preservation age (between 55 and 60 depending on your date of birth).</li>
  <li>You can contribute up to 25K AUD per year to your superannuation fund, including the employer contribution. For example, a Research Fellow with a salary of 100K AUD per year and 17% employer contribution can self contribute up to 8K AUD per year and get a tax benefit of 17.5% of 8K AUD = 1.4K AUD.</li>
  <li>There are tax-free components and taxable components in the super. When withdrawing money from the super, the tax-free components are mostly tax-free, while the taxable components are taxed depending on several factors such as type of withdrawal (lump sum or income stream), age, the element taxed or untaxed, etc.</li>
  <li>Take-away advice: Contribute as much as you can to your superannuation fund.</li>
</ul>

<p>(#Finance) <a href="https://youtu.be/k8MWPTdy7dw?si=lhCfNgJ2AF83i2Gl" rel="external nofollow noopener" target="_blank">How to reduce tax via property investment</a> by Kuan Tian again. The third video in just one morning, I am falling into a rabbit hole because his videos are really good. (Who doesn’t love <img class="emoji" title=":money_with_wings:" alt=":money_with_wings:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4b8.png" height="20" width="20">, especially a procrastinated almost-graduated PhD student <img class="emoji" title=":joy:" alt=":joy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png" height="20" width="20">)</p>

<ul>
  <li>The main idea is to use the tax benefit of negative gearing to reduce the tax.</li>
  <li>Negative gearing is a tax benefit that allows you to deduct the loss from your investment property from your taxable income. For example, if you have a salary of 100K AUD per year and a loss of 10K AUD from your investment property, then your taxable income is 90K AUD.</li>
  <li>The loss from your investment property can be the interest of your loan, the cost of maintenance, etc, minus the rental income.</li>
  <li>This strategy is good for high-income earners especially when you are in the highest tax bracket (45% for the income above 180K AUD).</li>
</ul>

<h2 id="2023-09-27">2023-09-27</h2>

<p>(#Research) A new perspective on the motivation of VAE (by Dinh)</p>

<ul>
  <li>Assume that \(x\) was generated from \(z\) through a generative process \(p(x \mid z)\).</li>
  <li>Before observing \(x\), we have a prior belief about \(z\), i.e., \(z\) can be sampled from a Gaussian distribution \(p(z) = \mathcal{N}(0, I)\).</li>
  <li>After observing \(x\), we want to correct our prior belief about \(z\) to a posterior belief \(p(z \mid x)\).</li>
  <li>However, we cannot directly compute \(p(z \mid x)\) because it is intractable. Therefore, we use a variational distribution \(q(z \mid x)\) to approximate \(p(z \mid x)\). The variational distribution \(q(z \mid x)\) is parameterized by an encoder \(e(z \mid x)\). The encoder \(e(z \mid x)\) is trained to minimize the KL divergence between \(q(z \mid x)\) and \(p(z \mid x)\). This is the motivation of VAE.</li>
</ul>

<p>Mathematically, we want to minimize the KL divergence between \(q_{\theta} (z \mid x)\) and \(p(z \mid x)\):</p>

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log \frac{q_{\theta} (z \mid x)}{p(z \mid x)} \right] = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(z \mid x) \right]\]

<p>Applying Bayes rule, we have:</p>

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(x \mid z) - \log p(z) + \log p(x) \right]\]

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(x \mid z) - \log p(z) \right] + \log p(x)\]

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = - \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log p(x \mid z) \right] + \mathcal{D}_{KL} \left[ q_{\theta} (z \mid x) \parallel p(z) \right] + \log p(x)\]

<p>So, minimizing \(\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) )\) is equivalent to maximizing the ELBO: \(\mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log p(x \mid z) \right] - \mathcal{D}_{KL} \left[ q_{\theta} (z \mid x) \parallel p(z) \right]\).</p>

<p>Another perspective on the motivation of VAE can be seen from the development of the Auto Encoder (AE) model.</p>

<ul>
  <li>The AE model is trained to minimize the reconstruction error between the input \(x\) and the output \(\hat{x}\).</li>
  <li>The AE process is deterministic, i.e., given \(x\), the output \(\hat{x}\) is always the same.</li>
  <li>Therefore, the AE model does not have contiguity and completeness properties as desired in a generative model.</li>
  <li>To solve this problem, we change the deterministic encoder of the AE model to a stochastic encoder, i.e., instead of mapping \(x\) to a single point \(z\), the encoder maps \(x\) to a distribution \(q_{\theta} (z \mid x)\). This distribution should be close to the prior distribution \(p(z)\). This is the motivation of VAE.</li>
</ul>

<h2 id="2023-09-23">2023-09-23</h2>

<p>(#Finance) Reading <a href="https://news.tonydinh.com/p/my-solopreneur-story-zero-to-45kmo" rel="external nofollow noopener" target="_blank">My solopreneur story: zero to $$45K/mo in 2 years</a></p>

<p>This is a very inspring story about how Tony Dinh built his side project to a profitable software product and become a solopreneur. Some key takeaways:</p>

<ul>
  <li>The unfair advantage: What is the unfair advantage? To me, it is the thing that you can do better than others in the same field and it is hard for others to catch up. In Tony’s case when the goal is to build a profitable software product, at first I thought that his unfair advantage is his software engineering skills. It is no doubt that Tony has a great skill set that enables him to build a product really fast. However, when reading his story again, I realized that his true unfair advantage is his ability to write and share his knowledge and experiences and from that build up his follower-community. It can be seen that his very first product DevUtils started to fly when he discovered Twitter and started to share funny interesting stuff on that platform. Compared to other software engineers who might have the same tech skills as Tony, Tony has a much better skill in writing and sharing, while compared to other writers, Tony has a much better skill in software engineering. So his unfair advantage in getting a profitable software product is the combination of these two skill sets. So what is my unfair advantage? In the context of being a researcher? I am still on the way to find out.</li>
  <li>Start small, commit to the process, and be consistent. Tony’s first “real” business - Black Magic is a very interesting story and a good demonstration for this lesson. The product at first was very simple which is just a bar on Twitter’s avatar that shows progress on getting 1k followers. He did that because of a very human reason: to celebrate of getting hist first 1k followers. It turned out that people loved this idea and even happy to pay for the subscription. Tony started to get recurring revenue from that. However, he didn’t stop there. He kept improving the product and adding more features to it that help users create more engagements. The product became more and more complex and useful that attracted more users. After just few months, the Black Magic from a fun side project became a profitable business generate 4K USD per month. This story brings me a lot of thoughts. First, think big but start small and do not wait to be perfect. Second, be committed and consistent to the process.</li>
</ul>

<p>(#F4T) <a href="https://mcdonnellcallum.medium.com/naval-ravikant-how-to-get-rich-without-getting-lucky-e8d32bba14d1" rel="external nofollow noopener" target="_blank">How to get rich (without getting lucky) by Naval Ravikant</a>. Some I like with my current knowledge and experience:</p>

<ul>
  <li>You will get rich by giving society what it wants but does not yet know how to get. At scale.</li>
  <li>Play iterated games. All the returns in life, whether in wealth, relationships, or knowledge, come from compound interest.</li>
  <li>Arm yourself with specific knowledge, accountability, and leverage.</li>
  <li>About specific knowledge:
    <ul>
      <li>Specific knowledge is knowledge that you cannot be trained for. If society can train you, it can train someone else, and replace you.</li>
      <li>When specific knowledge is taught, it’s through apprenticeships, not schools.</li>
      <li>Specific knowledge is often highly technical or creative. It cannot be outsourced or automated.</li>
    </ul>
  </li>
  <li>Work as hard as you can. Even though who you work with and what you work on are more important than how hard you work.</li>
</ul>

<p>(#F4T) Less is More principle</p>

<h2 id="2023-09-22">2023-09-22</h2>

<p>(#Research) On Reading: FLOW MATCHING FOR GENERATIVE MODELING (ICLR 2023)</p>

<ul>
  <li>Link to the paper: <a href="https://openreview.net/pdf?id=PqvMRDCJT9t" rel="external nofollow noopener" target="_blank">https://openreview.net/pdf?id=PqvMRDCJT9t</a>
</li>
  <li>Link to my blog post: <a href="https://tuananhbui89.github.io/blog/2023/flowmatching/">https://tuananhbui89.github.io/blog/2023/flowmatching/</a>
</li>
</ul>

<h2 id="2023-09-17">2023-09-17</h2>

<p>(#Research) Parameter-Efficient Fine-Tuning. <a href="https://youtu.be/StdrAJZsmw4?si=1lVR7I_we48_DUhJ" rel="external nofollow noopener" target="_blank">Link to Youtube video</a></p>

<p>(#Research) Tree of Thoughts: Deliberate Problem Solving with Large Language Models. <a href="https://youtu.be/ut5kp56wW_4?si=i9ZVAMMBJ0PZpRao" rel="external nofollow noopener" target="_blank">Link to Yannic’s video</a></p>

<ul>
  <li>From Chain-of-Thought, Self-Consistency CoT, to Tree of Thoughts.</li>
  <li>Intuition: LLMs are good at self-evaluation given a specific goal, for example, given a set of several intermediate thoughts, which one is the best to achieve the goal. So, we can integrate classical tree search algorithm (BFS, DFS, etc.) with LLMs to find the best path to achieve the goal.</li>
  <li>The paper is also good at experimental design when choosing specific tasks to best fit to the algorithm, i.e., Ga</li>
</ul>

<h2 id="2023-09-14">2023-09-14</h2>

<p>(#Research) On Reading: Diffusion Models Beat GANs on Image Synthesis. Link to <a href="https://tuananhbui89.github.io/blog/2023/conditional-diffusion/">blog post</a></p>

<h2 id="2023-09-08">2023-09-08</h2>

<p>(#Idea) Erasing a concept from a generative model by given a set of images.</p>

<h2 id="2023-09-04">2023-09-04</h2>

<p>(#Research) Chasing General Intelligence by <a href="https://ruishu.io/" rel="external nofollow noopener" target="_blank">Dr. Rui Shu</a> (OpenAI) - Guest lecture at Monash FIT 3181 - Deep Learning Unit.</p>

<p>Disclaimer: Rui gave a great talk and I want to take notes on it. However, because of detaching from the context, this note not necessarily reflects Rui’s opinions.</p>

<p><strong>Part 1: Rui’ Research Journey:</strong></p>

<ul>
  <li>Inspired by the Kingma’s VAE paper (2014) (style-content disentanglement)</li>
  <li>First research project: Extend VAE to semi-supervised learning setting.</li>
  <li>Second research project: Domain Adaptation: DIRT-T</li>
  <li>First thought: Take a field people care about and then make some sort of improvement. Safe approach in research.</li>
</ul>

<p><strong>Part 2: What is going wrong?</strong></p>

<ul>
  <li>Why those things work?</li>
  <li>NNs generalization makes zero sense:
    <ul>
      <li>
<a href="https://openreview.net/forum?id=Sy8gdB9xx" rel="external nofollow noopener" target="_blank">Understanding Deep Learning requires rethinking generalization</a> (Zhang et al. 2017): You can train on random labels and the model still clusters the data sensibly before fitting to random labels.</li>
      <li>Second thought: What are other fields in ML that works but not in the way we think it works? Style-content disentanglement.</li>
      <li><a href="https://arxiv.org/abs/1811.12359" rel="external nofollow noopener" target="_blank">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</a></li>
    </ul>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/ruishu_talk/disentanglement-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/ruishu_talk/disentanglement-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/ruishu_talk/disentanglement-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/ruishu_talk/disentanglement.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Style-content disentanglement (Image from Rui's talk
</div>

<p><strong>Part 3:</strong></p>

<p>Yang Song’s paper on estimating gradient of data distribution to learn generative model</p>

<ul>
  <li><a href="https://arxiv.org/abs/1907.05600" rel="external nofollow noopener" target="_blank">Generative Modeling by Estimating Gradients of the Data Distribution (Song et al. 2019)</a></li>
  <li>Sensitive to the choice of the model architecture which makes Rui thought that it might not going to far compared to VAEs or GANs. And he then really regretted about this thought. Because this paper is the foundation of the current hype of Generative Models, i.e., Diffusion Models.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/ruishu_talk/diffusion-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/ruishu_talk/diffusion-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/ruishu_talk/diffusion-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/ruishu_talk/diffusion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Thought on Diffusion Model (Image from Rui's talk)
</div>

<p>Rui’s thought after his lesson on Diffusion Model:</p>

<ul>
  <li>Just because you don’t understand it, doesn’t mean you can’t use it.</li>
  <li>Sometimes you gotta be a bit more open-minded and appreciate methods which have shown empirical success.</li>
  <li>But also be critical and not open-minded too much.</li>
</ul>

<p><strong>Where are we now?</strong></p>

<ul>
  <li>Image gen is quite good now, what about text? Transformer paper.</li>
  <li>Transformer powers GPT-4, and change the mantra “Garbage in, garbage out” to “Garbage in, Diamond out”.</li>
  <li>We are really close to AGI but not solve yet. GPT-4, ChatGPT sometime shows responses that look like it has reasoning ability but it is not really.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/ruishu_talk/gpt-drawing-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/ruishu_talk/gpt-drawing-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/ruishu_talk/gpt-drawing-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/ruishu_talk/gpt-drawing.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    GPT-4 Drawing (Image from Rui's talk)
</div>

<ul>
  <li>The last step before AGI: Reasoning? But it is the hardest step.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2309/ruishu_talk/the-last-step-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2309/ruishu_talk/the-last-step-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2309/ruishu_talk/the-last-step-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2309/ruishu_talk/the-last-step.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The last step before AGI (Image from Rui's talk)
</div>

<p><strong>How to get there?</strong></p>

<ul>
  <li>The large everything model
    <ul>
      <li>Multimodality: Image, Text, Speech, Audio, Sensor data, etc.</li>
      <li>How we design all types of data modalities into a single model?</li>
      <li>Which requires a lot of engineering effort.</li>
    </ul>
  </li>
  <li>The future of data synthesis:
    <ul>
      <li>Using data generated by the model and then improve the model itself, <strong>after filter</strong> (or Human-in-the-loop). Including human knowledge into the process is the key! (Yan Lecun also mentioned that Language model will never achieve AGI but Language model with human-in-the-loop (RFHL) is other thing).</li>
    </ul>
  </li>
</ul>

<p><strong>The inferefence heavy future?</strong></p>

<ul>
  <li>So important research direction is to make model’s inference more efficient.</li>
</ul>

<p><strong>Final Advice</strong></p>

<ul>
  <li>Learn probability theory and applied probability. (Build a strong background/foundation)</li>
  <li>Learn experimental design and how to science (e.g., Adversarial examples are not bugs, they are features. Where the authors proposed way to explore the way model making prediction based on invisible features)</li>
  <li>Familiar with working with big codebase (Learn modern skills)</li>
  <li>Keep thing simple (inspired by Rich Sutton’s The Bitter Lesson): The human-knowledge approach tends to complicated methods in the way that makes them less suited to leverage the power of computation. The simple approach which can be scaled up to large data and large computation is better.</li>
</ul>

<p>(#Research) Connection between Latent Diffusion formulation and the Rate-Distortion theory (Trung’s idea). Below are some personal notes for memorization without leaking the idea.</p>

<ul>
  <li>The Rate-Distortion theory is a theory that describes the trade-off between the rate of the latent code and the distortion of the reconstruction. The higher the rate, the more information of \(x\) is preserved in \(z\). However, if the rate is high, it lessen the generalization ability of the \(\log p(x \mid z, \theta)\).</li>
  <li>In latent diffusion formulation, the distortion \(D\) is measured on the first latent code \(z_1\) encoded from \(x\) by the encode-decoder architecture.</li>
  <li>The rate \(R\) measures the different between the forward process and the backward process in diffusion model, eventually it is the matching term that trains the diffusion model.</li>
  <li>There is a closed-form of the mutual information \(I(Z,X)\). We can utilize this formulation to design a specific regularization term to control the rate-distortion trade-off benefiting downstream tasks.</li>
</ul>

<h2 id="2023-09-01">2023-09-01</h2>

<p>(#Research) On reading: <a href="https://openreview.net/forum?id=eWtMdr6yCmL" rel="external nofollow noopener" target="_blank">TRADING INFORMATION BETWEEN LATENTS IN HIERARCHICAL VARIATIONAL AUTOENCODERS</a> published on ICLR 2023.</p>

<p>Revisit Rate-Distortion trade-off theory:</p>

<ul>
  <li>Problem setting of Rate-Distortion trade-off
    <ul>
      <li>How to learn a “useful” representation of data for downstream tasks?</li>
      <li>Using powerful encoder-decoder such as VAE, PixelCNN, etc. can easily ignore \(z\) and still obtain high marginal likelihood \(p(x \mid \theta)\). Therefore, we need to use a regularization term to encourage the encoder to learn a “useful” representation of \(z\), for example, as in Beta-VAE.</li>
    </ul>
  </li>
</ul>

<p>Rate distortion theory?</p>

\[H - D \leq I(z,x) \leq R\]

<p>where \(H\) is the entropy of data \(x\) and \(D\) is the distortion of the reconstruction \(x\) from \(z\). \(R\) is the rate of the latent code \(z\) (e.g., compression rate).</p>

<p>\(R = \log \frac{e(z \mid x)}{m(z)}\) where \(e(z \mid x)\) is the encoder and \(m(z)\) is the prior distribution of \(z\). The higher the rate, the more information of \(x\) is preserved in \(z\). However, if the rate is high, it lessen the generalization ability of the \(\log p(x \mid z, \theta)\).</p>

<p>The mutual information has upper bound by the rate of the latent code \(z\). For example, if \(R=0\) then \(I(z,x)=0\). This is because \(e(z \mid x) = m(z)\), which means that the encoder cannot learn anything from the data \(x\).</p>

<p>Motivation of the paper:</p>

<ul>
  <li>Reconsider the rate distortion theory in the context of hierarchical VAEs where there are multiple levels of latent codes \(z_1, z_2, \dots, z_L\).</li>
  <li>The authors proposed a direct links between the input \(x\) and the latent codes \(z_1, z_2, \dots, z_L\). With this architecture, they can decompose the total rate to the rate of each latent code \(z_1, z_2, \dots, z_L\). Unlike the standard hierarchical VAEs, where the rate of each latent code is not directly related to the input \(x\) but the previous latent code \(z_{l-1}\).</li>
  <li>Then they can control the rate of each latent code.</li>
</ul>

<p>Standard hierarchical VAEs:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/230901/standard-hvae-objective-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/230901/standard-hvae-objective-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/230901/standard-hvae-objective-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/230901/standard-hvae-objective.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Standard hierarchical VAEs
</div>

<p>Generalized Hierarchical VAEs:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/230901/all-three-hvae-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/230901/all-three-hvae-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/230901/all-three-hvae-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/230901/all-three-hvae.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Generalized Hierarchical VAEs
</div>

<h2 id="2023-08-30">2023-08-30</h2>

<p>(#Research) NVIDIA event: Transforming Your Development and Business with Large Language Models</p>

<ul>
  <li>Link to the event: <a href="https://transformingthefuturelargelang.splashthat.com/" rel="external nofollow noopener" target="_blank">https://transformingthefuturelargelang.splashthat.com/</a>
</li>
  <li>Speaker: Dr. Ettikan Kandasamy Karuppiah and Dr. Johan Barthelemy</li>
</ul>

<p>Introduction, Demystifying LLM and Data Curation</p>

<ul>
  <li>Except researchers, now almost everyone (developers) just care about API provided by Big Tech, i.e., OpenAI ChatGPT API?</li>
  <li>GenAI Ecosystem: Language, Media, Biology, Tools and Platforms (1600+ companies building their GenAI on Nvidia platform)</li>
  <li>Ratio of various data that LLMs are trained on. <a href="https://arxiv.org/abs/2303.18223" rel="external nofollow noopener" target="_blank">Ref</a>.</li>
</ul>

<p>LLM Training and Inference at Scale. Customized LLM with Prompt-Learning</p>

<ul>
  <li>Why do data curation? E.g., Duplication, Low quality, Bad unicode</li>
  <li>NeMo provides tools to quality filtering or reformating.</li>
  <li>Currently, the data curator works on CPUs only.</li>
  <li>Data Blending: Learnings from Bloomberg GPT, use specific data, even small amount of data does help alot to customize to specific domain, application (i.e., Bloomberg GPT for finance)</li>
  <li>3D parallelism techniques: Data parallelism, Tensor and Pipeline parallelism, Sequence parallelism or Selective Adaptative Recomputation?</li>
  <li>Auto-Configuarator tool: Auto search and optimize model configurations on any given compute or time constraints. (something like recommendation given an experience precomputed on a common used model, i.e., GPT3, not something like can optimize on-time, e.g., changing learning rate, optimizer based on data and current performance).</li>
  <li>Most important part of the tool is supporting model customization (see the slide)
    <ul>
      <li>Prompt learning: Prompt tuning, p-tuning, tune companion model</li>
      <li>prompt engineering: few-shot learning, chain-of-thought reasoning</li>
      <li>Parameter efficient fine-tuning: LoRA, IA3, Adapters</li>
      <li>Fine-tuning: SFT, RLHF</li>
    </ul>
  </li>
</ul>

<p>Questions:</p>

<ul>
  <li>What is Nvidia platform that alot companies based on?</li>
  <li>If every developers use the same API, where is the difference between them? If using the same suggestions from GenerativeAI?</li>
  <li>Free GPU? Does Nvidia provide any free GPU or support for research?</li>
</ul>

<p>Nvidia Framework:</p>

<ul>
  <li>NeMo <a href="https://developer.nvidia.com/nemo" rel="external nofollow noopener" target="_blank">https://developer.nvidia.com/nemo</a>
</li>
  <li>BioNeMo</li>
  <li>Picasso</li>
</ul>

<h2 id="2023-08-26">2023-08-26</h2>

<p>(#F4T) Review 10 best ideas/concepts from Charlie Munger. Link to the blog post: <a href="https://tuananhbui89.github.io/blog/2023/f4t/">https://tuananhbui89.github.io/blog/2023/f4t/</a></p>

<h2 id="2023-08-25">2023-08-25</h2>

<p>(#Research) Data-Free Knowledge Distillation</p>

<ul>
  <li>Reference: <a href="https://arxiv.org/abs/2011.14779" rel="external nofollow noopener" target="_blank">Data-Free Model Extraction</a>
</li>
  <li>What is Data-Free KD? It is a method to transfer knowledge from a teacher model to a student model without using any data. The idea is learn a generator that can generate synthetic data that is similar to the data from the teacher model. Then, we can use the synthetic data to train the student model.
\(L_S = L_{KL} (T(\hat{x}), S(\hat{x}))\)</li>
</ul>

<p>Where \(T(\hat{x})\) is the teacher model and \(S(\hat{x})\) is the student model. \(\hat{x}\) is the synthetic data generated by generator \(G\).</p>

\[L_G = L_{CE} (T(\hat{x}), y) - L_{KL} (T(\hat{x}), S(\hat{x}))\]

<p>Where \(y\) is the label of the synthetic data. Minimizing first term encourages the generator generate data that fall into the target class \(y\), while maximizing the second term encourages the generator generate diverse data? 
Compared to GAN, we can think both teacher and student models are acted as discriminators.</p>

<p>This adversarial game need to intergrate to the training process in each iteration. For example, after each iteration, you need to minimizing \(L_G\) to generate a new synthetic data. And then using \(\hat{x}\) to train the student. This is to ensure that the synthetic data is new to the student model.
Therefore, one of the drawbacks of DFKD is that it is very slow.</p>

<!-- Tuan (Henry)' work on improving Data-Free KD:

- Introducing noisy layer which is a linear layer that transforms the input (label-text embedding vector from CLIP) before feeding to the generator as previous work. (Input -> Noisy Layer -> Generator -> Teacher/Student -> $$L_G$$).
- One important point is that the Noisy layer need to reset its weight every time we generate a new batch of synthetic data (while fixing the generator). This is to ensure the diversity of the synthetic data.
- One interesting finding is that the noisy layer can be applied to all kinds of label-text embedding from different classes, while if using individual noise layers for each class, the performance is worse. -->

<h2 id="2023-08-21">2023-08-21</h2>

<p>(#Research) On reading: <a href="https://arxiv.org/abs/2305.13948" rel="external nofollow noopener" target="_blank">Decoupled Kullback-Leibler Divergence Loss</a>.</p>

<ul>
  <li>Paper link: <a href="https://arxiv.org/abs/2305.13948" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2305.13948</a>
</li>
  <li>Main idea: Minimize the difference between two logit distributions (weighted MSE loss)</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/AML/DecoupledKL-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/AML/DecoupledKL-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/AML/DecoupledKL-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/AML/DecoupledKL.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Decoupled Kullback-Leibler (DKL) <a href="https://arxiv.org/abs/2305.13948" rel="external nofollow noopener" target="_blank">(reference)</a>.
</div>

<h2 id="2023-08-20">2023-08-20</h2>

<p>(#Research) On reading: <a href="https://arxiv.org/abs/2207.12598" rel="external nofollow noopener" target="_blank">Classifier-Free Diffusion Guidance</a>.</p>

<ul>
  <li>Paper link: <a href="https://arxiv.org/abs/2207.12598" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2207.12598</a>
</li>
  <li>Motivation: Training a diffusion model to generate images from a specific class.
Dhariwal \&amp; Nichol (2021) proposed classifier guidance method, which uses an auxiliary classifier</li>
  <li>Previous work (i.e., <a href="https://arxiv.org/abs/2105.05233" rel="external nofollow noopener" target="_blank">Classifier Guided Diffusion</a>) train a classifier \(f_\phi (y \mid x_t,t)\) on noisy images x and use the gradient \(\nabla_{x} log f_{\phi}(x_t)\) to guide the sampling process towards the target class y. However, this method requires a classifier jointly trained with the diffusion model.</li>
</ul>

<p>(#Idea) Mixup Class-Guidance Diffusion model.</p>

<ul>
  <li>Main idea: Train a diffusion model that can generate not only images from a specific class but also images from a mixup of two classes. It can be applied to Continual Learning setting as in <a href="https://openreview.net/pdf?id=RlqgQXZx6r" rel="external nofollow noopener" target="_blank">DDGR</a> or Domain Generalization setting.</li>
  <li>Nice way to generate mixup labels with just y and lambda. Normally, we need to have two classes \(y_i,y_j\) and a parameter \(\lambda\) to generate a mixup label \(y_{mixup} = \lambda y_i + (1-\lambda) y_j\). However, we can have another way to generate mixup label, with one label \(y\), and one parameter \(\gamma\) to control the mixup ratio. For example, given a set of all labels \(Y=\{ y\_i \}\_{i=1}^N\), we can arrange them in a circle, and then we can generate a mixup label \(y_{mixup}\) by moving \(\gamma\) steps clockwise from \(y\). What is the benefit of this method?
    <ul>
      <li>Recall the equation in Classifier-Free Guidance paper 
\(\nabla_{x_t} \log p(y|x_t)=\nabla_{x_t} \log p(x_t|y) - \nabla_{x_t} \log p(x_t)\), 
where \(\nabla\_{x\_t} \log p(y \mid x\_t)\) is the gradient of the implicit classifier.</li>
      <li>In case of mixup label, we have \(\nabla_{x_t} \log p(y_{mixup}|x_t)=\nabla_{x_t} \log p(x_t|y_{mixup}) - \nabla_{x_t} \log p(x_t)\), 
where \(y_{mixup} = \gamma y_i + (1-\gamma) y_j\), which requires two gradients \(\nabla_{x_t} \log p(y_{i}  \mid x_t)\) and \(\nabla_{x_t} \log p(y_{j} \mid x_t)\).</li>
    </ul>
  </li>
  <li>Some observations/ideas from other work can be applied:
    <ul>
      <li>
<a href="https://proceedings.mlr.press/v202/dinh23a/dinh23a.pdf" rel="external nofollow noopener" target="_blank">PixelAsParam: A Gradient View on Diffusion Sampling with Guidance</a>: In this paper, the authors observed that the gradient of the auxiliary classifier \(\nabla_{x_t} \log p(y \mid x_t)\) and the gradient of the denoising process \(\nabla_{x_t} \log p(x_t)\) are conflicting (refer to Figure 2). It can be interpreted as the gradient of the auxiliary classifier is trying to move the sample towards a target class, while the gradient of the denoising process is trying to make the sample more diverse, i.e., their goals are contradictory. In this paper, they applied a multi-objective optimization method to project the gradient of auxiliary classifier onto a direction that is less conflicting. The result is that the generated images are in target class but still diverse. (better in both FID - distinguish between synthetic and real images - and Inception Score - distinguish between different classes of synthetic images). The time when the conflicting happens is also interesting. It is more conflicting at the beginning of the sampling process, and then it becomes less conflicting.</li>
      <li>Mixing between background and foreground using FFT/IFFT transformation.</li>
    </ul>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion/confliction_gradient_diffusion-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion/confliction_gradient_diffusion-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion/confliction_gradient_diffusion-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion/confliction_gradient_diffusion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Confliction of gradients <a href="https://proceedings.mlr.press/v202/dinh23a/dinh23a.pdf" rel="external nofollow noopener" target="_blank">(reference)</a>.
</div>

<h2 id="2023-08-19">2023-08-19</h2>

<p>(#Coding) How to show an image in Github page. Reference to this post: <a href="https://tuananhbui89.github.io/blog/2023/learn-code/">https://tuananhbui89.github.io/blog/2023/learn-code/</a></p>

<h2 id="2023-08-18">2023-08-18</h2>

<p>(#Research) On Reading: DDGR: Continual Learning with Deep Diffusion-based Generative Replay</p>

<ul>
  <li>Paper link: <a href="https://openreview.net/pdf?id=RlqgQXZx6r" rel="external nofollow noopener" target="_blank">https://openreview.net/pdf?id=RlqgQXZx6r</a>
</li>
  <li>Problem setting: Continual learning, more specific, task incremental learning. For example, CIFAR100 dataset with new task is each 5 classes to be learned sequentially.</li>
  <li>Questions:
    <ul>
      <li>Do we know the number of tasks in advance?</li>
      <li>Do we need to change the model architecture for each task?</li>
      <li>Can we access the data from previous tasks? Normally, we cannot. But there is a variant of CL called CL with repitition (or replay) where we can access the data from previous tasks but in different forms.</li>
    </ul>
  </li>
  <li>Main Idea: Using Class-Guidance Diffusion model to generate data from previous tasks to be used to train classifier and also reinforce the generative model.</li>
  <li>The main point is that the Diffusion model has some advantages over GAN or VAE in specific CL setting:
    <ul>
      <li>It does not have mode collapse problem compared to GAN.</li>
      <li>It can generate (overfitting) data from previous tasks quite well.</li>
      <li>It has class-guidance mechanism that can guild diffusion model to learn new distribution from new task and not overlapping with previous tasks. (so generator doesn’t face a serious catastrophic forgetting problem)</li>
    </ul>
  </li>
</ul>

<p>(#Idea) We can use Class-Guidance Diffusion model to learn mixup data and then can use that model to generate not only data from pure classes but also from mixup classes. It is well accepted that mixup technique can improve the generalization of classifier, so it can be applied to CL setting as well.</p>

<h2 id="2023-08-17">2023-08-17</h2>

<p>(#Code) How to disable NSFW detection in Huggingface.</p>

<ul>
  <li>context: I am trying to generate inappropriate images using Stable Diffusion with prompts from the I2P benchmark. However, the NSFW detection in Huggingface is too sensitive and it filters out all of the images, and return a black image instead. Therefore, I need to disable it.</li>
  <li>solution: modify the pipeline_stable_diffusion.py file in the Huggingface library. just return image and None in the run_safety_checker function.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># line 426 in the pipeline_stable_diffusion.py
</span><span class="k">def</span> <span class="nf">run_safety_checker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="bp">None</span>

    <span class="c1"># The following original code will be ignored
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">safety_checker</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">has_nsfw_concept</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">is_tensor</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
            <span class="n">feature_extractor_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_processor</span><span class="p">.</span><span class="nf">postprocess</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="sh">"</span><span class="s">pil</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">feature_extractor_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_processor</span><span class="p">.</span><span class="nf">numpy_to_pil</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">safety_checker_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_extractor</span><span class="p">(</span><span class="n">feature_extractor_input</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">has_nsfw_concept</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">safety_checker</span><span class="p">(</span>
            <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">clip_input</span><span class="o">=</span><span class="n">safety_checker_input</span><span class="p">.</span><span class="n">pixel_values</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">has_nsfw_concept</span>
</code></pre></div></div>

<p>(#Idea, #GenAI, #TML) Completely erase a concept (i.e., NSFW) from latent space of Stable Diffusion.</p>

<ul>
  <li>Problem: Current methods such as ESD (Erasing Concepts from Diffusion Models) can erase quite well a concept from the Stable Diffusion. However, recent work (Circumventing Concept Erasure Methods for Text-to-Image Generative Models) has shown that it is possible to recover the erased concept by using a simple Textual Inversion method.</li>
  <li>Firstly, personally, I think that the approach in Pham et al. (2023) is not very convincing. Because, they need to use additional data (25 samples/concept) to learn a new token associated with the removed concept. So, it is not surprising that they can generate images with the removed concept. It is becaused of the power of the personalized method, not because of the weakness of the ESD method. It would be better if we can compare performance on recovering concept A (concept A is totally new to the base Stable Diffusion model such as your personal images) on two models: a SD model injected with concept A and a model fine-tuned with concept A and then erased concept A and then injected concept A back. If the latter model can not generate images with concept A better than inject concept A directly to the base model, then we can say that the ESD method is effective.</li>
</ul>

<h2 id="2023-08-16">2023-08-16</h2>

<p>(#Research) The Inaproppriate Image Prompts (I2P) benchmark.</p>

<ul>
  <li>Including 4703 unique prompts to generate inappropriate images with Stable Diffusion. There are combinations of 7 categories including: hate, harrassment, violence, self-harm, sexual, shocking and illegal activities.</li>
  <li>Research paper: <a href="https://arxiv.org/abs/2211.05105" rel="external nofollow noopener" target="_blank">Safe Latent Diffusion:
Mitigating Inappropriate Degeneration in Diffusion Models</a>, CVPR 2023.</li>
  <li>Huggingface page: <a href="https://huggingface.co/datasets/AIML-TUDA/i2p" rel="external nofollow noopener" target="_blank">https://huggingface.co/datasets/AIML-TUDA/i2p</a>
</li>
</ul>

<h2 id="2023-08-14">2023-08-14</h2>

<p>(#Research) Some trends in KDD 2023: Graph Neural Networks and Casual Inference from Industrial Applications.</p>

<p>(#Research) Graph Neural Networks, definition of neighborhood aggregation. Most of GNN methods work on million of nodes, to scale to billion of nodes, there are a lot of tricks under the hood (from Dinh’s working experience in Trustingsocial).</p>

<p>(#Research) (With Trung and Van Anh) We derive a nice framework that connect data-space distributional robustness (as in our ICLR 2022 paper) and model-space distributional robustness (as in SAM).</p>

<h2 id="2023-08-08">2023-08-08</h2>

<p>(#Research) On reading: Erasing Concepts from Diffusion Models (ICCV 2023).  <a href="https://erasing.baulab.info/" rel="external nofollow noopener" target="_blank">https://erasing.baulab.info/</a></p>

<p>(#Research) On reading: CIRCUMVENTING CONCEPT ERASURE METHODS FOR TEXT-TO-IMAGE GENERATIVE MODELS. Project page: <a href="https://nyu-dice-lab.github.io/CCE/" rel="external nofollow noopener" target="_blank">https://nyu-dice-lab.github.io/CCE/</a></p>

<h2 id="2023-08-06">2023-08-06</h2>

<p>(#Coding) Strange bug in generating adversarial examples using Huggingface.</p>

<ul>
  <li>Context: I am trying to implement similar idea as in the <a href="https://anti-dreambooth.github.io/" rel="external nofollow noopener" target="_blank">Anti-Dreambooth project</a> to generate adversarial perturbation for the Textual Inversion project. However, I got this bug which cost me 2 days to figure out.</li>
  <li>Bug: the gradient on the input tensor is None, even required_grad is set to True.</li>
  <li>Cause: Because of the Huggingface accelerator. (Ref: <a href="https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation" rel="external nofollow noopener" target="_blank">gradient_accumulation</a>). The accelerator is to help to accelerate the training process by accumulating the gradient over multiple batches. It requires the model and the train dataloader to be prepared with function <code class="language-plaintext highlighter-rouge">accelerator.prepare()</code>. However, in our case, we do not use the train dataloader (see the <a href="https://tuananhbui89.github.io/blog/2023/anti-dreambooth/">blog post about Anti-Dreambooth</a>). Therefore, when we still use the accelerator.accumulate() in the training loop, the gradient is accumulated over multiple batches, and the gradient on the input tensor is None.</li>
  <li>Fix: remove the accelerator.accumulate() in the training loop. No it doesn’t work!</li>
</ul>

<h2 id="2023-08-05">2023-08-05</h2>

<p>(#Coding) Understand the implementation of the <a href="https://anti-dreambooth.github.io/" rel="external nofollow noopener" target="_blank">Anti-Dreambooth project</a>. Ref to the <a href="https://tuananhbui89.github.io/blog/2023/anti-dreambooth/">blog post</a></p>

<h2 id="2023-08-04">2023-08-04</h2>

<p>(#Research) Three views of Diffusion Models:</p>

<ul>
  <li>Probabilistic view point as in DDPM</li>
  <li>Denoising Score matching</li>
  <li>Stochastic Differential Equations (SDE) view point (which is the most general one)</li>
</ul>

<h2 id="2023-08-03">2023-08-03</h2>

<p>(#Research) Trusted Autonomous Systems</p>

<ul>
  <li>Trusted Autonomous Systems (TAS) is Australia’s first Defence Cooperative Research Centre.</li>
  <li>There are many trustworthy related projects undergoing in this center.</li>
  <li>Reference: <a href="https://tasdcrc.com.au/about-us/" rel="external nofollow noopener" target="_blank">https://tasdcrc.com.au/ </a>
</li>
</ul>

<h2 id="2023-08-01">2023-08-01</h2>

<p>(#Research) Helmholtz Visiting Researcher Grant</p>

<ul>
  <li>https://www.helmholtz-hida.de/en/new-horizons/hida-visiting-program/</li>
  <li>1-3 months visiting grant for Ph.D. students and postdocs in one of 18 Helmholtz centers in Germany.</li>
  <li>Deadline: 16 August 2023 and will end on 15 October 2023.</li>
  <li>CISPA - Helmholtz Center for Information Security https://cispa.de/en/people</li>
</ul>

<h2 id="2023-07-31">2023-07-31</h2>

<p>(#Research) Australia Research Council (ARC) Discovery Project (DP) 2023.</p>

<ul>
  <li>The ARC DP is a very competitive grant in Australia.</li>
  <li>List of successful funded projects in 2023: https://rms.arc.gov.au/RMS/Report/Download/Report/1b0c8b2e-7bb0-4f2d-8f52-ad207cfbb41d/243</li>
  <li>This is a good source to find potential postdoc positions or Ph.D. scholarships as well as to find potential collaborators.</li>
  <li>For example, regarding Trustworthy Machine Learning, there are two projects including Dinh’s project.</li>
</ul>

<h2 id="2023-07-30">2023-07-30</h2>

<p>() First Home Buyer Super Saver Scheme.</p>

<ul>
  <li>Save mony for first home buyer inside superannuation fund. Apply 15% tax rate instead of marginal tax rate. When withdraw, apply another 15% tax rate. Each individual can save up to 50k for cross all years.</li>
</ul>

<h2 id="2023-07-27">2023-07-27</h2>

<p>(#GenAI) How to run textual inversion using Huggingface library locally without login to Huggingface with token. Including:</p>

<ul>
  <li>How to install git-lfs on Linux to download large files from Github. Reference: https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md</li>
  <li>How to download pretrained model from Huggingface model hub. Reference: https://huggingface.co/docs/hub/models-downloading</li>
  <li>Setup environment to run textual inversion locally. Setup dependencies.</li>
  <li>Setup script</li>
</ul>

<h2 id="2023-07-24">2023-07-24</h2>

<p>(#Productivity) How to present a slide and take notes on the same screen simultaneously (e.g., it is very useful when teaching or giving a talk). At Monash, the lecture theatre has MirrorOp installed on all screens that can connect wirelessly with a laptop but it is not convenient when we want to take notes.</p>

<ul>
  <li>Best solution: connect Ipad to the screen and use Ipad to present the slide. We can also see the slide’s note on the Ipad (required an adapter USB-C to HDMI and a HDMI cable).</li>
  <li>Alternative solution: join Zoom meeting on personal computer (for presentation) and on Ipad (for taking notes) and share screen from Ipad on Zoom if needed. PC can connect to the screen using HDMI cable or MirrorOp.</li>
</ul>

<h2 id="2023-07-23">2023-07-23</h2>

<p>Micromouse competition.</p>

<ul>
  <li>First introduced by Claude Shannon in 1950s.</li>
  <li>At the begining, it was just a simple maze solving competition. However, after 50 years of growing and competing, it has become a very competitive competition with many different categories: speed, efficiency, size. And along with its, many great ideas have been introduced and applied to the competition. It involes many different fields: mechanical, electrical, software, and AI all in just a small robot.</li>
  <li>The Fosbury Flop in high jump. When everyone use the same jump technique, the performance becomes saturated. Then Fosbury introduced a new technique (backward flop) that no one had ever thought of before. And it became the new standard (even named after him). This phenomenon also happens in the Micromouse competition.</li>
  <li>The two most important game changing ideas in the history of micromouse competition: capability to diagonal movement and using fan (vacumn) to suck the mouse to the path so that the mouse can move faster as in a racing car.</li>
</ul>

<p>Reference:</p>

<ul>
  <li><a href="https://youtu.be/ZMQbHMgK2rw" rel="external nofollow noopener" target="_blank">The Fastest Maze-Solving Competition On Earth by Veritasium.</a></li>
  <li><a href="https://invention.si.edu/fosbury-flop-game-changing-technique" rel="external nofollow noopener" target="_blank">The Fosbury Flop—A Game-Changing Technique</a></li>
</ul>

<h2 id="2023-07-22">2023-07-22</h2>

<p>(#Parenting) ATAR and university admission.</p>

<p>(#AML) Rethinking Backdoor Attacks, Mardy’s group. 
https://arxiv.org/pdf/2307.10163.pdf</p>

<p>(#Productivity) How to synchonize PowerPoint in Teams among multiple editers working simultaneously (i.e., share file in Teams, and open file in local using PowerPoint), in this way can retain the math equations.</p>

<p>If you have some math equations in your PowerPoint, and open it on Google Slide, the equations will be converted to images. If you accidently sync the file with your original file, all math equations will be lost.</p>

<h2 id="2023-07-21">2023-07-21</h2>

<p>(#F4T) You cannot solve a problem with the same thinking that created it. Albert Einstein.</p>

<p>Context: Our lab had a workshop last week and Dinh gave a talk about his favorite book “The 7 habits of highly effective people”. One of the habits is “Sharpen the saw” means that you always need to improve yourself from all aspects: physical, mental, spiritual, and social. That is the way you can overcome your limits and obstacles that you are facing.</p>

<ul>
  <li>You cannot solve a research problem in your thesis with the same knowledge that you have when starting your thesis. You need to grow and learn.</li>
  <li>If you want to upgrade for your paycheck, you need to learn new skills, new knowledge before applying for a new job.</li>
</ul>

<p>(#Experience) The first department metting as a new Research Fellow.</p>

<p>Context: I have just started my new position as a RF at the Department of Data Science and AI, Monash University. Today is the first time to be exposed to what really happen beyond student’s perspective.</p>

<ul>
  <li>Teaching matter (because new semester is about to start)</li>
  <li>Head of departmemt’s presentation about current activities (especially hiring and open positions, and budget)</li>
  <li>Ph.D. student recruitment, how competitive it is and how to rank the candidates (some insights: academic record and research record)</li>
</ul>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2023-06-02-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "alshedivat/al-folio",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
