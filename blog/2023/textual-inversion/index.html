<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Textual Inversion | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Personalizing Text-to-Image Generation using Textual Inversion">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2023/textual-inversion/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Textual Inversion</h1>
    <p class="post-meta">August 7, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          <a href="/blog/tag/tml">
          <i class="fas fa-hashtag fa-sm"></i> tml</a>  
          <a href="/blog/tag/diffusion">
          <i class="fas fa-hashtag fa-sm"></i> diffusion</a>  
          <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#about-the-paper">About the paper</a></li>
<li class="toc-entry toc-h2">
<a href="#how-to-implement">How to implement</a>
<ul>
<li class="toc-entry toc-h3"><a href="#how-to-set-up-the-specific-token-and-the-input-prompt">How to set up the specific token and the input prompt</a></li>
<li class="toc-entry toc-h3"><a href="#tokenizer-thing">Tokenizer thing</a></li>
<li class="toc-entry toc-h3"><a href="#difference-in-prompting-process-between-textual-inversion-and-dreambooth-projects">Difference in prompting process between “Textual Inversion” and “Dreambooth” projects</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#problem-of-textual-inversions-implementation">Problem of Textual Inversion’s implementation</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Published at ICLR 2023</li>
  <li>Affiliations: Tel Aviv University, Nvidia.</li>
  <li>Main idea: Every visual concept can be represented by a paragraph of text. The authors propose a method to learn a specific token that can represent a visual concept (It can learned so that with this specific token, the text-to-image can reconstruct the input images). The token is then used to generate a new image that contains the visual concept.</li>
</ul>

<p>I gave a talk about “Exploring Controllability of Conditioned Diffusion Models” covering this paper at the event of Postdoc-NeT-AI program. <a href="https://www.dropbox.com/s/299og2x7gx64nko/2023-June-Postdoc-AINet_v3.pdf?dl=0" rel="external nofollow noopener" target="_blank">Slide can be found here</a>. Therefore, in this blog post, I would like to focus on the implementation side of the paper.</p>

<h2 id="how-to-implement">How to implement</h2>

<p>In this blog post, I would like to break down some main steps in the implementation provided by Huggingface in the example code <a href="https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion" rel="external nofollow noopener" target="_blank">here</a>. There are also two notebooks for training (learning conceptual token) and inference (using conceptual token to generate new images) <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb" rel="external nofollow noopener" target="_blank">here</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb" rel="external nofollow noopener" target="_blank">here</a>.</p>

<p>There are several main points as follows:</p>

<ul>
  <li>How to set up the specific token in the input prompt.</li>
  <li>How to prepare the dataset</li>
  <li>How to train and learn the specific token</li>
</ul>

<h3 id="how-to-set-up-the-specific-token-and-the-input-prompt">How to set up the specific token and the input prompt</h3>

<p>In this project, there is an assumption that every visual concept can be represented by a paragraph of text. For example, the visual concept of your Shiba dog can be described as: “The Shiba dog boasts a striking and distinctive appearance that captivates all who gaze upon it. With a compact yet sturdy build, its confident stance exudes an air of self-assured elegance. A plush double coat of fur, often seen in shades of red, sesame, black and tan, or cream, adds to its allure. The fur frames a fox-like face, adorned with piercing almond-shaped eyes that gleam with intelligence and curiosity. Its erect, triangular ears stand at attention, poised to catch every sound that graces its surroundings. A tightly curled tail rests gracefully over its back, accentuating the Shiba’s poise and dignity.” (I use ChatGPT to write this paragraph about a Shiba dog).</p>

<p>After that, we can use the paragraph to represent the visual concept to generate new images. For example, if we want to generate your Shiba dog in the beach, “<em>The Shiba dog … poise and dignity</em> <strong>on the beach</strong>” can be used as the input prompt.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/textual_inversion/examples-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/textual_inversion/examples-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/textual_inversion/examples-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/textual_inversion/examples.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Examples of Textual Inversion.
</div>

<p>However, if we use the entire paragraph to represent the visual concept, it is not efficient. More importantly, even arbitrary long, the paragraph may not be able to capture the visual concept as expected. 
Therefore, instead of using the entire paragraph, the Textual Inversion paper propose to invert the inference process (i.e, from image to text prompt) to learn a specific token that can represent the visual concept (in the paper they used the token <code class="language-plaintext highlighter-rouge">S*</code>)
And then, we can just use the specific token to generate new images. For example, with the same query as before, we can use “<strong>a photo of <code class="language-plaintext highlighter-rouge">S*</code> on the beach</strong>” as the input prompt.</p>

<p>So the first step is to set up a placeholder for the specific token in the input prompt. In the implementation, it can be set by the argument <code class="language-plaintext highlighter-rouge">placeholder_token</code> (default value is <code class="language-plaintext highlighter-rouge">&lt;cat-toy&gt;</code>).</p>

<p>There is also an argument <code class="language-plaintext highlighter-rouge">learnable_property</code> (option <code class="language-plaintext highlighter-rouge">object</code> or <code class="language-plaintext highlighter-rouge">style</code>) which is used to choose type of neural prompt from two sets of templates <code class="language-plaintext highlighter-rouge">imagenet_templates_small</code> (if set to <code class="language-plaintext highlighter-rouge">object</code>) or <code class="language-plaintext highlighter-rouge">imagenet_style_templates_small</code> (if set to <code class="language-plaintext highlighter-rouge">style</code>). 
Some examples of the templates are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imagenet_templates_small</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">a photo of a {}</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">a rendering of a {}</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">a cropped photo of the {}</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">the photo of a {}</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">imagenet_style_templates_small</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">a painting in the style of {}</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">a rendering in the style of {}</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">a cropped painting in the style of {}</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">the painting in the style of {}</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">placeholder_token</code> will replace the <code class="language-plaintext highlighter-rouge">{}</code> in the templates. For example, if we set <code class="language-plaintext highlighter-rouge">placeholder_token</code> to <code class="language-plaintext highlighter-rouge">cat-toy</code> and <code class="language-plaintext highlighter-rouge">learnable_property</code> to <code class="language-plaintext highlighter-rouge">object</code>, the input prompt will be <code class="language-plaintext highlighter-rouge">a photo of a cat-toy</code>.
The input prompt then will be tokenized by the tokenizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">templates</span> <span class="o">=</span> <span class="n">imagenet_style_templates_small</span> <span class="k">if</span> <span class="n">learnable_property</span> <span class="o">==</span> <span class="sh">"</span><span class="s">style</span><span class="sh">"</span> <span class="k">else</span> <span class="n">imagenet_templates_small</span>

<span class="n">placeholder_string</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">placeholder_token</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">templates</span><span class="p">).</span><span class="nf">format</span><span class="p">(</span><span class="n">placeholder_string</span><span class="p">)</span>

<span class="n">example</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">model_max_length</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
<span class="p">).</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="tokenizer-thing">Tokenizer thing</h3>

<p>There is also an argument <code class="language-plaintext highlighter-rouge">initializer_token</code> (default value is <code class="language-plaintext highlighter-rouge">toy</code>) but not used anywhere else in the code.</p>

<p>The <code class="language-plaintext highlighter-rouge">placeholder_token</code> (i.e., <code class="language-plaintext highlighter-rouge">&lt;cat-toy&gt;</code>) is converted to indexes by the tokenizer <a href="https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_tokens_to_ids" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">convert_tokens_to_ids</code></a> method. Basically, this method converts the a sequence of tokens (i.e., <code class="language-plaintext highlighter-rouge">&lt;cat-toy&gt;</code>) in a sequence of ids, using the vocabulary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert the initializer_token, placeholder_token to ids
</span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">initializer_token</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># Check if initializer_token is a single token or a sequence of tokens
</span><span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">The initializer token must be a single token.</span><span class="sh">"</span><span class="p">)</span>

<span class="n">initializer_token_id</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">placeholder_token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_ids</span><span class="p">(</span><span class="n">placeholder_tokens</span><span class="p">)</span>

<span class="c1"># Resize the token embeddings as we are adding new special tokens to the tokenizer
</span><span class="n">text_encoder</span><span class="p">.</span><span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

<span class="c1"># Initialise the newly added placeholder token with the embeddings of the initializer token
</span><span class="n">token_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">placeholder_token_ids</span><span class="p">:</span>
        <span class="n">token_embeds</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_embeds</span><span class="p">[</span><span class="n">initializer_token_id</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">placeholder_token_ids</code> is then used to specify the position in the embedding matrix to be updated (corresponding to our specific token). In the end, the only thing we need to learn is the embedding matrix (actually only several specific rows in the embedding matrix, but the entire embedding matrix is small enough to store/save unlike the unet’ weights where it is much compacted using <a href="https://huggingface.co/blog/lora" rel="external nofollow noopener" target="_blank">LORA</a>). Later, we will learn how to use the learned embedding matrix to generate new images or upload to the hub.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's make sure we don't update any embedding weights besides the newly added token
</span><span class="n">index_no_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">index_no_updates</span><span class="p">[</span><span class="nf">min</span><span class="p">(</span><span class="n">placeholder_token_ids</span><span class="p">)</span> <span class="p">:</span> <span class="nf">max</span><span class="p">(</span><span class="n">placeholder_token_ids</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">accelerator</span><span class="p">.</span><span class="nf">unwrap_model</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">).</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">[</span>
        <span class="n">index_no_updates</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">orig_embeds_params</span><span class="p">[</span><span class="n">index_no_updates</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="difference-in-prompting-process-between-textual-inversion-and-dreambooth-projects">Difference in prompting process between “Textual Inversion” and “Dreambooth” projects</h3>

<p>In Dreambooth, there is an argument <code class="language-plaintext highlighter-rouge">instance_prompt</code> which is used as a neural prompt to associate with the given images. For example, the default value is <code class="language-plaintext highlighter-rouge">a photo of sks dog</code>, where <code class="language-plaintext highlighter-rouge">sks</code> is the unique identifier to specify the learned concept. The <code class="language-plaintext highlighter-rouge">instance_prompt</code> is then tokenized by the tokenizer and the token ids are used to specify the position in the embedding matrix to be updated (corresponding to the specific token).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># In the DreamBoothDataset class
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">example</span><span class="p">[</span><span class="sh">"</span><span class="s">instance_prompt_ids</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_hidden_states</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">text_inputs</span> <span class="o">=</span> <span class="nf">tokenize_prompt</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">instance_prompt</span><span class="p">,</span> <span class="n">tokenizer_max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">tokenizer_max_length</span>
        <span class="p">)</span>
        <span class="n">example</span><span class="p">[</span><span class="sh">"</span><span class="s">instance_prompt_ids</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_inputs</span><span class="p">.</span><span class="n">input_ids</span>
        <span class="n">example</span><span class="p">[</span><span class="sh">"</span><span class="s">instance_attention_mask</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_inputs</span><span class="p">.</span><span class="n">attention_mask</span>
</code></pre></div></div>

<p>So the difference between the two projects is that:</p>

<ul>
  <li>In Dreambooth, only one neural prompt is used, while in Textual Inversion, there is a list of neural prompts</li>
  <li>In Textual Inversion, it is important to specify the <code class="language-plaintext highlighter-rouge">placeholder_token</code> to reuse the same token in other prompts, while in Dreambooth, the identifier (i.e., <code class="language-plaintext highlighter-rouge">sks</code>) is used to specify the position in the embedding matrix to be updated (corresponding to the specific token). In inferencce, a prompt with the same identifier will be used to generate images, for example, <code class="language-plaintext highlighter-rouge">a photo of sks dog in the beach</code>. So to me, the whole prompt in Dreambooth is like a placeholder token in Textual Inversion. However, in this case, how the output looks like if we use a prompt that not contains the whole <code class="language-plaintext highlighter-rouge">instance_prompt</code>? For example, <code class="language-plaintext highlighter-rouge">a sks dog walking on the beach</code>?</li>
</ul>

<p><strong>Update on 05/04/2025</strong>:</p>

<ul>
  <li>It seems that the <code class="language-plaintext highlighter-rouge">instance_prompt</code> should be in the inference prompt to generate the image with the desired concept, e.g., if the <code class="language-plaintext highlighter-rouge">instance_prompt</code> is <code class="language-plaintext highlighter-rouge">a photo of sks dog</code>, then the inference prompt should be <code class="language-plaintext highlighter-rouge">a photo of sks dog in the beach</code>. Otherwise, the output will not show the desired/learned concept.</li>
</ul>

<h2 id="problem-of-textual-inversions-implementation">Problem of Textual Inversion’s implementation</h2>

<p>The below snippet of code is used to prevent updating the entire embedding matrix rather than the specific token - specified for the <code class="language-plaintext highlighter-rouge">placeholder_token</code> in the training process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's make sure we don't update any embedding weights besides the newly added token
</span><span class="n">index_no_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">index_no_updates</span><span class="p">[</span><span class="nf">min</span><span class="p">(</span><span class="n">placeholder_token_ids</span><span class="p">)</span> <span class="p">:</span> <span class="nf">max</span><span class="p">(</span><span class="n">placeholder_token_ids</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">accelerator</span><span class="p">.</span><span class="nf">unwrap_model</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">).</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">[</span>
        <span class="n">index_no_updates</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">orig_embeds_params</span><span class="p">[</span><span class="n">index_no_updates</span><span class="p">]</span>
</code></pre></div></div>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">Foundation of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/anti-dreambooth/">Dreambooth and Anti-Dreambooth</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
