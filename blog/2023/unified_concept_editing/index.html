<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Unified Concept Editing in Diffusion Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="How to remove unwanted concepts from a diffusion model?">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="http://localhost:4000/blog/2023/unified_concept_editing/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Unified Concept Editing in Diffusion Models",
      "description": "How to remove unwanted concepts from a diffusion model?",
      "published": "October 17, 2023",
      "authors": [
        {
          "author": "Tuan-Anh Bui",
          "authorURL": "https://tuananhbui89.github.io/",
          "affiliations": [
            {
              "name": "Monash University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Unified Concept Editing in Diffusion Models</h1>
        <p>How to remove unwanted concepts from a diffusion model?</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#about-the-paper">About the paper</a></div>
            <div><a href="#introduction">Introduction</a></div>
            <div><a href="#background">Background</a></div>
            <div><a href="#how-to-implement">How to implement</a></div>
            <div><a href="#summary">Summary</a></div>
            
          </nav>
        </d-contents>

        <h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Accepted to BMVC 2024. <a href="https://arxiv.org/pdf/2308.14761.pdf" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2308.14761.pdf</a>
</li>
  <li>Affiliation: Northeastern University, Technion and MIT. Same group with the ESD paper.</li>
  <li>Link to Github: <a href="https://github.com/rohitgandikota/unified-concept-editing" rel="external nofollow noopener" target="_blank">https://github.com/rohitgandikota/unified-concept-editing</a>
</li>
</ul>

<p><strong>Summary</strong>:</p>

<h2 id="introduction">Introduction</h2>

<h2 id="background">Background</h2>

<h3 id="u-net-and-cross-attention-layer">U-Net and Cross Attention Layer</h3>

<p>U-Net is a backbone architecture of Diffusion Models since proposed in DDPM paper.
Recently, for conditional generation, the U-Net is extended with Cross Attention Layer (CA) to embed the conditional information into the U-Net.
The CA layer consists of 3 components: Query, Key and Value.
The Query takes the information from input image while the Key and Value take the information from the conditional information.</p>

<h2 id="text-to-image-model-editing-time">Text-to-Image Model Editing (TIME)</h2>

<p>Main idea:</p>

<ul>
  <li>receives an under-specified “source” prompt, which is requested to be well-aligned with a “destination” prompt containing an attribute that the user wants to promote</li>
  <li>edit the model’s weights (i.e., cross-attention layer) such that its perception of a given concept in the world is changed. The change is expected to manifest in generated images for related prompts, while not affecting the characteristics or perceptual quality in the generation of different scenes. This would allow us to fix incorrect, biased, or outdated assumptions that text-to-image models may make.</li>
  <li>
<strong>Important</strong> this approach edits the projection matrices in the <strong>cross-attention</strong> layers to map the source prompt close to the destination, without substantially deviating from the original weights. Because these matrices <strong>operate on textual data</strong> irrespective of the diffusion process or the image contents, they constitute a compelling location for editing a model based on textual prompts.</li>
</ul>

<p>Pros:</p>

<ul>
  <li>It does not require training or finetuning, it can be applied in parallel for all cross-attention layers, and it modifies only a small portion of the diffusion model weights while leaving the language model unchanged. When applied on the publicly available Stable Diffusion, TIME edits a mere 2.2% of the diffusion model parameters, does not modify the text encoder, and applies the edit in a fraction of a second using a single consumergrade GPU.</li>
</ul>

<p>Cons:</p>

<ul>
  <li>It risks interference with surrounding concepts when editing a particular con- cept. For example, editing doctors to be female might also affect teachers to be female. <d-cite key="shafahi2019adversarial"></d-cite>
</li>
</ul>

<h2 id="proposed-method">Proposed method</h2>

<p>Given a pretrain layer \(W^{*}\), a set of concepts to be edited \(E\) and a set of concepts to be preserved \(P\), the goal is to find a new layer \(W\) that is close to \(W^{*}\) but does not contain any concept in \(E\) and preserve all concepts in \(P\).</p>

<p>To do that, <d-cite key="orgad2023editing"></d-cite> proposed to use the following optimization problem:</p>

\[\underset{W}{\min} \sum_{c_i \in E} \| W c_i - v_i^* \|^2_2 + \sum_{c_j \in P} \| W c_j - W^* c_j \|^2_2\]

<p>where \(v_i^*\) is the targeted vector for concept \(c_i\).</p>

<p>As derived in <d-cite key="orgad2023editing"></d-cite>, the solution of this optimization problem is:</p>

\[W = \left( \sum_{c_i \in E} v_i^* c_i^T + \sum_{c_j \in P} W^* c_j cj^T \right) \left( \sum_{c_i \in E} c_i c_i^T + \sum_{c_j \in P} c_j c_j^T \right)^{-1}\]

<p>By defining \(v^*\) differently, the authors proposed 3 types of editing:</p>

<ul>
  <li>
<strong>Erasing/Moderation</strong>: Choose \(v^* = W c^*\), where \(c^*\) is the targeted concept different from the concepts to be earased \(c_i \in E\)$. For example, harmful concept like “nudity” or “gun” can be erased to “safe/neutrual” concept like “flower” or “cat”, or artistic concept like “Kelly Mckernan” or “Van Gogh” to “non-artistic” concept like “art” or “ “.</li>
  <li>
<strong>Debiasing</strong>: Choose \(v^* = W (c_i + \sum_{t=1}^p \alpha_t a_t)\) where \(c_i\) is “doctor” and \(a_t\) is attributes that we want to distribute across such as “white”, “asian”, “black”. By this way, the original concept “doctor” no longer only associated with “white” but also with “asian” and “black”.</li>
</ul>

<!-- The new layer $$W$$ is found by solving the following optimization problem:

$$
\begin{aligned}
\min _{W} & \frac{1}{2}\|W-W^{*}\|_{F}^{2} \\
\text { s.t. } & \forall e \in E, \forall p \in P, \forall x \in \mathcal{X}, \forall y \in \mathcal{Y}, \forall z \in \mathcal{Z}, \\
& \operatorname{proj}_{\mathcal{X}}\left(W^{T} x\right) \cdot \operatorname{proj}_{\mathcal{Y}}\left(W^{T} y\right) \leq 0 \\
& \operatorname{proj}_{\mathcal{X}}\left(W^{T} x\right) \cdot \operatorname{proj}_{\mathcal{Z}}\left(W^{T} z\right) \geq 0 \\
& \operatorname{proj}_{\mathcal{Y}}\left(W^{T} y\right) \cdot \operatorname{proj}_{\mathcal{Z}}\left(W^{T} z\right) \geq 0
\end{aligned}
$$

where $$\mathcal{X}, \mathcal{Y}, \mathcal{Z}$$ are the sets of images that contain concept $$e$$, concept $$p$$ and neither concept $$e$$ nor concept $$p$$, respectively. -->

<h2 id="experiments">Experiments</h2>

<h3 id="metrics">Metrics</h3>

<p><strong>How to quantify the performance of erasing/editing method?</strong> Visualize the generated images from the model with and without erasing/editing can be tricky because of cherry-picking. Therefore, it is important to have a quantitative metric to evaluate the performance of erasing method. In the paper, the authors proposed to use the following metric:</p>

<ul>
  <li>
<a href="https://github.com/richzhang/PerceptualSimilarity" rel="external nofollow noopener" target="_blank">LPIPS</a> which is a perceptual metric that measures the distance between two images in the feature space of a pretrained network. The lower the LPIPS score, the more similar the two images are.</li>
  <li>CLIP score on COCO-30k prompts which is to measure the alignment between the prompt and the generated image using CLIP model. The higher the CLIP score, the more aligned the prompt and the generated image are. (Note that, the authors did not provide proper reference on the CLIP score or how to produce this metric which makes me a bit confused).</li>
  <li>FID score (on ? # images) to measure the quality of the generated images. The lower the FID score, the better the quality of the generated images is.</li>
  <li>Classification accuracy on erased objects. The authors used a pretrained ResNet-50 to classify whether the generated image contains the erased object or not (top-1 accuracy). The lower the classification accuracy, the better the erasing method is. The dataset used for this experiment is <a href="https://github.com/fastai/imagenette" rel="external nofollow noopener" target="_blank">Imagenette</a> which is 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute). They generated 500 images for each erased class to evaluate the classification accuracy. <strong>Note that, they did not generate images for other non-erased classes</strong> to quantify the effect of erasing on other classes which is another limitation of this paper.</li>
</ul>

<h3 id="erasing">Erasing</h3>

<p>The authors did two types of experiments: erasing artistic styles (e.g., “Kelly Mckernan” or “Van Gogh”) and erasing objects (e.g., erasing the concept of garbage trucks).</p>

<p>Note that, the authors smartly did not choose to erase personality (e.g., “Barack Obama” or “Tom Cruise”) which is easier to recognize success or failure of the erasing process than artistic styles.</p>

<h4 id="erasing-artistic-styles">Erasing artistic styles</h4>

<p><strong>Setup</strong>:</p>

<ul>
  <li>Dataset: None. The method utilizes the generated images from the model to fine-tune itself.</li>
  <li>Model: pretrained Stable Diffusion model. version 1.4 or 2.0</li>
  <li>Metrics:</li>
  <li>Baseline: ESD-x, Concept Ablation, and SDD</li>
</ul>

<p><strong>Experiment 1</strong> Qualitative comparison as shown in Figure 3 and quantitative analysis as shown in Figure 4.</p>

<h3 id="debiasing">Debiasing</h3>

<h3 id="moderation">Moderation</h3>

<h2 id="how-to-implement">How to implement</h2>

<p>The minimal code snippet as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2023-10-17-watermark.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
