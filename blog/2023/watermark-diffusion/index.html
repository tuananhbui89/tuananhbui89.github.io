<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="How to know whether an image is real or fake?">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2023/watermark-diffusion/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust",
      "description": "How to know whether an image is real or fake?",
      "published": "October 17, 2023",
      "authors": [
        {
          "author": "Tuan-Anh Bui",
          "authorURL": "https://tuananhbui89.github.io/",
          "affiliations": [
            {
              "name": "Monash University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust</h1>
        <p>How to know whether an image is real or fake?</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#about-the-paper">About the paper</a></div>
            <div><a href="#background">Background</a></div>
            <ul>
              <li><a href="#watermarking">Watermarking</a></li>
              <li><a href="#diffusion-inversion">Diffusion Inversion</a></li>
              
            </ul>
<div><a href="#tree-ring-watermark">Tree-Ring Watermark</a></div>
            <ul>
              <li><a href="#threat-model">Threat Model</a></li>
              <li><a href="#watermarking-process">Watermarking Process</a></li>
              <li><a href="#constructing-the-key">Constructing the key</a></li>
              <li><a href="#how-to-detect-the-watermark">How to detect the watermark?</a></li>
              
            </ul>
<div><a href="#how-to-implement">How to implement</a></div>
            <div><a href="#summary">Summary</a></div>
            
          </nav>
        </d-contents>

        <h2 id="about-the-paper">About the paper</h2>

<ul>
  <li>Published at NeurIPS 2023</li>
  <li>Affiliation: University of Maryland. Tom Goldstein’s group</li>
  <li>Link to the paper: <a href="https://arxiv.org/pdf/2305.20030.pdf" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2305.20030.pdf</a>
</li>
  <li>Link to Github: <a href="https://github.com/YuxinWenRick/tree-ring-watermark" rel="external nofollow noopener" target="_blank">https://github.com/YuxinWenRick/tree-ring-watermark</a>
</li>
  <li>Link to Yannic Kilcher’s video: <a href="https://youtu.be/WncUlZYpdq4?si=thX3fiKHS59SQ1IG" rel="external nofollow noopener" target="_blank">https://youtu.be/WncUlZYpdq4?si=thX3fiKHS59SQ1IG</a>
</li>
</ul>

<p><strong>Side information</strong>: Tom is one of the most famous and active researchers in the field of Trustworthy Machine Learning, particulaly Adversarial Machine Learning.
His group has published several notable papers, such as <a href="https://arxiv.org/abs/1910.14667" rel="external nofollow noopener" target="_blank">Invisible Cloak</a><d-cite key="wu2020making"></d-cite>, <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/22722a343513ed45f14905eb07621686-Paper.pdf" rel="external nofollow noopener" target="_blank">clean-label data poisoning</a><d-cite key="shafahi2018poison"></d-cite>, <a href="https://arxiv.org/abs/1904.12843" rel="external nofollow noopener" target="_blank">adversarial training for free</a><d-cite key="shafahi2019adversarial"></d-cite>, <a href="https://arxiv.org/abs/1712.09913" rel="external nofollow noopener" target="_blank">visualizing the loss landscape of neural networks</a><d-cite key="li2018visualizing"></d-cite>. Recently, his group has (moved) explored TML aspects of modern generative models, such as Diffusion Models <d-cite key="wen2023tree"></d-cite>, LLMs <d-cite key="jain2023baseline, shu2023exploitability, kirchenbauer2023watermark"></d-cite>.</p>

<p><strong>Summary</strong>:</p>

<ul>
  <li>
<strong>Problem setting</strong>: How to insert a watermark into a generated image such that the watermark is robust to the attack and invisible to the human eye?</li>
  <li>
<strong>Approach</strong>: The authors proposed a simple yet effective watermarking framework for diffusion models which consits generation phase and detection phase. The method is based on the idea of <strong>diffusion inversion</strong> which allows us to invert the diffusion process. The key idea is to embed a watermark into the initial noise in frequency domain and then use the diffusion inversion to extract the watermark from the generated image in detection phase.</li>
  <li>
<strong>Pros</strong>: The approach doesn’t require to change the weight of the diffusion model but just need to modify the input noise. Therefore, every user can have their own secret watermarking without changing the model.</li>
  <li>
<strong>Cons</strong>: The method is evaluated under a quite weak black-box attack. This method is only applicable to DDIM (deterministic version of DDPM) and not applicable to other generative models such as VAEs or Flow-based models.</li>
</ul>

<p><strong>Follow-up ideas</strong>:</p>

<ul>
  <li>How to fine-tune the foundation model (i.e., Stable Diffusion which does not have the watermark) to a new model with watermarking capability naturally? In this case, every generated output will have secret watermarking and from that we now can know whether an image is real or fake!</li>
  <li>How about stochastic diffusion model like DDPM?</li>
</ul>

<p>After all, we still don’t know whether an image is real or fake <img class="emoji" title=":joy:" alt=":joy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png" height="20" width="20">.</p>

<h2 id="background">Background</h2>

<h3 id="watermarking">Watermarking</h3>

<p><strong>What is Watermarking?</strong> Watermarking is a technique to embed some information into a signal (image, audio, video, etc.) in a way that the signal is not changed much, but the information can be extracted later. The information can be used for many purposes, such as authentication, copyright.</p>

<p><strong>Watermarking: Attack and Defense Game</strong> The watermarking process can be seen as an adversarial game between two parties: <strong>attacker and defender</strong>. The attacker tries to remove the watermark from the signal, while the defender tries to make the watermark robust to the attacker’s removal process. The game is illustrated in the following figure.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/watermark-attack-defense-flow-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/watermark-attack-defense-flow-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/watermark-attack-defense-flow-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/watermark-attack-defense-flow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Watermarking: Attack and Defense Game (image source <a href="https://www.researchgate.net/publication/343385316_Digital_Watermarking_-_Comparison_of_DCT_and_DWT_methods" rel="external nofollow noopener" target="_blank">Jovanovic et al. 2009</a>)
</div>

<p>The defender has two main goals: (1) to make the watermark <strong>robust to the attacker</strong>’s removal process, and (2) to make the watermark <strong>invisible to the human eye</strong>. The first goal is measured by the robustness of the watermark, while the second goal is measured by the fidelity of the watermark. Similar as the trade-off between accuracy and robustness in the adversarial machine learning, the <strong>robustness and fidelity are usually conflicting</strong>, i.e., the more robust the watermark is, the more visible it is.</p>

<p>To extract the watermark, the defender needs a secret key and a secret decoder which are usually known only to the defender. There are two types of attack settings: <strong>white-box</strong> and <strong>black-box</strong> attacks depending on whether the attacker knows the secret key and decoder or not. Again, similar as the adversarial machine learning, the white-box attack is usually more powerful but less practical than the black-box attack.</p>

<p><strong>Adaptive Attack</strong> is a special type of attack where the attacker knows everything about the defender, i.e., the secret key, decoder, and the defense algorithm and can adaptively change the attack strategy based on the defender’s strategy. This type of attack is usually the most powerful and the most difficult to defend (and almost impossible to defend in the adversarial machine learning). In this paper, the authors evaluated their method under non-adaptive white-box attack.</p>

<p><strong>Why Watermarking in Generative Models?</strong> Originally, watermarking is to protect the ownership of the authors on their digital products. However, in the context of generative models, where a product is generated from a model with users’ input, the ownership is not clear. And when digging deeper, I found that copyright of AI art is complicated. Some important points that I got from this article <a href="https://www.yankodesign.com/2023/05/27/who-owns-ai-generated-content-understanding-ownership-copyrighting-and-how-the-law-interprets-ai-generated-art/#:~:text=As%20far%20as%20art%20goes,of%20it%20or%20copyright%20it." rel="external nofollow noopener" target="_blank">WHO OWNS AI-GENERATED CONTENT? UNDERSTANDING OWNERSHIP, COPYRIGHTING, AND HOW THE LAW INTERPRETS AI-GENERATED ART</a></p>

<ul>
  <li>According to (US) copyright law, only humans can be granted copyrights. If it’s created by AI, nobody can claim ownership of it or copyright it.</li>
  <li>But, if a person uses AI as a tool and gives very distinct/creative inputs in the process to create something, then the person can (again, as my understanding) claim ownership of the final product. For example, as mentioned in the arcticle, <code class="language-plaintext highlighter-rouge">graphic-novel artist Kris Kashtanova was granted copyright for their AI-generated comic book “Zarya of the Dawn” for the simple reason that there was human input in creating the entire comic book and its underlying storyline. The entire comic book was “AI-assisted” and not “AI-generated”, which is why it was eligible for copyright.</code>
</li>
  <li>Specific to text-to-image models as Stable Diffusion, Dall-E, MidJourney, etc, the answer depends from case to case and if you care about the ownership, the first thing to do is read the <code class="language-plaintext highlighter-rouge">Terms and Conditons</code> carefully. In general, there are common points from these models:
    <ul>
      <li>User own all Assets the user create with the Services, <strong>to the extent possible under current law</strong>.</li>
      <li>However, user’s input (e.g., text prompt, input images) is granted to the company to use to improve and maintain their services.</li>
      <li>User is responsible for the content and ensuring that it does not violate any laws or intellectual property rights</li>
    </ul>
  </li>
</ul>

<p>Now, given the above points, back to the question: <strong>Why Watermarking in Generative Models?</strong>, I think the main purpose of the watermarking is to protect the ownership of the users on their generated products.</p>

<p>However, it is a much more interesting implication of watermarking in generative models than just authentication. If we can robustly and reliably detect a watermark in a generated image, we can know whether the image is real or fake, which is a very important problem in the field of Trustworthy Machine Learning.</p>

<p><strong>What is DFT and why watermarking loves DFT?</strong></p>

<p>Reference: <a href="https://vincmazet.github.io/bip/filtering/fourier.html" rel="external nofollow noopener" target="_blank">https://vincmazet.github.io/bip/filtering/fourier.html</a> and <a href="https://www.cs.unm.edu/~brayer/vision/fourier.html" rel="external nofollow noopener" target="_blank">https://www.cs.unm.edu/~brayer/vision/fourier.html</a></p>

<p>As studied in the classical watermarking literature, the watermarking process is usually done in the frequency domain. Some important points about the frequency domain are (to my understanding):</p>

<ul>
  <li>Modification in the frequency domain is more robust to image transformation such as rotation, translation, scaling, etc. than modification in the spatial domain (<a href="https://www.cs.unm.edu/~brayer/vision/fourier.html" rel="external nofollow noopener" target="_blank">ref</a>)</li>
  <li>Modification in the frequency domain is easier to make the watermark invisible to the human eye. DFT transformation converts an image to a phase and an amplitude. The amplitude represents the intensity of the different frequencies in the image while the phase represents the location of the frequencies. The human vision comprehends the shape of an object better than its intensity, therefore, the phase is more important than the amplitude. This is the reason why we can remove/add the watermark by modifying the amplitude while keeping the phase unchanged.</li>
</ul>

<!-- ### DDPM and DDIM

One very important note is that this framework has been based on the DDIM <d-cite key="song2020denoising"></d-cite>, which is a deterministic version of DDPM <d-cite key="ho2020denoising"></d-cite>. In the DDPM framework, the forward diffusion process has a nice property that:

$$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t$$

where $$x_0$$ is the initial image, $$\epsilon_t \sim \mathcal{N}(0, I)$$ is the noise at time $$t$$. This property allows us to `predict` noisy version of $$x_0$$ at any arbitrary time $$t$$. On the other hand, given $$\epsilon_t = \epsilon_\theta(x_t, t)$$ is the predicted noise at time $$t$$ by the denoising network $$\epsilon_\theta$$ and $$x_t$$, we can `predict` $$\tilde{x_0}$$ as follows:

$$\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}$$

Based on this observation, the authors <d-cite key="song2020denoising"></d-cite> proposed to sample $$x_{t-1}$$ from $$x_t$$ as follows:

$$x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \tilde{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta(x_t, t) + \sigma_t \epsilon_t$$

where $$\epsilon_t \sim \mathcal{N}(0, I)$$ is the noise at time $$t$$.

So $$x_{t-1} \sim q(x_{t-1} \mid x_t, x_0) = \mathcal{N} (x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \tilde{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta(x_t, t), \sigma_t^2 I)$$ -->

<h3 id="diffusion-inversion">Diffusion Inversion</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/gan-inversion-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/gan-inversion-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/gan-inversion-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/gan-inversion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Illustration of GAN inversion (Image source <d-cite key="xia2022gan"></d-cite>).
</div>

<p>Generative inversion is a technique that allows us to invert the generation process. In other words, given a pre-trained generative model \(g_\theta(z)\) and an image \(x\) which can be either real image or generated one, we can find the noise \(z\) such that \(g_\theta(z)\) is close to \(x\). This technique was first proposed for GANs in Zhu et al. (2016) <d-cite key="zhu2016generative"></d-cite>, Creswell et al. (2016) <d-cite key="creswell2018inverting"></d-cite> not long after the introduction of GAN in 2014. It can be seen that, obtaining the inverted latent code brings many useful implications such as capability to edit/manipulate generated images by editing the latent code, or adversarial perturbation removal <d-cite key="samangouei2018defense"></d-cite>.</p>

<p>Because requring the deterministic property: one noise \(z\) always generates the same image \(x\), this technique is not trivial to apply to other generative models such as VAEs or Flow-based models. For Diffusion Models, thanks to the deterministic property in DDIM, we can apply this technique to invert the diffusion process, i.e., given an image \(x_0\), we can find the noise \(x_T\) to reconstruct \(x_0\). And with the blooming of Diffusion Models in the last two years, we can see many cool applications of this technique such as Textual Inversion <d-cite key="gal2022image"></d-cite>, Image Editing <d-cite key="mokady2023null"></d-cite> or the work we are discussing - Watermarking <d-cite key="wen2023tree"></d-cite>).</p>

<p>In the DDPM framework, the forward diffusion process has a nice property that:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon_t\]

<p>where \(x_0\) is the initial image, \(\epsilon_t \sim \mathcal{N}(0, I)\) is the noise at time \(t\). This property allows us to <code class="language-plaintext highlighter-rouge">predict</code> noisy version of \(x_0\) at any arbitrary time \(t\). On the other hand, given \(\epsilon_t = \epsilon_\theta(x_t, t)\) is the predicted noise at time \(t\) by the denoising network \(\epsilon_\theta\) and \(x_t\), we can <code class="language-plaintext highlighter-rouge">predict</code> \(\tilde{x_0}\) as follows:</p>

\[\tilde{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\]

<p>Now we consider the next step in the forward diffusion process:</p>

\[x_{t+1} = \sqrt{\bar{\alpha}_{t+1}} x_0 + \sqrt{1 - \bar{\alpha}_{t+1}} \epsilon_{t+1}\]

<p>where \(\epsilon_{t+1} \sim \mathcal{N}(0, I)\) is the noise at time \(t+1\). If we replace the original \(x_0\) with the predicted \(\tilde{x}_0\) and assume that the diffusion process is large enough so that \(\epsilon_{t+1} \approx \epsilon_\theta(x_t, t)\), we can obtain the inverted diffusion process as follows:</p>

\[x_{t+1} = \sqrt{\bar{\alpha}_{t+1}} \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} + \sqrt{1 - \bar{\alpha}_{t+1}} \epsilon_\theta(x_t, t)\]

<p>which now depends only on \(x_t\) and \(\epsilon_\theta(x_t, t)\). Repeating this process from \(t=0\) to \(t=T\), we can obtain the inverted code \(x_T\) that reconstructs \(x_0\) (again it works for DDIM model only). This is the key technique used in this watermarking method.</p>

<h2 id="tree-ring-watermark">Tree-Ring Watermark</h2>

<h3 id="threat-model">Threat Model</h3>

<p>In adversarial machine learning, a threat model is a description of capabilities and objectives of all parties in the attack and defense game. From that, we can narrow down the defense space and scope to this specific threat model (It is because in the real world, we cannot know every possible attack and defend against it). In this paper, the authors considered the following threat model:</p>

<ul>
  <li>Model owner (generative phase): The generative model owner generates an image \(x\) with a secret watermark \(k\). The constraint is that the watermarking algorithm  should have a negligible effect on the generation process, so that quality is maintained and watermarking leaves no visible trace.</li>
  <li>Attacker or Forger (attack phase): The attacker tries to remove the watermark from the generated image \(x\) to get \(x'\) (and then can claim his ownership on \(x'\), etc.). The attacker uses data augmentations only and knows nothing about the watermarking algorithm and the generative model (a <strong>quite weak black-box attack</strong>).</li>
  <li>Model owner (detection phase): The model owner tries to detect the watermark in the image \(x'\) to know whether it was modified from the original image \(x\) or not. The model owner knows nothing about the attack including its techniques and hyper-parameters.</li>
</ul>

<h3 id="watermarking-process">Watermarking Process</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/fig1-pipeline-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/fig1-pipeline-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/fig1-pipeline-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/fig1-pipeline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>The watermarking process is illustrated in the above figure. The watermarking process consists of two main steps: <strong>watermark embedding</strong> and <strong>watermark detection</strong>.</p>

<p>In the watermark embedding step, an initial Gaussian noise \(x_T\) is first converted to the frequency domain using DFT \(\mathcal{F}\). Then, a pre-defined watermark \(k\) is embedded into the frequency domain of \(x_T\) by a simple binary masking operation. The watermarked noise \(x_T^k\) is then converted back to the time domain using inverse DFT \(\mathcal{IF}\). Finally, the watermarked noise \(x_T^k\) is used to generate the watermarked image \(x_0^k\) (now call \(x\)) using the standard <strong>DDIM model</strong> (again not stochastic one like DDPM).</p>

<p>In the watermark detection, the transformed image \(x'=\mathcal{A}(x_0^k)\) is first inverted to obtain the <strong>approximated</strong> noise \(x'_T\) using the <strong>DDIM inversion</strong>. Then, the watermark \(k'\) is extracted from the frequency domain of \(x'_T\) using the same binary masking operation as in the watermark embedding step. Finally, the extracted watermark \(k'\) is compared with the original watermark \(k\) to determine whether the image \(x'\) is from the original image \(x\) or not.</p>

<p>The simple process can be describe as follows:</p>

<blockquote class="block-quote">
  <p><strong>Watermark Embedding</strong></p>

  <p>Input: initial noise \(x_T\), secret key \(k\), mask \(M\), DDIM model \(\mathcal{D}\) <br>
\(x_T^f = \mathcal{F}(x_T)\) <br>
\(x_T^k = Masking(x_T^f, k, M)\) <br>
\(x_0^k = \mathcal{D}(\mathcal{IF}(x_T^k))\) <br>
Output: watermarked image \(x_0^k\) or \(x\)</p>
</blockquote>

<blockquote class="block-quote">
  <p><strong>Watermark Detection</strong></p>

  <p>Input: transformed image \(x'\), secret key \(k\), mask \(M\), DDIM inversion \(\mathcal{D}^I\) <br>
\(x'_T = \mathcal{D}^I (x')\) <br>
\({x'}_T^f = \mathcal{F}(x'_T)\) <br>
\(k' = UnMasking({x'}_T^f, k, M)\) <br>
calculate distance \(d(k, k')\) between \(k'\) and \(k\) <br>
Output: \(d(k, k')\)</p>
</blockquote>

<p>As described in the paper, the masking opearation will produce output:</p>

\[x_{i,T}^k \sim \left\{
  \begin{array}{ c l }
    k_i &amp; \quad \textrm{if } i \in M \\
    \mathcal{N} (0,1)                 &amp; \quad \textrm{otherwise}
  \end{array}
\right.\]

<p>where \(k_i\) is the \(i\)-th element of the key \(k\), \(M\) is the mask. Note that the Fourier transform of a Gaussian noise array is also distributed as
Gaussian noise. The distance function is the L1 distance \(d(k, k') = \frac{1}{\mid M \mid} \sum_{i \in M} \mid k_i - k'_i \mid\).</p>

<h3 id="constructing-the-key">Constructing the key</h3>

<p>As mentioned in the paper, choosing the key pattern \(k\)  (as similar the binary mask \(M\)) strongly effects the robustness and visibility of the watermark. The authors proposed to use a <strong>tree-ring pattern</strong> which is a circular mask with radius \(r\) centered on the low frequency modes as the key pattern. This pattern brings several benefits such as invariant to rotation, translation, and dilation (which was studied in classical watermarking literature). The authors proposed three variants of the tree-ring pattern:</p>

<ul>
  <li>Tree-ring Zero: all elements in the tree-ring pattern are zero.</li>
  <li>Tree-ring Rands: all elements in the tree-ring pattern are randomly sampled from \(\mathcal{N}(0,1)\).</li>
  <li>Tree-ring Rings: multiple rings with different radiuses.</li>
</ul>

<p><strong>Why ring pattern?</strong></p>

<h3 id="how-to-detect-the-watermark">How to detect the watermark?</h3>

<p>Given an image \(x'\) and from the watermark detection process, we can obtain \(k'\) which is the extracted pattern from \(x'\). Now, how we can decide whether \(k'\) is the same as the original pattern \(k\) or not?</p>

<p>To do that, the authors defined a null hypothesis \(H_0\) and find the P-value of the null hypothesis. The null hypothesis is defined as follows:</p>

\[H_0: k' \sim \mathcal{N}(0, \sigma^2 I)\]

<p>Here, the variance \(\sigma^2\) is unknown and be estimated from each image as \(\sigma^2 = \frac{1}{\mid M \mid} \sum_{i \in M} \mid k'_i \mid^2\).</p>

<blockquote class="block-quote">
  <p><strong>What is Null Hypothesis?</strong></p>

  <p>Null hypothesis is the claim that no relationship exists between two sets of data or variables being analyzed. For example, in the context of watermarking, the null hypothesis is a statement that the extracted pattern \(k'\) is just a random noise and not related to the original pattern \(k\). On the other hand, the alternative hypothesis is a statement that the extracted pattern \(k'\) is related to the original pattern \(k\).</p>
</blockquote>

<blockquote class="block-quote">
  <p><strong>What is P-value?</strong></p>

  <p>The P-value is the probability of obtaining results as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. The smaller the P-value, the stronger the evidence against the null hypothesis. The P-value is calculated from the null hypothesis and the observed data using a statistical test. For example, in the context of watermarking, the P-value is the probability of obtaining the extracted pattern \(k'\) from the null hypothesis \(H_0\).</p>
</blockquote>

<p>The P-value is calculated as follows:</p>

\[p = Pr \left( \chi^2_{\mid M \mid, \lambda} \leq \eta \mid H_0 \right) = \Phi_{\chi^2} (z)\]

<p>where \(\chi^2_{\mid M \mid, \lambda}\) is the chi-squared distribution with \(\mid M \mid\) degrees of freedom and non-centrality parameter \(\lambda\), \(\eta = \frac{1}{\sigma^2} \sum_{i \in M} \mid k_i - k'_i \mid^2\), \(z = \frac{\eta - \lambda}{\sqrt{2 \lambda}}\), and \(\Phi_{\chi^2}\) is the cumulative distribution function of the chi-squared distribution.</p>

<p>From that, an image is considered as a forgery if \(p &lt; \alpha\) where \(\alpha\) is a pre-defined threshold (too small \(\alpha\) will lead to many false positives, while too large \(\alpha\) will lead to many false negatives).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/fig3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/fig3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/fig3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/fig3-attack-watermark-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/fig3-attack-watermark-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/fig3-attack-watermark-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/fig3-attack-watermark.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Measuring P-value in different settings (w/o watermark, w/ watermark, w/ watermark and attack). The extreme low P-value in the last setting indicates that the watermark is robust to the attack.
</div>

<p>The figure above shows the P-value of the null hypothesis in three different settings including (1) without watermark, (2) with watermark, and (3) with watermark and attack. As we can see, the P-value of image with watermark is much lower than that of image without watermark, and the P-value in the last setting is extremely low, which indicates that the watermark is robust to the attack. The authors also provided a more quantitative analysis as Table 1 in the paper.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/watermark/tab1-main-results-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/watermark/tab1-main-results-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/watermark/tab1-main-results-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/watermark/tab1-main-results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>The metric was used to measure the performance is <strong>AUC/TPR@1%FPR</strong> which is the area under the ROC curve (AUC) or the true positive rate (TPR) at 1% false positive rate (FPR). The authors also used FID score to measure the quality of the generated images and CLIP score to measure the semantic similarity between the generated images and the prompts.</p>

<p><strong>That’s all for the paper!</strong> There are still many experiments and analysis in the paper, but the post is already too long. Further details can be found in the paper.</p>

<h2 id="summary">Summary</h2>

<p><strong>Summary</strong>:</p>

<ul>
  <li>
<strong>Problem setting</strong>: How to insert a watermark into a generated image such that the watermark is robust to the attack and invisible to the human eye?</li>
  <li>
<strong>Approach</strong>: The authors proposed a simple yet effective watermarking framework for diffusion models which consits generation phase and detection phase. The method is based on the idea of <strong>diffusion inversion</strong> which allows us to invert the diffusion process. The key idea is to embed a watermark into the initial noise in frequency domain and then use the diffusion inversion to extract the watermark from the generated image in detection phase.</li>
  <li>
<strong>Pros</strong>: The approach doesn’t require to change the weight of the diffusion model but just need to modify the input noise. Therefore, every user can have their own secret watermarking without changing the model.</li>
  <li>
<strong>Cons</strong>: The method is evaluated under a quite weak black-box attack. This method is only applicable to DDIM (deterministic version of DDPM) and not applicable to other generative models such as VAEs or Flow-based models.</li>
</ul>

<p><strong>Follow-up ideas</strong>:</p>

<ul>
  <li>How to fine-tune the foundation model (i.e., Stable Diffusion which does not have the watermark) to a new model with watermarking capability naturally? In this case, every generated output will have secret watermarking and from that we now can know whether an image is real or fake!</li>
  <li>How about stochastic diffusion model like DDPM?</li>
</ul>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2023-10-17-watermark.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
