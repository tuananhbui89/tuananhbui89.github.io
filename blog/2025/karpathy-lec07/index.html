<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Karpathy Series - Intro to Large Language Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/karpathy-lec07/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Karpathy Series - Intro to Large Language Models</h1>
    <p class="post-meta">December 15, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#large-language-models-are-distributed-as-a-parameter-file-plus-executable-code-and-can-run-locally-for-inference">Large language models are distributed as a parameter file plus executable code and can run locally for inference</a></li>
<li class="toc-entry toc-h1"><a href="#obtaining-model-parameters-requires-a-large-scale-training-run-that-compresses-training-corpora-into-the-model-weights">Obtaining model parameters requires a large-scale training run that compresses training corpora into the model weights</a></li>
<li class="toc-entry toc-h1"><a href="#large-language-models-are-trained-as-next-token-predictors-and-thereby-internalize-knowledge-that-enables-diverse-behaviors">Large language models are trained as next-token predictors and thereby internalize knowledge that enables diverse behaviors</a></li>
<li class="toc-entry toc-h1"><a href="#transformer-architectures-implement-the-computations-of-modern-large-language-models-but-the-internal-representations-remain-difficult-to-fully-interpret">Transformer architectures implement the computations of modern large language models but the internal representations remain difficult to fully interpret</a></li>
<li class="toc-entry toc-h1"><a href="#production-quality-conversational-assistants-are-produced-by-fine-tuning-pretrained-base-models-on-curated-qa-datasets">Production-quality conversational assistants are produced by fine-tuning pretrained base models on curated Q&amp;A datasets</a></li>
<li class="toc-entry toc-h1"><a href="#human-comparison-labels-and-reinforcement-learning-from-human-feedback-rlhf-provide-an-additional-alignment-stage">Human comparison labels and reinforcement learning from human feedback (RLHF) provide an additional alignment stage</a></li>
<li class="toc-entry toc-h1"><a href="#benchmark-leaderboards-reveal-a-two-tier-ecosystem-of-closed-proprietary-models-and-open-weight-models">Benchmark leaderboards reveal a two-tier ecosystem of closed proprietary models and open-weight models</a></li>
<li class="toc-entry toc-h1"><a href="#scaling-laws-describe-predictable-improvements-from-increasing-model-size-and-training-data">Scaling laws describe predictable improvements from increasing model size and training data</a></li>
<li class="toc-entry toc-h1"><a href="#tool-use-augments-language-models-by-giving-them-access-to-external-computation-browsing-and-visualization">Tool use augments language models by giving them access to external computation, browsing, and visualization</a></li>
<li class="toc-entry toc-h1"><a href="#multimodalityimage-and-audio-understandinggenerationextends-model-capabilities-beyond-text-and-enables-new-applications">Multimodality—image and audio understanding/generation—extends model capabilities beyond text and enables new applications</a></li>
<li class="toc-entry toc-h1"><a href="#system-1-versus-system-2-describes-desiderata-for-time-dependent-deliberation-and-improved-reasoning">System 1 versus system 2 describes desiderata for time-dependent deliberation and improved reasoning</a></li>
<li class="toc-entry toc-h1"><a href="#self-improvement-via-reinforcement-and-sandboxed-evaluation-is-possible-in-closed-environments-but-remains-challenging-for-general-language-tasks">Self-improvement via reinforcement and sandboxed evaluation is possible in closed environments but remains challenging for general language tasks</a></li>
<li class="toc-entry toc-h1"><a href="#customization-enables-specialized-expert-models-via-retrieval-custom-instructions-and-fine-tuning">Customization enables specialized expert models via retrieval, custom instructions, and fine-tuning</a></li>
<li class="toc-entry toc-h1"><a href="#large-language-models-can-be-conceived-as-kernel-processes-of-an-emerging-llm-operating-system-that-orchestrates-memory-tools-and-agents">Large language models can be conceived as kernel processes of an emerging ‘LLM operating system’ that orchestrates memory, tools, and agents</a></li>
<li class="toc-entry toc-h1"><a href="#llm-deployments-introduce-a-novel-security-attack-surface-including-jailbreaks-adversarial-encodings-and-adversarial-artifacts-in-images">LLM deployments introduce a novel security attack surface including jailbreaks, adversarial encodings, and adversarial artifacts in images</a></li>
<li class="toc-entry toc-h1"><a href="#prompt-injection-attacks-hijack-model-behavior-by-embedding-instructions-within-retrieved-documents-or-media-and-can-enable-phishing-and-data-exfiltration">Prompt injection attacks hijack model behavior by embedding instructions within retrieved documents or media and can enable phishing and data exfiltration</a></li>
<li class="toc-entry toc-h1"><a href="#data-poisoning-and-backdoor-trigger-phrases-can-corrupt-models-during-training-or-fine-tuning-producing-conditional-failures">Data poisoning and backdoor trigger phrases can corrupt models during training or fine-tuning, producing conditional failures</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/zjkBMFhNj_g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="large-language-models-are-distributed-as-a-parameter-file-plus-executable-code-and-can-run-locally-for-inference">Large language models are distributed as a parameter file plus executable code and can run locally for inference</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-02-05-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-02-05-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-02-05-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-02-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A large language model typically consists of two artifacts: a <strong>parameters file</strong> containing the trained weights and a <strong>runtime implementation</strong> that performs the forward pass.<br></p>

<ul>
  <li>
<strong>Parameters file</strong>
    <ul>
      <li>Stores the numeric coefficients of the neural network (commonly <strong>float16</strong>).<br>
</li>
      <li>Can be very large — for example, a <strong>70B-parameter</strong> model stored as <strong>2 bytes per parameter</strong> yields on the order of <strong>140 GB</strong>.<br>
</li>
    </ul>
  </li>
  <li>
<strong>Runtime implementation</strong>
    <ul>
      <li>Can be a small, dependency-free program (for example a compact <strong>C binary</strong>) that loads the parameters, implements the model architecture, and executes <strong>autoregressive sampling</strong> to produce text.<br>
</li>
    </ul>
  </li>
</ul>

<p>Because the architecture and weights are self-contained, <strong>inference can be performed offline</strong> on a capable workstation without network connectivity.<br><br>
However, larger models impose heavier <strong>CPU/GPU and memory</strong> requirements — practical consumer usage commonly employs <strong>smaller variants</strong> of the model to achieve interactive latency.<br></p>

<hr>

<h1 id="obtaining-model-parameters-requires-a-large-scale-training-run-that-compresses-training-corpora-into-the-model-weights">Obtaining model parameters requires a large-scale training run that compresses training corpora into the model weights</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-05-18-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-05-18-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-05-18-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-05-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Model training</strong> is an expensive, large-scale optimization process that ingests massive text corpora and produces the parameter file used for inference.<br></p>

<ul>
  <li>Data and compute scale
    <ul>
      <li>Training state-of-the-art models involves collecting <strong>petabyte-scale raw data</strong> (for illustration, pretraining datasets are on the order of <strong>multiple terabytes</strong> of text).<br>
</li>
      <li>It requires provisioning very large GPU clusters (often <strong>tens of thousands of accelerators</strong> at the highest scale) and running distributed optimization for <strong>days or weeks</strong> at <strong>multimillion-dollar</strong> cost.<br>
</li>
    </ul>
  </li>
  <li>Nature of trained parameters
    <ul>
      <li>The trained parameters function as a <strong>lossy compression</strong> of the training distribution: they retain statistical structure and knowledge useful for generation but <strong>do not store exact copies</strong> of source documents.<br>
</li>
    </ul>
  </li>
  <li>Industry practice
    <ul>
      <li>Current practice often scales both <strong>model size (parameters)</strong> and <strong>dataset size</strong> according to predictable <strong>scaling laws</strong> to improve next-token prediction accuracy, driving large compute investments in addition to algorithmic work.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="large-language-models-are-trained-as-next-token-predictors-and-thereby-internalize-knowledge-that-enables-diverse-behaviors">Large language models are trained as next-token predictors and thereby internalize knowledge that enables diverse behaviors</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-09-04-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-09-04-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-09-04-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-09-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The fundamental pretraining objective is <strong>next-token prediction</strong>: given a sequence of tokens the model estimates a probability distribution over the next token and is optimized to minimize prediction error.<br></p>

<ul>
  <li>Consequences of the objective
    <ul>
      <li>This objective forces the model to capture <strong>syntactic and semantic regularities</strong> across vast corpora, so the resulting parameters encode substantial <strong>factual and procedural knowledge</strong> even though the task is purely predictive.<br>
</li>
    </ul>
  </li>
  <li>Inference behavior and risks
    <ul>
      <li>At inference time the model samples tokens <strong>autoregressively</strong>, producing fluent text that can resemble training documents but may <strong>hallucinate fabricated facts</strong> (for example invented ISBNs or plausible-sounding product metadata).<br>
</li>
    </ul>
  </li>
  <li>Why models can perform many tasks
    <ul>
      <li>The <strong>predictive/compression equivalence</strong> explains why next-token training yields models that can answer questions, provide factual descriptions, and perform many downstream tasks after appropriate adaptation or prompting.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="transformer-architectures-implement-the-computations-of-modern-large-language-models-but-the-internal-representations-remain-difficult-to-fully-interpret">Transformer architectures implement the computations of modern large language models but the internal representations remain difficult to fully interpret</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-12-49-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-12-49-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-12-49-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-12-49.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Modern large language models are implemented with the <strong>Transformer</strong> neural network architecture, whose mathematical operations and forward-pass computations are well specified.<br></p>

<ul>
  <li>Parameter distribution and structure
    <ul>
      <li>
<strong>Billions to hundreds of billions of parameters</strong> are distributed across <strong>attention layers</strong>, <strong>feedforward networks</strong>, and <strong>embeddings</strong>.<br>
</li>
      <li>Optimization adjusts these parameters to reduce next-token loss but does not yield a human-interpretable decomposition of function.<br>
</li>
    </ul>
  </li>
  <li>Empirical nature and interpretability
    <ul>
      <li>Consequently, models are largely <strong>empirical artifacts</strong>: their behavior is best characterized by <strong>input/output evaluation</strong> rather than full mechanistic understanding.<br>
</li>
      <li>This motivates the subfield of <strong>mechanistic interpretability</strong>, which attempts to map parameter substructures to functional capabilities.<br>
</li>
    </ul>
  </li>
  <li>Deployment implication
    <ul>
      <li>The relative <strong>inscrutability</strong> means rigorous, scenario-specific evaluation suites and empirical safety testing are essential when deploying these systems.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="production-quality-conversational-assistants-are-produced-by-fine-tuning-pretrained-base-models-on-curated-qa-datasets">Production-quality conversational assistants are produced by fine-tuning pretrained base models on curated Q&amp;A datasets</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-17-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-17-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-17-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-17-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A typical two-stage training paradigm separates broad knowledge acquisition from alignment to desired behaviors.<br></p>

<ol>
  <li>
<strong>Pretraining</strong>
    <ul>
      <li>Performed on broad, noisy web-scale corpora to build a <strong>knowledge-rich base model</strong>.<br>
</li>
    </ul>
  </li>
  <li>
<strong>Fine-tuning</strong>
    <ul>
      <li>Uses <strong>smaller, much higher-quality</strong> datasets consisting of labeled examples (e.g., user prompts paired with ideal assistant responses).<br>
</li>
      <li>Examples are generated following <strong>detailed labeling instructions</strong> and often rely on trained annotators.<br>
</li>
      <li>Fine-tuning preserves factual knowledge from pretraining while shifting the model’s output distribution toward <strong>helpful, formatted, and policy-compliant</strong> assistant responses.<br>
</li>
      <li>It is <strong>computationally far cheaper and faster</strong> than pretraining.<br>
</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Iterative deployment cycle
    <ul>
      <li>Includes monitoring, collecting failure cases, and incorporating corrected responses into subsequent fine-tuning cycles to iteratively improve behavior.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="human-comparison-labels-and-reinforcement-learning-from-human-feedback-rlhf-provide-an-additional-alignment-stage">Human comparison labels and reinforcement learning from human feedback (RLHF) provide an additional alignment stage</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-21-57-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-21-57-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-21-57-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-21-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A further alignment stage uses <strong>pairwise comparison labels</strong>, where human raters choose the better output among model candidates rather than authoring full responses.<br></p>

<ul>
  <li>Reinforcement Learning from Human Feedback (<strong>RLHF</strong>)
    <ul>
      <li>These comparisons are used to train a <strong>reward model</strong> that operationalizes human preferences.<br>
</li>
      <li>The reward model guides policy optimization so the model prefers outputs judged superior by humans, often improving alignment beyond direct supervised fine-tuning.<br>
</li>
    </ul>
  </li>
  <li>Labeling practices and tooling
    <ul>
      <li>
<strong>Labeling instructions</strong> encode desiderata such as <strong>helpfulness</strong>, <strong>truthfulness</strong>, and <strong>harmlessness</strong>, and are typically extensive and nuanced to capture real-world safety requirements.<br>
</li>
      <li>Human–machine workflows accelerate labeling quality by having models propose candidates which humans curate or combine, reducing manual effort while scaling comparison collection.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="benchmark-leaderboards-reveal-a-two-tier-ecosystem-of-closed-proprietary-models-and-open-weight-models">Benchmark leaderboards reveal a two-tier ecosystem of closed proprietary models and open-weight models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-24-36-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-24-36-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-24-36-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-24-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Public comparison platforms rank chat models using <strong>pairwise human judgments</strong> and compute <strong>ELO-style ratings</strong> to compare performance empirically.<br></p>

<ul>
  <li>Current landscape
    <ul>
      <li>The highest-rated models on many leaderboards are <strong>closed-source, commercially hosted</strong> systems whose weights are not publicly available; these systems often lead in aggregate capability metrics.<br>
</li>
    </ul>
  </li>
  <li>Open-weight ecosystem
    <ul>
      <li>Beneath them sits a growing <strong>open-weight</strong> ecosystem (model families released with downloadable checkpoints and papers) that trades some performance for greater <strong>transparency, modifiability, and on-premise deployment</strong>.<br>
</li>
    </ul>
  </li>
  <li>Ecosystem dynamics
    <ul>
      <li>The dynamic is one of <strong>competition and convergence</strong>: open-source projects iterate to close the gap while proprietary systems retain performance advantages and controlled deployment environments.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="scaling-laws-describe-predictable-improvements-from-increasing-model-size-and-training-data">Scaling laws describe predictable improvements from increasing model size and training data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-26-40-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-26-40-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-26-40-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-26-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Scaling laws</strong> empirically relate next-token prediction loss to two primary variables: <strong>model parameter count</strong> and <strong>amount of training data</strong>.<br></p>

<ul>
  <li>Properties and utility
    <ul>
      <li>These relationships are smooth and extrapolatable over wide ranges, allowing practitioners to <strong>forecast expected predictive accuracy</strong> for a given compute and data budget.<br>
</li>
      <li>Because many downstream evaluation metrics correlate with next-token accuracy, scaling up model size and dataset volume has been a reliable path to capability improvements.<br>
</li>
    </ul>
  </li>
  <li>Implication for investment
    <ul>
      <li>This predictable scaling behavior helps explain industry incentives to acquire large GPU clusters and massive corpora; algorithmic progress can accelerate gains, but the scaling relationship provides a robust baseline strategy relatively independent of specific architectural innovations.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="tool-use-augments-language-models-by-giving-them-access-to-external-computation-browsing-and-visualization">Tool use augments language models by giving them access to external computation, browsing, and visualization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-30-38-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-30-38-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-30-38-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-30-38.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Modern assistant systems are designed to <strong>orchestrate external tools</strong> rather than rely solely on internal token-based computation.<br></p>

<ul>
  <li>Common tool types
    <ul>
      <li>Web browsers for information retrieval<br>
</li>
      <li>Calculator or Python interpreters for precise numeric computation<br>
</li>
      <li>Plotting libraries for visualization<br>
</li>
      <li>Image-generation APIs for multimodal outputs<br>
</li>
    </ul>
  </li>
  <li>Interaction pattern
    <ul>
      <li>The assistant issues <strong>structured tool calls</strong> or emits special tokens that indicate desired tool invocation, consumes the tool outputs as additional context, and continues generation to produce a final answer — mirroring human workflows.<br>
</li>
    </ul>
  </li>
  <li>Benefits and implementation considerations
    <ul>
      <li>Tool use expands reliability and capability because <strong>deterministic external systems</strong> (e.g., a calculator or web search) provide accurate inputs and let the language model act as an <strong>orchestrator</strong> coordinating heterogeneous resources.<br>
</li>
      <li>Implementations require careful <strong>interface design</strong>, grounding of tool outputs, and <strong>citation/provenance tracking</strong> to maintain trustworthiness.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="multimodalityimage-and-audio-understandinggenerationextends-model-capabilities-beyond-text-and-enables-new-applications">Multimodality—image and audio understanding/generation—extends model capabilities beyond text and enables new applications</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-34-16-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-34-16-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-34-16-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-34-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Multimodal models</strong> ingest and produce modalities such as <strong>images, audio, and video</strong> alongside text, enabling a wide range of tasks.<br></p>

<ul>
  <li>Example capabilities
    <ul>
      <li>Code generation from diagram images, fine-grained image captioning, speech-to-text and text-to-speech conversational interfaces, and multimodal content creation.<br>
</li>
    </ul>
  </li>
  <li>Integration approach
    <ul>
      <li>Integration requires <strong>joint training</strong> and modality-specific encoders that project diverse inputs into a <strong>shared representation space</strong> for cross-modal reasoning.<br>
</li>
    </ul>
  </li>
  <li>Trade-offs and deployment
    <ul>
      <li>Multimodality broadens usability for real-world workflows (for example generating a functioning website from a hand-drawn sketch) but increases system complexity by adding new tool and data pipelines and new evaluation metrics for correctness across modalities.<br>
</li>
      <li>Practical deployment requires <strong>modality-specific safety checks</strong> and preprocessing to mitigate risks unique to non-text inputs.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="system-1-versus-system-2-describes-desiderata-for-time-dependent-deliberation-and-improved-reasoning">System 1 versus system 2 describes desiderata for time-dependent deliberation and improved reasoning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-36-33-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-36-33-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-36-33-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-36-33.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The <strong>system 1 / system 2</strong> distinction frames a capability spectrum:<br></p>

<ul>
  <li>
<strong>System 1</strong>: fast, intuitive token-level prediction (current autoregressive models operate here).<br>
</li>
  <li>
    <p><strong>System 2</strong>: slower, deliberative reasoning that trades time for accuracy and supports multi-step planning and evaluation.<br></p>
  </li>
  <li>Current limitations and research directions
    <ul>
      <li>Autoregressive models produce each token with roughly uniform compute and <strong>lack native mechanisms for multi-step internal deliberation</strong> or explicit tree-search over reasoning trajectories.<br>
</li>
      <li>Research aims to enable models to spend longer deliberation time and to organize intermediate reasoning (for example <strong>chain-of-thought</strong> or <strong>tree-of-thoughts</strong> algorithms) so accuracy increases monotonically with invested computation.<br>
</li>
      <li>Achieving practical system 2 behavior requires mechanisms for maintaining and manipulating <strong>intermediate state</strong>, planning, and multi-step evaluation, often leveraging tool use and external deterministic modules to improve reliability.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="self-improvement-via-reinforcement-and-sandboxed-evaluation-is-possible-in-closed-environments-but-remains-challenging-for-general-language-tasks">Self-improvement via reinforcement and sandboxed evaluation is possible in closed environments but remains challenging for general language tasks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-39-26-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-39-26-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-39-26-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-39-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Self-improvement</strong> in closed domains often follows the AlphaGo pattern: imitate experts initially, then surpass them by optimizing directly against an automated reward function that is cheap to evaluate (for example win/loss in a game).<br></p>

<ul>
  <li>Limits for general language tasks
    <ul>
      <li>For general language tasks there is no single universally available, cheap reward signal analogous to game outcomes, which complicates automated self-improvement at scale.<br>
</li>
      <li>Narrow domains with reliable, automatically computable reward functions may permit iterative self-play or objective-driven optimization to exceed human-level performance, but generalizing this approach to open-ended language tasks remains an active research question.<br>
</li>
    </ul>
  </li>
  <li>Current approaches
    <ul>
      <li>Progress toward self-improving LLMs focuses on <strong>constrained tasks</strong>, <strong>surrogate reward models</strong>, or <strong>human-in-the-loop reinforcement signals</strong> rather than fully autonomous universal improvement.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="customization-enables-specialized-expert-models-via-retrieval-custom-instructions-and-fine-tuning">Customization enables specialized expert models via retrieval, custom instructions, and fine-tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-41-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-41-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-41-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-41-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Customization</strong> lets teams tailor a general base model to specific tasks, domains, or organizational knowledge through several mechanisms:<br></p>

<ul>
  <li>Mechanisms for customization
    <ul>
      <li>
<strong>Custom instructions</strong> that alter behavior.<br>
</li>
      <li>
<strong>Retrieval-augmented generation (RAG)</strong> that surfaces user-provided documents at runtime and keeps the base weights unchanged.<br>
</li>
      <li>
<strong>Supervised fine-tuning</strong> on domain-specific example pairs to produce task experts.<br>
</li>
    </ul>
  </li>
  <li>Product patterns
    <ul>
      <li>Product efforts like <strong>user-configurable GPTs</strong> or <strong>app-store paradigms</strong> encapsulate these levers so developers can compose assistants with specified behavior and knowledge.<br>
</li>
      <li>Deeper customization (targeted fine-tuning or prompt engineering) can create high-performing, task-specific experts that outperform generic assistants on narrow workloads.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="large-language-models-can-be-conceived-as-kernel-processes-of-an-emerging-llm-operating-system-that-orchestrates-memory-tools-and-agents">Large language models can be conceived as kernel processes of an emerging ‘LLM operating system’ that orchestrates memory, tools, and agents</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-44-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-44-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-44-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-44-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>An <strong>LLM-centered system</strong> functions analogously to an operating system kernel that coordinates storage, working memory, and peripheral services.<br></p>

<ul>
  <li>Analogies
    <ul>
      <li>The <strong>Internet and document stores</strong> act as mass storage.<br>
</li>
      <li>The <strong>context window</strong> acts as working memory or <strong>RAM</strong>.<br>
</li>
      <li>External tools are analogous to <strong>system calls</strong> or device drivers.<br>
</li>
    </ul>
  </li>
  <li>Management patterns
    <ul>
      <li>The context window is a finite, high-value resource that the LLM orchestrator must <strong>page in and out relevant content</strong> from larger stores via retrieval and summarization to solve tasks efficiently.<br>
</li>
      <li>Multithreading equivalents appear in parallel tool invocation and speculative generation; user-space/kernel-space analogies map to user prompts versus system-level instruction contexts.<br>
</li>
    </ul>
  </li>
  <li>Design implication
    <ul>
      <li>Viewing LLMs as orchestration layers clarifies patterns for <strong>tool integration</strong>, <strong>memory management</strong>, and <strong>extensibility</strong> in complex application stacks built on language models.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="llm-deployments-introduce-a-novel-security-attack-surface-including-jailbreaks-adversarial-encodings-and-adversarial-artifacts-in-images">LLM deployments introduce a novel security attack surface including jailbreaks, adversarial encodings, and adversarial artifacts in images</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-48-37-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-48-37-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-48-37-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-48-37.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Language models open new vectors of compromise because they accept rich, flexible inputs and execute semantics learned from heterogeneous web data; attackers exploit this by crafting inputs or encodings that bypass refusal logic.<br></p>

<ul>
  <li>Attack types and examples
    <ul>
      <li>
<strong>Jailbreaks</strong> achieved via roleplay or instruction framing that cause a model to violate safety policies.<br>
</li>
      <li>
<strong>Adversarial encodings</strong> (for example base64 or optimized token suffixes) that hide malicious intent from detectors trained on natural-language refusals.<br>
</li>
      <li>
<strong>Visual adversarial patterns</strong> embedded in images can act as conditioned triggers when models process multimodal inputs; researchers can optimize such patterns to reliably elicit undesired behavior.<br>
</li>
    </ul>
  </li>
  <li>Defensive needs
    <ul>
      <li>Defending requires comprehensive, <strong>multimodal adversarial training</strong>, sanitization pipelines, and robust <strong>multi-language and multi-encoding</strong> safety datasets rather than English-only refusal examples.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="prompt-injection-attacks-hijack-model-behavior-by-embedding-instructions-within-retrieved-documents-or-media-and-can-enable-phishing-and-data-exfiltration">Prompt injection attacks hijack model behavior by embedding instructions within retrieved documents or media and can enable phishing and data exfiltration</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-53-57-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-53-57-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-53-57-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-53-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Prompt injection</strong> occurs when content that the model retrieves or receives contains instructions that override or influence the intended prompt context, causing the model to reveal sensitive information, follow malicious directives, or publish attacker-controlled links.<br></p>

<ul>
  <li>Attack vectors and examples
    <ul>
      <li>Invisible or faint text in images that instruct the model to produce spam.<br>
</li>
      <li>Web pages that include malicious instructions which appear during browsing.<br>
</li>
      <li>Shared documents that embed executable payloads to exfiltrate data.<br>
</li>
    </ul>
  </li>
  <li>Exploited behaviors
    <ul>
      <li>Attackers exploit downstream processing behaviors (e.g., rendering external images, embedding URLs, or converting content that executes within trusted domains) to bypass content security policies and cause the model to leak data to attacker-controlled endpoints.<br>
</li>
    </ul>
  </li>
  <li>Mitigations
    <ul>
      <li>Combine strict <strong>input sanitization</strong>, <strong>provenance-aware retrieval filters</strong>, conservative rendering policies, and runtime behavior constraints that separate untrusted content from system-level instructions.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="data-poisoning-and-backdoor-trigger-phrases-can-corrupt-models-during-training-or-fine-tuning-producing-conditional-failures">Data poisoning and backdoor trigger phrases can corrupt models during training or fine-tuning, producing conditional failures</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec07/00-58-02-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec07/00-58-02-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec07/00-58-02-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec07/00-58-02.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A <strong>data poisoning attack</strong> injects adversarially crafted examples into training or fine-tuning data so the trained model behaves maliciously or incorrectly when presented with specific trigger phrases or contexts.<br></p>

<ul>
  <li>Behavior of backdoors
    <ul>
      <li>These backdoors act like sleeper agents: the model behaves normally most of the time, but the presence of a <strong>trigger token or phrase</strong> (for example an injected phrase used during fine-tuning) causes the model to output nonsensical or attacker-chosen content and to bypass safety checks.<br>
</li>
    </ul>
  </li>
  <li>Attack surface and ease
    <ul>
      <li>Poisoning is easier to mount when training corpora are scraped from uncontrolled web sources or when fine-tuning allows external contributors.<br>
</li>
    </ul>
  </li>
  <li>Defenses
    <ul>
      <li>Dataset <strong>provenance tracking</strong>, robust data filtering, <strong>differential validation</strong> on held-out clean sets, and forensic auditing to detect anomalous correlations.<br>
</li>
      <li>While documented attacks are more common in fine-tuning scenarios, studying pretraining-level risks and developing secure data curation pipelines is an active area of research.<br>
</li>
    </ul>
  </li>
</ul>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
