<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Foundation of Diffusion Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/diffusion-foundation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Foundation of Diffusion Models</h1>
    <p class="post-meta">March 8, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          <a href="/blog/tag/diffusion">
          <i class="fas fa-hashtag fa-sm"></i> diffusion</a>  
          <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#what-are-diffusion-models">What are Diffusion Models?</a>
<ul>
<li class="toc-entry toc-h3"><a href="#advantages-of-diffusion-models">Advantages of Diffusion Models</a></li>
<li class="toc-entry toc-h3"><a href="#disadvantages-of-diffusion-models">Disadvantages of Diffusion Models</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#mathematical-foundation">Mathematical Foundation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#differential-equations-odes-and-sdes">Differential Equations: ODEs and SDEs</a></li>
<li class="toc-entry toc-h3"><a href="#forward-and-reverse-diffusion-processes">Forward and Reverse Diffusion Processes</a></li>
<li class="toc-entry toc-h3">
<a href="#euler-method-for-numerical-integration">Euler Method for Numerical Integration</a>
<ul>
<li class="toc-entry toc-h4"><a href="#implementation-of-the-euler-method">Implementation of the Euler method</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#fokker-planck-equation-from-trajectories-to-distributions">Fokker-Planck Equation: From Trajectories to Distributions</a></li>
<li class="toc-entry toc-h3"><a href="#score-matching-and-denoising">Score Matching and Denoising</a></li>
<li class="toc-entry toc-h3"><a href="#variational-perspective-and-kl-minimization">Variational Perspective and KL Minimization</a></li>
<li class="toc-entry toc-h3"><a href="#tweedies-formula">Tweedie’s formula</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#variants-of-diffusion-models">Variants of Diffusion Models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#ddpm">DDPM</a></li>
<li class="toc-entry toc-h3"><a href="#ddim">DDIM</a></li>
<li class="toc-entry toc-h3"><a href="#score-matching">Score Matching</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#flow-matching">Flow Matching</a>
<ul>
<li class="toc-entry toc-h3"><a href="#fundammentals-concepts-in-flow-matching">Fundammentals Concepts in Flow Matching</a></li>
<li class="toc-entry toc-h3"><a href="#derivation-of-the-flow-matching-objective">Derivation of the Flow Matching Objective</a></li>
<li class="toc-entry toc-h3"><a href="#how-to-sampling-from-flow-matching-model">How to sampling from Flow Matching model</a></li>
<li class="toc-entry toc-h3"><a href="#flow-matching-code-example">Flow Matching Code Example</a></li>
<li class="toc-entry toc-h3"><a href="#conditional-flow-matching">Conditional Flow Matching</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#differences-between-score-matching-diffusion-models-and-flow-matching">Differences between Score Matching, Diffusion Models and Flow Matching</a>
<ul>
<li class="toc-entry toc-h3"><a href="#summary-of-main-differences">Summary of Main Differences</a></li>
<li class="toc-entry toc-h3">
<a href="#detailed-differences">Detailed Differences</a>
<ul>
<li class="toc-entry toc-h4"><a href="#1-time-convention">1. Time Convention</a></li>
<li class="toc-entry toc-h4"><a href="#2-forward-process-vs-probability-paths">2. Forward Process vs. Probability Paths</a></li>
<li class="toc-entry toc-h4"><a href="#3-training-objective">3. Training Objective</a></li>
<li class="toc-entry toc-h4"><a href="#4-sampling-process">4. Sampling Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#noise-scheduling">Noise scheduling</a></li>
<li class="toc-entry toc-h2">
<a href="#guidanced-diffusion">Guidanced Diffusion</a>
<ul>
<li class="toc-entry toc-h3"><a href="#classifier-guidance">Classifier Guidance</a></li>
<li class="toc-entry toc-h3"><a href="#classifier-free-guidance-cfg">Classifier-Free Guidance (CFG)</a></li>
<li class="toc-entry toc-h3"><a href="#intuition-recap">Intuition Recap</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#latent-diffusion">Latent Diffusion</a></li>
<li class="toc-entry toc-h2">
<a href="#conditional-diffusion">Conditional Diffusion</a>
<ul>
<li class="toc-entry toc-h3"><a href="#control-net">Control-Net</a></li>
<li class="toc-entry toc-h3"><a href="#image-prompt">Image Prompt</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#diffusion-transformers">Diffusion Transformers</a>
<ul>
<li class="toc-entry toc-h3"><a href="#data-processing-in-dit">Data Processing in DiT</a></li>
<li class="toc-entry toc-h3"><a href="#the-dit-architecture">The DiT Architecture</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#diffusion-flux">Diffusion Flux</a></li>
<li class="toc-entry toc-h2">
<a href="#image-inpainting-with-diffusion-models">Image Inpainting with Diffusion Models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#training-pipeline">Training Pipeline</a></li>
<li class="toc-entry toc-h3"><a href="#challenges-in-image-inpainting">Challenges in Image Inpainting</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#accelerating-diffusion-models">Accelerating Diffusion Models</a>
<ul>
<li class="toc-entry toc-h3">
<a href="#consistency-models">Consistency Models</a>
<ul>
<li class="toc-entry toc-h4"><a href="#concept-and-mathematical-definition">Concept and Mathematical Definition</a></li>
<li class="toc-entry toc-h4"><a href="#sampling-with-consistency-models">Sampling with Consistency Models</a></li>
<li class="toc-entry toc-h4"><a href="#training-consistency-models">Training Consistency Models</a></li>
<li class="toc-entry toc-h4"><a href="#implementation-of-consistency-models">Implementation of Consistency Models</a></li>
</ul>
</li>
<li class="toc-entry toc-h3">
<a href="#diffusion-distillation">Diffusion Distillation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#progressive-distillation">Progressive Distillation</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#rectified-diffusion">Rectified Diffusion</a></li>
<li class="toc-entry toc-h3">
<a href="#caching-in-diffusion-models">Caching in Diffusion Models</a>
<ul>
<li class="toc-entry toc-h4"><a href="#u-net-refresher">U-Net Refresher</a></li>
<li class="toc-entry toc-h4"><a href="#observation-feature-reuse-across-timesteps">Observation: Feature Reuse Across Timesteps</a></li>
<li class="toc-entry toc-h4"><a href="#implementation-overview">Implementation Overview</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#multi-modal-diffusion">Multi-modal Diffusion</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <p>(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)</p>

<h2 id="what-are-diffusion-models">What are Diffusion Models?</h2>

<p>Diffusion models are a class of generative models that generate data by progressively denoising a sample from pure noise. They are inspired by <a href="https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics" rel="external nofollow noopener" target="_blank"><strong>non-equilibrium thermodynamics</strong></a> and are based on a forward and reverse diffusion process:</p>

<ol>
  <li>Forward Process (Diffusion Process): A data sample (e.g., an image) is gradually corrupted by adding Gaussian noise over multiple timesteps until it becomes nearly pure noise.</li>
  <li>Reverse Process (Denoising Process): A neural network learns to reverse this corruption by gradually removing noise step by step, reconstructing the original data distribution.</li>
</ol>

<figure style="text-align: center;">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/JnIkGtkO-Js?si=faOgaMvGtqTLcG1T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>Diffusion - How molecules actually move</figcaption>
</figure>

<p><strong>Analogy: Ink Dissolving in Water</strong>
Imagine dropping a blob of ink into a glass of water:</p>

<ul>
  <li>Forward process (Diffusion Process): Initially, the ink is concentrated in one place (structured data). Over time, it spreads out randomly, blending with the water (adding noise). Eventually, the entire glass becomes a uniformly colored mixture, losing its original structure (complete noise).</li>
  <li>Reverse process (Denoising Process): If we had a way to perfectly reverse time, we could watch the ink particles retrace their paths, reassembling into the original drop (generating the original data from noise). Diffusion models learn to perform this “reverse process” step by step using machine learning.</li>
</ul>

<blockquote>
  <p><strong>Non-Equilibrium Thermodynamics</strong></p>

  <p>Thermodynamics studies <strong>how energy moves and changes</strong> in a system. In equilibrium thermodynamics, systems are in balance—nothing is changing. Non-equilibrium thermodynamics, on the other hand, deals with <strong>systems that are constantly evolving, moving between states of disorder and order</strong>.</p>

  <p>In diffusion models, the forward process (adding noise to data) and the reverse process (removing noise) resemble a non-equilibrium thermodynamic system because they describe an evolving state that moves from order (structured data) to disorder (pure noise) and back to order (reconstructed data).</p>
</blockquote>

<blockquote>
  <p><strong>Brownian Motion</strong></p>

  <p>Brownian motion <strong>describes the random movement</strong> of tiny particles (like pollen grains in water) due to <strong>collisions with molecules</strong>. This randomness is similar to how noise is added in diffusion models.</p>
</blockquote>

<h3 id="advantages-of-diffusion-models">Advantages of Diffusion Models</h3>

<p>Diffusion models offer several key advantages over traditional generative models like GANs and VAEs:</p>

<ol>
  <li>
    <p><strong>High-Fidelity Samples</strong>: Unlike VAEs and GANs which generate samples in one step, diffusion models create samples gradually by denoising. This step-by-step process allows the model to first establish coarse image structure before refining fine details, resulting in higher quality outputs.</p>
  </li>
  <li>
    <p><strong>Training Stability</strong>: Diffusion models are easier to train compared to GANs as they use a single tractable likelihood loss. They don’t suffer from training instabilities like mode collapse that often plague GANs.</p>
  </li>
  <li>
    <p><strong>Sample Diversity</strong>: Similar to VAEs, diffusion models maximize likelihood which ensures coverage of all modes in the training dataset. This leads to more diverse outputs compared to GANs which can suffer from mode collapse.</p>
  </li>
  <li>
    <p><strong>Flexible Architecture</strong>: The multi-step denoising process enables additional functionalities like inpainting or image-to-image generation by manipulating the input noise, without requiring architectural changes.</p>
  </li>
  <li>
    <p><strong>Consistent Quality</strong>: The gradual denoising process is more robust and consistent compared to GANs where quality can vary significantly between samples.</p>
  </li>
</ol>

<p>The main trade-off is generation speed - diffusion models require multiple neural network passes to generate samples, making them slower than single-pass models like GANs and VAEs. However, various sampling optimization techniques have been developed to significantly reduce this computational overhead.</p>

<h3 id="disadvantages-of-diffusion-models">Disadvantages of Diffusion Models</h3>

<p>While diffusion models have significant advantages, they also come with some trade-offs:</p>

<ul>
  <li>Slow Sampling: The reverse process requires multiple denoising steps, making inference slower compared to GANs.</li>
  <li>Compute Intensive: Training requires large amounts of data and computational power.</li>
  <li>Memory Usage: They require storing multiple intermediate noise distributions, making them more memory-intensive.</li>
  <li>Complex Implementation: The multi-step nature of diffusion models makes them more complex to implement compared to single-step models.</li>
</ul>

<h2 id="mathematical-foundation">Mathematical Foundation</h2>

<p>Diffusion models are built on a deep interplay between <strong>differential equations</strong>, <strong>probability theory</strong>, and <strong>variational inference</strong>. To understand why the model works, we need to trace how these ideas connect: from describing how systems evolve over time, to modeling probability densities, to designing trainable objectives.</p>

<h3 id="differential-equations-odes-and-sdes">Differential Equations: ODEs and SDEs</h3>

<p>We start with <strong>ordinary differential equations (ODEs)</strong>, which describe how a system changes deterministically over time based on its current state.</p>

\[\frac{dx}{dt} = f(x, t)\]

<p>where \(x(t)\) is the state of the system - the function we want to solve -and \(t\) is time. \(f(x, t)\) defines how \(x\) changes over time.</p>

<p>This is a useful starting point, but in real-world data generation, we must account for <strong>randomness</strong>. That brings us to <strong>stochastic differential equations (SDEs)</strong>, which incorporate random fluctuations into the system.</p>

\[dx = f(x, t) dt + g(x, t) dW_t\]

<p>where the <em>drift term</em> \(f(x, t) dt\) captures the deterministic trends, while the <em>diffusion term</em> \(g(x, t) dW_t\) captures the random fluctuations via a Wiener process \(W_t\).</p>

<p>👉 Motivation: Diffusion models inject noise step by step, so SDEs provide the natural language to describe this stochastic corruption process. More specifically, the drift term \(f(x, t)\) is the shift of the mean of the distribution, and the diffusion term \(g(x, t)\) is the spread of the distribution - injecting Gaussian noise.</p>

<h3 id="forward-and-reverse-diffusion-processes">Forward and Reverse Diffusion Processes</h3>

<p><strong>Forward Process (Adding Noise)</strong></p>

<p>The forward diffusion process transform a data sample \(x_0\) into pure noise \(x_T\) over time:</p>

\[dx = f(x, t)dt + g(t) dW_t\]

<p>Intuitively, the drift term \(f(x, t) dt\) shifts the mean of the distribution by a deterministic amount \(f(x,t)\) (i.e., is a function of \(x\) and current time \(t\)) to a zero mean distribution. The diffusion term \(g(t) dW_t\) spreads the distribution by injecting Gaussian noise, increasing the variance of the distribution.</p>

<p>Note that \(g(t)\) is a function of current time \(t\) only, to guarantee that each noised distribution remains Gaussian with a know mean and variance. This make the forward process to be simple and tractable, that we can have exact sampling formula for each specific time step \(t\), i.e., \(q (x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)\).</p>

<p><strong>Reverse Process (Removing Noise)</strong>
In order to generate data from pure noise \(x_T\), we need to reverse the diffusion process by Reverse-Time SDE (Anderson 1982).</p>

\[dx = \left[ f(x,t) - \frac{1}{2} g^2(t) \nabla_x \log p_t(x) \right] dt + g(t) d\tilde{W}_t\]

<p>where \(\nabla_x \log p_t(x)\) is the <strong>score function</strong>, which estimates the structure of data at time \(t\) - how likely different data points are at each step. \(d\tilde{W}_t\) is another Wiener process but in the reverse direction.</p>

<p>👉 Motivation: Since \(f(x,t)\) and \(g(t))\) are known, to reverse noise, we must know the score function. So we train a neural network to approximate the score function \(\nabla_x \log p_t(x)\). This is the core of the diffusion model.</p>

<h3 id="euler-method-for-numerical-integration">Euler Method for Numerical Integration</h3>

<p>To simulate the reverse process, we need a numerical method to integrate the SDE backward through time. The Euler–Maruyama method (a stochastic generalization of the Euler method) provides a simple, first-order approximation.</p>

<p>For the variance-preserving (VP) SDE used in diffusion models, the deterministic part of the reverse process (ignoring noise sampling) can be expressed as an ODE:</p>

\[\frac{dx}{dt} = f(x,t) - \frac{1}{2} g^2(t) \nabla_x \log p_t(x)\]

<p>Now, using the Euler method to integrate this ODE backward in time from \(t_{n+1}\) to \(t_n\):</p>

\[x_{t_n} = x_{t_{n+1}} + (t_{n+1} - t_{n}) \frac{dx}{dt} \bigg|_{x=x_{t_{n+1}}, t=t_{n+1}}\]

<p>Substituing the ODE into the Euler method, we get:</p>

\[x_{t_n} = x_{t_{n+1}} + (t_{n+1} - t_{n}) \left[ f(x_{t_{n+1}}, t_{n+1}) - \frac{1}{2} g^2(t_{n+1}) \nabla_x \log p_{t_{n+1}}(x_{t_{n+1}}) \right]\]

<p>This stepwise update forms the foundation of diffusion sampling algorithms like DDPM and DDIM, where the score term is replaced by the neural network’s prediction.</p>

<h4 id="implementation-of-the-euler-method">Implementation of the Euler method</h4>

<p>In the context of <strong>consistency models</strong>, the update rule is derived from the Probability Flow ODE (PF-ODE):</p>

\[\frac{dx}{dt} = -t s_{\phi}(x,t)\]

<p>where \(s_{\phi}(x,t)\) is a learned score function related to the model output \(f_{\theta}(x,t)\).
From the definition of the consistency function (please refer to the Consistency Models section), we have:</p>

\[s_{\phi}(x,t) = \frac{x - f_{\theta}(x,t)}{t^2}\]

<p>Substituing this into the PF-ODE Euler update:</p>

\[\hat{x}_{t_n} = x_{t_{n+1}} - (t_n - t_{n+1}) t_{n+1} s_{\phi}(x_{t_{n+1}}, t_{n+1})\]

<p>We get:
\(\hat{x}_{t_n} = x_{t_{n+1}} - (t_n - t_{n+1}) \frac{x - f_{\theta}(x,t)}{t}\)</p>

<p>This formula defines a <strong>single-step explicit Euler integration</strong>: it can predict the next step \(x_{t_n}\) from the current step \(x_{t_{n+1}}\).</p>

<p>Python implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@th.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">euler_solver</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">next_t</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">samples</span>
    <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># f_{\theta}(x,t) - consistency model output
</span>    <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div>

<p>👉 Interpretation:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">d</code> is the difference between the current step and the next step.</li>
  <li>
<code class="language-plaintext highlighter-rouge">next_t - t</code> is the time step size (negative for reverse time integration).</li>
  <li>The Euler method uses only the slope at the start of the interval, making it simple but potentially less accurate for large steps.</li>
</ul>

<p><strong>Heun Method (Improved Euler / Predictor–Corrector)</strong></p>

<p>The Heun method is a second-order numerical scheme that improves on Euler by estimating the derivative twice — at the beginning and at the end of the time interval — and then averaging them. This yields better stability and smaller integration error, especially for stiff or nonlinear dynamics like those in diffusion models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-07-12-13-48-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-07-12-13-48-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-07-12-13-48-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-07-12-13-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>The update consists of two stages:</p>

<ul>
  <li>Predictor (Euler step):</li>
</ul>

\[\tilde{x}_{t_n} = x_{t_{n+1}} - (t_n - t_{n+1}) d_{t_{n+1}}\]

<p>where \(d_{t_{n+1}} = \frac{x_{t_{n+1}} - f_{\theta}(x_{t_{n+1}}, t_{n+1})}{t_{n+1}}\).</p>

<ul>
  <li>Corrector (Second-order correction): Evaluate a new derivative \(d_{t_n}\) at the predicted point \(\tilde{x}_{t_n}\), then update the final point:</li>
</ul>

\[\hat{x}_{t_n} = x_{t_{n+1}} + \frac{1}{2}(t_n - t_{n+1}) (d_{t_{n+1}} + d_{t_n})\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@th.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">heun_solver</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">next_t</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">samples</span>
    <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># f_{\theta}(x,t) - consistency model output
</span>
    
    <span class="c1"># IMPORTANT - Euler method
</span>    <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">next_t</span><span class="p">)</span> <span class="c1"># f_{\theta}(x,t) - consistency model output
</span>
    <span class="n">next_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="n">next_d</span><span class="p">)</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">((</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div>

<h3 id="fokker-planck-equation-from-trajectories-to-distributions">Fokker-Planck Equation: From Trajectories to Distributions</h3>

<p>SDEs describe how the distribution of a system changes over time, but what about the <strong>distribution of data</strong> as noise accumulates? The <strong>Fokker-Planck equation</strong> bridges the gap between trajectories and distributions, explaining how noise pushes data distributions \(p_t(x)\) toward isotropic Gaussian distributions.</p>

\[\frac{\partial p_t(x)}{\partial t} = -\nabla_x \cdot (f(x,t) p_t(x)) + \frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\]

<p>where \(p_t(x)\) is the distribution of the data at time \(t\).</p>

<p>The first term \(-\nabla_x \cdot (f(x,t) p_t(x))\) describes the change of the probability density \(p_t(x)\) with the drift term \(f(x,t)\) (as the velocity of that mass). The divergence operator \(\nabla_x \cdot\) measures how much the mass is spreading out (positive divergence) or converging/concentrating (negative divergence) at any given point \(x\). The whole term \(- \nabla_x \cdot (f(x,t) p_t(x))\) describes a <strong>rate of change</strong> of the probability density \(p_t(x)\) at any given point \(x\), where the positive value means the mass is flowing away from \(x\), causing \(p_t(x)\) to decrease (hence the negative sign), and vice versa.</p>

<p>The second term \(\frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\) presents the spreading and smoothing effect of the probability density \(p_t(x)\) over time due to the influence of the random fluctuations. More specifically, \(g(t)\) controls the magnitude of the random noise. The \(\nabla_x p_t(x)\) term describes the <strong>steepness</strong> or slope of \(p_t(x)\) at any given point \(x\). The whole term \(\frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\) describes a <strong>rate of change</strong> of the probability density \(p_t(x)\), i.e., the larger the gradient, the more the density rising sharply around \(x\). Similar to the first term, \(\nabla_x \cdot\) measures how much the mass is spreading out (positive divergence) or converging/concentrating (negative divergence) at any given point \(x\) with two differences:</p>

<ul>
  <li>It contains the random fluctuations term \(g(t)^2\) instead of the drift term \(f(x,t)\), introducing randomness into the system.</li>
  <li>It proportional to the gradient \(\nabla_x p_t(x)\), meaning that the <strong>slope/sharp region is more affected/spread out than the flat region</strong> (which has smaller gradient \(\nabla_x p_t(x)\)).</li>
</ul>

<h3 id="score-matching-and-denoising">Score Matching and Denoising</h3>

<p>Since the reverse SDE depends on the score function \(\nabla_x \log p_t(x)\) (which is intractable), we need to design a training objective to approximate this function. Vincent et al. proposed <strong>denoising score matching</strong> <a href="https://ieeexplore.ieee.org/abstract/document/6795935/" rel="external nofollow noopener" target="_blank">(Vincent 2011)</a> to approximate by training a neural network \(s_{\theta}(x_t, t)\) to approximate the conditional score function \(\nabla_x \log q(x_t \mid x_0, \epsilon)\) (where \(q\) is a tractable forward process, \(dx = f(x, t)dt + g(t)dW_t\)) <strong>assuming</strong> that \(q(x_t \mid x_0) \approx p_t(x_t)\) at time \(t\) (which is a reasonable assumption).</p>

<p>This objective aims to minimize the difference:</p>

\[\mathbb{E}_{p(x_0), \epsilon \sim \mathcal{N}(0, I)} \left[ \left\| s_{\theta}(x_t, t) - \nabla_x \log p_t(x_t \mid x_0, \epsilon) \right\|^2 \right]\]

<h3 id="variational-perspective-and-kl-minimization">Variational Perspective and KL Minimization</h3>

<p>Another way to frame diffusion models is to consider them as a variational inference problem. The forward process \(q(x_{0:T})\) is a know noising chain of distributions, and the reverse process \(p_{\theta}(x_{0:T})\) is learned. Therefore, we can use the variational lower bound (ELBO) to train the model.</p>

\[\mathbb{E}_{q(x_{0:T})} \left[ D_{KL} \left( q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) \right]\]

<p><strong>ELBO</strong></p>

<p><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="external nofollow noopener" target="_blank"><strong>Evidence lower bound (ELBO)</strong></a> is a key concept in variational inference, which is used in VAEs to approximate the log-likelihood of the data.</p>

<p>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the marginal distribution of \(X\), and \(p_\theta(Z \mid X)\) is the conditional distribution of \(Z\) given \(X\). Then, for a sample \(x \sim p_{\text{data}}\), and any distribution \(q_\phi\), the ELBO is defined as</p>

\[L(\phi, \theta; x) := \mathbb{E}_{z\sim q_\phi(\cdot|x)} \left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>The ELBO can equivalently be written as</p>

\[\begin{aligned}
L(\phi, \theta; x) &amp;= \mathbb{E}_{z\sim q_\phi(\cdot|x)}[\ln p_\theta(x,z)] + H[q_\phi(z \mid x)] \\
&amp;= \ln p_\theta(x) - D_{KL}(q_\phi(z \mid x) || p_\theta(z \mid x)).
\end{aligned}\]

<p>In the first line, \(H[q_\phi(z \mid x)]\) is the entropy of \(q_\phi\), which relates the ELBO to the Helmholtz free energy. In the second line, \(\ln p_\theta(x)\) is called the evidence for \(x\), and \(D_{KL}(q_\phi(z \mid x) \mid\mid p_\theta(z \mid x))\) is the Kullback-Leibler divergence between \(q_\phi\) and \(p_\theta\). Since the Kullback-Leibler divergence is non-negative, \(L(\phi, \theta; x)\) forms a lower bound on the evidence (ELBO inequality)</p>

\[\ln p_\theta(x) \geq \mathbb{E}_{z\sim q_\phi(\cdot|x)}\left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>Deep-dive topics about VAE might including:</p>

<ul>
  <li>Reparameterization Trick: <a href="https://en.wikipedia.org/wiki/Reparameterization_trick" rel="external nofollow noopener" target="_blank">How to sample from a distribution in a differentiable way - Wiki</a>
</li>
  <li>The problem of KL divergence: <a href="https://andrewcharlesjones.github.io/journal/klqp.html" rel="external nofollow noopener" target="_blank">mode seeking vs mode covering</a> by Andy Jones</li>
  <li><a href="https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae" rel="external nofollow noopener" target="_blank">A nice property of VAEs: Disentanglement Representation Learning</a></li>
</ul>

<h3 id="tweedies-formula">Tweedie’s formula</h3>

<p>Finally, <strong>Tweedie’s formula</strong> gives a neat probabilistic justification for the denoising score matching objective:</p>

\[\mathbb{E} [ x_0 \mid x_t] = x_t + \sigma_t^2 s_{\theta}(x_t, t)\]

<p>where \(s_{\theta}(x_t, t)\) is the score function to be learned by the neural network. It shows that the posterior mean of clean data given a noisy data is just the noisy sample plus a correction term proportional to the score function.</p>

<p>In some papers such as ESD (Gandikota et al. 2023), where we need to fine-tune the pretrained model and match the score function of the original and the fine-tuned model, they use Tweedie’s formula to justify the matching term.</p>

<h2 id="variants-of-diffusion-models">Variants of Diffusion Models</h2>

<p>The original formulation of diffusion models can be implemented in several ways. Two of the most influential variants are <strong>Denoising Diffusion Probabilistic Models (DDPMs)</strong> and <strong>Denoising Diffusion Implicit Models (DDIMs)</strong>. Both share the same forward noising process but differ in how they perform the reverse (denoising) process during inference.</p>

<h3 id="ddpm">DDPM</h3>

<p>DDPM (Ho et al. 2020) is the classic diffusion model:</p>

<ul>
  <li>The Reverse process is defined as a <strong>Markov chain</strong>, where each step \(x_t \to x_{t-1}\) involves sampling from a Gaussian distribution conditioned on \(x_t\).</li>
  <li>Sampling os stochastic, even with the same starting noise \(x_T\), the generated data \(x_0\) is different.</li>
  <li>While highly effective and stable (compared to GANs), DDPMs require hundreds to thousands of steps to slowly add/remove noise, which makes inference slow.</li>
</ul>

<p>Read more about DDPM in another blog post <a href="/blog/2023/diffusion-tutorial/">here</a></p>

<h3 id="ddim">DDIM</h3>

<p>DDIM (Song et al. 2020) builds on DDPM but introduces a <strong>non-Markovian reverse process</strong>, enabling faster sampling.
It also allows us to use the same training process as DDPM, e.g., we can use pretrained DDPM models to generate data.</p>

<p>The sampling process of DDIM is as follows:</p>

\[x_{t-1} = \sqrt{\alpha_{t-1}} \left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right) + \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}(x_t) + \sigma_t\epsilon_t\]

<p>where the first term represents the “predicted \(x_0\)”, the second term is the “direction pointing to \(x_t\)”, and the last term is random noise.</p>

<p>By setting \(\sigma_t = 0\) for all \(t\), DDIM becomes a deterministic process given \(x_{t-1}\) and \(x_0\), except for \(t=1\). In other words, the intermediate steps \(x_{T-1}, x_{T-2}, \ldots, x_1\) are deterministic given starting noise \(x_T\).</p>

<p>Read more about DDIM in another blog post <a href="/blog/2023/diffusion-tutorial-p2/">here</a></p>

<h3 id="score-matching">Score Matching</h3>

<p>Score-based generative models (<a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf" rel="external nofollow noopener" target="_blank">Song and Ermon 2019</a>, <a href="https://openreview.net/forum?id=PxTIG12RRHS" rel="external nofollow noopener" target="_blank">Song et al. 2021</a>(published in ICLR 2021, are a family of generative models that learn to estimate the score function — the gradient of the log-density of the data distribution \(s_\theta(x, t) \approx \nabla_x \log p_t(x)\). Intuitively, the score function tells us which direction in the input space increases the likelihood of the data. By learning this function at different noise levels (at different time steps \(t\)), the model learns how to denoise a sample toward realistic data.</p>

<p><strong>Training Objective</strong></p>

<p>The ideal objective of score matching is:</p>

\[\min_{\theta} \mathbb{E}_{p_t(x)} \left[ \| s_\theta(x, t) - \nabla_x \log p_t(x) \|^2 \right]\]

<p>which can be shown to be equivalent to</p>

\[\min_{\theta} \mathbb{E}_{p_t(x)} \left[ \text{trace}(\nabla_x s_{\theta}(x, t)) + \frac{1}{2} \| s_{\theta}(x, t) \|^2 \right]\]

<p>With deep networks and high-dimensional data, it is difficult to obtain the expectation over the data distribution, especially with the \(\text{trace}(\nabla_x s_{\theta}(x, t))\).</p>

<p>One of the popular ways to overcome this is to use <strong>Denoising Score Matching (DSM)</strong> (Vincent 2011). The idea is first to perturbs the data \(x\) with a <strong>known Gaussian noise process</strong> \(q(x_t \mid x_0)\), then train the score network \(s_\theta(x_t, t)\) to denoise the perturbed data \(x_t\) back to the original data \(x_0\).</p>

\[\min_{\theta} \mathbb{E}_{q(x_t \mid x), p_t(x)} \left[ \| s_\theta(x_t, t) - \nabla_{x_t} \log q(x_t \mid x) \|^2 \right]\]

<p>For Gaussian perturbations:</p>

\[\nabla_{x_t} \log q(x_t \mid x) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2}\]

<p>Hence, we can train \(s_\theta(x_t, t)\) to match this target directly.</p>

<p><strong>Sampling</strong></p>

<p>In Song and Ermon 2019, they use Langevin dynamics to samples data with the score function \(s_\theta(x, t)\). Given a fixed step size \(\epsilon\), and an inital value \(x_T \sim \mathcal{N}(0, I)\), the Langevin dynamics recursively update the data as follows:</p>

\[x_{t-1} = x_t + \frac{\epsilon}{2} s_{\theta}(x_t, t) + \sqrt{\epsilon} z_t\]

<p>where \(z_t \sim \mathcal{N}(0, I)\).</p>

<p><strong>\(\epsilon\)-prediction</strong></p>

<p>Instead of predicting the score directly, DDPM variants predict the noise \(\epsilon\) added to the data, which is equivalent to predicting the score but more numerically stable.</p>

<p>The data corruption process is:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]

<p>Then the score function relates to the noise predictor as:</p>

\[s_\theta(x_t, t) = -\frac{\epsilon_{\theta}(x_t, t)}{\sqrt{1 - \bar{\alpha}_t}}\]

<p>The training loss becomes:</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon_\theta(x_t, t) - \epsilon \|^2 \right]\]

<p><strong>From DDPM to Score Matching</strong></p>

<p>It can be seen that the key idea of both DDPM and SMLD is to perturb the data with a known noise distribution, and then denoise back to the original data.
In the following work (Song et al. 2021), the authors unified the two frameworks into a single framework called <strong>Score Matching</strong>, with an infinite number of noise scales.</p>

<p>First, the diffusion process can be defined as a SDE:</p>

\[dx = f(x, t) dt + g(t) dW_t\]

<p>where \(f(x, t)\) is the drift term and \(g(t)\) is the diffusion term and \(W_t\) is the Wiener process.</p>

<p>The reverse diffusion process, which denoises the data from \(x_T \sim \mathcal{N}(0, I)\) to \(x_0 \sim p(x)\), can be defined as a reverse-time SDE:</p>

\[dx = [f(x, t) - g(t)^2 \nabla_x \log p_t(x)] dt + g(t) d\bar{W}_t\]

<p>where \(\bar{W}_t\) is the reverse-time Wiener process.</p>

<p>Different diffusion model families have different choices of \(f(x, t)\) and \(g(t)\):</p>

<ul>
  <li>Variance Exploding (VE) SDE: \(f(x, t) = 0, g(t) = \sqrt{\frac{d\sigma_t^2}{dt}}\) as in SMLD</li>
  <li>Variance Preserving (VP) SDE: \(f(x, t) = -\frac{1}{2}\beta_t x, g(t) = \sqrt{\beta_t}\) as in DDPM</li>
</ul>

<p><strong>Sampling with the Euler-Maruyama method</strong></p>

<p>Once trained, the score network defines the reverse-time dynamics that transform Gaussian noise \(x_T \sim \mathcal{N}(0, I)\) to the data sample \(x_0 \sim p(x)\).
We can simulate the reverse SDE numerically using the Euler-Maruyama method:</p>

\[x_{t-1} = x_t + (t_n - t_{n+1}) \frac{dx}{dt} \big| _{x=x_t, t = t_n}\]

\[x_{t-1} = x_t + (t_n - t_{n+1}) \left[ f(x_t, t_n) - g(t_n)^2 \nabla_x \log p_{t_n}(x_t) \right] + g(t_n) \sqrt{t_n - t_{n+1}} z_t\]

<p>where \(z_t \sim \mathcal{N}(0, I)\).</p>

<p>This adds both a <strong>deterministic drift</strong> (using the learned score function) and a <strong>stochastic noise term</strong> to preserve the sample diversity.</p>

<p>If we remove the noise term (set \(z_t = 0\)), we obtain the <strong>Probability Flow ODE</strong>, a deterministic trajectory equivalent in distribution to the reverse-time SDE, which is the foundation of DDIM deterministic sampling.</p>

<p><strong>Implementation of Score Matching</strong></p>

<p>The official implementation of Score Matching is <a href="https://github.com/yang-song/score_sde_pytorch" rel="external nofollow noopener" target="_blank">https://github.com/yang-song/score_sde_pytorch</a>.</p>

<p>In Diffusers lib, the Score SDV VP and VE schedulers can be found <a href="https://huggingface.co/docs/diffusers/en/api/schedulers/score_sde_ve" rel="external nofollow noopener" target="_blank">here</a> and <a href="https://huggingface.co/docs/diffusers/en/api/schedulers/score_sde_vp" rel="external nofollow noopener" target="_blank">here</a>.</p>

<p>Notable functions/methods in the official implementation (PyTorch version):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">discretize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.

    Useful for reverse diffusion sampling and probabiliy flow sampling.
    Defaults to Euler-Maruyama discretization.

    Args:
      x: a torch tensor
      t: a torch float representing the time step (from 0 to `self.T`)

    Returns:
      f, G
    </span><span class="sh">"""</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">N</span>
    <span class="n">drift</span><span class="p">,</span> <span class="n">diffusion</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sde</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">drift</span> <span class="o">*</span> <span class="n">dt</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">diffusion</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">t</span><span class="p">.</span><span class="n">device</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">G</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  <span class="k">def</span> <span class="nf">reverse</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score_fn</span><span class="p">,</span> <span class="n">probability_flow</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Create the reverse-time SDE/ODE.

    Args:
      score_fn: A time-dependent score-based model that takes x and t and returns the score.
      probability_flow: If `True`, create the reverse-time ODE used for probability flow sampling.
    </span><span class="sh">"""</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">N</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span>
    <span class="n">sde_fn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sde</span>
    <span class="n">discretize_fn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">discretize</span>

    <span class="c1"># Build the class for reverse-time SDE.
</span>    <span class="k">class</span> <span class="nc">RSDE</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">__class__</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="n">self</span><span class="p">.</span><span class="n">probability_flow</span> <span class="o">=</span> <span class="n">probability_flow</span>

      <span class="nd">@property</span>
      <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">T</span>

      <span class="k">def</span> <span class="nf">sde</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Create the drift and diffusion functions for the reverse SDE/ODE.</span><span class="sh">"""</span>
        <span class="n">drift</span><span class="p">,</span> <span class="n">diffusion</span> <span class="o">=</span> <span class="nf">sde_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="nf">score_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">drift</span> <span class="o">=</span> <span class="n">drift</span> <span class="o">-</span> <span class="n">diffusion</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">score</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">probability_flow</span> <span class="k">else</span> <span class="mf">1.</span><span class="p">)</span>
        <span class="c1"># Set the diffusion function to zero for ODEs.
</span>        <span class="n">diffusion</span> <span class="o">=</span> <span class="mf">0.</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">probability_flow</span> <span class="k">else</span> <span class="n">diffusion</span>
        <span class="k">return</span> <span class="n">drift</span><span class="p">,</span> <span class="n">diffusion</span>

      <span class="k">def</span> <span class="nf">discretize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Create discretized iteration rules for the reverse diffusion sampler.</span><span class="sh">"""</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">G</span> <span class="o">=</span> <span class="nf">discretize_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">rev_f</span> <span class="o">=</span> <span class="n">f</span> <span class="o">-</span> <span class="n">G</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="nf">score_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">probability_flow</span> <span class="k">else</span> <span class="mf">1.</span><span class="p">)</span>
        <span class="n">rev_G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">G</span><span class="p">)</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">probability_flow</span> <span class="k">else</span> <span class="n">G</span>
        <span class="k">return</span> <span class="n">rev_f</span><span class="p">,</span> <span class="n">rev_G</span>

    <span class="k">return</span> <span class="nc">RSDE</span><span class="p">()</span>
</code></pre></div></div>

<p>IMPORTANT NOTE: For models trained with VP SDE, the marginal distribution of \(x_t\) given clean data \(x_0\) is Gaussian: \(x_t \sim \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, \sigma_t^2 I)\) or in general SDE notation: \(x_t = \text{mean}(x_0, t) + \text{std}(t) \dot z\) where \(z \sim \mathcal{N}(0, I)\).</p>

<p>Therefore, when training, we sample</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="nf">marginal_prob</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">perturbed_data</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">z</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">VPSDE</span><span class="p">(</span><span class="n">SDE</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">beta_min</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">beta_max</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Construct a Variance Preserving SDE.

    Args:
      beta_min: value of beta(0)
      beta_max: value of beta(1)
      N: number of discretization steps
    </span><span class="sh">"""</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">beta_0</span> <span class="o">=</span> <span class="n">beta_min</span>
    <span class="n">self</span><span class="p">.</span><span class="n">beta_1</span> <span class="o">=</span> <span class="n">beta_max</span>
    <span class="n">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
    <span class="n">self</span><span class="p">.</span><span class="n">discrete_betas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">beta_min</span> <span class="o">/</span> <span class="n">N</span><span class="p">,</span> <span class="n">beta_max</span> <span class="o">/</span> <span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">discrete_betas</span>
    <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">sqrt_1m_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span>

  <span class="k">def</span> <span class="nf">sde</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">beta_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta_1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">beta_0</span><span class="p">)</span>
    <span class="n">drift</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">beta_t</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">diffusion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">drift</span><span class="p">,</span> <span class="n">diffusion</span>

  <span class="k">def</span> <span class="nf">marginal_prob</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">log_mean_coeff</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta_1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">beta_0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">beta_0</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">log_mean_coeff</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">log_mean_coeff</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>

  <span class="k">def</span> <span class="nf">prior_sampling</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">prior_logp</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">logps</span> <span class="o">=</span> <span class="o">-</span><span class="n">N</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">return</span> <span class="n">logps</span>

  <span class="k">def</span> <span class="nf">discretize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DDPM discretization.</span><span class="sh">"""</span>
    <span class="n">timestep</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">discrete_betas</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)[</span><span class="n">timestep</span><span class="p">]</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)[</span><span class="n">timestep</span><span class="p">]</span>
    <span class="n">sqrt_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">sqrt_beta</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">G</span>

</code></pre></div></div>

<p><strong>The SDE loss function</strong>. IMPORTANT, in the implementation, <code class="language-plaintext highlighter-rouge">losses = (score x std + z)**2</code> is used. Why is that?</p>

<p>It is because of the Gaussian property where \(p(x_t \mid x_0) = \mathcal{N}(\mu_t, \sigma_t^2 I)\).</p>

\[\nabla_x \log p(x_t \mid x_0) = -\frac{1}{\sigma_t^2} (x_t - \mu_t) = - \frac{z}{\sigma_t}\]

<p>Therefore, the final loss function is:</p>

\[\mathcal{L}_{SDE}(\theta) = \mathbb{E}_{t, x_0, z} \left[ \| s_\theta(x_t, t) + \frac{z}{\sigma_t} \|^2 \right]\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">get_sde_loss_fn</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">reduce_mean</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">continuous</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">likelihood_weighting</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">Create a loss function for training with arbirary SDEs.

  Args:
    sde: An `sde_lib.SDE` object that represents the forward SDE.
    train: `True` for training loss and `False` for evaluation loss.
    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.
    continuous: `True` indicates that the model is defined to take continuous time steps. Otherwise it requires
      ad-hoc interpolation to take continuous time steps.
    likelihood_weighting: If `True`, weight the mixture of score matching losses
      according to https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended in our paper.
    eps: A `float` number. The smallest time step to sample from.

  Returns:
    A loss function.
  </span><span class="sh">"""</span>
  <span class="n">reduce_op</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span> <span class="k">if</span> <span class="n">reduce_mean</span> <span class="k">else</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute the loss function.

    Args:
      model: A score model.
      batch: A mini-batch of training data.

    Returns:
      loss: A scalar that represents the average loss value across the mini-batch.
    </span><span class="sh">"""</span>
    <span class="n">score_fn</span> <span class="o">=</span> <span class="n">mutils</span><span class="p">.</span><span class="nf">get_score_fn</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">continuous</span><span class="o">=</span><span class="n">continuous</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">sde</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="nf">marginal_prob</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">perturbed_data</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">std</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">z</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">score_fn</span><span class="p">(</span><span class="n">perturbed_data</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">likelihood_weighting</span><span class="p">:</span>
      <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">score</span> <span class="o">*</span> <span class="n">std</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span>
      <span class="n">losses</span> <span class="o">=</span> <span class="nf">reduce_op</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">g2</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="nf">sde</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">t</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
      <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">score</span> <span class="o">+</span> <span class="n">z</span> <span class="o">/</span> <span class="n">std</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">])</span>
      <span class="n">losses</span> <span class="o">=</span> <span class="nf">reduce_op</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

  <span class="k">return</span> <span class="n">loss_fn</span>
</code></pre></div></div>

<p>The score function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">def</span> <span class="nf">get_score_fn</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">continuous</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">Wraps `score_fn` so that the model output corresponds to a real time-dependent score function.

  Args:
    sde: An `sde_lib.SDE` object that represents the forward SDE.
    model: A score model.
    train: `True` for training and `False` for evaluation.
    continuous: If `True`, the score-based model is expected to directly take continuous time steps.

  Returns:
    A score function.
  </span><span class="sh">"""</span>
  <span class="n">model_fn</span> <span class="o">=</span> <span class="nf">get_model_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>

  <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">sde_lib</span><span class="p">.</span><span class="n">VPSDE</span><span class="p">)</span> <span class="ow">or</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">sde_lib</span><span class="p">.</span><span class="n">subVPSDE</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">score_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
      <span class="c1"># Scale neural network output by standard deviation and flip sign
</span>      <span class="k">if</span> <span class="n">continuous</span> <span class="ow">or</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">sde_lib</span><span class="p">.</span><span class="n">subVPSDE</span><span class="p">):</span>
        <span class="c1"># For VP-trained models, t=0 corresponds to the lowest noise level
</span>        <span class="c1"># The maximum value of time embedding is assumed to 999 for
</span>        <span class="c1"># continuously-trained models.
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="mi">999</span>
        <span class="n">score</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="nf">marginal_prob</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">t</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># For VP-trained models, t=0 corresponds to the lowest noise level
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="n">sde</span><span class="p">.</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="n">sqrt_1m_alphas_cumprod</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="n">device</span><span class="p">)[</span><span class="n">labels</span><span class="p">.</span><span class="nf">long</span><span class="p">()]</span>

      <span class="n">score</span> <span class="o">=</span> <span class="o">-</span><span class="n">score</span> <span class="o">/</span> <span class="n">std</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">score</span>

  <span class="k">elif</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">sde</span><span class="p">,</span> <span class="n">sde_lib</span><span class="p">.</span><span class="n">VESDE</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">score_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">continuous</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="nf">marginal_prob</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">t</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># For VE-trained models, t=0 corresponds to the highest noise level
</span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">sde</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">t</span>
        <span class="n">labels</span> <span class="o">*=</span> <span class="n">sde</span><span class="p">.</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

      <span class="n">score</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">score</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">SDE class </span><span class="si">{</span><span class="n">sde</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="si">}</span><span class="s"> not yet supported.</span><span class="sh">"</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">score_fn</span>
</code></pre></div></div>

<p><strong>Implementation of SMLD</strong></p>

<p>From the same repository of SDE Pytorch</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">get_smld_loss_fn</span><span class="p">(</span><span class="n">vesde</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">reduce_mean</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">Legacy code to reproduce previous results on SMLD(NCSN). Not recommended for new work.</span><span class="sh">"""</span>
  <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">vesde</span><span class="p">,</span> <span class="n">VESDE</span><span class="p">),</span> <span class="sh">"</span><span class="s">SMLD training only works for VESDEs.</span><span class="sh">"</span>

  <span class="c1"># Previous SMLD models assume descending sigmas
</span>  <span class="n">smld_sigma_array</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flip</span><span class="p">(</span><span class="n">vesde</span><span class="p">.</span><span class="n">discrete_sigmas</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
  <span class="n">reduce_op</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span> <span class="k">if</span> <span class="n">reduce_mean</span> <span class="k">else</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">model_fn</span> <span class="o">=</span> <span class="n">mutils</span><span class="p">.</span><span class="nf">get_model_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vesde</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">sigmas</span> <span class="o">=</span> <span class="n">smld_sigma_array</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">device</span><span class="p">)[</span><span class="n">labels</span><span class="p">]</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmas</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">perturbed_data</span> <span class="o">=</span> <span class="n">noise</span> <span class="o">+</span> <span class="n">batch</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">perturbed_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="o">-</span><span class="n">noise</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigmas</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">score</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="nf">reduce_op</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmas</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

  <span class="k">return</span> <span class="n">loss_fn</span>
</code></pre></div></div>

<p><strong>Implementation of DDPM</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">get_ddpm_loss_fn</span><span class="p">(</span><span class="n">vpsde</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">reduce_mean</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">Legacy code to reproduce previous results on DDPM. Not recommended for new work.</span><span class="sh">"""</span>
  <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">vpsde</span><span class="p">,</span> <span class="n">VPSDE</span><span class="p">),</span> <span class="sh">"</span><span class="s">DDPM training only works for VPSDEs.</span><span class="sh">"</span>

  <span class="n">reduce_op</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span> <span class="k">if</span> <span class="n">reduce_mean</span> <span class="k">else</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">model_fn</span> <span class="o">=</span> <span class="n">mutils</span><span class="p">.</span><span class="nf">get_model_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vpsde</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">sqrt_alphas_cumprod</span> <span class="o">=</span> <span class="n">vpsde</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">sqrt_1m_alphas_cumprod</span> <span class="o">=</span> <span class="n">vpsde</span><span class="p">.</span><span class="n">sqrt_1m_alphas_cumprod</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">perturbed_data</span> <span class="o">=</span> <span class="n">sqrt_alphas_cumprod</span><span class="p">[</span><span class="n">labels</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch</span> <span class="o">+</span> \
                     <span class="n">sqrt_1m_alphas_cumprod</span><span class="p">[</span><span class="n">labels</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">noise</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">perturbed_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">score</span> <span class="o">-</span> <span class="n">noise</span><span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="nf">reduce_op</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">losses</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

  <span class="k">return</span> <span class="n">loss_fn</span>
</code></pre></div></div>

<h2 id="flow-matching">Flow Matching</h2>

<h3 id="fundammentals-concepts-in-flow-matching">Fundammentals Concepts in Flow Matching</h3>

<p><strong>Normalizing Flow</strong>: A class of generative models that learns a transformation (or “flow”) to map a know prior distribution \(p_0\) to a target distribution \(p_1\) through a family of intermediate marginal distributions \(p_t\), where \(t \in [0, 1]\). A key requirement is that the transformation must be invertible (bijective).</p>

<p><strong>Continuous Normalizing Flow</strong>: Uses ordinary differential equation (ODE) to define continuous-time transformations between distributions.</p>

<p><strong>Flow and Velocity Field</strong>:</p>

<ul>
  <li>The flow \(\psi_t(x)\) describes the trajectory of a point \(x\) over time.</li>
  <li>The velocity field \(u_t(x)\) specifies the instantaneous direction and speed of movement</li>
  <li>These are related by the ODE: \(\frac{d}{dt} \psi_t(x) = u_t (\psi_t (x))\)</li>
  <li>The induced density \(p_t(x)\) evolves according to the continuity equation: \(\frac{\partial p_t(x)}{\partial t} + \nabla_x \cdot \big( u_t(x) \, p_t(x) \big) = 0.\)</li>
</ul>

<p>This equation shows how a point moves along the flow path: \(x_t \rightarrow x_{t+1} = x_t + dt * u_t(x_t)\) at time \(t\).</p>

<p><strong>Key insight</strong>: The velocity field \(u_t(x)\) is the only component neccessary to sample from \(p_t\) by solving the ODE. Therefore, <strong>flow matching aims to learn the velocity field \(u_t(x)\)</strong>.</p>

<h3 id="derivation-of-the-flow-matching-objective">Derivation of the Flow Matching Objective</h3>

<p><strong>Starting Objective</strong>: Approximate the velocity field \(u_t(x)\) with the learned velocity field \(v_{\theta}(t, x)\).</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t) \|^2 \right]\]

<p><strong>Step 1</strong>: Expand the squared norm:</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t) \|^2 \right] = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) \|^2 - 2 \langle v_{\theta}(t, x_t), u_t(x_t) \rangle + \| u_t(x_t) \|^2 \right]\]

<p><strong>Step 2</strong>: Express the velocity field as a conditional expectation:</p>

\[u_t(x_t) = \int u_t(x_t \mid x_1) \frac{p_t (x_t \mid x_1) q(x_1)}{p_t(x_t)} dx_1\]

<p><strong>Interpretation</strong>: The velocity at \(x_t\) is a weighted average of conditional velocities \(u_t(x_t \mid x_1)\) from all possible data points \(x_1\). Point \(x_1\) that are “closer” to \(x_t\) (higher probability \(p_t (x_t \mid x_1)\)) contribute more to the velocity at \(x_t\).</p>

<p><strong>Step 3</strong>: Substitute into the cross-term expectation (correlation between \(v_{\theta}(t, x_t)\) and \(u_t(x_t)\))</p>

\[\mathbb{E}_{x_t \sim p_t(x)} \left[ \langle v_{\theta}(t, x_t), u_t(x_t) \rangle \right] = \int p_t(x_t) v_{\theta}(t, x_t) \cdot u_t(x_t) dx_t\]

<p>Substitute \(u_t(x_t)\) to the above equation:</p>

\[= \int \int v_{\theta}(t, x_t) \cdot u_t(x_t \mid x_1) \cdot p_t(x_t \mid x_1) \cdot q(x_1) dx_1 dx_t\]

\[= \mathbb{E}_{x_t \sim p_t(x_t \mid x_1), x_1 \sim q(x_1)} \left[ v_{\theta}(t, x_t) \cdot u_t(x_t \mid x_1) \right]\]

<p><strong>Step 4</strong>: Rewrite the full objective using conditional expectation:</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) \|^2 - 2 \langle v_{\theta}(t, x_t), u_t(x_t \mid x_1) \rangle + \| u_t(x_t) \|^2 \right]\]

<p><strong>Step 5</strong>: Add and subtract the term \(\| u_t(x_t \mid x_1) \|^2\)</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t \mid x_1) \|^2 \right] + \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| u_t(x_t) \|^2 - \| u_t(x_t \mid x_1) \|^2\right]\]

<p><strong>Step 6</strong>: Drop constant terms that are independent of \(\theta\):</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t \mid x_1) \|^2 \right]\]

<p><strong>Practical Implementation</strong>:</p>

<p>Simply choose the linear interpolation path \(X_t = (1 - t) X_0 + t X_1\), then the velocity field \(u_t(x_t)\) is:</p>

\[u_t(x_t \mid x_1) = \frac{d}{dt} X_t = X_1 - X_0\]

<p>This give us a tractable training objective where we sample:</p>

<ul>
  <li>A time step \(t \sim \mathcal{U}(0, 1)\)</li>
  <li>A data ppont \(x_1 \sim q(x_1)\)</li>
  <li>A noise sample \(x_0 \sim \mathcal{N}(0, I)\)</li>
  <li>Construct the interpolated sample \(x_t = (1 - t) x_0 + t x_1\)</li>
  <li>Train to predict the velocity field \(v_{\theta}(t, x_t) = x_1 - x_0\)</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-02-11-15-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-02-10-03-19.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<h3 id="how-to-sampling-from-flow-matching-model">How to sampling from Flow Matching model</h3>

<p>The sampling process of Flow Matching model is similar to the diffusion model, where we start from a noise sample \(x_0 \sim \mathcal{N}(0, I)\) and iteratively sample the next step \(x_{t+dt}\) by Euler method:</p>

\[x_{t+dt} = x_t + dt * v_{\theta}(t+dt/2, x_t+dt/2 * v_{\theta}(t, x_t))\]

<p>where \(v_{\theta}(t, x_t)\) is the velocity field predicted by the neural network.</p>

<h3 id="flow-matching-code-example">Flow Matching Code Example</h3>

<p><a href="https://github.com/facebookresearch/flow_matching/blob/main/examples/standalone_flow_matching.ipynb" rel="external nofollow noopener" target="_blank">A Standalone Flow Matching code</a> - from [4]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Define the Flow
</span><span class="k">class</span> <span class="nc">Flow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_start</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_end</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_start</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_t</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="nf">self</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="nf">self</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">flow</span> <span class="o">=</span> <span class="nc">Flow</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">flow</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="nf">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x_1</span>
    <span class="n">dx_t</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="nf">loss_fn</span><span class="p">(</span><span class="nf">flow</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">),</span> <span class="n">dx_t</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="c1"># Sampling
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">t_start</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">t_end</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p>In the above code, the <code class="language-plaintext highlighter-rouge">forward</code> function is for the velocity field \(v_{\theta}(t, x)\), and the <code class="language-plaintext highlighter-rouge">step</code> function is to get the next step \(X_{t+dt}\) from the current step \(X_t\) by Euler method</p>

\[X_{t+dt} = X_t + dt * v_{\theta}(t+dt/2, X_t+dt/2 * v_{\theta}(t, X_t))\]

<h3 id="conditional-flow-matching">Conditional Flow Matching</h3>

<p>(Note that the <strong>Conditional</strong> in the name of Conditional Flow Matching is meaning the condition \(c\) is given, not the conditional vector field \(u_t(x \mid x_1)\) from previous step)</p>

<p>In conditional flow matching, we incorporate a condition \(c\) into the velocity field \(v_{\theta}(t, x, c)\).
In practice, the three inputs \(t, x, c\) are concatenated together as the input to the neural network.</p>

<p>Sampling function:</p>

\[X_{t+dt} = X_t + dt * v_{\theta}(t+dt/2, X_t+dt/2 * v_{\theta}(t, X_t, c), c)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Define the Flow
</span><span class="k">class</span> <span class="nc">Flow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_start</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_end</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_start</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_t</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="nf">self</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="nf">self</span><span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">flow</span> <span class="o">=</span> <span class="nc">Flow</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">flow</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">x_1</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nf">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x_1</span>
    <span class="n">dx_t</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="nf">loss_fn</span><span class="p">(</span><span class="nf">flow</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">),</span> <span class="n">dx_t</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Sampling
# --- evaluation / visualisation section --------------------------
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">256</span>                    

<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">x</span>      <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>     <span class="c1"># (n_samples, 2)
</span>
<span class="c1"># if you just want random labels –– otherwise load real labels here
</span><span class="n">c_eval</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (n_samples, 1)
</span>
<span class="c1"># colours for the scatter (same length as x)
</span><span class="n">colors</span>  <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span> <span class="k">if</span> <span class="n">lbl</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">'</span><span class="s">orange</span><span class="sh">'</span> <span class="k">for</span> <span class="n">lbl</span> <span class="ow">in</span> <span class="n">c_eval</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()]</span>

<span class="c1"># -----------------------------------------------------------------
</span><span class="n">n_steps</span>      <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_every</span>   <span class="o">=</span> <span class="mi">20</span>
<span class="n">plot_indices</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">plot_every</span><span class="p">))</span>
<span class="k">if</span> <span class="n">plot_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_steps</span><span class="p">:</span>
    <span class="n">plot_indices</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span>   <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">plot_indices</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">plot_indices</span><span class="p">),</span> <span class="mi">4</span><span class="p">),</span>
                           <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">time_steps</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># initial frame
</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">plot_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>                         <span class="c1"># no gradients while sampling
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                      <span class="n">t_start</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                      <span class="n">t_end</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">c_eval</span><span class="p">)</span>               <span class="c1"># 2️⃣ use the same‑sized label tensor
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">in</span> <span class="n">plot_indices</span><span class="p">:</span>
            <span class="n">plot_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<p>References:</p>

<ul>
  <li>[1]<a href="https://arxiv.org/abs/2210.02747" rel="external nofollow noopener" target="_blank">Flow Matching for Generative Modeling</a> paper</li>
  <li>[2]<a href="https://youtu.be/7cMzfkWFWhI?si=rRnZQKxs9p-_zjhZ" rel="external nofollow noopener" target="_blank">A cool explanation of Flow Matching</a>
</li>
  <li>[3]<a href="https://diffusionflow.github.io/" rel="external nofollow noopener" target="_blank">Diffusion Meets Flow Matching: Two Sides of the Same Coin</a>
</li>
  <li>[4]<a href="https://neurips.cc/media/neurips-2024/Slides/99531.pdf" rel="external nofollow noopener" target="_blank">A NeurIPS 2024 tutorial on Flow Matching</a>
</li>
</ul>

<h2 id="differences-between-score-matching-diffusion-models-and-flow-matching">Differences between Score Matching, Diffusion Models and Flow Matching</h2>

<h3 id="summary-of-main-differences">Summary of Main Differences</h3>

<p>All three methods learn generative models by establishing a connection between a simple noise distribution and a complex data distribution, but they differ fundamentally in their formulation and training approach:</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Diffusion Models (DDPM/DDIM)</th>
      <th>Score Matching (SDE)</th>
      <th>Flow Matching (CFM)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Core Learning Target</strong></td>
      <td>Learn to predict noise \(\epsilon_t\) or data \(x_t\) from previous step \(x_{t-1}\)</td>
      <td>Learn score function \(\nabla_x \log p_t(x)\)</td>
      <td>Learn velocity field \(u_t(x)\)</td>
    </tr>
    <tr>
      <td><strong>Process Type</strong></td>
      <td>Discrete Markov chain</td>
      <td>Continuous SDE</td>
      <td>Continuous ODE</td>
    </tr>
    <tr>
      <td><strong>Forward Process</strong></td>
      <td>Add Gaussian noise step-by-step</td>
      <td>Stochastic diffusion (SDE with drift + noise)</td>
      <td>Deterministic interpolation path</td>
    </tr>
    <tr>
      <td><strong>Backward Process</strong></td>
      <td>Reverse Markov chain</td>
      <td>Reverse SDE</td>
      <td>ODE integration</td>
    </tr>
    <tr>
      <td><strong>Tractability</strong></td>
      <td>Forward process tractable</td>
      <td>Forward SDE tractable</td>
      <td>Conditional paths tractable</td>
    </tr>
    <tr>
      <td><strong>Training Paradigm</strong></td>
      <td>Denoising autoencoder</td>
      <td>Denoising score matching</td>
      <td>Conditional flow matching</td>
    </tr>
    <tr>
      <td><strong>Sampling</strong></td>
      <td>Iterative denoising (stochastic or deterministic)</td>
      <td>SDE/ODE integration</td>
      <td>ODE integration (typically straight paths)</td>
    </tr>
    <tr>
      <td><strong>Path Geometry</strong></td>
      <td>Curved noising trajectory</td>
      <td>Stochastic curved paths</td>
      <td>Straight/optimal transport paths</td>
    </tr>
    <tr>
      <td><strong>Key Advantage</strong></td>
      <td>Simple, well-understood</td>
      <td>Theoretically grounded, flexible</td>
      <td>Fast sampling, simple training</td>
    </tr>
  </tbody>
</table>

<p><strong>Relationship</strong>:</p>
<ul>
  <li>DDIM can be viewed as a discretization of a probability flow ODE derived from the Score Matching SDE</li>
  <li>Flow Matching with Gaussian probability paths recovers Score Matching formulations</li>
  <li>Flow Matching with linear interpolation paths gives the deterministic version similar to DDIM</li>
  <li>All three can be unified under a common framework of learning to transform distributions</li>
</ul>

<hr>

<h3 id="detailed-differences">Detailed Differences</h3>

<h4 id="1-time-convention">1. Time Convention</h4>

<p>Understanding time conventions is crucial for comparing these methods:</p>

<p><strong>Flow Matching:</strong></p>
<ul>
  <li>
<strong>Continuous time</strong> \(t \in [0, 1]\)</li>
  <li>\(t = 0\): <strong>noise</strong> distribution \(p_0(x) = \mathcal{N}(0, I)\)</li>
  <li>\(t = 1\): <strong>data</strong> distribution \(p_1(x) = q(x)\)</li>
  <li>Forward in time moves from noise → data</li>
</ul>

<p><strong>Diffusion Models (DDPM, DDIM):</strong></p>
<ul>
  <li>
<strong>Discrete time</strong> with steps \(t \in \{0, 1, 2, ..., T\}\)</li>
  <li>\(t = 0\): <strong>data</strong> distribution \(q(x_0)\)</li>
  <li>\(t = T\): <strong>noise</strong> distribution \(\mathcal{N}(0, I)\)</li>
  <li>Forward in time moves from data → noise (opposite of Flow Matching!)</li>
  <li>To align with Flow Matching convention, we use \(r = T - t\), so:
    <ul>
      <li>\(r = 0\) corresponds to noise</li>
      <li>\(r = T\) corresponds to data</li>
    </ul>
  </li>
</ul>

<p><strong>Score Matching (SDE):</strong></p>
<ul>
  <li>
<strong>Continuous time</strong> \(t \in [0, T]\) (often \(T = 1\))</li>
  <li>\(t = 0\): <strong>data</strong> distribution \(p_0(x) = q(x)\)</li>
  <li>\(t = T\): <strong>noise</strong> distribution \(p_T(x) \approx \mathcal{N}(0, \sigma^2 I)\)</li>
  <li>Forward in time moves from data → noise</li>
  <li>Using \(r = T - t\) for consistency: \(r = 0\) is noise, \(r = T\) is data</li>
</ul>

<hr>

<h4 id="2-forward-process-vs-probability-paths">2. Forward Process vs. Probability Paths</h4>

<p><strong>Diffusion Models (DDPM, DDIM):</strong></p>

<p>The forward process progressively corrupts data by adding Gaussian noise through a <strong>Markov chain</strong>:</p>

\[q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]

<p>With reparameterization:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\]

<p>where \(\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)\).</p>

<ul>
  <li>
<strong>Discrete steps</strong>: Each transition is a single Gaussian convolution</li>
  <li>
<strong>Tractable</strong>: \(q(x_t \mid x_0)\) has closed form</li>
  <li>
<strong>Markovian</strong>: Each step depends only on the previous state</li>
</ul>

<p><strong>Score Matching (SDE):</strong></p>

<p>The forward process is a <strong>continuous stochastic differential equation (SDE)</strong>:</p>

\[dx = f(x, t)dt + g(t)dW_t\]

<p>where:</p>
<ul>
  <li>\(f(x, t)\) is the <strong>drift coefficient</strong> (deterministic component)</li>
  <li>\(g(t)\) is the <strong>diffusion coefficient</strong> (stochastic component)</li>
  <li>\(W_t\) is the <strong>Wiener process</strong> (Brownian motion)</li>
</ul>

<p>Common example (Variance Exploding - VE):
\(dx = 0 \cdot dt + \sqrt{\frac{d\sigma_t^2}{dt}} dW_t\)</p>

<p>Common example (Variance Preserving - VP):
\(dx = -\frac{1}{2}\beta_t x \, dt + \sqrt{\beta_t} dW_t\)</p>

<ul>
  <li>
<strong>Continuous time</strong>: Infinitesimal noise additions</li>
  <li>
<strong>Stochastic</strong>: Includes random Brownian motion</li>
  <li>
<strong>Non-Markovian</strong> in discrete time but Markovian in continuous time</li>
</ul>

<p><strong>Flow Matching:</strong></p>

<p>The forward process defines <strong>probability paths</strong> that interpolate between distributions:</p>

\[p_t(x) = \int p_t(x \mid x_1) q(x_1) dx_1\]

<p>Where the <strong>conditional probability path</strong> is often chosen as:</p>

\[p_t(x_t \mid x_1) = \mathcal{N}(x_t; \mu_t(x_1), \sigma_t^2(x_1) I)\]

<p>For <strong>linear interpolation</strong> (Optimal Transport path):
\(x_t = (1-t)x_0 + t x_1, \quad x_0 \sim \mathcal{N}(0, I)\)</p>

<p>This gives:
\(\mu_t(x_1) = t x_1, \quad \sigma_t = 1 - t\)</p>

<ul>
  <li>
<strong>Deterministic paths</strong> (no stochastic component in the ODE)</li>
  <li>
<strong>Conditional paths</strong> are tractable by design</li>
  <li>
<strong>Straight trajectories</strong> (shortest path in many metrics)</li>
</ul>

<p><strong>Connections:</strong></p>
<ul>
  <li>The <strong>velocity field</strong> \(u_t(x)\) in Flow Matching corresponds to the <strong>drift term</strong> \(f(x, t)\) in Score Matching</li>
  <li>The <strong>conditional probability path</strong> \(p_t(x_t \mid x_1)\) in Flow Matching corresponds to the SDE solution initialized at \(x_1\)</li>
  <li>The <strong>marginal probability path</strong> \(p_t(x)\) in Flow Matching corresponds to the SDE marginal when initialized from data \(x_0 \sim q(x)\)</li>
</ul>

<p><strong>Special Cases:</strong></p>
<ul>
  <li>Flow Matching with <strong>Gaussian probability paths</strong> (with appropriate \(\mu_t, \sigma_t\)) recovers the forward SDE from Score Matching</li>
  <li>Flow Matching with <strong>linear interpolation</strong> gives the deterministic probability flow ODE, similar to DDIM</li>
</ul>

<hr>

<h4 id="3-training-objective">3. Training Objective</h4>

<p><strong>Diffusion Models (DDPM):</strong></p>

<p>Train a neural network to predict the noise added in the forward process:</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon_\theta(x_t, t) - \epsilon \|^2 \right]\]

<p>where \(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\) (<strong>\(\epsilon\)-prediction</strong>).</p>

<p>Alternative formulation (<strong>\(x_0\)-prediction</strong>):</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \hat{x}_\theta(x_t, t) - x_0 \|^2 \right]\]

<ul>
  <li>
<strong>Target</strong>: Noise \(\epsilon\) or clean data \(x_0\)</li>
  <li>
<strong>Simple</strong>: Direct regression on known quantities</li>
  <li>
<strong>Weighted MSE</strong>: Can add time-dependent weighting</li>
</ul>

<p><strong>Score Matching (DSM - Denoising Score Matching):</strong></p>

<p>The reverse SDE requires the <strong>score function</strong> \(\nabla_x \log p_t(x)\), which is intractable. Vincent (2011) proposed training a network \(s_\theta(x_t, t)\) to approximate the <strong>conditional score</strong>:</p>

\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t, x_0, x_t \mid x_0} \left[ \| s_\theta(x_t, t) - \nabla_{x_t} \log q(x_t \mid x_0) \|^2 \right]\]

<p>Under the assumption that \(q(x_t \mid x_0) \approx p_t(x_t)\) (reasonable for small noise), this approximates the true score.</p>

<p>For Gaussian perturbations \(q(x_t \mid x_0) = \mathcal{N}(\alpha_t x_0, \sigma_t^2 I)\):</p>

\[\nabla_{x_t} \log q(x_t \mid x_0) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2} = -\frac{\epsilon}{\sigma_t}\]

<p>So the objective becomes:</p>

\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \left\| s_\theta(\alpha_t x_0 + \sigma_t \epsilon, t) + \frac{\epsilon}{\sigma_t} \right\|^2 \right]\]

<ul>
  <li>
<strong>Target</strong>: Score function (gradient of log probability)</li>
  <li>
<strong>Theoretical</strong>: Grounded in score-based generative modeling theory</li>
  <li>
<strong>Flexible</strong>: Works with any forward SDE</li>
</ul>

<p><strong>Flow Matching (CFM):</strong></p>

<p>Train a network to predict the velocity field:</p>

\[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, x_1, x_t \mid x_1} \left[ \| v_\theta(t, x_t) - u_t(x_t \mid x_1) \|^2 \right]\]

<p>For linear interpolation \(x_t = (1-t)x_0 + t x_1\):</p>

\[u_t(x_t \mid x_1) = \frac{d x_t}{dt} = x_1 - x_0\]

<p>So:</p>

\[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, x_0, x_1} \left[ \| v_\theta(t, x_t) - (x_1 - x_0) \|^2 \right]\]

<ul>
  <li>
<strong>Target</strong>: Velocity (direction and magnitude of flow)</li>
  <li>
<strong>Direct</strong>: Straightforward regression on vector field</li>
  <li>
<strong>Efficient</strong>: Often requires fewer sampling steps</li>
</ul>

<p><strong>Equivalence:</strong></p>

<p>The training objectives are closely related through reparameterizations:</p>

<ul>
  <li>
<strong>Score to Noise</strong>: \(s_\theta(x_t, t) = -\frac{\epsilon_\theta(x_t, t)}{\sigma_t}\)</li>
  <li>
<strong>Velocity to Noise</strong>: \(v_\theta(t, x_t) = \frac{\alpha_t}{\sigma_t} \epsilon_\theta(x_t, t)\) (approximately, for certain schedulers)</li>
  <li>Score Matching with Gaussian paths is <strong>equivalent</strong> to Flow Matching with appropriate probability path parameterization</li>
</ul>

<hr>

<h4 id="4-sampling-process">4. Sampling Process</h4>

<p><strong>Diffusion Models:</strong></p>

<p><strong>DDPM (Stochastic Sampling):</strong></p>

<p>Iteratively denoise by reversing the forward Markov chain:</p>

\[x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z\]

<p>where \(z \sim \mathcal{N}(0, I)\) and \(\sigma_t\) is the noise variance.</p>

<ul>
  <li>
<strong>Stochastic</strong>: Adds noise at each step</li>
  <li>
<strong>Many steps</strong>: Typically 1000 steps (can be reduced with techniques)</li>
</ul>

<p><strong>DDIM (Deterministic Sampling):</strong></p>

<p>Use a deterministic update rule:</p>

\[x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{predicted } x_0} + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_\theta(x_t, t)\]

<ul>
  <li>
<strong>Deterministic</strong>: No added noise</li>
  <li>
<strong>Fewer steps</strong>: 10-50 steps often sufficient</li>
  <li><strong>Equivalent to probability flow ODE</strong></li>
</ul>

<p><strong>Score Matching:</strong></p>

<p><strong>Reverse-time SDE:</strong></p>

\[dx = [f(x, t) - g(t)^2 \nabla_x \log p_t(x)] dt + g(t) d\bar{W}_t\]

<p>where \(\bar{W}_t\) is a reverse-time Brownian motion.</p>

<ul>
  <li>
<strong>Stochastic</strong>: Includes diffusion term</li>
  <li>
<strong>Continuous</strong>: Integrated numerically (Euler-Maruyama, etc.)</li>
</ul>

<p><strong>Probability Flow ODE (Deterministic):</strong></p>

\[\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)\]

<ul>
  <li>
<strong>Deterministic</strong>: No stochastic component</li>
  <li>
<strong>Same marginals</strong> as the SDE</li>
  <li>
<strong>Flexible solvers</strong>: Use any ODE solver (Runge-Kutta, adaptive methods)</li>
</ul>

<p><strong>Flow Matching:</strong></p>

<p>Integrate the learned velocity field ODE:</p>

\[\frac{dx}{dt} = v_\theta(t, x)\]

<p>Starting from \(x_0 \sim \mathcal{N}(0, I)\) at \(t=0\), integrate to \(t=1\):</p>

<p><strong>References</strong></p>

<ul>
  <li>[1]<a href="https://neurips.cc/media/neurips-2024/Slides/99531.pdf" rel="external nofollow noopener" target="_blank">A NeurIPS 2024 tutorial on Flow Matching</a>
</li>
</ul>

<h2 id="noise-scheduling">Noise scheduling</h2>

<p>Noise scheduling in diffusion models refers to how noise is gradually added to data in the forward process and how it is removed in the reverse process. The choice of noise schedule significantly impacts the model’s performance, sample quality, and training efficiency.</p>

<p>We follow the DDIM convention, where \(0 &lt; \bar{\alpha}_t &lt; 1, \beta_t = 1 - \bar{\alpha}_t\) and \(\alpha_t = \prod_{i=1}^{t} \bar{\alpha}_i\) is the cumulative noise level at time \(t\), and \(\beta_t\) is the noise level at time \(t\). With this convention, \(x_t = \sqrt(\alpha_t) x_0 + \sqrt(1-\alpha_t) \epsilon\), and \(\alpha_T \approx 0\) when \(t \rightarrow T\) while \(\alpha_0 \approx 1\) when \(t \rightarrow 0\).</p>

<p>Common principles of noise scheduling:</p>

<ul>
  <li>Add large amount of noise at \(t\) large while small amount of noise at \(t\) small. \(t=0\) means clean data, \(t=T\) means pure noise.</li>
  <li>The speed of change (acceleration, or \(\frac{d\beta_t}{dt}\)) should also has some proper speed (but I am not sure :D)</li>
</ul>

<p>Common noise schedules:</p>

<ul>
  <li>
<strong>Linear</strong>: \(\alpha_t = \frac{t}{T}\) or \(\beta_t = \beta_{\min} + (\beta_{\max} - \beta_{\min})\frac{t}{T}\). Issue: early timesteps do not add enough noise, and late timesteps can add too much noise.</li>
  <li>
<strong>Cosine</strong>: \(\beta_t = \beta_{\min} + 0.5 (\beta_{\max} - \beta_{\min}) ( 1 + \cos(\frac{t}{T} \pi))\). Intuition is that adding more gradually at the start and <strong>faster at the end</strong>.</li>
  <li>
<strong>Exponential</strong>: \(\beta_t = \beta_{\max} (\beta_{\min} / \beta_{\max})^{\frac{t}{T}}\)</li>
</ul>

<hr>

<h2 id="guidanced-diffusion">Guidanced Diffusion</h2>

<p>Resources:</p>

<ul>
  <li>A great blog from Sander Dieleman: <a href="https://sander.ai/2022/05/26/guidance.html" rel="external nofollow noopener" target="_blank">Guidance: a cheat code for diffusion models</a> and <a href="https://sander.ai/2023/08/28/geometry.html" rel="external nofollow noopener" target="_blank">the geometry of diffusion guidance</a>.</li>
</ul>

<p><strong>Why Guidance?</strong></p>

<p>Guidance is a method to control the generation process so that the ouput is sample from a conditional distribution \(p(x \mid y)\), where \(y\) is a condition - such as a text prompt - rather than a generic \(p(x)\).</p>

<h3 id="classifier-guidance">Classifier Guidance</h3>

<p>In order to get the conditional score function \(\nabla_x \ln p(x \mid y)\), we can use Bayes rule to decompose the score function into an unconditional component and a conditional one:</p>

\[p(x \mid y) = \frac{p(y \mid x) p(x)}{p(y)}\]

\[\log p(x \mid y) = \log p(y \mid x) + \log p(x) - \log p(y)\]

\[\nabla_x \log p(x \mid y) = \nabla_x \log p(y \mid x) + \nabla_x \log p(x) - \nabla_x \log p(y)\]

<p>where \(\nabla_x \log p(x)\) is the score function of the unconditional model. \(\nabla_x \log p(y) = 0\) since \(p(y)\) is independent of \(x\).</p>

<p>The term \(\nabla_x \log p(y \mid x)\) means the direction pointing to \(y\) given \(x\).</p>

<ul>
  <li>In the begining of the inference process, i.e., large \(t\), when \(x_t\) still has a lot of noise, \(\nabla_x \log p(y \mid x)\) is close to \(0\), means that there is no clear information of \(y\).</li>
  <li>In the later stages, i.e., small \(t\), when \(x_t\) is less noisy and closer to \(x_0\), \(\nabla_x \log p(y \mid x)\) is larger, means that \(x_t\) has more information of \(y\), i.e., larger \(p(y \mid x)\).</li>
</ul>

<p><strong>How to obtain \(\nabla_x \log p(y \mid x)\)?</strong></p>

<p>\(p(y \mid x)\) means the probability of a condition \(y\) given \(x\).
In a simple case, where \(y\) is just a image class, like a <code class="language-plaintext highlighter-rouge">cat</code>, the probability \(p(y=\text{cat} \mid x)\) can be simply obtained from a pre-trained classifier.</p>

<p>However, in a more complex case, where \(y\) is a text prompt like a black cat with red eyes and blue fur, a pre-trained classifier is not expressive enough, i.e., it cannot distinguish between \(y_1\) <code class="language-plaintext highlighter-rouge">a black cat with red eyes and blue fur</code> vs \(y_2\) <code class="language-plaintext highlighter-rouge">a white cat with blue eyes and red fur</code> or mathematically \(p(y_1 \mid x) \neq p(y_2 \mid x)\).</p>

<p>In other words, the quality - diversity of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\). For example:</p>

<ul>
  <li>If \(p_\phi(y \mid x)\) is a binary classifier <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code>, then output image \(x \sim p_\theta(x \mid y)\) can be either <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code> only, even \(p_\theta(x)\) was trained from a massive dataset with many more classes rather than just two classes.</li>
  <li>If you want to generate an image \(x\) from a complex prompt \(y\), you need a powerful model like CLIP as the conditional model \(p_\phi(y \mid x)\).</li>
</ul>

<p>To balance between the specificity (i.e., high \(p(y \mid x\))) and diversity/quality (i.e., \(p(x \mid y) \approx p(x)\)), we use a guidance scale \(\gamma\) to control the trade-off between the two.</p>

\[\nabla_x \log p_{\textcolor{red}{\gamma}}(x \mid y) =  \nabla_x \log p(x) + \gamma \nabla_x \log p(y \mid x)\]

<p>where \(\gamma\) is the guidance scale. A big \(\gamma\) means the model is less creative but more following the condition \(y\).</p>

<h3 id="classifier-free-guidance-cfg">Classifier-Free Guidance (CFG)</h3>

<p>Classifier guidance has two key limitations:</p>

<ol>
  <li>It requires a powerful external classifier \(p_\phi(y \mid x)\).</li>
  <li>The generative model \(p_\theta(x)\) may not match the domain of interest (e.g., trained on ImageNet but asked to generate medical images).</li>
</ol>

<p><strong>Classifier-free guidance</strong> solves both by eliminating the explicit classifier. Instead, we train the diffusion model itself in two modes:</p>

<ul>
  <li>
<strong>Unconditional:</strong> modeling \(p_\theta(x)\)</li>
  <li>
<strong>Conditional:</strong> modeling \(p_\theta(x \mid y)\)</li>
</ul>

<p>This is achieved simply by randomly dropping out the condition \(y\) during training (with some probability, e.g., 10–20%). Rather than truly unconditional generation \(p_\theta(x)\) which can be complicated in implementation, we can replace with a <strong>null condition</strong> \(\emptyset\) which is an empty string, i.e., \(p_\theta(x) = p_\theta(x \mid \emptyset)\). <strong>However, in my intuition, this might implicitly implies the null concept lies in the low-density region of the data manifold.</strong> More specifically, without the null condition, the interpolation between \(p_\theta(x \mid y_1)\) and \(p_\theta(x \mid y_2)\) such as \(p_\theta(x \mid (1-t)y_1 + ty_2)\) might be a smooth transition between two concepts \(y_1\) and \(y_2\). However, with the null condition, the interpolation might be a jump from one concept to another.</p>

<p>At inference time, we can <em>combine</em> these two models into a guided score. The derivation is as follows:</p>

<p>We start with Bayes rule for the conditional score that we want to approximate:</p>

\[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\]

<p>Applying log-likelihood and taking the derivative w.r.t. \(x\):</p>

\[\nabla_x \log p(y \mid x) = \nabla_x \log p(x \mid y) + \nabla_x \log p(y) - \nabla_x \log p(x)\]

<p>Dropping the term \(\nabla_x \log p(y)\) since \(p(y)\) is independent of \(x\):</p>

\[\nabla_x \log p(y \mid x) = \nabla_x \log p(x \mid y) - \nabla_x \log p(x)\]

<p>Replacing this into the Classifier-guidance formula:</p>

\[\nabla_x \log p_{\textcolor{red}{\gamma}}(x \mid y) =  \nabla_x \log p(x) + \gamma (\nabla_x \log p(x \mid y) - \nabla_x \log p(x))\]

<p>that is:</p>

\[\nabla_x \log p_{\textcolor{red}{\gamma}}(x \mid y) =  (1 - \gamma) \nabla_x \log p(x) + \gamma \nabla_x \log p(x \mid y)\]

<p>where \(\gamma \geq 0\) is the <strong>guidance scale</strong>.</p>

<ul>
  <li>\(\gamma = 0\) → purely unconditional generation \(p(x \mid \emptyset)\)</li>
  <li>\(\gamma = 1\) → purely conditional generation \(p(x \mid y)\).</li>
  <li>\(\gamma &gt; 1\) → amplifies the effect of the condition but less creative, trading off diversity for fidelity.</li>
  <li>Geometrically, CFG interpolates between the unconditional and conditional score vectors, pushing the sample further in the direction that aligns with \(y\).</li>
</ul>

<p><strong>Why Classifier-Free Guidance works better than Classifier Guidance?</strong><br>
(Extending the intuition from Sander Dieleman) — the key lies in the difference between the gradient from a standard/external classifier \(\phi\) and the gradient from the generative model itself \(\theta\).</p>

<ul>
  <li>
    <p><strong>Classifier Guidance</strong> relies on<br>
\(\nabla_x \log p_{\phi}(y \mid x),\)<br>
where the classifier is trained independently from the generative process.</p>
  </li>
  <li>
    <p><strong>Classifier-Free Guidance (CFG)</strong> instead leverages the <em>implicit classifier</em> inside the generative model:<br>
\(\nabla_x \log p_{\theta}(x \mid y) + \nabla_x \log p_{\theta}(y) - \nabla_x \log p_{\theta}(x).\)</p>
  </li>
</ul>

<p>A well-known phenomenon of discriminative classifiers is <strong>shortcut learning</strong> (<a href="https://arxiv.org/abs/2004.07780" rel="external nofollow noopener" target="_blank">Geirhos et al., 2020</a>): gradient-descent-trained classifiers often find “shortcuts” that optimize the loss but fail to align with human perception. For example, they may overfit to local texture cues rather than global shape, producing gradients that push generation toward features humans do not find semantically meaningful.</p>

<p>By contrast, the generative classifier (implicit in CFG) is trained to <em>model and reconstruct the data distribution itself</em> conditioned on human-meaningful labels/prompts. Its gradients are therefore better aligned with human perceptual semantics.</p>

<p>👉 In short: CFG works better under the <strong>human perspective</strong> (more semantically aligned generations), whereas Classifier Guidance works better under the <strong>classifier perspective</strong> (gradients aligned with a discriminative model that may exploit spurious correlations).</p>

<p><strong>When Classifier Guidance is better?</strong><br>
I’ve explored a proposal linking this to <strong>Machine Unlearning</strong> (although not yet published). The idea is to guide unlearning with the gradient of an <em>external classifier</em>, rather than relying solely on the generative classifier as in the [ESD paper]. This approach can be particularly beneficial in two cases:</p>

<ul>
  <li>
<strong>Unlearning rare concepts</strong>: where the generative model assigns very low probability \(p_{\theta}(x \mid y)\), making CFG ineffective.</li>
  <li>
<strong>Unlearning ambiguous or multi-expressed concepts</strong>: e.g., “nudity” vs “naked”. A discriminative classifier can unify these expressions under a shared semantic decision boundary, while the generative model may treat them as distinct.</li>
</ul>

<p>Thus, while CFG dominates in general generation tasks, Classifier Guidance can provide unique advantages for <em>targeted unlearning</em>.</p>

<p><strong>Why \(\gamma &gt; 1\) but not between \(0\) and \(1\)?</strong></p>

<p>In typical interpolation scenarios, we expect \(\gamma \in [0,1]\) to balance unconditional and conditional influences. Surprisingly, in CFG, values of \(\gamma &gt; 1\) (e.g., 7 or 7.5) are commonly used — and empirically yield sharper, more faithful generations.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://sander.ai/images/panda1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://sander.ai/images/panda1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://sander.ai/images/panda1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://sander.ai/images/panda1.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://sander.ai/images/panda3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://sander.ai/images/panda3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://sander.ai/images/panda3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://sander.ai/images/panda3.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Two sets of samples from OpenAI's GLIDE model, for the prompt 'A stained glass window of a panda eating bamboo.', taken from their paper. Guidance scale 1 (no guidance) on the left, guidance scale 3 on the right. Image source: from Sander Dieleman's blog.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://sander.ai/images/corgi1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://sander.ai/images/corgi1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://sander.ai/images/corgi1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://sander.ai/images/corgi1.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://sander.ai/images/corgi3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://sander.ai/images/corgi3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://sander.ai/images/corgi3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://sander.ai/images/corgi3.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Two sets of samples from OpenAI's GLIDE model, for the prompt '“A cozy living room with a painting of a corgi on the wall above a couch and a round coffee table in front of a couch and a vase of flowers on a coffee table.', taken from their paper. Guidance scale 1 (no guidance) on the left, guidance scale 3 on the right. Image source: from Sander Dieleman's blog.
</div>

<p>As demonstrated in <a href="https://arxiv.org/pdf/2105.05233" rel="external nofollow noopener" target="_blank">Dhariwal &amp; Nichol, 2021</a>, larger \(\gamma\) produces outputs with higher fidelity and stronger alignment to prompts, albeit at the cost of reduced diversity. The intuition is that \(\log p_{\gamma}(y \mid x)\) becomes <em>sharper</em> than \(\log p_{1}(y \mid x)\) when \(\gamma &gt; 1\), effectively amplifying conditional gradients and biasing the generation toward features that match the conditioning signal more strongly.</p>

<p>This explains why CFG practitioners often “turn up the guidance dial” above 1 — it helps the model stay on track with the prompt, even if some creativity/diversity is sacrificed.</p>

<h3 id="intuition-recap">Intuition Recap</h3>

<ul>
  <li>
<strong>Classifier guidance:</strong> adds an external force from a classifier \(p(y \mid x)\).</li>
  <li>
<strong>Classifier-free guidance:</strong> reuses the generative model itself, trained with and without condition, to simulate that force.</li>
  <li>Both balance a trade-off between diversity and conditional alignment, controlled by the guidance scale \(\gamma\).</li>
</ul>

<hr>

<h2 id="latent-diffusion">Latent Diffusion</h2>

<h2 id="conditional-diffusion">Conditional Diffusion</h2>

<h3 id="control-net">Control-Net</h3>

<h3 id="image-prompt">Image Prompt</h3>

<p>Beyond controlling the generation process using text prompt, there is a hot topic in the community to control using image information/layout/prompt - which has a huge potential in applications, e.g., image inpainting, image-to-image generation, etc. In the standard Stable Diffusion, the condition embedding \(c_t\) is just a text embedding \(c_t = E_t(y)\) where \(y\) is the text prompt and \(E_t\) is a pre-trained text encoder such as CLIP.
IP-Adapter [1] proposes to use an additional image encoder to extract the image embedding from a reference image \(c_i = E_i(x)\) and then project it into the original condition space.
The objective function for IP-Adapter is:</p>

\[\mathcal{L}_{IP} = \mathbb{E}_{z, c, \epsilon, t} \left[ \mid \mid \epsilon - \epsilon_\theta(z_t \mid c_i, c_t, t) \mid \mid_2^2 \right]\]

<p>The cross-attention layer is also modified from the one in Stable Diffusion to include the image embedding \(c_i\) as a condition.</p>

\[\text{Attention}(Q, K_i, K_t, V_i, V_t) = \lambda \text{softmax}\left(\frac{QK_i^T}{\sqrt{d}} + c_i\right)V_i + \text{softmax}\left(\frac{QK_t^T}{\sqrt{d}}\right)V_t\]

<p>where \(Q=z W_Q\), \(K_i = c_i W_K^i\), \(K_t = c_t W_K^t\), \(V_i = c_i W_V^i\), \(V_t = c_t W_V^t\), and \(W_Q\), \(W_K^i\), \(W_K^t\), \(W_V^i\), \(W_V^t\) are the weights of the linear layers.
The model becomes the original Stable Diffusion when \(\lambda = 0\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-03-20-07-02-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2308.06721" rel="external nofollow noopener" target="_blank">IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</a>
</li>
  <li>[2] <a href="https://openreview.net/pdf?id=PJqP0wyQek#page=1.93" rel="external nofollow noopener" target="_blank">MS-DIFFUSION: MULTI-SUBJECT ZERO-SHOT IMAGE PERSONALIZATION WITH LAYOUT GUIDANCE</a>
</li>
</ul>

<hr>

<h2 id="diffusion-transformers">Diffusion Transformers</h2>

<p>The Diffusion Transformers (DiTs) is a class of diffusion models that replace the traditional U-Net convolutional architecture with a Vision Transformer (ViT) architecture as a backbone.</p>

<h3 id="data-processing-in-dit">Data Processing in DiT</h3>

<p>Similar to Latent Diffusion model, the diffusion process in DiT is on the latent space. Therefore, the first step is using pre-trained convolutional Variational Autoencoder (VAE) as in LDM to convert the spatial input into the latent space (i.e., \(256 \times 256 \times 3\) to \(32 \times 32 \times 4\)).</p>

<p><strong>Patchifying</strong> converting the(latent) spatial input into a sequence of \(T\) tokens/patches, each of dimension \(d\), by linearly embedding each patch in the input with a linear layer.</p>

<p><strong>Positional Encoding</strong> the standard sinusoidal positional embeddings are added to the token embeddings to provide the model with the positional information.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-01-15-06-00.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>Beside the visual tokens, the DiT also uses the <strong>conditional information</strong> such as timestep \(t\) and the textual prompt \(c\) associated with the input image. These information are added to the DiT block through a embedding layer.</p>

<h3 id="the-dit-architecture">The DiT Architecture</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-01-15-03-57.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>There are three types have been studied in the DiT paper including:</p>

<p><strong>In-Context Conditioning</strong> (The far right in the above figure) Append the vector embedding of \(t\) and \(c\) in the input sequence, <strong>treating them as additional visual tokens</strong>. This is similar to the <code class="language-plaintext highlighter-rouge">cls</code> tokens in ViT.</p>

<p><strong>Cross-Attention</strong> Concatenate the conditional embedding \(t\) and \(c\) into a length-two sequence, separate from the image token sequence. Then modify the cross-attention layer to inject these conditioning information into the visual path.</p>

<p><strong>Adaptive layer norm (adaLN) block</strong> Following the widespread success of Adaptive normalization layer in Diffusion with U-Net backbones, DiT also replaces the standard layer norm in transformer blocks with an adaptive layer norm. Rather than directly learn dimension-wise scaling and shift \(\gamma\) and \(\beta\), the adaLN block regresses them from the sum of the embedding of the conditioning information \(t\) and \(c\).</p>

<p><strong>adaLN-Zero block</strong> Prior work on ResNets has found that initializing each residual block as the identity function is beneficial. This version uses the same adaptive layer norm as the adaLN block but with zero initialization.</p>

<p><strong>Transformer Decoder</strong> After the final DiT block, we need to decode the sequence of image tokens into an output latent noise prediction and output diagonal covariance prediction (two outputs). This can be done by a standard linear layer with output dimension \(p \times p \times 2C\) where \(C\) is the number of channels of the image.</p>

<p>References:</p>

<ul>
  <li>DIT paper: <a href="https://arxiv.org/pdf/2212.09748" rel="external nofollow noopener" target="_blank">Scalable Diffusion Models with Transformers</a>
</li>
  <li>Official implementation: <a href="https://github.com/facebookresearch/DiT" rel="external nofollow noopener" target="_blank">https://github.com/facebookresearch/DiT</a>
</li>
</ul>

<hr>

<h2 id="diffusion-flux">Diffusion Flux</h2>

<p>References:</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2507.09595v1" rel="external nofollow noopener" target="_blank">Demystifying Flux Architecture</a></li>
  <li>Flux official implementation: <a href="https://github.com/black-forest-labs/flux" rel="external nofollow noopener" target="_blank">https://github.com/black-forest-labs/flux</a>
</li>
</ul>

<h2 id="image-inpainting-with-diffusion-models">Image Inpainting with Diffusion Models</h2>

<h3 id="training-pipeline">Training Pipeline</h3>

<p><strong>Training data</strong> for inpainting is a combination of three components: <strong>original image</strong> as ground truth, <strong>masked image</strong> as input, and <strong>prompt</strong> as condition to provide the context of the missing region.</p>

<p>To ensure the model is robust, a variety of mask shapes and sizes can be used, including <strong>Rectangular</strong>, <strong>Free-form masks</strong>, and <strong>arbitrary shapes</strong></p>

<p><strong>Loss function</strong> for inpainting is a combination of <strong>pixel-wise reconstruction loss</strong> and <strong>perceptual loss</strong> (or <strong>Style loss</strong>). If using GANs, the <strong>adversarial loss</strong> is used to ensure the inpainted regions are perceptually realistic under the discriminator perspective.</p>

<h3 id="challenges-in-image-inpainting">Challenges in Image Inpainting</h3>

<p><strong>Semantic and Structural Consistency</strong>: A primary challenge for generative models is to fill in missing regions in a way that is not only visually plausible but also semantically and structurally consistent with the rest of the image.</p>

<p><strong>Semantic ambiguity</strong> means that the missing region can be filled in multiple ways, e.g., filling a gap in a street scene could be extending a road, adding a pedestrian, or a vehicle. Even when the input prompt is given, the task remains difficult, when <strong>concept leaking</strong> occurs, i.e., “a black cat on a white background” vs “a white cat on a black background”.</p>

<p><strong>Long-range dependency and global structure</strong>: is another significant hurdle. While generative models excel at local details, they can be struggling with the broader context, lighting, and perspective.</p>

<p><strong>Perceptual realism</strong>: is another key challenge. Even if the inpainted regions are visually consistent, they may not align with human perception. For example, an inpainting might produce unrealistic shadows or reflections or overly smooth, or having artificial artifacts.</p>

<p><strong>Large missing regions</strong>: The size of missing area is directly proportional to the difficulty of the task.</p>

<h2 id="accelerating-diffusion-models">Accelerating Diffusion Models</h2>

<h3 id="consistency-models">Consistency Models</h3>

<p>The core idea behind <strong>Consistency Models (CMs)</strong> is elegantly simple yet powerful:</p>

<blockquote>
  <p><strong>“Points on the same trajectory should map to the same initial point.”</strong></p>
</blockquote>

<h4 id="concept-and-mathematical-definition">Concept and Mathematical Definition</h4>

<p>Formally, consider a solution trajectory \(\{ x_t \}_{t \in [\epsilon, T]}\) of the <strong>Probability Flow ODE</strong>:</p>

\[\frac{dx}{dt} = \mu(x, t) - \frac{1}{2} \sigma(t)^2 \nabla_x \log p_t(x)\]

<p>We define the <strong>consistency function</strong> as</p>

\[f: (x_t, t) \mapsto x_{\epsilon}\]

<p>Intuitively, this function maps any point along a diffusion trajectory to its corresponding starting point at time \(\epsilon\).</p>

<p>A valid consistency function must satisfy the <strong>self-consistency property</strong>:</p>

\[f(x_t, t) = f(x_{t'}, t') \quad \forall t, t' \in [\epsilon, T]\]

<p>That is, any two points on the same trajectory—no matter when they occur—should yield the same mapped output.</p>

<p>The objective of a <strong>consistency model</strong> \(f_{\theta}\) is to learn this mapping from data while enforcing this self-consistency constraint.</p>

<p><strong>Determinism and Relation to Probability Flow ODE</strong></p>

<p>Unlike the stochastic nature of the diffusion SDE, the <strong>Probability Flow ODE</strong> is <em>deterministic</em>.<br>
Given a fixed starting point \(x_T\), the trajectory and its corresponding final point \(x_{\epsilon}\) are uniquely determined for all \(t \in [\epsilon, T]\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-05-13-34-52-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-05-13-34-52-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-05-13-34-52-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-05-13-34-52.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-05-13-35-06-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-05-13-35-06-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-05-13-35-06-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-05-13-35-06.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<h4 id="sampling-with-consistency-models">Sampling with Consistency Models</h4>

<p>Once trained, a consistency model \(f_{\theta}\) can generate samples in a <strong>single step</strong>:</p>

<ol>
  <li>Sample a random latent point \(x_T \sim \mathcal{N}(0, I)\)</li>
  <li>Map it to data space with<br>
\(x_{\epsilon} = f_{\theta}(x_T, T)\)</li>
</ol>

<p>This one-step sampling process is deterministic and efficient.<br>
Alternatively, we can perform multi-step sampling by injecting small amounts of noise at each step, introducing stochasticity to improve sample diversity.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-06-10-44-39-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-06-10-44-39-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-06-10-44-39-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-06-10-44-39.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<h4 id="training-consistency-models">Training Consistency Models</h4>

<p>There are two main strategies to train consistency models:</p>

<ol>
  <li>
<strong>Distillation from Pre-Trained Diffusion Models</strong> uses knowledge from a pre-trained diffusion model.</li>
  <li>
<strong>Training from Scratch</strong> relies on an unbiased estimator of the score function.</li>
</ol>

<p>In summary, the key step is how to get the two adjacent points of the PF-ODE trajectory, then enforce the consistency function as its definition. In distillation, we leverage the pre-trained score model \(s_{\phi}(x,t)\) to approximate the ground-truth score function \(\nabla_x \log p_t(x)\). In training from scratch, we leverage the following unbiased estimator</p>

\[\nabla_x \log p_t(x) = \mathbb{E} \left[ \frac{x_t - x}{t^2} \mid x_t \right]\]

<p>where \(x \sim \mathcal{D}\) and \(x_t \sim \mathcal{N}(x; t^2 I)\). At the end, we approximate \(x_{t_n} = x + t_n z\) and \(x_{t_{n+1}} = x + t_{n+1} z\) where \(z \sim \mathcal{N}(0, I)\).</p>

<p><strong>Distribution Matching Distillation</strong></p>

<p>This approach leverages an existing diffusion model to generate <em>adjacent points</em> \((\hat{x}_{t_n}, x_{t_{n+1}})\) along a Probability Flow ODE trajectory.</p>

<p>The goal is to enforce:</p>

\[f_{\theta}(\hat{x}_{t_n}, t_n) = f_{\theta}(x_{t_{n+1}}, t_{n+1})\]

<p>so that \(f_{\theta}\) behaves as a true consistency function.</p>

<p><strong>Step-by-step process:</strong></p>

<p><strong>Step 1 — Obtain point</strong> \(x_{t_{n+1}}\)<br>
Sample from the SDE transition density:
\(x_{t_{n+1}} \sim \mathcal{N}(x; t_{n+1}^2 I), \quad x \sim \mathcal{D}\)</p>

<p><strong>Step 2 — Estimate the adjacent point</strong> \(\hat{x}_{t_n}\)<br>
Using an ODE solver \(\Phi(x, t, \phi)\) parameterized by the pre-trained diffusion model:</p>

\[\hat{x}_{t_n} = x_{t_{n+1}} + (t_n - t_{n+1}) \Phi(x_{t_{n+1}}, t_{n+1}, \phi)\]

<p>If the Euler method is used,<br>
\(\Phi(x, t, \phi) = -t\, s_{\phi}(x, t)\)<br>
where \(s_{\phi}(x, t)\) is the score function.</p>

<p>Hence,<br>
\(\hat{x}_{t_n} = x_{t_{n+1}} - (t_n - t_{n+1}) t_{n+1} s_{\phi}(x_{t_{n+1}}, t_{n+1})\)</p>

<p><strong>Step 3 — Define the loss</strong><br>
The <strong>consistency loss</strong> ensures outputs from adjacent points match:</p>

\[\mathcal{L} = d\big(f_{\theta}(\hat{x}_{t_n}, t_n), f_{\theta}(x_{t_{n+1}}, t_{n+1})\big)\]

<p>where \(d(\cdot,\cdot)\) is a distance metric (e.g., L2 norm).</p>

<p><strong>Step 4 — Update with EMA</strong><br>
Rather than standard gradient descent, the paper proposes an <strong>Exponential Moving Average (EMA)</strong> update between two model parameters \(\theta\) and \(\theta^-\), as shown in Algorithm 2.</p>

<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-06-11-03-48-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-06-11-03-48-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-06-11-03-48-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-06-11-03-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

  </div>
</div>

<p><strong>Training from scratch</strong></p>

<p>When no pre-trained diffusion model is available, we can directly estimate the score function using an <strong>unbiased estimator</strong>:</p>

\[\nabla_x \log p_t(x) = \mathbb{E}\left[\frac{x_t - x}{t^2} \mid x_t\right]\]

<p>where</p>
<ul>
  <li>\(x \sim \mathcal{D}\) (data distribution)</li>
  <li>\(x_t \sim \mathcal{N}(x; t^2 I)\) (noisy version of data)</li>
</ul>

<p>We can then approximate:
\(x_{t_n} = x + t_n z, \quad x_{t_{n+1}} = x + t_{n+1} z, \quad z \sim \mathcal{N}(0, I)\)</p>

<p>This formulation allows consistency models to be trained <em>entirely from noise-perturbed data samples</em>.</p>

<p><strong>Implementation of the Euler method</strong></p>

<p>Implementation of the Euler method (from <a href="https://github.com/openai/consistency_models/blob/e32b69ee436d518377db86fb2127a3972d0d8716/cm/karras_diffusion.py#L163" rel="external nofollow noopener" target="_blank">here</a>).
Note that this version implies \(s_{\phi}(x,t) = \frac{x - f_{\theta}(x,t)}{t^2}\) and therefore we don’t need the pre-trained diffusion model to estimate the score function.</p>

<p>Substituing this into the PF-ODE Euler update:</p>

\[\hat{x}_{t_n} = x_{t_{n+1}} - (t_n - t_{n+1}) t_{n+1} s_{\phi}(x_{t_{n+1}}, t_{n+1})\]

<p>We get:</p>

\[\hat{x}_{t_n} = x_{t_{n+1}} - (t_n - t_{n+1}) \frac{x - f_{\theta}(x,t)}{t}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@th.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">euler_solver</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">next_t</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">samples</span>
    <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># f_{\theta}(x,t) - consistency model output
</span>    <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div>

<p>Heun method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@th.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">heun_solver</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">next_t</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">samples</span>
    <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># f_{\theta}(x,t) - consistency model output
</span>
    
    <span class="c1"># IMPORTANT - Euler method
</span>    <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">next_t</span><span class="p">)</span> <span class="c1"># f_{\theta}(x,t) - consistency model output
</span>
    <span class="n">next_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="n">next_d</span><span class="p">)</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">((</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div>

<h4 id="implementation-of-consistency-models">Implementation of Consistency Models</h4>

<p>The official implementation is available <a href="https://github.com/openai/consistency_models" rel="external nofollow noopener" target="_blank">here</a>.
The train loop is defined in the <code class="language-plaintext highlighter-rouge">train_util.py</code> <a href="https://github.com/openai/consistency_models/blob/main/cm/train_util.py#L267" rel="external nofollow noopener" target="_blank">file</a>.</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">diffusion.progdist_losses</code> is the loss function for progressive distillation.</li>
  <li>
<code class="language-plaintext highlighter-rouge">consistency_losses</code> is the loss function for consistency distillation.</li>
  <li><code class="language-plaintext highlighter-rouge">target_model</code></li>
  <li><code class="language-plaintext highlighter-rouge">teacher_model</code></li>
  <li><code class="language-plaintext highlighter-rouge">teacher_diffusion</code></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">forward_backward</code> method</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward_backward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">cond</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mp_trainer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">):</span>
            <span class="n">micro</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">dist_util</span><span class="p">.</span><span class="nf">dev</span><span class="p">())</span>
            <span class="n">micro_cond</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">dist_util</span><span class="p">.</span><span class="nf">dev</span><span class="p">())</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cond</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">last_batch</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">schedule_sampler</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">micro</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dist_util</span><span class="p">.</span><span class="nf">dev</span><span class="p">())</span>

            <span class="n">ema</span><span class="p">,</span> <span class="n">num_scales</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ema_scale_fn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">global_step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training_mode</span> <span class="o">==</span> <span class="sh">"</span><span class="s">progdist</span><span class="sh">"</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">num_scales</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="nf">ema_scale_fn</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">compute_losses</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">progdist_losses</span><span class="p">,</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">ddp_model</span><span class="p">,</span>
                        <span class="n">micro</span><span class="p">,</span>
                        <span class="n">num_scales</span><span class="p">,</span>
                        <span class="n">target_model</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">teacher_model</span><span class="p">,</span>
                        <span class="n">target_diffusion</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">teacher_diffusion</span><span class="p">,</span>
                        <span class="n">model_kwargs</span><span class="o">=</span><span class="n">micro_cond</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">compute_losses</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">progdist_losses</span><span class="p">,</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">ddp_model</span><span class="p">,</span>
                        <span class="n">micro</span><span class="p">,</span>
                        <span class="n">num_scales</span><span class="p">,</span>
                        <span class="n">target_model</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">target_model</span><span class="p">,</span>
                        <span class="n">target_diffusion</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">diffusion</span><span class="p">,</span>
                        <span class="n">model_kwargs</span><span class="o">=</span><span class="n">micro_cond</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">training_mode</span> <span class="o">==</span> <span class="sh">"</span><span class="s">consistency_distillation</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">compute_losses</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">consistency_losses</span><span class="p">,</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">ddp_model</span><span class="p">,</span>
                    <span class="n">micro</span><span class="p">,</span>
                    <span class="n">num_scales</span><span class="p">,</span>
                    <span class="n">target_model</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">target_model</span><span class="p">,</span>
                    <span class="n">teacher_model</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">teacher_model</span><span class="p">,</span>
                    <span class="n">teacher_diffusion</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">teacher_diffusion</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">micro_cond</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">training_mode</span> <span class="o">==</span> <span class="sh">"</span><span class="s">consistency_training</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">compute_losses</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">diffusion</span><span class="p">.</span><span class="n">consistency_losses</span><span class="p">,</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">ddp_model</span><span class="p">,</span>
                    <span class="n">micro</span><span class="p">,</span>
                    <span class="n">num_scales</span><span class="p">,</span>
                    <span class="n">target_model</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">target_model</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">micro_cond</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Unknown training mode </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">training_mode</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">last_batch</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">use_ddp</span><span class="p">:</span>
                <span class="n">losses</span> <span class="o">=</span> <span class="nf">compute_losses</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">self</span><span class="p">.</span><span class="n">ddp_model</span><span class="p">.</span><span class="nf">no_sync</span><span class="p">():</span>
                    <span class="n">losses</span> <span class="o">=</span> <span class="nf">compute_losses</span><span class="p">()</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">consistency_losses</code>.</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">distiller = denoise_fn(x_t, t)</code> and <code class="language-plaintext highlighter-rouge">distiller_target = target_denoise_fn(x_t2, t2)</code> are the consistency model output and the target model output, respectively.</li>
  <li>
<code class="language-plaintext highlighter-rouge">t2</code> is the adjacent time step of <code class="language-plaintext highlighter-rouge">t</code>.</li>
  <li>
<code class="language-plaintext highlighter-rouge">x_t2</code> is the predicted adjacent point of <code class="language-plaintext highlighter-rouge">x_t</code>.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="k">def</span> <span class="nf">consistency_losses</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">x_start</span><span class="p">,</span>
        <span class="n">num_scales</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">target_model</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">teacher_model</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">teacher_diffusion</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">model_kwargs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>

        <span class="n">dims</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">.</span><span class="n">ndim</span>

        <span class="c1"># IMPORTANT - f_{\theta}(x,t) - consistency model output
</span>        <span class="k">def</span> <span class="nf">denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">denoise</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">target_model</span><span class="p">:</span>

            <span class="nd">@th.no_grad</span><span class="p">()</span>
            <span class="k">def</span> <span class="nf">target_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">denoise</span><span class="p">(</span><span class="n">target_model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">(</span><span class="sh">"</span><span class="s">Must have a target model</span><span class="sh">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">teacher_model</span><span class="p">:</span>

            <span class="nd">@th.no_grad</span><span class="p">()</span>
            <span class="k">def</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">teacher_diffusion</span><span class="p">.</span><span class="nf">denoise</span><span class="p">(</span><span class="n">teacher_model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="nd">@th.no_grad</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">heun_solver</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">next_t</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">samples</span>
            <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

            
            <span class="c1"># IMPORTANT - Euler method
</span>            <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">next_t</span><span class="p">)</span>

            <span class="n">next_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="n">next_d</span><span class="p">)</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">((</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">samples</span>

        <span class="nd">@th.no_grad</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">euler_solver</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">next_t</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">samples</span>
            <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">denoiser</span> <span class="o">=</span> <span class="n">x0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">denoiser</span> <span class="o">=</span> <span class="nf">teacher_denoise_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">denoiser</span><span class="p">)</span> <span class="o">/</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">next_t</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">samples</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">num_scales</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">device</span><span class="o">=</span><span class="n">x_start</span><span class="p">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sigma_max</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span> <span class="o">+</span> <span class="n">indices</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_scales</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">sigma_min</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">sigma_max</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">**</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>

        <span class="n">t2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sigma_max</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_scales</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">sigma_min</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">sigma_max</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">t2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">**</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>

        <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_start</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">*</span> <span class="nf">append_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>

        <span class="n">dropout_state</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">get_rng_state</span><span class="p">()</span>
        <span class="n">distiller</span> <span class="o">=</span> <span class="nf">denoise_fn</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">teacher_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">x_t2</span> <span class="o">=</span> <span class="nf">euler_solver</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">x_start</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_t2</span> <span class="o">=</span> <span class="nf">heun_solver</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">x_start</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

        <span class="n">th</span><span class="p">.</span><span class="nf">set_rng_state</span><span class="p">(</span><span class="n">dropout_state</span><span class="p">)</span>
        <span class="n">distiller_target</span> <span class="o">=</span> <span class="nf">target_denoise_fn</span><span class="p">(</span><span class="n">x_t2</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>
        <span class="n">distiller_target</span> <span class="o">=</span> <span class="n">distiller_target</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>

        <span class="n">snrs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_snr</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="nf">get_weightings</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weight_schedule</span><span class="p">,</span> <span class="n">snrs</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sigma_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">loss_norm</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l1</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">diffs</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">distiller</span> <span class="o">-</span> <span class="n">distiller_target</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">mean_flat</span><span class="p">(</span><span class="n">diffs</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span>
        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">loss_norm</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l2</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">diffs</span> <span class="o">=</span> <span class="p">(</span><span class="n">distiller</span> <span class="o">-</span> <span class="n">distiller_target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">mean_flat</span><span class="p">(</span><span class="n">diffs</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span>
        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">loss_norm</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l2-32</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">distiller</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">distiller</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">bilinear</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">distiller_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span>
                <span class="n">distiller_target</span><span class="p">,</span>
                <span class="n">size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">bilinear</span><span class="sh">"</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">diffs</span> <span class="o">=</span> <span class="p">(</span><span class="n">distiller</span> <span class="o">-</span> <span class="n">distiller_target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">mean_flat</span><span class="p">(</span><span class="n">diffs</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span>
        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">loss_norm</span> <span class="o">==</span> <span class="sh">"</span><span class="s">lpips</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">:</span>
                <span class="n">distiller</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">distiller</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">bilinear</span><span class="sh">"</span><span class="p">)</span>
                <span class="n">distiller_target</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span>
                    <span class="n">distiller_target</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">bilinear</span><span class="sh">"</span>
                <span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">lpips_loss</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">distiller</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">distiller_target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="n">weights</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Unknown loss norm </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">loss_norm</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">terms</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">terms</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>

        <span class="k">return</span> <span class="n">terms</span>
</code></pre></div></div>

<h3 id="diffusion-distillation">Diffusion Distillation</h3>

<h4 id="progressive-distillation">Progressive Distillation</h4>

<p>The idea of progressive distillation is to</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-06-12-36-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-06-12-36-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-06-12-36-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-06-12-36-31.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>References:</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2202.00512" rel="external nofollow noopener" target="_blank">PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS, ICLR 2022</a></li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf" rel="external nofollow noopener" target="_blank">On Distillation of Guided Diffusion Models, CVPR 2023</a></li>
  <li><a href="https://openreview.net/forum?id=1k4yZbbDqX" rel="external nofollow noopener" target="_blank">InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation, ICLR 2024</a></li>
  <li><a href="https://arxiv.org/pdf/2311.17042" rel="external nofollow noopener" target="_blank">Adversarial Diffusion Distillation</a></li>
  <li><a href="https://arxiv.org/pdf/2405.14867" rel="external nofollow noopener" target="_blank">Improved Distribution Matching Distillation for Fast Image Synthesis</a></li>
</ul>

<h3 id="rectified-diffusion">Rectified Diffusion</h3>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2209.03003" rel="external nofollow noopener" target="_blank">Flow straight and fast: Learning to generate and transfer data with rectified flow</a>
</li>
  <li>[2] <a href="https://arxiv.org/pdf/2403.03206" rel="external nofollow noopener" target="_blank">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a>
</li>
</ul>

<h3 id="caching-in-diffusion-models">Caching in Diffusion Models</h3>

<p>This technique takes advantage of the <strong>U-Net architecture</strong> used in diffusion models, particularly its <strong>skip connections</strong>, which transfer intermediate features from the encoder to the decoder.</p>

<p>The core idea:<br>
👉 <em>Store intermediate results from step</em> \(t\) <em>(e.g., decoder features)</em> <em>and reuse them at step</em> \(t-1\) <em>instead of recomputing the entire U-Net.</em></p>

<h4 id="u-net-refresher">U-Net Refresher</h4>

<p>A U-Net has two main components:</p>

<ul>
  <li>
<strong>Encoder (Down Blocks)</strong> — progressively downsamples the input to a compact high-level representation.</li>
  <li>
<strong>Decoder (Up Blocks)</strong> — upsamples the features to reconstruct the image.</li>
</ul>

<p>Each pair of down and up blocks \(D_i, U_i\) connects through:</p>

<ul>
  <li>a <strong>main path</strong>: \(D_1 \to D_d \to U_d \to U_1\)</li>
  <li>
<strong>skip connections</strong>: \(D_i \to U_i\)</li>
</ul>

<p>At each layer, the output combines both paths:
\(U_i = \text{Concat}(D_i, U_{i+1})\)</p>

<h4 id="observation-feature-reuse-across-timesteps">Observation: Feature Reuse Across Timesteps</h4>

<p>During denoising, <strong>adjacent timesteps produce very similar high-level features</strong>:
\(U_i^{(t)} \approx U_i^{(t-1)}\)</p>

<p>So instead of recomputing these expensive decoder features every step, we can <strong>cache</strong> them:</p>

\[F_c^{(t)} \leftarrow U_i^{(t)} \\
U_i^{(t-1)} = \text{Concat}(D_i^{(t-1)}, F_c^{(t)})\]

<p>This simple reuse cuts redundant computation and <strong>significantly speeds up inference</strong>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-06-20-14-04-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-06-20-14-04-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-06-20-14-04-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-06-20-14-04.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<h4 id="implementation-overview">Implementation Overview</h4>

<p>I was curious more about the implementation details than the idea. You can find the full implementation in the <a href="https://github.com/horseee/DeepCache/tree/master/DeepCache" rel="external nofollow noopener" target="_blank">DeepCache repository</a>. 
First, we need to modify the Stable Diffusion pipeline, specifically the <strong>denoising loop</strong>, where cached features are passed to the U-Net.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="c1"># predict the noise residual
</span>    <span class="n">noise_pred</span><span class="p">,</span> <span class="n">prv_features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">unet</span><span class="p">(</span>
        <span class="n">latent_model_input</span><span class="p">,</span>
        <span class="n">t</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
        <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
        <span class="n">replicate_prv_feature</span><span class="o">=</span><span class="n">prv_features</span><span class="p">,</span>
        <span class="n">quick_replicate</span><span class="o">=</span> <span class="n">cache_interval</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">cache_layer_id</span><span class="o">=</span><span class="n">cache_layer_id</span><span class="p">,</span>
        <span class="n">cache_block_id</span><span class="o">=</span><span class="n">cache_block_id</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>The U-Net model is defined in the <code class="language-plaintext highlighter-rouge">unet_2d_condition.py</code> <a href="https://github.com/horseee/DeepCache/blob/master/DeepCache/sd/unet_2d_condition.py" rel="external nofollow noopener" target="_blank">file</a>, where <code class="language-plaintext highlighter-rouge">forward</code> method has been modified to support the caching feature.
Note that the caching applied to <code class="language-plaintext highlighter-rouge">cross-attention</code> layer only.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">quick_replicate</span> <span class="ow">and</span> <span class="n">replicate_prv_feature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Downsampling - nothing change 
</span>
    <span class="c1"># Middle - No middle 
</span>
    <span class="c1"># Upsampling
</span>    <span class="n">sample</span> <span class="o">=</span> <span class="n">replicate_prv_feature</span>
    <span class="c1">#down_block_res_samples = down_block_res_samples[:-1]
</span>    <span class="k">if</span> <span class="n">cache_block_id</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_blocks</span><span class="p">[</span><span class="n">cache_layer_id</span><span class="p">].</span><span class="n">attentions</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">cache_block_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">cache_layer_id</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache_block_id</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">upsample_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_blocks</span><span class="p">):</span>

        <span class="c1"># Skip the blocks that are not the cache layer # IMPORTANT - This is where speed is gained
</span>        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cache_layer_id</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cache_layer_id</span><span class="p">:</span>
            <span class="n">trunc_upsample_block</span> <span class="o">=</span> <span class="n">cache_block_id</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">trunc_upsample_block</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">upsample_block</span><span class="p">.</span><span class="n">resnets</span><span class="p">)</span>

        <span class="n">is_final_block</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="n">res_samples</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[</span><span class="o">-</span><span class="n">trunc_upsample_block</span><span class="p">:]</span>
        <span class="n">down_block_res_samples</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[:</span> <span class="o">-</span><span class="n">trunc_upsample_block</span><span class="p">]</span>

        <span class="c1"># if we have not reached the final block and need to forward the
</span>        <span class="c1"># upsample size, we do it here
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_final_block</span> <span class="ow">and</span> <span class="n">forward_upsample_size</span><span class="p">:</span>
            <span class="n">upsample_size</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

        <span class="k">if</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">upsample_block</span><span class="p">,</span> <span class="sh">"</span><span class="s">has_cross_attention</span><span class="sh">"</span><span class="p">)</span> <span class="ow">and</span> <span class="n">upsample_block</span><span class="p">.</span><span class="n">has_cross_attention</span><span class="p">:</span>
            <span class="c1">#print(sample.shape, [res_sample.shape for res_sample in res_samples])
</span>            <span class="n">sample</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">upsample_block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span>
                <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
                <span class="n">res_hidden_states_tuple</span><span class="o">=</span><span class="n">res_samples</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
                <span class="n">upsample_size</span><span class="o">=</span><span class="n">upsample_size</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">enter_block_number</span><span class="o">=</span><span class="n">cache_block_id</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cache_layer_id</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="nf">upsample_block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span>
                <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
                <span class="n">res_hidden_states_tuple</span><span class="o">=</span><span class="n">res_samples</span><span class="p">,</span>
                <span class="n">upsample_size</span><span class="o">=</span><span class="n">upsample_size</span><span class="p">,</span>
                <span class="n">scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="n">prv_f</span> <span class="o">=</span> <span class="n">replicate_prv_feature</span>
</code></pre></div></div>

<p>References:</p>

<ul>
  <li>[1] <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.pdf" rel="external nofollow noopener" target="_blank">DeepCache: Accelerating Diffusion Models for Free</a>
</li>
</ul>

<p>Rectified Flows define the forward process as straight paths between the data distribution and a standard normal distribution [2], i.e.,</p>

\[z_t = (1 - t) x_0 + t \epsilon\]

<p>where \(\epsilon\) is a standard normal random variable and \(t\) is the time step in [0, 1].</p>

<h2 id="multi-modal-diffusion">Multi-modal Diffusion</h2>

<!-- mkdir -p assets/img/diffusion-foundation -->
<!-- mv _posts/2025-10-01-*.png assets/img/diffusion-foundation/ -->

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
