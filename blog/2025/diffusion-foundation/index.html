<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Foundation of Diffusion Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/diffusion-foundation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Foundation of Diffusion Models</h1>
    <p class="post-meta">March 8, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          <a href="/blog/tag/diffusion">
          <i class="fas fa-hashtag fa-sm"></i> diffusion</a>  
          <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#what-are-diffusion-models">What are Diffusion Models?</a>
<ul>
<li class="toc-entry toc-h3"><a href="#advantages-of-diffusion-models">Advantages of Diffusion Models</a></li>
<li class="toc-entry toc-h3"><a href="#disadvantages-of-diffusion-models">Disadvantages of Diffusion Models</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#mathematical-foundation">Mathematical Foundation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#differential-equations-odes-and-sdes">Differential Equations: ODEs and SDEs</a></li>
<li class="toc-entry toc-h3"><a href="#forward-and-reverse-diffusion-processes">Forward and Reverse Diffusion Processes</a></li>
<li class="toc-entry toc-h3"><a href="#fokker-planck-equation-from-trajectories-to-distributions">Fokker-Planck Equation: From Trajectories to Distributions</a></li>
<li class="toc-entry toc-h3"><a href="#score-matching-and-denoising">Score Matching and Denoising</a></li>
<li class="toc-entry toc-h3"><a href="#variational-perspective-and-kl-minimization">Variational Perspective and KL Minimization</a></li>
<li class="toc-entry toc-h3"><a href="#tweedies-formula">Tweedie’s formula</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#variants-of-diffusion-models">Variants of Diffusion Models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#ddpm">DDPM</a></li>
<li class="toc-entry toc-h3"><a href="#ddim">DDIM</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#flow-matching">Flow Matching</a>
<ul>
<li class="toc-entry toc-h3"><a href="#fundammentals-concepts-in-flow-matching">Fundammentals Concepts in Flow Matching</a></li>
<li class="toc-entry toc-h3"><a href="#derivation-of-the-flow-matching-objective">Derivation of the Flow Matching Objective</a></li>
<li class="toc-entry toc-h3"><a href="#how-to-sampling-from-flow-matching-model">How to sampling from Flow Matching model</a></li>
<li class="toc-entry toc-h3"><a href="#flow-matching-code-example">Flow Matching Code Example</a></li>
<li class="toc-entry toc-h3"><a href="#conditional-flow-matching">Conditional Flow Matching</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#differences-between-score-matching-ddpm-and-flow-matching">Differences between Score Matching, DDPM and Flow Matching</a>
<ul>
<li class="toc-entry toc-h3"><a href="#summary-of-main-differences">Summary of Main Differences</a></li>
<li class="toc-entry toc-h3">
<a href="#detailed-differences">Detailed Differences</a>
<ul>
<li class="toc-entry toc-h4"><a href="#1-time-convention">1. Time Convention</a></li>
<li class="toc-entry toc-h4"><a href="#2-forward-process-vs-probability-paths">2. Forward Process vs. Probability Paths</a></li>
<li class="toc-entry toc-h4"><a href="#3-training-objective">3. Training Objective</a></li>
<li class="toc-entry toc-h4"><a href="#4-sampling-process">4. Sampling Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#noise-scheduling">Noise scheduling</a></li>
<li class="toc-entry toc-h2">
<a href="#guidanced-diffusion">Guidanced Diffusion</a>
<ul>
<li class="toc-entry toc-h3"><a href="#classifier-guidance">Classifier Guidance</a></li>
<li class="toc-entry toc-h3"><a href="#classifier-free-guidance">Classifier-free Guidance</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#latent-diffusion">Latent Diffusion</a></li>
<li class="toc-entry toc-h2">
<a href="#conditional-diffusion">Conditional Diffusion</a>
<ul>
<li class="toc-entry toc-h3"><a href="#control-net">Control-Net</a></li>
<li class="toc-entry toc-h3"><a href="#image-prompt">Image Prompt</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#diffusion-transformers">Diffusion Transformers</a>
<ul>
<li class="toc-entry toc-h3"><a href="#data-processing-in-dit">Data Processing in DiT</a></li>
<li class="toc-entry toc-h3"><a href="#the-dit-architecture">The DiT Architecture</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#diffusion-flux">Diffusion Flux</a></li>
<li class="toc-entry toc-h2">
<a href="#image-inpainting-with-diffusion-models">Image Inpainting with Diffusion Models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#training-pipeline">Training Pipeline</a></li>
<li class="toc-entry toc-h3"><a href="#challenges-in-image-inpainting">Challenges in Image Inpainting</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#accelerating-diffusion-models">Accelerating Diffusion Models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#diffusion-distillation">Diffusion Distillation</a></li>
<li class="toc-entry toc-h3"><a href="#rectified-diffusion">Rectified Diffusion</a></li>
</ul>
</li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <p>(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)</p>

<h2 id="what-are-diffusion-models">What are Diffusion Models?</h2>

<p>Diffusion models are a class of generative models that generate data by progressively denoising a sample from pure noise. They are inspired by <a href="https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics" rel="external nofollow noopener" target="_blank"><strong>non-equilibrium thermodynamics</strong></a> and are based on a forward and reverse diffusion process:</p>

<ol>
  <li>Forward Process (Diffusion Process): A data sample (e.g., an image) is gradually corrupted by adding Gaussian noise over multiple timesteps until it becomes nearly pure noise.</li>
  <li>Reverse Process (Denoising Process): A neural network learns to reverse this corruption by gradually removing noise step by step, reconstructing the original data distribution.</li>
</ol>

<figure style="text-align: center;">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/JnIkGtkO-Js?si=faOgaMvGtqTLcG1T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>Diffusion - How molecules actually move</figcaption>
</figure>

<p><strong>Analogy: Ink Dissolving in Water</strong>
Imagine dropping a blob of ink into a glass of water:</p>

<ul>
  <li>Forward process (Diffusion Process): Initially, the ink is concentrated in one place (structured data). Over time, it spreads out randomly, blending with the water (adding noise). Eventually, the entire glass becomes a uniformly colored mixture, losing its original structure (complete noise).</li>
  <li>Reverse process (Denoising Process): If we had a way to perfectly reverse time, we could watch the ink particles retrace their paths, reassembling into the original drop (generating the original data from noise). Diffusion models learn to perform this “reverse process” step by step using machine learning.</li>
</ul>

<blockquote>
  <p><strong>Non-Equilibrium Thermodynamics</strong></p>

  <p>Thermodynamics studies <strong>how energy moves and changes</strong> in a system. In equilibrium thermodynamics, systems are in balance—nothing is changing. Non-equilibrium thermodynamics, on the other hand, deals with <strong>systems that are constantly evolving, moving between states of disorder and order</strong>.</p>

  <p>In diffusion models, the forward process (adding noise to data) and the reverse process (removing noise) resemble a non-equilibrium thermodynamic system because they describe an evolving state that moves from order (structured data) to disorder (pure noise) and back to order (reconstructed data).</p>
</blockquote>

<blockquote>
  <p><strong>Brownian Motion</strong></p>

  <p>Brownian motion <strong>describes the random movement</strong> of tiny particles (like pollen grains in water) due to <strong>collisions with molecules</strong>. This randomness is similar to how noise is added in diffusion models.</p>
</blockquote>

<h3 id="advantages-of-diffusion-models">Advantages of Diffusion Models</h3>

<p>Diffusion models offer several key advantages over traditional generative models like GANs and VAEs:</p>

<ol>
  <li>
    <p><strong>High-Fidelity Samples</strong>: Unlike VAEs and GANs which generate samples in one step, diffusion models create samples gradually by denoising. This step-by-step process allows the model to first establish coarse image structure before refining fine details, resulting in higher quality outputs.</p>
  </li>
  <li>
    <p><strong>Training Stability</strong>: Diffusion models are easier to train compared to GANs as they use a single tractable likelihood loss. They don’t suffer from training instabilities like mode collapse that often plague GANs.</p>
  </li>
  <li>
    <p><strong>Sample Diversity</strong>: Similar to VAEs, diffusion models maximize likelihood which ensures coverage of all modes in the training dataset. This leads to more diverse outputs compared to GANs which can suffer from mode collapse.</p>
  </li>
  <li>
    <p><strong>Flexible Architecture</strong>: The multi-step denoising process enables additional functionalities like inpainting or image-to-image generation by manipulating the input noise, without requiring architectural changes.</p>
  </li>
  <li>
    <p><strong>Consistent Quality</strong>: The gradual denoising process is more robust and consistent compared to GANs where quality can vary significantly between samples.</p>
  </li>
</ol>

<p>The main trade-off is generation speed - diffusion models require multiple neural network passes to generate samples, making them slower than single-pass models like GANs and VAEs. However, various sampling optimization techniques have been developed to significantly reduce this computational overhead.</p>

<h3 id="disadvantages-of-diffusion-models">Disadvantages of Diffusion Models</h3>

<p>While diffusion models have significant advantages, they also come with some trade-offs:</p>

<ul>
  <li>Slow Sampling: The reverse process requires multiple denoising steps, making inference slower compared to GANs.</li>
  <li>Compute Intensive: Training requires large amounts of data and computational power.</li>
  <li>Memory Usage: They require storing multiple intermediate noise distributions, making them more memory-intensive.</li>
  <li>Complex Implementation: The multi-step nature of diffusion models makes them more complex to implement compared to single-step models.</li>
</ul>

<h2 id="mathematical-foundation">Mathematical Foundation</h2>

<p>Diffusion models are built on a deep interplay between <strong>differential equations</strong>, <strong>probability theory</strong>, and <strong>variational inference</strong>. To understand why the model works, we need to trace how these ideas connect: from describing how systems evolve over time, to modeling probability densities, to designing trainable objectives.</p>

<h3 id="differential-equations-odes-and-sdes">Differential Equations: ODEs and SDEs</h3>

<p>We start with <strong>ordinary differential equations (ODEs)</strong>, which describe how a system changes deterministically over time based on its current state.</p>

\[\frac{dx}{dt} = f(x, t)\]

<p>where \(x(t)\) is the state of the system - the function we want to solve -and \(t\) is time. \(f(x, t)\) defines how \(x\) changes over time.</p>

<p>This is a useful starting point, but in real-world data generation, we must account for <strong>randomness</strong>. That brings us to <strong>stochastic differential equations (SDEs)</strong>, which incorporate random fluctuations into the system.</p>

\[dx = f(x, t) dt + g(x, t) dW_t\]

<p>where the <em>drift term</em> \(f(x, t) dt\) captures the deterministic trends, while the <em>diffusion term</em> \(g(x, t) dW_t\) captures the random fluctuations via a Wiener process \(W_t\).</p>

<p>👉 Motivation: Diffusion models inject noise step by step, so SDEs provide the natural language to describe this stochastic corruption process. More specifically, the drift term \(f(x, t)\) is the shift of the mean of the distribution, and the diffusion term \(g(x, t)\) is the spread of the distribution - injecting Gaussian noise.</p>

<h3 id="forward-and-reverse-diffusion-processes">Forward and Reverse Diffusion Processes</h3>

<p><strong>Forward Process (Adding Noise)</strong></p>

<p>The forward diffusion process transform a data sample \(x_0\) into pure noise \(x_T\) over time:</p>

\[dx = f(x, t)dt + g(t) dW_t\]

<p>Intuitively, the drift term \(f(x, t) dt\) shifts the mean of the distribution by a deterministic amount \(f(x,t)\) (i.e., is a function of \(x\) and current time \(t\)) to a zero mean distribution. The diffusion term \(g(t) dW_t\) spreads the distribution by injecting Gaussian noise, increasing the variance of the distribution.</p>

<p>Note that \(g(t)\) is a function of current time \(t\) only, to guarantee that each noised distribution remains Gaussian with a know mean and variance. This make the forward process to be simple and tractable, that we can have exact sampling formula for each specific time step \(t\), i.e., \(q (x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)\).</p>

<p><strong>Reverse Process (Removing Noise)</strong>
In order to generate data from pure noise \(x_T\), we need to reverse the diffusion process by Reverse-Time SDE (Anderson 1982).</p>

\[dx = \left[ f(x,t) - g^2(t) \nabla_x \log p_t(x) \right] dt + g(t) d\tilde{W}_t\]

<p>where \(\nabla_x \log p_t(x)\) is the <strong>score function</strong>, which estimates the structure of data at time \(t\) - how likely different data points are at each step. \(d\tilde{W}_t\) is another Wiener process but in the reverse direction.</p>

<p>👉 Motivation: Since \(f(x,t)\) and \(g(t))\) are known, to reverse noise, we must know the score function. So we train a neural network to approximate the score function \(\nabla_x \log p_t(x)\). This is the core of the diffusion model.</p>

<h3 id="fokker-planck-equation-from-trajectories-to-distributions">Fokker-Planck Equation: From Trajectories to Distributions</h3>

<p>SDEs describe how the distribution of a system changes over time, but what about the <strong>distribution of data</strong> as noise accumulates? The <strong>Fokker-Planck equation</strong> bridges the gap between trajectories and distributions, explaining how noise pushes data distributions \(p_t(x)\) toward isotropic Gaussian distributions.</p>

\[\frac{\partial p_t(x)}{\partial t} = -\nabla_x \cdot (f(x,t) p_t(x)) + \frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\]

<p>where \(p_t(x)\) is the distribution of the data at time \(t\).</p>

<p>The first term \(-\nabla_x \cdot (f(x,t) p_t(x))\) describes the change of the probability density \(p_t(x)\) with the drift term \(f(x,t)\) (as the velocity of that mass). The divergence operator \(\nabla_x \cdot\) measures how much the mass is spreading out (positive divergence) or converging/concentrating (negative divergence) at any given point \(x\). The whole term \(- \nabla_x \cdot (f(x,t) p_t(x))\) describes a <strong>rate of change</strong> of the probability density \(p_t(x)\) at any given point \(x\), where the positive value means the mass is flowing away from \(x\), causing \(p_t(x)\) to decrease (hence the negative sign), and vice versa.</p>

<p>The second term \(\frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\) presents the spreading and smoothing effect of the probability density \(p_t(x)\) over time due to the influence of the random fluctuations. More specifically, \(g(t)\) controls the magnitude of the random noise. The \(\nabla_x p_t(x)\) term describes the <strong>steepness</strong> or slope of \(p_t(x)\) at any given point \(x\). The whole term \(\frac{1}{2} \nabla_x \cdot (g(t)^2 \nabla_x p_t(x))\) describes a <strong>rate of change</strong> of the probability density \(p_t(x)\), i.e., the larger the gradient, the more the density rising sharply around \(x\). Similar to the first term, \(\nabla_x \cdot\) measures how much the mass is spreading out (positive divergence) or converging/concentrating (negative divergence) at any given point \(x\) with two differences:</p>

<ul>
  <li>It contains the random fluctuations term \(g(t)^2\) instead of the drift term \(f(x,t)\), introducing randomness into the system.</li>
  <li>It proportional to the gradient \(\nabla_x p_t(x)\), meaning that the <strong>slope/sharp region is more affected/spread out than the flat region</strong> (which has smaller gradient \(\nabla_x p_t(x)\)).</li>
</ul>

<h3 id="score-matching-and-denoising">Score Matching and Denoising</h3>

<p>Since the reverse SDE depends on the score function \(\nabla_x \log p_t(x)\) (which is intractable), we need to design a training objective to approximate this function. Vincent et al. proposed <strong>denoising score matching</strong> <a href="https://ieeexplore.ieee.org/abstract/document/6795935/" rel="external nofollow noopener" target="_blank">(Vincent 2011)</a> to approximate by training a neural network \(s_{\theta}(x_t, t)\) to approximate the conditional score function \(\nabla_x \log q(x_t \mid x_0, \epsilon)\) (where \(q\) is a tractable forward process, \(dx = f(x, t)dt + g(t)dW_t\)) <strong>assuming</strong> that \(q(x_t \mid x_0) \approx p_t(x_t)\) at time \(t\) (which is a reasonable assumption).</p>

<p>This objective aims to minimize the difference:</p>

\[\mathbb{E}_{p(x_0), \epsilon \sim \mathcal{N}(0, I)} \left[ \left\| s_{\theta}(x_t, t) - \nabla_x \log p_t(x_t \mid x_0, \epsilon) \right\|^2 \right]\]

<h3 id="variational-perspective-and-kl-minimization">Variational Perspective and KL Minimization</h3>

<p>Another way to frame diffusion models is to consider them as a variational inference problem. The forward process \(q(x_{0:T})\) is a know noising chain of distributions, and the reverse process \(p_{\theta}(x_{0:T})\) is learned. Therefore, we can use the variational lower bound (ELBO) to train the model.</p>

\[\mathbb{E}_{q(x_{0:T})} \left[ D_{KL} \left( q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) \right]\]

<p><strong>ELBO</strong></p>

<p><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="external nofollow noopener" target="_blank"><strong>Evidence lower bound (ELBO)</strong></a> is a key concept in variational inference, which is used in VAEs to approximate the log-likelihood of the data.</p>

<p>Let \(X\) and \(Z\) be random variables, jointly distributed with distribution \(p_\theta\). For example, \(p_\theta(X)\) is the marginal distribution of \(X\), and \(p_\theta(Z \mid X)\) is the conditional distribution of \(Z\) given \(X\). Then, for a sample \(x \sim p_{\text{data}}\), and any distribution \(q_\phi\), the ELBO is defined as</p>

\[L(\phi, \theta; x) := \mathbb{E}_{z\sim q_\phi(\cdot|x)} \left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>The ELBO can equivalently be written as</p>

\[\begin{aligned}
L(\phi, \theta; x) &amp;= \mathbb{E}_{z\sim q_\phi(\cdot|x)}[\ln p_\theta(x,z)] + H[q_\phi(z \mid x)] \\
&amp;= \ln p_\theta(x) - D_{KL}(q_\phi(z \mid x) || p_\theta(z \mid x)).
\end{aligned}\]

<p>In the first line, \(H[q_\phi(z \mid x)]\) is the entropy of \(q_\phi\), which relates the ELBO to the Helmholtz free energy. In the second line, \(\ln p_\theta(x)\) is called the evidence for \(x\), and \(D_{KL}(q_\phi(z \mid x) \mid\mid p_\theta(z \mid x))\) is the Kullback-Leibler divergence between \(q_\phi\) and \(p_\theta\). Since the Kullback-Leibler divergence is non-negative, \(L(\phi, \theta; x)\) forms a lower bound on the evidence (ELBO inequality)</p>

\[\ln p_\theta(x) \geq \mathbb{E}_{z\sim q_\phi(\cdot|x)}\left[\ln \frac{p_\theta(x,z)}{q_\phi(z|x)}\right].\]

<p>Deep-dive topics about VAE might including:</p>

<ul>
  <li>Reparameterization Trick: <a href="https://en.wikipedia.org/wiki/Reparameterization_trick" rel="external nofollow noopener" target="_blank">How to sample from a distribution in a differentiable way - Wiki</a>
</li>
  <li>The problem of KL divergence: <a href="https://andrewcharlesjones.github.io/journal/klqp.html" rel="external nofollow noopener" target="_blank">mode seeking vs mode covering</a> by Andy Jones</li>
  <li><a href="https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae" rel="external nofollow noopener" target="_blank">A nice property of VAEs: Disentanglement Representation Learning</a></li>
</ul>

<h3 id="tweedies-formula">Tweedie’s formula</h3>

<p>Finally, <strong>Tweedie’s formula</strong> gives a neat probabilistic justification for the denoising score matching objective:</p>

\[\mathbb{E} [ x_0 \mid x_t] = x_t + \sigma_t^2 s_{\theta}(x_t, t)\]

<p>where \(s_{\theta}(x_t, t)\) is the score function to be learned by the neural network. It shows that the posterior mean of clean data given a noisy data is just the noisy sample plus a correction term proportional to the score function.</p>

<p>In some papers such as ESD (Gandikota et al. 2023), where we need to fine-tune the pretrained model and match the score function of the original and the fine-tuned model, they use Tweedie’s formula to justify the matching term.</p>

<h2 id="variants-of-diffusion-models">Variants of Diffusion Models</h2>

<p>The original formulation of diffusion models can be implemented in several ways. Two of the most influential variants are <strong>Denoising Diffusion Probabilistic Models (DDPMs)</strong> and <strong>Denoising Diffusion Implicit Models (DDIMs)</strong>. Both share the same forward noising process but differ in how they perform the reverse (denoising) process during inference.</p>

<h3 id="ddpm">DDPM</h3>

<p>DDPM (Ho et al. 2020) is the classic diffusion model:</p>

<ul>
  <li>The Reverse process is defined as a <strong>Markov chain</strong>, where each step \(x_t \to x_{t-1}\) involves sampling from a Gaussian distribution conditioned on \(x_t\).</li>
  <li>Sampling os stochastic, even with the same starting noise \(x_T\), the generated data \(x_0\) is different.</li>
  <li>While highly effective and stable (compared to GANs), DDPMs require hundreds to thousands of steps to slowly add/remove noise, which makes inference slow.</li>
</ul>

<p>Read more about DDPM in another blog post <a href="/blog/2023/diffusion-tutorial/">here</a></p>

<h3 id="ddim">DDIM</h3>

<p>DDIM (Song et al. 2020) builds on DDPM but introduces a <strong>non-Markovian reverse process</strong>, enabling faster sampling.
It also allows us to use the same training process as DDPM, e.g., we can use pretrained DDPM models to generate data.</p>

<p>The sampling process of DDIM is as follows:</p>

\[x_{t-1} = \sqrt{\alpha_{t-1}} \left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right) + \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}(x_t) + \sigma_t\epsilon_t\]

<p>where the first term represents the “predicted \(x_0\)”, the second term is the “direction pointing to \(x_t\)”, and the last term is random noise.</p>

<p>By setting \(\sigma_t = 0\) for all \(t\), DDIM becomes a deterministic process given \(x_{t-1}\) and \(x_0\), except for \(t=1\). In other words, the intermediate steps \(x_{T-1}, x_{T-2}, \ldots, x_1\) are deterministic given starting noise \(x_T\).</p>

<p>Read more about DDIM in another blog post <a href="/blog/2023/diffusion-tutorial-p2/">here</a></p>

<h2 id="flow-matching">Flow Matching</h2>

<h3 id="fundammentals-concepts-in-flow-matching">Fundammentals Concepts in Flow Matching</h3>

<p><strong>Normalizing Flow</strong>: A class of generative models that learns a transformation (or “flow”) to map a know prior distribution \(p_0\) to a target distribution \(p_1\) through a family of intermediate marginal distributions \(p_t\), where \(t \in [0, 1]\). A key requirement is that the transformation must be invertible (bijective).</p>

<p><strong>Continuous Normalizing Flow</strong>: Uses ordinary differential equation (ODE) to define continuous-time transformations between distributions.</p>

<p><strong>Flow and Velocity Field</strong>:</p>

<ul>
  <li>The flow \(\psi_t(x)\) describes the trajectory of a point \(x\) over time.</li>
  <li>The velocity field \(u_t(x)\) specifies the instantaneous direction and speed of movement</li>
  <li>These are related by the ODE: \(\frac{d}{dt} \psi_t(x) = u_t (\psi_t (x))\)</li>
  <li>The induced density \(p_t(x)\) evolves according to the continuity equation: \(\frac{\partial p_t(x)}{\partial t} + \nabla_x \cdot \big( u_t(x) \, p_t(x) \big) = 0.\)</li>
</ul>

<p>This equation shows how a point moves along the flow path: \(x_t \rightarrow x_{t+1} = x_t + dt * u_t(x_t)\) at time \(t\).</p>

<p><strong>Key insight</strong>: The velocity field \(u_t(x)\) is the only component neccessary to sample from \(p_t\) by solving the ODE. Therefore, <strong>flow matching aims to learn the velocity field \(u_t(x)\)</strong>.</p>

<h3 id="derivation-of-the-flow-matching-objective">Derivation of the Flow Matching Objective</h3>

<p><strong>Starting Objective</strong>: Approximate the velocity field \(u_t(x)\) with the learned velocity field \(v_{\theta}(t, x)\).</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t) \|^2 \right]\]

<p><strong>Step 1</strong>: Expand the squared norm:</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t) \|^2 \right] = \mathbb{E}_{x_t \sim p_t(x)} \left[ \| v_{\theta}(t, x_t) \|^2 - 2 \langle v_{\theta}(t, x_t), u_t(x_t) \rangle + \| u_t(x_t) \|^2 \right]\]

<p><strong>Step 2</strong>: Express the velocity field as a conditional expectation:</p>

\[u_t(x_t) = \int u_t(x_t \mid x_1) \frac{p_t (x_t \mid x_1) q(x_1)}{p_t(x_t)} dx_1\]

<p><strong>Interpretation</strong>: The velocity at \(x_t\) is a weighted average of conditional velocities \(u_t(x_t \mid x_1)\) from all possible data points \(x_1\). Point \(x_1\) that are “closer” to \(x_t\) (higher probability \(p_t (x_t \mid x_1)\)) contribute more to the velocity at \(x_t\).</p>

<p><strong>Step 3</strong>: Substitute into the cross-term expectation (correlation between \(v_{\theta}(t, x_t)\) and \(u_t(x_t)\))</p>

\[\mathbb{E}_{x_t \sim p_t(x)} \left[ \langle v_{\theta}(t, x_t), u_t(x_t) \rangle \right] = \int p_t(x_t) v_{\theta}(t, x_t) \cdot u_t(x_t) dx_t\]

<p>Substitute \(u_t(x_t)\) to the above equation:</p>

\[= \int \int v_{\theta}(t, x_t) \cdot u_t(x_t \mid x_1) \cdot p_t(x_t \mid x_1) \cdot q(x_1) dx_1 dx_t\]

\[= \mathbb{E}_{x_t \sim p_t(x_t \mid x_1), x_1 \sim q(x_1)} \left[ v_{\theta}(t, x_t) \cdot u_t(x_t \mid x_1) \right]\]

<p><strong>Step 4</strong>: Rewrite the full objective using conditional expectation:</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) \|^2 - 2 \langle v_{\theta}(t, x_t), u_t(x_t \mid x_1) \rangle + \| u_t(x_t) \|^2 \right]\]

<p><strong>Step 5</strong>: Add and subtract the term \(\| u_t(x_t \mid x_1) \|^2\)</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t \mid x_1) \|^2 \right] + \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| u_t(x_t) \|^2 - \| u_t(x_t \mid x_1) \|^2\right]\]

<p><strong>Step 6</strong>: Drop constant terms that are independent of \(\theta\):</p>

\[\mathcal{L}_{FM} (\theta) = \mathbb{E}_{x_1 \sim q(x_1), x_t \sim p_t(x_t \mid x_1)} \left[ \| v_{\theta}(t, x_t) - u_t(x_t \mid x_1) \|^2 \right]\]

<p><strong>Practical Implementation</strong>:</p>

<p>Simply choose the linear interpolation path \(X_t = (1 - t) X_0 + t X_1\), then the velocity field \(u_t(x_t)\) is:</p>

\[u_t(x_t \mid x_1) = \frac{d}{dt} X_t = X_1 - X_0\]

<p>This give us a tractable training objective where we sample:</p>

<ul>
  <li>A time step \(t \sim \mathcal{U}(0, 1)\)</li>
  <li>A data ppont \(x_1 \sim q(x_1)\)</li>
  <li>A noise sample \(x_0 \sim \mathcal{N}(0, I)\)</li>
  <li>Construct the interpolated sample \(x_t = (1 - t) x_0 + t x_1\)</li>
  <li>Train to predict the velocity field \(v_{\theta}(t, x_t) = x_1 - x_0\)</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-02-11-15-03-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-02-11-15-03.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-02-10-03-19-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-02-10-03-19.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<h3 id="how-to-sampling-from-flow-matching-model">How to sampling from Flow Matching model</h3>

<p>The sampling process of Flow Matching model is similar to the diffusion model, where we start from a noise sample \(x_0 \sim \mathcal{N}(0, I)\) and iteratively sample the next step \(x_{t+dt}\) by Euler method:</p>

\[x_{t+dt} = x_t + dt * v_{\theta}(t+dt/2, x_t+dt/2 * v_{\theta}(t, x_t))\]

<p>where \(v_{\theta}(t, x_t)\) is the velocity field predicted by the neural network.</p>

<h3 id="flow-matching-code-example">Flow Matching Code Example</h3>

<p><a href="https://github.com/facebookresearch/flow_matching/blob/main/examples/standalone_flow_matching.ipynb" rel="external nofollow noopener" target="_blank">A Standalone Flow Matching code</a> - from [4]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Define the Flow
</span><span class="k">class</span> <span class="nc">Flow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_start</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_end</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_start</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_t</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="nf">self</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="nf">self</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">flow</span> <span class="o">=</span> <span class="nc">Flow</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">flow</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="nf">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x_1</span>
    <span class="n">dx_t</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="nf">loss_fn</span><span class="p">(</span><span class="nf">flow</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">),</span> <span class="n">dx_t</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="c1"># Sampling
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">t_start</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">t_end</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p>In the above code, the <code class="language-plaintext highlighter-rouge">forward</code> function is for the velocity field \(v_{\theta}(t, x)\), and the <code class="language-plaintext highlighter-rouge">step</code> function is to get the next step \(X_{t+dt}\) from the current step \(X_t\) by Euler method</p>

\[X_{t+dt} = X_t + dt * v_{\theta}(t+dt/2, X_t+dt/2 * v_{\theta}(t, X_t))\]

<h3 id="conditional-flow-matching">Conditional Flow Matching</h3>

<p>(Note that the <strong>Conditional</strong> in the name of Conditional Flow Matching is meaning the condition \(c\) is given, not the conditional vector field \(u_t(x \mid x_1)\) from previous step)</p>

<p>In conditional flow matching, we incorporate a condition \(c\) into the velocity field \(v_{\theta}(t, x, c)\).
In practice, the three inputs \(t, x, c\) are concatenated together as the input to the neural network.</p>

<p>Sampling function:</p>

\[X_{t+dt} = X_t + dt * v_{\theta}(t+dt/2, X_t+dt/2 * v_{\theta}(t, X_t, c), c)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Define the Flow
</span><span class="k">class</span> <span class="nc">Flow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_start</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_end</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_start</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_t</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="nf">self</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="nf">self</span><span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">flow</span> <span class="o">=</span> <span class="nc">Flow</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">flow</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">x_1</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nf">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x_1</span>
    <span class="n">dx_t</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="nf">loss_fn</span><span class="p">(</span><span class="nf">flow</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">),</span> <span class="n">dx_t</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Sampling
# --- evaluation / visualisation section --------------------------
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">256</span>                    

<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">x</span>      <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>     <span class="c1"># (n_samples, 2)
</span>
<span class="c1"># if you just want random labels –– otherwise load real labels here
</span><span class="n">c_eval</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (n_samples, 1)
</span>
<span class="c1"># colours for the scatter (same length as x)
</span><span class="n">colors</span>  <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span> <span class="k">if</span> <span class="n">lbl</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">'</span><span class="s">orange</span><span class="sh">'</span> <span class="k">for</span> <span class="n">lbl</span> <span class="ow">in</span> <span class="n">c_eval</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()]</span>

<span class="c1"># -----------------------------------------------------------------
</span><span class="n">n_steps</span>      <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_every</span>   <span class="o">=</span> <span class="mi">20</span>
<span class="n">plot_indices</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">plot_every</span><span class="p">))</span>
<span class="k">if</span> <span class="n">plot_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_steps</span><span class="p">:</span>
    <span class="n">plot_indices</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span>   <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">plot_indices</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">plot_indices</span><span class="p">),</span> <span class="mi">4</span><span class="p">),</span>
                           <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">time_steps</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># initial frame
</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">plot_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>                         <span class="c1"># no gradients while sampling
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">x_t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                      <span class="n">t_start</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                      <span class="n">t_end</span><span class="o">=</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">c_eval</span><span class="p">)</span>               <span class="c1"># 2️⃣ use the same‑sized label tensor
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">in</span> <span class="n">plot_indices</span><span class="p">:</span>
            <span class="n">plot_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t = </span><span class="si">{</span><span class="n">time_steps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">plot_count</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<p>References:</p>

<ul>
  <li>[1]<a href="https://arxiv.org/abs/2210.02747" rel="external nofollow noopener" target="_blank">Flow Matching for Generative Modeling</a> paper</li>
  <li>[2]<a href="https://youtu.be/7cMzfkWFWhI?si=rRnZQKxs9p-_zjhZ" rel="external nofollow noopener" target="_blank">A cool explanation of Flow Matching</a>
</li>
  <li>[3]<a href="https://diffusionflow.github.io/" rel="external nofollow noopener" target="_blank">Diffusion Meets Flow Matching: Two Sides of the Same Coin</a>
</li>
  <li>[4]<a href="https://neurips.cc/media/neurips-2024/Slides/99531.pdf" rel="external nofollow noopener" target="_blank">A NeurIPS 2024 tutorial on Flow Matching</a>
</li>
</ul>

<h2 id="differences-between-score-matching-ddpm-and-flow-matching">Differences between Score Matching, DDPM and Flow Matching</h2>

<h3 id="summary-of-main-differences">Summary of Main Differences</h3>

<p>All three methods learn generative models by establishing a connection between a simple noise distribution and a complex data distribution, but they differ fundamentally in their formulation and training approach:</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Diffusion Models (DDPM/DDIM)</th>
      <th>Score Matching (SDE)</th>
      <th>Flow Matching (CFM)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Core Learning Target</strong></td>
      <td>Learn to predict noise \(\epsilon_t\) or data \(x_t\) from previous step \(x_{t-1}\)</td>
      <td>Learn score function \(\nabla_x \log p_t(x)\)</td>
      <td>Learn velocity field \(u_t(x)\)</td>
    </tr>
    <tr>
      <td><strong>Process Type</strong></td>
      <td>Discrete Markov chain</td>
      <td>Continuous SDE</td>
      <td>Continuous ODE</td>
    </tr>
    <tr>
      <td><strong>Forward Process</strong></td>
      <td>Add Gaussian noise step-by-step</td>
      <td>Stochastic diffusion (SDE with drift + noise)</td>
      <td>Deterministic interpolation path</td>
    </tr>
    <tr>
      <td><strong>Backward Process</strong></td>
      <td>Reverse Markov chain</td>
      <td>Reverse SDE</td>
      <td>ODE integration</td>
    </tr>
    <tr>
      <td><strong>Tractability</strong></td>
      <td>Forward process tractable</td>
      <td>Forward SDE tractable</td>
      <td>Conditional paths tractable</td>
    </tr>
    <tr>
      <td><strong>Training Paradigm</strong></td>
      <td>Denoising autoencoder</td>
      <td>Denoising score matching</td>
      <td>Conditional flow matching</td>
    </tr>
    <tr>
      <td><strong>Sampling</strong></td>
      <td>Iterative denoising (stochastic or deterministic)</td>
      <td>SDE/ODE integration</td>
      <td>ODE integration (typically straight paths)</td>
    </tr>
    <tr>
      <td><strong>Path Geometry</strong></td>
      <td>Curved noising trajectory</td>
      <td>Stochastic curved paths</td>
      <td>Straight/optimal transport paths</td>
    </tr>
    <tr>
      <td><strong>Key Advantage</strong></td>
      <td>Simple, well-understood</td>
      <td>Theoretically grounded, flexible</td>
      <td>Fast sampling, simple training</td>
    </tr>
  </tbody>
</table>

<p><strong>Relationship</strong>:</p>
<ul>
  <li>DDIM can be viewed as a discretization of a probability flow ODE derived from the Score Matching SDE</li>
  <li>Flow Matching with Gaussian probability paths recovers Score Matching formulations</li>
  <li>Flow Matching with linear interpolation paths gives the deterministic version similar to DDIM</li>
  <li>All three can be unified under a common framework of learning to transform distributions</li>
</ul>

<hr>

<h3 id="detailed-differences">Detailed Differences</h3>

<h4 id="1-time-convention">1. Time Convention</h4>

<p>Understanding time conventions is crucial for comparing these methods:</p>

<p><strong>Flow Matching:</strong></p>
<ul>
  <li>
<strong>Continuous time</strong> \(t \in [0, 1]\)</li>
  <li>\(t = 0\): <strong>noise</strong> distribution \(p_0(x) = \mathcal{N}(0, I)\)</li>
  <li>\(t = 1\): <strong>data</strong> distribution \(p_1(x) = q(x)\)</li>
  <li>Forward in time moves from noise → data</li>
</ul>

<p><strong>Diffusion Models (DDPM, DDIM):</strong></p>
<ul>
  <li>
<strong>Discrete time</strong> with steps \(t \in \{0, 1, 2, ..., T\}\)</li>
  <li>\(t = 0\): <strong>data</strong> distribution \(q(x_0)\)</li>
  <li>\(t = T\): <strong>noise</strong> distribution \(\mathcal{N}(0, I)\)</li>
  <li>Forward in time moves from data → noise (opposite of Flow Matching!)</li>
  <li>To align with Flow Matching convention, we use \(r = T - t\), so:
    <ul>
      <li>\(r = 0\) corresponds to noise</li>
      <li>\(r = T\) corresponds to data</li>
    </ul>
  </li>
</ul>

<p><strong>Score Matching (SDE):</strong></p>
<ul>
  <li>
<strong>Continuous time</strong> \(t \in [0, T]\) (often \(T = 1\))</li>
  <li>\(t = 0\): <strong>data</strong> distribution \(p_0(x) = q(x)\)</li>
  <li>\(t = T\): <strong>noise</strong> distribution \(p_T(x) \approx \mathcal{N}(0, \sigma^2 I)\)</li>
  <li>Forward in time moves from data → noise</li>
  <li>Using \(r = T - t\) for consistency: \(r = 0\) is noise, \(r = T\) is data</li>
</ul>

<hr>

<h4 id="2-forward-process-vs-probability-paths">2. Forward Process vs. Probability Paths</h4>

<p><strong>Diffusion Models (DDPM, DDIM):</strong></p>

<p>The forward process progressively corrupts data by adding Gaussian noise through a <strong>Markov chain</strong>:</p>

\[q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]

<p>With reparameterization:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\]

<p>where \(\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)\).</p>

<ul>
  <li>
<strong>Discrete steps</strong>: Each transition is a single Gaussian convolution</li>
  <li>
<strong>Tractable</strong>: \(q(x_t \mid x_0)\) has closed form</li>
  <li>
<strong>Markovian</strong>: Each step depends only on the previous state</li>
</ul>

<p><strong>Score Matching (SDE):</strong></p>

<p>The forward process is a <strong>continuous stochastic differential equation (SDE)</strong>:</p>

\[dx = f(x, t)dt + g(t)dW_t\]

<p>where:</p>
<ul>
  <li>\(f(x, t)\) is the <strong>drift coefficient</strong> (deterministic component)</li>
  <li>\(g(t)\) is the <strong>diffusion coefficient</strong> (stochastic component)</li>
  <li>\(W_t\) is the <strong>Wiener process</strong> (Brownian motion)</li>
</ul>

<p>Common example (Variance Exploding - VE):
\(dx = 0 \cdot dt + \sqrt{\frac{d\sigma_t^2}{dt}} dW_t\)</p>

<p>Common example (Variance Preserving - VP):
\(dx = -\frac{1}{2}\beta_t x \, dt + \sqrt{\beta_t} dW_t\)</p>

<ul>
  <li>
<strong>Continuous time</strong>: Infinitesimal noise additions</li>
  <li>
<strong>Stochastic</strong>: Includes random Brownian motion</li>
  <li>
<strong>Non-Markovian</strong> in discrete time but Markovian in continuous time</li>
</ul>

<p><strong>Flow Matching:</strong></p>

<p>The forward process defines <strong>probability paths</strong> that interpolate between distributions:</p>

\[p_t(x) = \int p_t(x \mid x_1) q(x_1) dx_1\]

<p>Where the <strong>conditional probability path</strong> is often chosen as:</p>

\[p_t(x_t \mid x_1) = \mathcal{N}(x_t; \mu_t(x_1), \sigma_t^2(x_1) I)\]

<p>For <strong>linear interpolation</strong> (Optimal Transport path):
\(x_t = (1-t)x_0 + t x_1, \quad x_0 \sim \mathcal{N}(0, I)\)</p>

<p>This gives:
\(\mu_t(x_1) = t x_1, \quad \sigma_t = 1 - t\)</p>

<ul>
  <li>
<strong>Deterministic paths</strong> (no stochastic component in the ODE)</li>
  <li>
<strong>Conditional paths</strong> are tractable by design</li>
  <li>
<strong>Straight trajectories</strong> (shortest path in many metrics)</li>
</ul>

<p><strong>Connections:</strong></p>
<ul>
  <li>The <strong>velocity field</strong> \(u_t(x)\) in Flow Matching corresponds to the <strong>drift term</strong> \(f(x, t)\) in Score Matching</li>
  <li>The <strong>conditional probability path</strong> \(p_t(x_t \mid x_1)\) in Flow Matching corresponds to the SDE solution initialized at \(x_1\)</li>
  <li>The <strong>marginal probability path</strong> \(p_t(x)\) in Flow Matching corresponds to the SDE marginal when initialized from data \(x_0 \sim q(x)\)</li>
</ul>

<p><strong>Special Cases:</strong></p>
<ul>
  <li>Flow Matching with <strong>Gaussian probability paths</strong> (with appropriate \(\mu_t, \sigma_t\)) recovers the forward SDE from Score Matching</li>
  <li>Flow Matching with <strong>linear interpolation</strong> gives the deterministic probability flow ODE, similar to DDIM</li>
</ul>

<hr>

<h4 id="3-training-objective">3. Training Objective</h4>

<p><strong>Diffusion Models (DDPM):</strong></p>

<p>Train a neural network to predict the noise added in the forward process:</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon_\theta(x_t, t) - \epsilon \|^2 \right]\]

<p>where \(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\) (<strong>\(\epsilon\)-prediction</strong>).</p>

<p>Alternative formulation (<strong>\(x_0\)-prediction</strong>):</p>

\[\mathcal{L}_{DDPM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \hat{x}_\theta(x_t, t) - x_0 \|^2 \right]\]

<ul>
  <li>
<strong>Target</strong>: Noise \(\epsilon\) or clean data \(x_0\)</li>
  <li>
<strong>Simple</strong>: Direct regression on known quantities</li>
  <li>
<strong>Weighted MSE</strong>: Can add time-dependent weighting</li>
</ul>

<p><strong>Score Matching (DSM - Denoising Score Matching):</strong></p>

<p>The reverse SDE requires the <strong>score function</strong> \(\nabla_x \log p_t(x)\), which is intractable. Vincent (2011) proposed training a network \(s_\theta(x_t, t)\) to approximate the <strong>conditional score</strong>:</p>

\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t, x_0, x_t \mid x_0} \left[ \| s_\theta(x_t, t) - \nabla_{x_t} \log q(x_t \mid x_0) \|^2 \right]\]

<p>Under the assumption that \(q(x_t \mid x_0) \approx p_t(x_t)\) (reasonable for small noise), this approximates the true score.</p>

<p>For Gaussian perturbations \(q(x_t \mid x_0) = \mathcal{N}(\alpha_t x_0, \sigma_t^2 I)\):</p>

\[\nabla_{x_t} \log q(x_t \mid x_0) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2} = -\frac{\epsilon}{\sigma_t}\]

<p>So the objective becomes:</p>

\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \left\| s_\theta(\alpha_t x_0 + \sigma_t \epsilon, t) + \frac{\epsilon}{\sigma_t} \right\|^2 \right]\]

<ul>
  <li>
<strong>Target</strong>: Score function (gradient of log probability)</li>
  <li>
<strong>Theoretical</strong>: Grounded in score-based generative modeling theory</li>
  <li>
<strong>Flexible</strong>: Works with any forward SDE</li>
</ul>

<p><strong>Flow Matching (CFM):</strong></p>

<p>Train a network to predict the velocity field:</p>

\[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, x_1, x_t \mid x_1} \left[ \| v_\theta(t, x_t) - u_t(x_t \mid x_1) \|^2 \right]\]

<p>For linear interpolation \(x_t = (1-t)x_0 + t x_1\):</p>

\[u_t(x_t \mid x_1) = \frac{d x_t}{dt} = x_1 - x_0\]

<p>So:</p>

\[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, x_0, x_1} \left[ \| v_\theta(t, x_t) - (x_1 - x_0) \|^2 \right]\]

<ul>
  <li>
<strong>Target</strong>: Velocity (direction and magnitude of flow)</li>
  <li>
<strong>Direct</strong>: Straightforward regression on vector field</li>
  <li>
<strong>Efficient</strong>: Often requires fewer sampling steps</li>
</ul>

<p><strong>Equivalence:</strong></p>

<p>The training objectives are closely related through reparameterizations:</p>

<ul>
  <li>
<strong>Score to Noise</strong>: \(s_\theta(x_t, t) = -\frac{\epsilon_\theta(x_t, t)}{\sigma_t}\)</li>
  <li>
<strong>Velocity to Noise</strong>: \(v_\theta(t, x_t) = \frac{\alpha_t}{\sigma_t} \epsilon_\theta(x_t, t)\) (approximately, for certain schedulers)</li>
  <li>Score Matching with Gaussian paths is <strong>equivalent</strong> to Flow Matching with appropriate probability path parameterization</li>
</ul>

<hr>

<h4 id="4-sampling-process">4. Sampling Process</h4>

<p><strong>Diffusion Models:</strong></p>

<p><strong>DDPM (Stochastic Sampling):</strong></p>

<p>Iteratively denoise by reversing the forward Markov chain:</p>

\[x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z\]

<p>where \(z \sim \mathcal{N}(0, I)\) and \(\sigma_t\) is the noise variance.</p>

<ul>
  <li>
<strong>Stochastic</strong>: Adds noise at each step</li>
  <li>
<strong>Many steps</strong>: Typically 1000 steps (can be reduced with techniques)</li>
</ul>

<p><strong>DDIM (Deterministic Sampling):</strong></p>

<p>Use a deterministic update rule:</p>

\[x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{predicted } x_0} + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_\theta(x_t, t)\]

<ul>
  <li>
<strong>Deterministic</strong>: No added noise</li>
  <li>
<strong>Fewer steps</strong>: 10-50 steps often sufficient</li>
  <li><strong>Equivalent to probability flow ODE</strong></li>
</ul>

<p><strong>Score Matching:</strong></p>

<p><strong>Reverse-time SDE:</strong></p>

\[dx = [f(x, t) - g(t)^2 \nabla_x \log p_t(x)] dt + g(t) d\bar{W}_t\]

<p>where \(\bar{W}_t\) is a reverse-time Brownian motion.</p>

<ul>
  <li>
<strong>Stochastic</strong>: Includes diffusion term</li>
  <li>
<strong>Continuous</strong>: Integrated numerically (Euler-Maruyama, etc.)</li>
</ul>

<p><strong>Probability Flow ODE (Deterministic):</strong></p>

\[\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)\]

<ul>
  <li>
<strong>Deterministic</strong>: No stochastic component</li>
  <li>
<strong>Same marginals</strong> as the SDE</li>
  <li>
<strong>Flexible solvers</strong>: Use any ODE solver (Runge-Kutta, adaptive methods)</li>
</ul>

<p><strong>Flow Matching:</strong></p>

<p>Integrate the learned velocity field ODE:</p>

\[\frac{dx}{dt} = v_\theta(t, x)\]

<p>Starting from \(x_0 \sim \mathcal{N}(0, I)\) at \(t=0\), integrate to \(t=1\):</p>

<p><strong>References</strong></p>

<ul>
  <li>[1]<a href="https://neurips.cc/media/neurips-2024/Slides/99531.pdf" rel="external nofollow noopener" target="_blank">A NeurIPS 2024 tutorial on Flow Matching</a>
</li>
</ul>

<h2 id="noise-scheduling">Noise scheduling</h2>

<p>Noise scheduling in diffusion models refers to how noise is gradually added to data in the forward process and how it is removed in the reverse process. The choice of noise schedule significantly impacts the model’s performance, sample quality, and training efficiency.</p>

<p>We follow the DDIM convention, where \(0 &lt; \bar{\alpha}_t &lt; 1, \beta_t = 1 - \bar{\alpha}_t\) and \(\alpha_t = \prod_{i=1}^{t} \bar{\alpha}_i\) is the cumulative noise level at time \(t\), and \(\beta_t\) is the noise level at time \(t\). With this convention, \(x_t = \sqrt(\alpha_t) x_0 + \sqrt(1-\alpha_t) \epsilon\), and \(\alpha_T \approx 0\) when \(t \rightarrow T\) while \(\alpha_0 \approx 1\) when \(t \rightarrow 0\).</p>

<p>Common principles of noise scheduling:</p>

<ul>
  <li>Add large amount of noise at \(t\) large while small amount of noise at \(t\) small. \(t=0\) means clean data, \(t=T\) means pure noise.</li>
  <li>The speed of change (acceleration, or \(\frac{d\beta_t}{dt}\)) should also has some proper speed (but I am not sure :D)</li>
</ul>

<p>Common noise schedules:</p>

<ul>
  <li>
<strong>Linear</strong>: \(\alpha_t = \frac{t}{T}\) or \(\beta_t = \beta_{\min} + (\beta_{\max} - \beta_{\min})\frac{t}{T}\). Issue: early timesteps do not add enough noise, and late timesteps can add too much noise.</li>
  <li>
<strong>Cosine</strong>: \(\beta_t = \beta_{\min} + 0.5 (\beta_{\max} - \beta_{\min}) ( 1 + \cos(\frac{t}{T} \pi))\). Intuition is that adding more gradually at the start and <strong>faster at the end</strong>.</li>
  <li>
<strong>Exponential</strong>: \(\beta_t = \beta_{\max} (\beta_{\min} / \beta_{\max})^{\frac{t}{T}}\)</li>
</ul>

<h2 id="guidanced-diffusion">Guidanced Diffusion</h2>

<p>Resources:</p>

<ul>
  <li>A great blog from Sander Dieleman: <a href="https://sander.ai/2022/05/26/guidance.html" rel="external nofollow noopener" target="_blank">Guidance: a cheat code for diffusion models</a> and <a href="https://sander.ai/2023/08/28/geometry.html" rel="external nofollow noopener" target="_blank">the geometry of diffusion guidance</a>.</li>
</ul>

<p><strong>Why Guidance?</strong></p>

<p>Guidance is a method to control the generation process so that the ouput is sample from a conditional distribution \(p(x \mid y)\), where \(y\) is a condition - such as a text prompt - rather than a generic \(p(x)\).</p>

<h3 id="classifier-guidance">Classifier Guidance</h3>

<p>In order to get the conditional score function \(\nabla_x \ln p(x \mid y)\), we can use Bayes rule to decompose the score function into an unconditional component and a conditional one:</p>

\[p(x \mid y) = \frac{p(y \mid x) p(x)}{p(y)}\]

\[\log p(x \mid y) = \log p(y \mid x) + \log p(x) - \log p(y)\]

\[\nabla_x \log p(x \mid y) = \nabla_x \log p(y \mid x) + \nabla_x \log p(x) - \nabla_x \log p(y)\]

<p>where \(\nabla_x \log p(x)\) is the score function of the unconditional model. \(\nabla_x \log p(y) = 0\) since \(p(y)\) is independent of \(x\).</p>

<p>The term \(\nabla_x \log p(y \mid x)\) means the direction pointing to \(y\) given \(x\).</p>

<ul>
  <li>In the begining of the inference process, i.e., large \(t\), when \(x_t\) still has a lot of noise, \(\nabla_x \log p(y \mid x)\) is close to \(0\), means that there is no clear information of \(y\).</li>
  <li>In the later stages, i.e., small \(t\), when \(x_t\) is less noisy and closer to \(x_0\), \(\nabla_x \log p(y \mid x)\) is larger, means that \(x_t\) has more information of \(y\), i.e., larger \(p(y \mid x)\).</li>
</ul>

<p><strong>How to obtain \(\nabla_x \log p(y \mid x)\)?</strong></p>

<p>\(p(y \mid x)\) means the probability of a condition \(y\) given \(x\).
In a simple case, where \(y\) is just a image class, like a <code class="language-plaintext highlighter-rouge">cat</code>, the probability \(p(y=\text{cat} \mid x)\) can be simply obtained from a pre-trained classifier.</p>

<p>However, in a more complex case, where \(y\) is a text prompt like a black cat with red eyes and blue fur, a pre-trained classifier is not expressive enough, i.e., it cannot distinguish between \(y_1\) <code class="language-plaintext highlighter-rouge">a black cat with red eyes and blue fur</code> vs \(y_2\) <code class="language-plaintext highlighter-rouge">a white cat with blue eyes and red fur</code> or mathematically \(p(y_1 \mid x) \neq p(y_2 \mid x)\).</p>

<p>In other words, the quality - diversity of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\). For example:</p>

<ul>
  <li>If \(p_\phi(y \mid x)\) is a binary classifier <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code>, then output image \(x \sim p_\theta(x \mid y)\) can be either <code class="language-plaintext highlighter-rouge">hot dog</code> or <code class="language-plaintext highlighter-rouge">not hot dog</code> only, even \(p_\theta(x)\) was trained from a massive dataset with many more classes rather than just two classes.</li>
  <li>If you want to generate an image \(x\) from a complex prompt \(y\), you need a powerful model like CLIP as the conditional model \(p_\phi(y \mid x)\).</li>
</ul>

<p>To balance between the specificity (i.e., high \($p(y \mid x\))) and diversity/quality (i.e., \(p(x \mid y) \approx p(x)\)), we use a guidance scale \(\gamma\) to control the trade-off between the two.</p>

\[\nabla_x \log p(x \mid y) =  \nabla_x \log p(x) + \gamma \nabla_x \log p(y \mid x)\]

<p>where \(\gamma\) is the guidance scale. A big \(\gamma\) means the model is less creative but more following the condition \(y\).</p>

<h3 id="classifier-free-guidance">Classifier-free Guidance</h3>

<p>The main limitation of the above approach is that the quality of the generated image \(x\) strongly depends on the capability of the conditional model \(p(y \mid x)\).</p>

<p>If your model \(p(x)\) was trained on Image-Net dataset, but you want to generate an CT-scan medical image, then even with a powerful conditional model \(p(y \mid x)\), you will not get that.</p>

<p>The idea of classifier-free guidance cames from the <strong>Bayes Classifier</strong> - if you have trained a powerful unconditional generative model \(p(x)\) then you can use it as a classifier \(p(y \mid x)\) as follows:</p>

\[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\]

<h2 id="latent-diffusion">Latent Diffusion</h2>

<h2 id="conditional-diffusion">Conditional Diffusion</h2>

<h3 id="control-net">Control-Net</h3>

<h3 id="image-prompt">Image Prompt</h3>

<p>Beyond controlling the generation process using text prompt, there is a hot topic in the community to control using image information/layout/prompt - which has a huge potential in applications, e.g., image inpainting, image-to-image generation, etc. In the standard Stable Diffusion, the condition embedding \(c_t\) is just a text embedding \(c_t = E_t(y)\) where \(y\) is the text prompt and \(E_t\) is a pre-trained text encoder such as CLIP.
IP-Adapter [1] proposes to use an additional image encoder to extract the image embedding from a reference image \(c_i = E_i(x)\) and then project it into the original condition space.
The objective function for IP-Adapter is:</p>

\[\mathcal{L}_{IP} = \mathbb{E}_{z, c, \epsilon, t} \left[ \mid \mid \epsilon - \epsilon_\theta(z_t \mid c_i, c_t, t) \mid \mid_2^2 \right]\]

<p>The cross-attention layer is also modified from the one in Stable Diffusion to include the image embedding \(c_i\) as a condition.</p>

\[\text{Attention}(Q, K_i, K_t, V_i, V_t) = \lambda \text{softmax}\left(\frac{QK_i^T}{\sqrt{d}} + c_i\right)V_i + \text{softmax}\left(\frac{QK_t^T}{\sqrt{d}}\right)V_t\]

<p>where \(Q=z W_Q\), \(K_i = c_i W_K^i\), \(K_t = c_t W_K^t\), \(V_i = c_i W_V^i\), \(V_t = c_t W_V^t\), and \(W_Q\), \(W_K^i\), \(W_K^t\), \(W_V^i\), \(W_V^t\) are the weights of the linear layers.
The model becomes the original Stable Diffusion when \(\lambda = 0\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-03-20-07-02-10-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-03-20-07-02-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2308.06721" rel="external nofollow noopener" target="_blank">IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</a>
</li>
  <li>[2] <a href="https://openreview.net/pdf?id=PJqP0wyQek#page=1.93" rel="external nofollow noopener" target="_blank">MS-DIFFUSION: MULTI-SUBJECT ZERO-SHOT IMAGE PERSONALIZATION WITH LAYOUT GUIDANCE</a>
</li>
</ul>

<h2 id="diffusion-transformers">Diffusion Transformers</h2>

<p>The Diffusion Transformers (DiTs) is a class of diffusion models that replace the traditional U-Net convolutional architecture with a Vision Transformer (ViT) architecture as a backbone.</p>

<h3 id="data-processing-in-dit">Data Processing in DiT</h3>

<p>Similar to Latent Diffusion model, the diffusion process in DiT is on the latent space. Therefore, the first step is using pre-trained convolutional Variational Autoencoder (VAE) as in LDM to convert the spatial input into the latent space (i.e., \(256 \times 256 \times 3\) to \(32 \times 32 \times 4\)).</p>

<p><strong>Patchifying</strong> converting the(latent) spatial input into a sequence of \(T\) tokens/patches, each of dimension \(d\), by linearly embedding each patch in the input with a linear layer.</p>

<p><strong>Positional Encoding</strong> the standard sinusoidal positional embeddings are added to the token embeddings to provide the model with the positional information.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-06-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-01-15-06-00.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>Beside the visual tokens, the DiT also uses the <strong>conditional information</strong> such as timestep \(t\) and the textual prompt \(c\) associated with the input image. These information are added to the DiT block through a embedding layer.</p>

<h3 id="the-dit-architecture">The DiT Architecture</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion-foundation/2025-10-01-15-03-57-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/diffusion-foundation/2025-10-01-15-03-57.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>There are three types have been studied in the DiT paper including:</p>

<p><strong>In-Context Conditioning</strong> (The far right in the above figure) Append the vector embedding of \(t\) and \(c\) in the input sequence, <strong>treating them as additional visual tokens</strong>. This is similar to the <code class="language-plaintext highlighter-rouge">cls</code> tokens in ViT.</p>

<p><strong>Cross-Attention</strong> Concatenate the conditional embedding \(t\) and \(c\) into a length-two sequence, separate from the image token sequence. Then modify the cross-attention layer to inject these conditioning information into the visual path.</p>

<p><strong>Adaptive layer norm (adaLN) block</strong> Following the widespread success of Adaptive normalization layer in Diffusion with U-Net backbones, DiT also replaces the standard layer norm in transformer blocks with an adaptive layer norm. Rather than directly learn dimension-wise scaling and shift \(\gamma\) and \(\beta\), the adaLN block regresses them from the sum of the embedding of the conditioning information \(t\) and \(c\).</p>

<p><strong>adaLN-Zero block</strong> Prior work on ResNets has found that initializing each residual block as the identity function is beneficial. This version uses the same adaptive layer norm as the adaLN block but with zero initialization.</p>

<p><strong>Transformer Decoder</strong> After the final DiT block, we need to decode the sequence of image tokens into an output latent noise prediction and output diagonal covariance prediction (two outputs). This can be done by a standard linear layer with output dimension \(p \times p \times 2C\) where \(C\) is the number of channels of the image.</p>

<p>References:</p>

<ul>
  <li>DIT paper: <a href="https://arxiv.org/pdf/2212.09748" rel="external nofollow noopener" target="_blank">Scalable Diffusion Models with Transformers</a>
</li>
  <li>Official implementation: <a href="https://github.com/facebookresearch/DiT" rel="external nofollow noopener" target="_blank">https://github.com/facebookresearch/DiT</a>
</li>
</ul>

<h2 id="diffusion-flux">Diffusion Flux</h2>

<p>References:</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2507.09595v1" rel="external nofollow noopener" target="_blank">Demystifying Flux Architecture</a></li>
  <li>Flux official implementation: <a href="https://github.com/black-forest-labs/flux" rel="external nofollow noopener" target="_blank">https://github.com/black-forest-labs/flux</a>
</li>
</ul>

<h2 id="image-inpainting-with-diffusion-models">Image Inpainting with Diffusion Models</h2>

<h3 id="training-pipeline">Training Pipeline</h3>

<p><strong>Training data</strong> for inpainting is a combination of three components: <strong>original image</strong> as ground truth, <strong>masked image</strong> as input, and <strong>prompt</strong> as condition to provide the context of the missing region.</p>

<p>To ensure the model is robust, a variety of mask shapes and sizes can be used, including <strong>Rectangular</strong>, <strong>Free-form masks</strong>, and <strong>arbitrary shapes</strong></p>

<p><strong>Loss function</strong> for inpainting is a combination of <strong>pixel-wise reconstruction loss</strong> and <strong>perceptual loss</strong> (or <strong>Style loss</strong>). If using GANs, the <strong>adversarial loss</strong> is used to ensure the inpainted regions are perceptually realistic under the discriminator perspective.</p>

<h3 id="challenges-in-image-inpainting">Challenges in Image Inpainting</h3>

<p><strong>Semantic and Structural Consistency</strong>: A primary challenge for generative models is to fill in missing regions in a way that is not only visually plausible but also semantically and structurally consistent with the rest of the image.</p>

<p><strong>Semantic ambiguity</strong> means that the missing region can be filled in multiple ways, e.g., filling a gap in a street scene could be extending a road, adding a pedestrian, or a vehicle. Even when the input prompt is given, the task remains difficult, when <strong>concept leaking</strong> occurs, i.e., “a black cat on a white background” vs “a white cat on a black background”.</p>

<p><strong>Long-range dependency and global structure</strong>: is another significant hurdle. While generative models excel at local details, they can be struggling with the broader context, lighting, and perspective.</p>

<p><strong>Perceptual realism</strong>: is another key challenge. Even if the inpainted regions are visually consistent, they may not align with human perception. For example, an inpainting might produce unrealistic shadows or reflections or overly smooth, or having artificial artifacts.</p>

<p><strong>Large missing regions</strong>: The size of missing area is directly proportional to the difficulty of the task.</p>

<h2 id="accelerating-diffusion-models">Accelerating Diffusion Models</h2>

<h3 id="diffusion-distillation">Diffusion Distillation</h3>

<h3 id="rectified-diffusion">Rectified Diffusion</h3>

<p>References:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2209.03003" rel="external nofollow noopener" target="_blank">Flow straight and fast: Learning to generate and transfer data with rectified flow</a>
</li>
  <li>[2] <a href="https://arxiv.org/pdf/2403.03206" rel="external nofollow noopener" target="_blank">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a>
</li>
</ul>

<p>Rectified Flows define the forward process as straight paths between the data distribution and a standard normal distribution [2], i.e.,</p>

\[z_t = (1 - t) x_0 + t \epsilon\]

<p>where \(\epsilon\) is a standard normal random variable and \(t\) is the time step in [0, 1].</p>

<!-- mkdir -p assets/img/diffusion-foundation -->
<!-- mv _posts/2025-10-01-*.png assets/img/diffusion-foundation/ -->

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
