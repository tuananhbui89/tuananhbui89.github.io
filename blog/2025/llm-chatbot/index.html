<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>LLM Series - Part 3 - Build a Chatbot with Ollama | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="How to break into $100k+ salary roles - part 3">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/llm-chatbot/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">LLM Series - Part 3 - Build a Chatbot with Ollama</h1>
    <p class="post-meta">January 17, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          <a href="/blog/tag/coding">
          <i class="fas fa-hashtag fa-sm"></i> coding</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#background">Background</a>
<ul>
<li class="toc-entry toc-h3"><a href="#ollama">Ollama</a></li>
<li class="toc-entry toc-h3"><a href="#create-a-python-chatbot-with-ollama">Create a Python Chatbot with Ollama</a></li>
<li class="toc-entry toc-h3"><a href="#vllm">vLLM</a></li>
<li class="toc-entry toc-h3"><a href="#prompt-engineering">Prompt Engineering</a></li>
<li class="toc-entry toc-h3">
<a href="#role-playing">Role Playing</a>
<ul>
<li class="toc-entry toc-h4"><a href="#example-customer-support-classification-bot">Example: Customer Support Classification Bot</a></li>
<li class="toc-entry toc-h4"><a href="#openais-api-format">OpenAI’s API format</a></li>
<li class="toc-entry toc-h4"><a href="#useful-strategies">Useful strategies</a></li>
<li class="toc-entry toc-h4"><a href="#some-real-world-prompting-examples">Some real-world prompting examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <h2 id="background">Background</h2>

<h3 id="ollama">Ollama</h3>

<p><strong>References:</strong></p>

<ul>
  <li><a href="https://ollama.com/" rel="external nofollow noopener" target="_blank">Ollama</a></li>
  <li><a href="https://github.com/ollama/ollama" rel="external nofollow noopener" target="_blank">Ollama Github</a></li>
</ul>

<p><strong>What is Ollama?</strong></p>

<p>Ollama is an application designed to make running LLMs locally easy. It is a lightweight and fast alternative to large cloud-based LLMs.</p>

<p><strong>Advantages of Ollama:</strong></p>

<ul>
  <li>
<strong>Cross-platform</strong>: Ollama is available on Windows, Linux, and macOS.</li>
  <li>
<strong>Multiple LLMs</strong>: Ollama supports a wide range of LLMs, including Llama, GPT, and DeepSeek.</li>
</ul>

<p><strong>What can Ollama do?</strong></p>

<ul>
  <li>Run LLMs Locally
    <ul>
      <li>Supports various open-source LLMs (e.g., LLaMA, Mistral, Gemma, Phi-2).</li>
      <li>No need for an internet connection once the model is downloaded.</li>
      <li>Efficient memory management for running LLMs on laptops and desktops.</li>
    </ul>
  </li>
  <li>Easy Model Management
    <ul>
      <li>Install models with simple commands (<code class="language-plaintext highlighter-rouge">ollama pull &lt;model-name&gt;</code>)</li>
      <li>Supports custom model creation with fine-tuned weights and configurations</li>
    </ul>
  </li>
  <li>Flexible API for Developers
    <ul>
      <li>Provides a CLI (Command Line Interface) and a Python API.</li>
      <li>Can be integrated into applications for chatbots, text generation, and NLP tasks.</li>
    </ul>
  </li>
  <li>Prompt Engineering &amp; Fine-Tuning
    <ul>
      <li>Allows users to customize system prompts for better responses.</li>
      <li>Supports parameter tuning to control model behavior.</li>
    </ul>
  </li>
</ul>

<p><strong>Use Cases:</strong></p>

<ul>
  <li>Chatbots &amp; Assistants – Build local AI-powered assistants.</li>
  <li>Text Generation – Summarization, paraphrasing, creative writing.</li>
  <li>Code Generation – AI-assisted coding with models like CodeLLaMA.</li>
  <li>Privacy-Sensitive Applications – Run LLMs without sending data to the cloud.</li>
</ul>

<h3 id="create-a-python-chatbot-with-ollama">Create a Python Chatbot with Ollama</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">requests.exceptions</span> <span class="kn">import</span> <span class="nb">ConnectionError</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="k">def</span> <span class="nf">chat_with_ollama</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">mistral</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:11434/api/generate</span><span class="sh">"</span>  <span class="c1"># Changed back to default Ollama port
</span>    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span>
    <span class="p">}</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">response</span><span class="p">.</span><span class="n">ok</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Error: API returned status code </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span>

        <span class="c1"># Handle streaming response
</span>        <span class="n">full_response</span> <span class="o">=</span> <span class="sh">""</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">iter_lines</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">line</span><span class="p">:</span>
                <span class="c1"># Decode the line and parse it as JSON
</span>                <span class="n">json_response</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="k">if</span> <span class="sh">"</span><span class="s">response</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">json_response</span><span class="p">:</span>
                    <span class="n">full_response</span> <span class="o">+=</span> <span class="n">json_response</span><span class="p">[</span><span class="sh">"</span><span class="s">response</span><span class="sh">"</span><span class="p">]</span>
                
        <span class="k">return</span> <span class="n">full_response</span>
    <span class="k">except</span> <span class="nb">ConnectionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">Error: Cannot connect to Ollama server. Please ensure it</span><span class="sh">'</span><span class="s">s running on port 11434.</span><span class="sh">"</span>
    <span class="k">except</span> <span class="n">json</span><span class="p">.</span><span class="n">JSONDecodeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Error: Invalid JSON response from server. Details: </span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Error: </span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># Chat loop
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Chatbot (type </span><span class="sh">'</span><span class="s">exit</span><span class="sh">'</span><span class="s"> to quit):</span><span class="sh">"</span><span class="p">)</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">user_input</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">You: </span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">user_input</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="o">==</span> <span class="sh">"</span><span class="s">exit</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">response</span> <span class="o">=</span> <span class="nf">chat_with_ollama</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Bot: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>How it works:</strong></p>

<ul>
  <li>The script sends user input to Ollama’s local API.</li>
  <li>The model generates a response and returns it.</li>
  <li>The chatbot runs in a loop until the user types “exit”.</li>
</ul>

<p><strong>Running the Chatbot:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama pull mistral
ollama serve
python chatbot.py
</code></pre></div></div>

<p><strong>Useful Commands:</strong></p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">ollama pull mistral</code> - Pull the mistral model</li>
  <li>
<code class="language-plaintext highlighter-rouge">ollama serve</code> - Start the Ollama server</li>
  <li>
<code class="language-plaintext highlighter-rouge">ollama ps</code> - see what models are currently loaded into memory.</li>
  <li>
<code class="language-plaintext highlighter-rouge">ollama stop &lt;container_id&gt;</code> - Stop a container</li>
  <li>
<code class="language-plaintext highlighter-rouge">ollama rm &lt;container_id&gt;</code> - Remove a container</li>
  <li>
<code class="language-plaintext highlighter-rouge">ollama list</code> - List all models. This is useful because when you pull a model, e.g., <code class="language-plaintext highlighter-rouge">ollama pull mistral</code>, it eventually shows up as <code class="language-plaintext highlighter-rouge">mistral:latest</code>.</li>
</ul>

<p><strong>Issues:</strong></p>

<ul>
  <li>Error: listen tcp 127.0.0.1:11434: bind: address already in use
    <ul>
      <li>This means that the server is already running on the port.</li>
      <li>To fix this, you can either stop the server or run it on a different port.</li>
      <li>
<code class="language-plaintext highlighter-rouge">ollama stop &lt;container_id&gt;</code> - Stop a container</li>
      <li>
<code class="language-plaintext highlighter-rouge">OLLAMA_HOST=127.0.0.1:11500 ollama serve</code> - Run the server on a different port</li>
      <li>Setup environment variables on Linux: https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux</li>
    </ul>
  </li>
</ul>

<h3 id="vllm">vLLM</h3>

<p><strong>References:</strong></p>

<ul>
  <li><a href="https://vllm.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">vLLM</a></li>
  <li><a href="https://github.com/vllm-project/vllm" rel="external nofollow noopener" target="_blank">vLLM Github</a></li>
</ul>

<p><strong>What is vLLM?</strong></p>

<p>vLLM is a fast and easy-to-use library for LLM inference and serving.</p>

<p><strong>What can vLLM do?</strong></p>

<ul>
  <li>Seamless integration with popular HuggingFace models</li>
  <li>High-throughput serving with various decoding algorithms, including parallel sampling, beam search, and more</li>
  <li>Tensor parallelism and pipeline parallelism support for distributed inference</li>
  <li>Streaming outputs</li>
  <li>OpenAI-compatible API server</li>
  <li>Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs, Gaudi® accelerators and GPUs, PowerPC CPUs, TPU, and AWS Trainium and Inferentia Accelerators.</li>
  <li>Prefix caching support</li>
  <li>Multi-lora support</li>
</ul>

<h3 id="prompt-engineering">Prompt Engineering</h3>

<h3 id="role-playing">Role Playing</h3>

<p>LLMs can perform various roles depending on their context, training data, and prompting. The role can be specified in the system prompt.
For example, <strong>Mistral</strong> provides several useful scenarios to show their prompting capabilities as in the guide: <a href="https://docs.mistral.ai/guides/prompting_capabilities/" rel="external nofollow noopener" target="_blank">https://docs.mistral.ai/guides/prompting_capabilities/</a>.</p>

<h4 id="example-customer-support-classification-bot">Example: Customer Support Classification Bot</h4>

<p>Mistral models can easily categorize text into distinct classes. Take a customer support bot for a bank as an illustration: we can establish a series of predetermined categories within the prompt and then instruct Mistral AI models to categorize the customer’s question accordingly.</p>

<p>In the following example, when presented with the customer inquiry, Mistral AI models correctly categorizes it as “country support”:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">requests.exceptions</span> <span class="kn">import</span> <span class="nb">ConnectionError</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="k">def</span> <span class="nf">chat_with_ollama</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">system_prompt</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">mistral</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:11434/api/generate</span><span class="sh">"</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">:</span> <span class="n">system_prompt</span>
    <span class="p">}</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">response</span><span class="p">.</span><span class="n">ok</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Error: API returned status code </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span>

        <span class="c1"># Handle streaming response
</span>        <span class="n">full_response</span> <span class="o">=</span> <span class="sh">""</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">iter_lines</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">line</span><span class="p">:</span>
                <span class="c1"># Decode the line and parse it as JSON
</span>                <span class="n">json_response</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="k">if</span> <span class="sh">"</span><span class="s">response</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">json_response</span><span class="p">:</span>
                    <span class="n">full_response</span> <span class="o">+=</span> <span class="n">json_response</span><span class="p">[</span><span class="sh">"</span><span class="s">response</span><span class="sh">"</span><span class="p">]</span>
                
        <span class="k">return</span> <span class="n">full_response</span>
    <span class="k">except</span> <span class="nb">ConnectionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">Error: Cannot connect to Ollama server. Please ensure it</span><span class="sh">'</span><span class="s">s running on port 11434.</span><span class="sh">"</span>
    <span class="k">except</span> <span class="n">json</span><span class="p">.</span><span class="n">JSONDecodeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Error: Invalid JSON response from server. Details: </span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Error: </span><span class="si">{</span><span class="nf">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># Initialize system prompt
</span><span class="n">system_prompt</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are a bank customer service bot. Your task is to assess customer intent and categorize customer inquiry after &lt;&lt;&lt;&gt;&gt;&gt; into one of the following predefined categories:

card arrival
change pin
exchange rate
country support
cancel transfer
charge dispute

If the text doesn</span><span class="sh">'</span><span class="s">t fit into any of the above categories, classify it as:
customer service

You will only respond with the category. Do not include the word </span><span class="sh">"</span><span class="s">Category</span><span class="sh">"</span><span class="s">. Do not provide explanations or notes.</span><span class="sh">"""</span>

<span class="c1"># Modified chat loop
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Chatbot (type </span><span class="sh">'</span><span class="s">exit</span><span class="sh">'</span><span class="s"> to quit):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">To change system prompt, type </span><span class="sh">'</span><span class="s">change_prompt</span><span class="sh">'"</span><span class="p">)</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">user_input</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">You: </span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">user_input</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="o">==</span> <span class="sh">"</span><span class="s">exit</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">elif</span> <span class="n">user_input</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="o">==</span> <span class="sh">"</span><span class="s">change_prompt</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">system_prompt</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">Enter new system prompt: </span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">System prompt updated!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">continue</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="nf">chat_with_ollama</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Bot: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="openais-api-format">OpenAI’s API format</h4>

<p>When sending requests to OpenAI’s API, we can specify the format of the response in the <code class="language-plaintext highlighter-rouge">data</code> playload parameter like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"model"</span><span class="p">:</span><span class="w"> </span><span class="s2">"gpt-4"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"messages"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"role"</span><span class="p">:</span><span class="w"> </span><span class="s2">"system"</span><span class="p">,</span><span class="w"> </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"You are a helpful assistant."</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"role"</span><span class="p">:</span><span class="w"> </span><span class="s2">"user"</span><span class="p">,</span><span class="w"> </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"What is the capital of France?"</span><span class="p">}</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nl">"temperature"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span><span class="w">
  </span><span class="nl">"max_tokens"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w">
  </span><span class="nl">"top_p"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"n"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"stream"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Where the parameters are:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>“model”</td>
      <td><code class="language-plaintext highlighter-rouge">string</code></td>
      <td>The model to use (“gpt-4”, “gpt-3.5-turbo”, etc.)</td>
    </tr>
    <tr>
      <td>“messages”</td>
      <td><code class="language-plaintext highlighter-rouge">list</code></td>
      <td>List of messages forming the conversation history</td>
    </tr>
    <tr>
      <td>“role”</td>
      <td><code class="language-plaintext highlighter-rouge">string</code></td>
      <td>Role of each message: “system”, “user”, “assistant”</td>
    </tr>
    <tr>
      <td>“content”</td>
      <td><code class="language-plaintext highlighter-rouge">string</code></td>
      <td>The actual text content of the message</td>
    </tr>
    <tr>
      <td>“temperature”</td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>Controls randomness (0 = deterministic, 1 = highly random)</td>
    </tr>
    <tr>
      <td>“max_tokens”</td>
      <td><code class="language-plaintext highlighter-rouge">int</code></td>
      <td>The max number of tokens the response can have</td>
    </tr>
    <tr>
      <td>“top_p”</td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>Probability mass for nucleus sampling (alternative to temperature)</td>
    </tr>
    <tr>
      <td>“n”</td>
      <td><code class="language-plaintext highlighter-rouge">int</code></td>
      <td>Number of responses to generate</td>
    </tr>
    <tr>
      <td>“stream”</td>
      <td><code class="language-plaintext highlighter-rouge">bool</code></td>
      <td>If true, streams back tokens as they are generated</td>
    </tr>
  </tbody>
</table>

<p><strong>Multi-turn conversations</strong> to help the model understand the context of the conversation:</p>

<ul>
  <li>The conversation history helps maintain context</li>
</ul>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"model"</span><span class="p">:</span><span class="w"> </span><span class="s2">"gpt-4"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"messages"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"role"</span><span class="p">:</span><span class="w"> </span><span class="s2">"system"</span><span class="p">,</span><span class="w"> </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"You are an AI that provides programming advice."</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"role"</span><span class="p">:</span><span class="w"> </span><span class="s2">"user"</span><span class="p">,</span><span class="w"> </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"How do I write a Python function?"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"role"</span><span class="p">:</span><span class="w"> </span><span class="s2">"assistant"</span><span class="p">,</span><span class="w"> </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"You can define a function using the `def` keyword."</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"role"</span><span class="p">:</span><span class="w"> </span><span class="s2">"user"</span><span class="p">,</span><span class="w"> </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Can you give me an example?"</span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h4 id="useful-strategies">Useful strategies</h4>

<ul>
  <li>
<strong>Few shot learning</strong>: Few-shot learning or in-context learning is when we give a few examples in the prompts, and the LLM can generate corresponding output based on the example demonstrations.</li>
  <li>
<strong>Step-by-step instructions</strong>: This strategy is inspired by the <strong>chain-of-thought</strong> prompting that enables LLMs to use a series of intermediate reasoning steps to tackle complex tasks. It’s often easier to solve complex problems when we decompose them into simpler and small steps and it’s easier for us to debug and inspect the model behavior.</li>
  <li>
<strong>Output formatting</strong>: We can ask LLMs to output in a certain format by directly asking “write a report in the Markdown format”.</li>
  <li>
<strong>Example generation</strong>: We can ask LLMs to automatically guide the reasoning and understanding process by generating examples with the explanations and steps.</li>
</ul>

<h4 id="some-real-world-prompting-examples">Some real-world prompting examples</h4>

<ul>
  <li>
<a href="https://github.com/SalesforceAIResearch/CodeChain/tree/main" rel="external nofollow noopener" target="_blank">Codechain by Salesforce</a> at <a href="https://github.com/SalesforceAIResearch/CodeChain/blob/main/prompts/codechain_gen.txt" rel="external nofollow noopener" target="_blank">https://github.com/SalesforceAIResearch/CodeChain/blob/main/prompts/codechain_gen.txt</a>
</li>
</ul>

<p>And the above prompt file is used in this file <a href="https://github.com/SalesforceAIResearch/CodeChain/blob/main/src/generate.py" rel="external nofollow noopener" target="_blank">https://github.com/SalesforceAIResearch/CodeChain/blob/main/src/generate.py</a>.</p>

<p>The flow of the code as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Load the prompt file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">prompt_file</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">infile</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="c1"># replace the placeholders in the prompt with the actual values
</span>
<span class="n">curr_prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;&lt;problem&gt;&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">question</span><span class="p">)</span>  

<span class="k">if</span> <span class="sh">'</span><span class="s">&lt;&lt;starter_code&gt;&gt;</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">:</span>
    <span class="n">starter_code</span> <span class="o">=</span> <span class="n">problem</span><span class="p">[</span><span class="sh">'</span><span class="s">starter_code</span><span class="sh">'</span><span class="p">]</span> 
    <span class="n">curr_prompt</span> <span class="o">=</span> <span class="n">curr_prompt</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;&lt;starter_code&gt;&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">starter_code</span><span class="p">)</span>
    
<span class="k">if</span> <span class="sh">'</span><span class="s">&lt;&lt;starter_code_task&gt;&gt;</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">:</span>
    <span class="n">starter_code</span> <span class="o">=</span> <span class="n">problem</span><span class="p">[</span><span class="sh">'</span><span class="s">starter_code</span><span class="sh">'</span><span class="p">]</span> 
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">starter_code</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">starter_code_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Notes:</span><span class="se">\n</span><span class="s">The final python function should begin with: </span><span class="se">\n</span><span class="s">```python</span><span class="se">\n</span><span class="si">{</span><span class="n">starter_code</span><span class="si">}</span><span class="se">\n</span><span class="s">```</span><span class="sh">"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">starter_code_prompt</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="n">curr_prompt</span> <span class="o">=</span> <span class="n">curr_prompt</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;&lt;starter_code_task&gt;&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">starter_code_prompt</span><span class="p">)</span>

<span class="k">if</span> <span class="sh">'</span><span class="s">&lt;&lt;question_guide&gt;&gt;</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">:</span>
    <span class="n">starter_code</span> <span class="o">=</span> <span class="n">problem</span><span class="p">[</span><span class="sh">'</span><span class="s">starter_code</span><span class="sh">'</span><span class="p">]</span> 
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">starter_code</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">question_guide</span> <span class="o">=</span> <span class="sh">'</span><span class="s">use the provided function signature</span><span class="sh">'</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">question_guide</span> <span class="o">=</span> <span class="sh">'</span><span class="s">read from and write to standard IO</span><span class="sh">'</span>
    <span class="n">curr_prompt</span> <span class="o">=</span> <span class="n">curr_prompt</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;&lt;question_guide&gt;&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">question_guide</span><span class="p">)</span>    

<span class="k">if</span> <span class="sh">'</span><span class="s">&lt;&lt;modules&gt;&gt;</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">curr_prompt</span><span class="p">:</span> 
    <span class="k">if</span> <span class="n">problem_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span> <span class="k">continue</span> 
    <span class="n">curr_modules</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">problem_id</span><span class="p">])</span>
    <span class="n">module_seq</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">curr_modules</span><span class="p">:</span> 
        <span class="n">module_seq</span> <span class="o">+=</span> <span class="sh">"</span><span class="se">\n</span><span class="s">```module</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="n">module</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="s">```</span><span class="se">\n</span><span class="sh">"</span>
    <span class="n">curr_prompt</span> <span class="o">=</span> <span class="n">curr_prompt</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">&lt;&lt;modules&gt;&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="n">module_seq</span><span class="p">)</span>

<span class="c1"># Call the API
</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
                  <span class="n">model</span><span class="o">=</span><span class="n">model_mapping</span><span class="p">[</span><span class="n">args</span><span class="p">.</span><span class="n">model</span><span class="p">],</span> 
                  <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> 
                         <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful AI assistant to help developers to solve challenging coding problems.</span><span class="sh">"</span><span class="p">},</span>
                        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> 
                         <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">curr_prompt</span><span class="p">}</span>
                    <span class="p">],</span>
                  <span class="n">n</span><span class="o">=</span><span class="mi">5</span> <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">num_gen_samples</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>

</code></pre></div></div>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llm-implementations/">LLM Series - Part 2 - Common Implementations in LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llm-jailbreak/">LLM Series - Part 4 - How to Jailbreak LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ml-foundation/">Foundations of Machine Learning</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
