<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Kolmogorov-Arnold Network (KAN) | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/KAN/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Kolmogorov-Arnold Network (KAN)</h1>
    <p class="post-meta">February 21, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#mlp-vs-kan">MLP vs KAN</a>
<ul>
<li class="toc-entry toc-h3"><a href="#kan-or-mlp-a-fairer-comparison">KAN or MLP: A Fairer Comparison</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#kan">KAN</a>
<ul>
<li class="toc-entry toc-h3">
<a href="#implementation-of-kan">Implementation of KAN</a>
<ul>
<li class="toc-entry toc-h4"><a href="#residual-activation-function">Residual Activation Function</a></li>
<li class="toc-entry toc-h4"><a href="#b-spline">B-spline</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#philosophical-thoughts-on-kan-by-kans-author">Philosophical thoughts on KAN by Kan’s author</a></li>
<li class="toc-entry toc-h3"><a href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
</li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <p>Resources:</p>

<ul>
  <li>[1] <a href="https://arxiv.org/abs/2404.19756" rel="external nofollow noopener" target="_blank">KAN Paper</a>
</li>
  <li>[2] <a href="https://github.com/KindXiaoming/pykan" rel="external nofollow noopener" target="_blank">KAN Github</a>
</li>
  <li>[3] <a href="https://github.com/mintisan/awesome-kan" rel="external nofollow noopener" target="_blank">Awesome KAN(Kolmogorov-Arnold Network)</a>
</li>
  <li>[4] <a href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/" rel="external nofollow noopener" target="_blank">Philosophical thoughts on Kolmogorov-Arnold Networks by Ziming Liu</a>
</li>
  <li>[5] <a href="https://www.digitalocean.com/community/tutorials/kolmogorov-arnold-networks-kan-revolutionizing-deep-learning" rel="external nofollow noopener" target="_blank">Kolmogorov-Arnold Networks (KAN) Promising Alternative to Multi-Layer Perceptron? by DigitalOcean</a>
</li>
  <li>[6] <a href="https://arxiv.org/pdf/2407.16674" rel="external nofollow noopener" target="_blank">KAN or MLP: A Fairer Comparison</a>
</li>
</ul>

<h2 id="mlp-vs-kan">MLP vs KAN</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-21-16-23-16-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-21-16-23-16.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison between MLP and KAN.
</div>

<p><strong>Limitations of MLP</strong>:</p>

<ul>
  <li>
<strong>Interpretability</strong>: MLPs are often considered “black boxes” due to their complex internal workings, making it difficult to understand how they arrive at their predictions.</li>
  <li>
<strong>Curse of Dimensionality</strong>: MLPs can struggle with high-dimensional data, as the number of parameters required to capture complex relationships grows exponentially with the input dimension.</li>
  <li>
<strong>Local Optimization</strong>: MLPs rely on gradient-based optimization algorithms, which can get stuck in local minima, potentially leading to suboptimal solutions.</li>
  <li>
<strong>Catastrophic Forgetting</strong>: MLPs can be prone to catastrophic forgetting, where learning new information can overwrite previously learned knowledge, hindering their ability to perform continual learning.</li>
</ul>

<p><strong>Advantages of KAN over MLP</strong>:</p>

<ul>
  <li>
<strong>Interpretability</strong>: KANs are more interpretable than MLPs due to their structure and the use of learnable activation functions. The absence of linear weight matrices and the explicit representation of univariate functions make it easier to understand how KANs arrive at their predictions.</li>
  <li>
<strong>Neural Scaling Laws</strong>: KANs exhibit faster neural scaling laws than MLPs, meaning that their performance improves more rapidly with increasing model size. This faster scaling can lead to significant gains in accuracy by simply scaling up the model.</li>
  <li>
<strong>Continual Learning</strong>: KANs can naturally perform continual learning without catastrophic forgetting, unlike MLPs. This ability stems from the locality of spline basis functions, which allows KANs to update knowledge in specific regions without affecting previously learned information.</li>
</ul>

<p><strong>Limitations of KAN</strong>:</p>

<ul>
  <li>
<strong>Computational Efficiency</strong>: KANs can be computationally more expensive to train than MLPs due to the complexity of learning and evaluating spline functions. The current implementation of this spline function can be found <a href="https://github.com/KindXiaoming/pykan/blob/ecde4ec3274d3bef1ad737479cf126aed38ab530/kan/spline.py#L4" rel="external nofollow noopener" target="_blank">here</a>, which requires recursive computation of a higher-order spline from lower-order splines. This process does not leverage the parallelization of modern GPUs.</li>
  <li>
<strong>Theoretical Limitations</strong>: The Kolmogorov-Arnold Representation Theorem (KAT) primarily applies to single layer KANs, and therefore the multi-layer KANs are not guaranteed to be able to represent any continuous function. For example, the input of the activation function should be bounded, which is not trivial for multi-layer KANs.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-07-46-07-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-07-46-07.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Should we use KAN or MLP? Image from [1].
</div>

<h3 id="kan-or-mlp-a-fairer-comparison">KAN or MLP: A Fairer Comparison</h3>

<p>Paper [6] provides a <strong>fairer</strong> comparison between KAN and MLP by considering the same number of parameters and FLOPs to make sure that the computational complexity is the same.
The tasks for comparison are also more comprehensive, including tasks in ML, CV, NLP and symbolic formula representation.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-09-20-43-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-09-20-43.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Other comparison between KAN and MLP from a fairer perspective/setting.
</div>

<p>The key findings are follows, which somewhat contradict to the observation in the original KAN paper [1].</p>

<ul>
  <li>
<strong>Symbolic Formula Representation</strong>: KANs outperform MLPs when approximating symbolic formulas.</li>
  <li>
<strong>Other Tasks</strong>: MLPs generally outperform KANs on other tasks, including machine learning, computer vision, natural language processing, and audio processing.</li>
  <li>
<strong>Impact of B-spline Activation</strong>: KANs’ advantage in symbolic formula representation comes from their use of B-spline activation functions.  When MLPs use B-spline activation functions, their performance on symbolic formula representation matches or exceeds that of KANs.  However, B-spline activation functions do not significantly improve MLPs’ performance on other tasks.</li>
  <li>
<strong>Continual Learning</strong>: KANs do not outperform MLPs in continual learning tasks. In a standard class-incremental continual learning setting, KANs forget old tasks more quickly than MLPs.</li>
</ul>

<h2 id="kan">KAN</h2>

<!-- The **Universal Approximation Theorem** (UAT) states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary accuracy.
This is the foundation theorem of the Multi-Layer Perceptron (MLP). More specifically, the multivariate continuous function $$f: [0,1]^n \rightarrow \mathbb{R}$$ can be represented as follows in MLP:

$$
f(x) = f(x_1, x_2, \cdots, x_n) = \sum_{i=1}^{m} \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j + b_i \right)
$$

where $$\sigma$$ is the non-linear activation function, $$w_{i,j}$$ is the weight, and $$b_i$$ is the bias. -->

<p>Before we dive into the KAN, let’s first understand the two definitions <strong>“edge”</strong> and <strong>“node”</strong> in MLP and KAN.
Given a MLP with \(n\) input nodes and \(m\) output nodes, the MLP can be represented as a directed acyclic graph (DAG) as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-06-46-10-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-06-46-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    MLP layer
</div>

<p>Mathematically, the node \(y_i\) of the output (hidden) layer can be represented as \(y_i = \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j\right)\) where \(x_j\) is the input node, \(w_{i,j}\) is the weight. We ignore the bias term for simplicity.
The connection between the input <strong>nodes</strong> \(x_j\) and the output <strong>node</strong> \(y_i\) is called an <strong>edge</strong>, which is scaled by the <strong>learnable weight</strong> \(w_{i,j}\).
After applying the sum operation over all the edges, the output node \(y_i = \sigma \left( \sum_{j=1}^{n} w_{i,j} x_j \right)\) is obtained by applying the non-linear activation function \(\sigma\) on the weighted sum.
Note that the activation function \(\sigma\) is pointwise applied and not learnable.</p>

<p>For the Kolmogorov-Arnold Network (KAN), it is based on the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem" rel="external nofollow noopener" target="_blank">Kolmogorov-Arnold Representation Theorem (KAT)</a>.
KAT states that any continuous function can be represented as a sum of a trigonometric polynomial and a spline function.
More specifically, the multivariate continuous function \(f: [0,1]^n \rightarrow \mathbb{R}\) can be represented as:</p>

\[f(x) = f(x_1, x_2, \cdots, x_n) = \sum_{q=0}^{2n+1} \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right)\]

<p>where \(\phi_{q,}:[0,1] \rightarrow \mathbb{R}\) are the learnable activation functions over <strong>edges</strong>, and the \(\Phi_q\) is the learnable activation function over output <strong>nodes</strong>.</p>

<p>In KAN, the <strong>edge</strong> connection between the input <strong>nodes</strong> \(x_p\) and the output <strong>node</strong> \(y_q\) is applied by the <strong>learnable activation function</strong> \(\phi_{q,p}\).
After applying the sum operation over all the edges, the output node \(y_q = \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right)\) is obtained by applying another learnable activation function \(\Phi_q\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-22-07-00-27-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-22-07-00-27.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    KAN layer
</div>

<p>So compared to MLP, while the process from input nodes to output nodes is quite similar (one output node connected to all the input nodes), and the activation function on edges <strong>\(\phi_{q,p}\)</strong> is also parameterized similar to \(w_{i,j}\) in MLP, the main difference lies in the activation function on output nodes <strong>\(\Phi_q\)</strong> that is learnable in KAN.</p>

<h3 id="implementation-of-kan">Implementation of KAN</h3>

<h4 id="residual-activation-function">Residual Activation Function</h4>

<p>Beside the spline function, the activation function also includes a basis function \(b(x)\) which gets the signal directly from the input nodes (without going through any weight matrix).</p>

\[\phi(x) = w_b b(x) + w_s \text{spline}(x)\]

<p>where \(w_b\) and \(w_s\) are the learnable weights. the basis function \(b(x) = \text{silu}(x) = x / (1 + e^{-x})\).</p>

<p>The most complicated part is the spline function, which is parameterized as a linear combination of <strong>B-splines</strong> such as:</p>

\[\text{spline}(x) = \sum_{i=1} c_i B_i(x)\]

<p>where \(B_i(x)\) is the \(i\)-th B-spline and \(c_i\) is the learnablecoefficient.</p>

<h4 id="b-spline">B-spline</h4>

<p>B-splines are essentially curves made up of polynomial segments, each with a specified level of smoothness. Picture each segment as a small curve, where multiple control points influence the shape. Unlike simpler spline curves, which rely on only two control points per segment, B-splines use more, leading to smoother and more adaptable curves.</p>

<p>The magic of B-splines lies in their local impact. Adjusting one control point affects only the nearby section of the curve, leaving the rest undisturbed. This property offers remarkable advantages, especially in maintaining smoothness and facilitating differentiability, which is crucial for effective backpropagation during training (From [4]).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/05/Screenshot-2024-05-14-at-8.19.58-PM.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    B-spline. Image from DigitalOcean [4].
</div>

<p>Mathematically, B-splines can be constructed by means of the Cox-de Boor recursion formula (<a href="https://en.wikipedia.org/wiki/B-spline#Definition" rel="external nofollow noopener" target="_blank">Wikipedia</a>), starting with the B-spline basis function of order 0. We start with the B-splines of degree \(p = 0\), i.e. piecewise constant polynomials:</p>

\[B_{i,0}(t) := \begin{cases}
1 &amp; \text{if } t_i \leq t &lt; t_{i+1}, \\
0 &amp; \text{otherwise.}
\end{cases}\]

<p>The higher \((p + 1)\)-degree B-splines are defined by recursion:</p>

\[B_{i,p}(t) := \frac{t - t_i}{t_{i+p} - t_i} B_{i,p-1}(t) + \frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} B_{i+1,p-1}(t).\]

<p>The implementation of the B-spline can be found <a href="https://github.com/KindXiaoming/pykan/blob/ecde4ec3274d3bef1ad737479cf126aed38ab530/kan/spline.py#L4" rel="external nofollow noopener" target="_blank">here</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KAN/2025-02-21-17-05-47-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/KAN/2025-02-21-17-05-47.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Implementation of B-spline.
</div>

<p><strong>Computational Expensiveness</strong>: Because of the recursive computation of the B-spline, the computational complexity is much higher than that of MLP.</p>

<p><strong>Grid Extension</strong></p>

<p>The grid extension in KAN is the process of refining the spline function by adding more knots, so that the spline function can have a higher resolution, fit the data better. 
It can be done by using higher-order B-splines, which is calculated by the lower-order B-splines (therefore, it is called extension).</p>

<h3 id="philosophical-thoughts-on-kan-by-kans-author">Philosophical thoughts on KAN by Kan’s author</h3>

<p>I found the philosophical thoughts on KAN by the author <a href="https://kindxiaoming.github.io/blog/2024/kolmogorov-arnold-networks/" rel="external nofollow noopener" target="_blank">here</a> very interesting and helpful to understand the KAN and its difference with MLP.
I just quote the part that I think is most relevant to the KAN here.</p>

<blockquote>
  <p><strong>Reductionism vs. Holism</strong> While MLPs are more aligned with holism, KANs are more aligned with reductionism. The design principle of MLPs is “more is different”. <strong>In an MLP, each neuron is simple</strong> because it has fixed activation functions. However, <strong>what matters is the complicated connection patterns among neurons</strong>. The magical power of MLPs performing a task is an emergent behavior which is attributed to collective contribution from all neurons. By contrast, <strong>in a KAN, each activation function is complicated</strong> because it has learnable functions. By sparsification and pruning, we hope the computation graph to be simple. In summary, MLPs have simple ‘atoms’ but complicated ways to combine these atoms; KANs have complicated (diverse) ‘atoms’ but simple ways to combine these atoms. In this sense, MLPs’ expressive power comes from the complicated connection patterns (fully-connected structure), which give rise to emergent bahavior (holism). In contrast, KANs’ expressive power comes from the complexity of fundamental units (learnable activation functions), but the way to decompose whole network to units is simple (reductionsim)</p>
</blockquote>

<h3 id="attention-mechanism">Attention Mechanism</h3>

<p>Because of the spline function \(\text{spline}(x)\), which is a linear combination of B-splines with different level of smoothness/resolution of the input \(x\), each resolution is weighted by the learnable coefficient \(c_i\), 
this mechanism can be regarded as a <strong>soft self attention</strong> mechanism, where the output attends to different parts of the input with different resolutions.</p>

<!-- img_path: /assets/img/KAN/ -->
<!-- mkdir -p ../assets/img/KAN/ -->
<!-- mv 2025-02-21-*.png ../assets/img/KAN/ -->

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">Foundation of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
