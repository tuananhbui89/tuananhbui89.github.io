<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>LLM Series - Part 1 - Important Concepts in NLP | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="How to break into $100k+ salary roles">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh¬†</span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">LLM Series - Part 1 - Important Concepts in NLP</h1>
    <p class="post-meta">January 15, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
      ¬† ¬∑ ¬†
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a> ¬†
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a> ¬†
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a> ¬†
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a> ¬†
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#nlp-foundations">NLP Foundations</a>
<ul>
<li class="toc-entry toc-h3"><a href="#corpus">Corpus</a></li>
<li class="toc-entry toc-h3"><a href="#common-pre-processing-techniques">Common Pre-processing Techniques</a></li>
<li class="toc-entry toc-h3"><a href="#word-meaning-and-word-sense-disambiguation">Word Meaning and Word Sense Disambiguation</a></li>
<li class="toc-entry toc-h3"><a href="#word-embeddings">Word Embeddings</a></li>
<li class="toc-entry toc-h3"><a href="#how-to-represent-sentences">How to represent sentences</a></li>
<li class="toc-entry toc-h3"><a href="#how-to-represent-documents">How to represent documents</a></li>
<li class="toc-entry toc-h3"><a href="#data-augmentation">Data Augmentation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#fundamental-tasks-in-nlp">Fundamental Tasks in NLP</a>
<ul>
<li class="toc-entry toc-h3">
<a href="#low-level-tasks">Low-level Tasks</a>
<ul>
<li class="toc-entry toc-h4"><a href="#tokenization">Tokenization</a></li>
<li class="toc-entry toc-h4"><a href="#part-of-speech-tagging">Part-of-Speech Tagging</a></li>
<li class="toc-entry toc-h4"><a href="#named-entity-recognition">Named Entity Recognition</a></li>
<li class="toc-entry toc-h4"><a href="#syntactic-parsing">Syntactic Parsing</a></li>
<li class="toc-entry toc-h4"><a href="#semantic-role-labeling">Semantic Role Labeling</a></li>
<li class="toc-entry toc-h4"><a href="#coreference-resolution">Coreference Resolution</a></li>
</ul>
</li>
<li class="toc-entry toc-h3">
<a href="#high-levelapplication-tasks">High-level/Application Tasks</a>
<ul>
<li class="toc-entry toc-h4"><a href="#sentiment-analysis">Sentiment Analysis</a></li>
<li class="toc-entry toc-h4"><a href="#machine-translation">Machine Translation</a></li>
<li class="toc-entry toc-h4"><a href="#text-summarization">Text Summarization</a></li>
<li class="toc-entry toc-h4"><a href="#question-answering">Question Answering</a></li>
<li class="toc-entry toc-h4"><a href="#text-generation">Text Generation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#evaluation">Evaluation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#perplexity">Perplexity</a></li>
<li class="toc-entry toc-h3">
<a href="#bleu-score">BLEU score</a>
<ul>
<li class="toc-entry toc-h4">
<a href="#calculation-of-bleu-score">Calculation of BLEU Score</a>
<ul>
<li class="toc-entry toc-h5"><a href="#n-gram-precision">N-gram Precision</a></li>
<li class="toc-entry toc-h5"><a href="#clipped-precision">Clipped Precision</a></li>
<li class="toc-entry toc-h5"><a href="#brevity-penalty-bp">Brevity Penalty (BP)</a></li>
<li class="toc-entry toc-h5"><a href="#final-bleu-score">Final BLEU Score</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#strengths-of-bleu-score">Strengths of BLEU Score</a></li>
<li class="toc-entry toc-h4"><a href="#weaknesses-of-bleu-score">Weaknesses of BLEU Score</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#wer-word-error-rate">WER (Word Error Rate)</a></li>
<li class="toc-entry toc-h3"><a href="#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</a></li>
<li class="toc-entry toc-h3">
<a href="#meteor-metric-for-evaluation-of-translation-with-explicit-ordering">METEOR (Metric for Evaluation of Translation with Explicit Ordering)</a>
<ul>
<li class="toc-entry toc-h4"><a href="#formula">Formula</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#levenshtein-distance-edit-distance">Levenshtein distance (Edit Distance)</a></li>
<li class="toc-entry toc-h3">
<a href="#g-eval">G-EVAL</a>
<ul>
<li class="toc-entry toc-h4"><a href="#how-g-eval-works">How G-Eval works</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#tokenization-1">Tokenization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#out-of-vocabulary-oov">Out-of-Vocabulary (OOV)</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#transformer">Transformer</a></li>
<li class="toc-entry toc-h2"><a href="#kv-caching">KV Caching</a></li>
<li class="toc-entry toc-h2">
<a href="#bert">BERT</a>
<ul>
<li class="toc-entry toc-h3"><a href="#tokenization-in-bert">Tokenization in BERT</a></li>
<li class="toc-entry toc-h3"><a href="#masked-prediction-in-bert">Masked Prediction in BERT</a></li>
<li class="toc-entry toc-h3"><a href="#pre-training-bert">Pre-training BERT</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#how-to-train-a-llm">How to train a LLM</a>
<ul>
<li class="toc-entry toc-h3"><a href="#unsupervised-pre-training">Unsupervised Pre-training</a></li>
<li class="toc-entry toc-h3"><a href="#supervised-fine-tuning-sft">Supervised Fine-tuning (SFT)</a></li>
<li class="toc-entry toc-h3"><a href="#reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toc-entry toc-h3">
<a href="#ppo-and-dpo">PPO and DPO</a>
<ul>
<li class="toc-entry toc-h4"><a href="#proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</a></li>
<li class="toc-entry toc-h4"><a href="#direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</a></li>
</ul>
</li>
<li class="toc-entry toc-h3">
<a href="#sft-vs-rlhf">SFT vs RLHF</a>
<ul>
<li class="toc-entry toc-h4"><a href="#when-is-rlhf-needed">When is RLHF needed?</a></li>
<li class="toc-entry toc-h4"><a href="#why-is-supervised-fine-tuning-not-enough">Why is Supervised Fine-Tuning Not Enough?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#lora-and-adapters">LoRA and Adapters</a>
<ul>
<li class="toc-entry toc-h3"><a href="#lora">LoRA</a></li>
<li class="toc-entry toc-h3"><a href="#adapters">Adapters</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#prompt-engineering">Prompt Engineering</a>
<ul>
<li class="toc-entry toc-h3"><a href="#zero-shot-and-few-shot-learning">Zero-shot and Few-shot Learning</a></li>
<li class="toc-entry toc-h3"><a href="#chain-of-thought">Chain of Thought</a></li>
<li class="toc-entry toc-h3"><a href="#meta-prompting">Meta Prompting</a></li>
<li class="toc-entry toc-h3"><a href="#self-consistency">Self-Consistency</a></li>
<li class="toc-entry toc-h3"><a href="#generated-knowledge-prompting">Generated Knowledge Prompting</a></li>
<li class="toc-entry toc-h3"><a href="#prompt-chaining">Prompt Chaining</a></li>
<li class="toc-entry toc-h3"><a href="#tree-of-thoughts">Tree of Thoughts</a></li>
<li class="toc-entry toc-h3"><a href="#mixture-of-reasoning-experts">Mixture of Reasoning Experts</a></li>
<li class="toc-entry toc-h3"><a href="#how-to-create-a-prompt">How to create a prompt</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#code-and-frameworks-for-llms">Code and Frameworks for LLMs</a>
<ul>
<li class="toc-entry toc-h3">
<a href="#langchain">LangChain</a>
<ul>
<li class="toc-entry toc-h4"><a href="#prompt-templates">Prompt Templates</a></li>
<li class="toc-entry toc-h4"><a href="#chains">Chains</a></li>
<li class="toc-entry toc-h4"><a href="#memory">Memory</a></li>
<li class="toc-entry toc-h4"><a href="#agents">Agents</a></li>
<li class="toc-entry toc-h4"><a href="#tools">Tools</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#llamaindex">LlamaIndex</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#fine-tuning-llms">Fine-tuning LLMs</a></li>
<li class="toc-entry toc-h2"><a href="#rag">RAG</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <h2 id="nlp-foundations">NLP Foundations</h2>

<p>References:</p>

<ul>
  <li><a href="https://www.geeksforgeeks.org/nlp-interview-questions/" rel="external nofollow noopener" target="_blank">https://www.geeksforgeeks.org/nlp-interview-questions/</a></li>
  <li>Stanford CS224N: Natural Language Processing with Deep Learning - Schedule - <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/index.html" rel="external nofollow noopener" target="_blank">link</a> - <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/readings/cs224n_winter2023_lecture1_notes_draft.pdf" rel="external nofollow noopener" target="_blank">Lecture 1</a>
</li>
  <li>63 Must-Know LLMs Interview Questions - Github Repo - <a href="https://github.com/Devinterview-io/llms-interview-questions" rel="external nofollow noopener" target="_blank">link</a>
</li>
</ul>

<h3 id="corpus">Corpus</h3>

<p>In NLP, a corpus is a huge collection of texts or documents. It is a structured dataset that acts as a sample of a specific language, domain, or issue. A corpus can include a variety of texts, including books, essays, web pages, and social media posts.</p>

<p>Popular corpus that used for training and evaluating NLP models:</p>

<ul>
  <li>
<strong>Common Crawl</strong>: A corpus of web pages collected by the Common Crawl project.</li>
  <li>
<strong>Wikipedia</strong>: A corpus of articles from the English Wikipedia.</li>
  <li>
<strong>Books</strong>: A corpus of books from various genres and languages.</li>
  <li>
<strong>News Articles</strong>: A corpus of news articles from various sources.</li>
  <li>
<strong>Social Media</strong>: A corpus of social media posts from various platforms.</li>
</ul>

<h3 id="common-pre-processing-techniques">Common Pre-processing Techniques</h3>

<ul>
  <li>
<strong>Tokenization</strong>: The process of splitting a text into individual words or tokens.</li>
  <li>
<strong>Normalization</strong>: The process of converting a text into a standard form.
    <ul>
      <li>
<strong>Lowercasing</strong>: The process of converting a text into lowercase.</li>
      <li>
<strong>Stemming</strong>: The process of reducing a word to its root form.</li>
      <li>
<strong>Lemmatization</strong>: The process of reducing a word to its base form.</li>
      <li>
<strong>Date and Time Normalization</strong>: The process of converting a date or time into a standard format.</li>
    </ul>
  </li>
  <li>
<strong>Stopword Removal</strong>: The process of removing common words that do not carry much meaning (e.g., ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúat‚Äù).</li>
  <li>Removal of Special Characters and Punctuation</li>
  <li>Removing HTML Tags or Markup</li>
  <li>Spell Correction</li>
  <li>Sentence Segmentation</li>
</ul>

<p><strong>What is named entity recognition in NLP?</strong></p>

<p>Named entity recognition (NER) is the task of identifying and classifying named entities in a text into predefined categories such as person, organization, location, etc.</p>

<h3 id="word-meaning-and-word-sense-disambiguation">Word Meaning and Word Sense Disambiguation</h3>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/2024-02-18-20-31-05-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/2024-02-18-20-31-05-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/2024-02-18-20-31-05-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/2024-02-18-20-31-05.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Word meaning
</div>

<ul>
  <li>
<strong>Word meaning</strong>: The meaning of a word is the concept it refers to. For example, the word ‚Äúdog‚Äù refers to the concept of a four-legged animal that barks.</li>
  <li>
<strong>Word sense</strong>: The sense of a word is the particular meaning it has in a specific context. For example, the word ‚Äúbank‚Äù can refer to a financial institution or the side of a river.</li>
  <li>
<strong>Word sense disambiguation</strong>: The task of determining which sense of a word is used in a particular context.</li>
  <li>
<strong>Lexical semantics</strong>: The study of word meaning and word sense disambiguation.</li>
  <li>
<strong>Distributional semantics</strong>: The study of word meaning based on the distributional properties of words in text.</li>
  <li>
<strong>Word embeddings</strong>: Dense vector representations of words that capture their meaning based on the distributional properties of words in text.</li>
  <li>
<strong>Word similarity</strong>: The similarity between words based on their meaning, often measured using word embeddings.</li>
  <li>
<strong>Word analogy</strong>: The relationship between words based on their meaning, often captured using word embeddings.</li>
  <li>
<strong>Word sense induction</strong>: The task of automatically identifying the different senses of a word in a corpus of text.</li>
</ul>

<p><strong>Distributional semantics</strong> is based on the idea that words that occur in similar contexts tend to have similar meanings.</p>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/2024-02-18-20-36-12-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/2024-02-18-20-36-12-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/2024-02-18-20-36-12-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/2024-02-18-20-36-12.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Distributional semantics
</div>

<p><strong>TF-IDF</strong></p>

<p>TF-IDF is a statistical measure that evaluates how important a word is to a document in a collection or corpus. It is the product of two metrics: term frequency (TF) and inverse document frequency (IDF).</p>

<ul>
  <li>
<strong>Term Frequency (TF)</strong>: The frequency of a term in a document.</li>
  <li>
<strong>Inverse Document Frequency (IDF)</strong>: The logarithm of the ratio of the total number of documents to the number of documents containing the term to measure how rare or unique a term is across the corpus.</li>
</ul>

\[\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)\]

<p>Where \(t\) is the term, \(d\) is the document, and \(D\) is the corpus.</p>

<p>Key insights:</p>

<ul>
  <li>Words that occur frequently in a document but are rare across the corpus will have high TF-IDF scores, making them important for identifying the document‚Äôs topic.</li>
  <li>Common words like ‚Äúthe,‚Äù ‚Äúand,‚Äù or ‚Äúis‚Äù (stopwords) will have low TF-IDF scores due to their high occurrence across documents.</li>
</ul>

<h3 id="word-embeddings">Word Embeddings</h3>

<ul>
  <li>
<strong>Word embeddings</strong>: Dense vector representations of words that capture their meaning based on the distributional properties of words in text.</li>
  <li>
<strong>Word2Vec</strong>: A word embedding model that uses a neural network to learn word embeddings by predicting the context of words in a corpus of text.</li>
  <li>
<strong>GloVe</strong>: A word embedding model that uses a global matrix factorization technique to learn word embeddings by predicting the co-occurrence of words in a corpus of text.</li>
  <li>
<strong>FastText</strong>: A word embedding model that uses a neural network to learn word embeddings by predicting the context of words in a corpus of text.</li>
  <li>
<strong>BERT</strong>: A word embedding model that uses a transformer architecture to learn word embeddings by predicting the masked words in a corpus of text.</li>
</ul>

<p><strong>Word2Vec</strong></p>

<ul>
  <li>
<strong>CBOW (Continuous Bag of Words)</strong>: Predicts the center word based on the context of the surrounding words.</li>
  <li>
<strong>Skip-Gram</strong>: Predicts the context of the center word based on the center word.</li>
</ul>

<p>The training objective of CBOW is:</p>

\[P(w_t = w \mid w_{t-n}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+n})\]

<p>Where \(w_t\) is the center word and \(w_{t-n}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+n}\) are the context words.</p>

<p>The training objective of Skip-Gram is:</p>

\[P(w_{t-n}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+n} \mid w_t)\]

<p>Because of the softmax function and the size of the vocabulary, the computation complexity is too high. Therefore, we use <strong>negative sampling</strong> to only <strong>update embeddings for a few negative samples</strong> rather than the entire vocabulary.
We also use <strong>sigmoid function</strong> instead of softmax function to treat the problem as a binary classification problem.</p>

<ul>
  <li>Given a real word pair, we want the model to output 1.</li>
  <li>Given a random negative sample word pair, we want the model to output 0.</li>
</ul>

<p><strong>GloVe</strong></p>

<ul>
  <li>
<strong>Co-occurrence Matrix</strong>: Represents the co-occurrence of words in a corpus of text.</li>
  <li>
<strong>Global Matrix Factorization</strong>: Decomposes the co-occurrence matrix into two lower-dimensional matrices, which represent the word embeddings.</li>
</ul>

<p>The objective function of GloVe is:</p>

\[J = \sum_{i,j=1}^V f(X_{ij})(w_i^T\tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\]

<p>where \(V\) is the size of the vocabulary and \(X_{ij}\) is the co-occurrence count between word \(i\) and word \(j\). \(w_i\) and \(\tilde{w}_j\) are the word embeddings of word \(i\) and word \(j\), \(b_i\) and \(\tilde{b}_j\) are the bias terms of word \(i\) and word \(j\).
The weighting function \(f(x)\) should satisfy the following properties:</p>

<ol>
  <li>
    <p>\(f(0) = 0\). If \(f\) is viewed as a continuous function, it should vanish as \(x \to 0\) fast enough that \(\lim_{x \to 0} f(x)\log^2 x\) is finite.</p>
  </li>
  <li>
    <p>\(f(x)\) should be non-decreasing so that rare co-occurrences are not overweighted.</p>
  </li>
  <li>
    <p>\(f(x)\) should be relatively small for large values of \(x\), so that frequent co-occurrences are not overweighted.</p>
  </li>
</ol>

<p>One class of functions that satisfies these properties is:</p>

\[f(x) = \begin{cases}
(x/x_{\text{max}})^\alpha &amp; \text{if } x &lt; x_{\text{max}} \\
1 &amp; \text{otherwise}
\end{cases}\]

<h3 id="how-to-represent-sentences">How to represent sentences</h3>

<ul>
  <li>
<strong>Bag of Words</strong>: Represents a sentence as a bag of words, ignoring the order of the words.</li>
  <li>
<strong>TF-IDF</strong>: Represents a sentence as a bag of words, but with the frequency of the words in the sentence.</li>
  <li>
<strong>N-grams</strong>: Represents a sentence as a sequence of N consecutive words.</li>
  <li>
<strong>Word Embeddings</strong>: Dense vector representations of sentences that capture their meaning based on the distributional properties of words in text. However, it has <strong>context-independent issue</strong>, i.e., the same word has the same embedding regardless of its usage</li>
  <li>
<strong>Contextual Embeddings</strong>: Use models that generate word embeddings based on the context within the sentence. Such as ELMo, BERT, etc. The embeddings can be the output of the [CLS] token or pool all word embeddings.</li>
  <li>
<strong>Direct Sentence Embedding</strong>: Directly encode entire sentences into a fixed-length vector, such as Sentence-BERT or Universal Sentence Encoder.</li>
</ul>

<p>N-gram model vs Neural model in terms of sentence representation:</p>

<ul>
  <li>Neural model can capture the meaning of a sentence by learning the context of the words in the sentence while N-gram model cannot.</li>
</ul>

<h3 id="how-to-represent-documents">How to represent documents</h3>

<p>Beside the above methods that we can use to obtain embeddings of all sentences in a document then arregate them, we can also use the following methods:</p>

<ul>
  <li>
<strong>Doc2Vec</strong>: Extends Word2Vec to represent entire documents as dense vectors, such as Doc2Vec, Paragraph Vector, etc.</li>
  <li>
<strong>Topic Modeling</strong>: Extracts the topics from the document, such as Latent Dirichlet Allocation (LDA), Non-Negative Matrix Factorization (NMF), etc.</li>
</ul>

<h3 id="data-augmentation">Data Augmentation</h3>

<ul>
  <li>
<strong>Synonym Replacement</strong>: Replaces words in the document with their synonyms. E.g., ‚ÄúThe cat sat on the mat.‚Äù -&gt; ‚ÄúThe feline rested on the rug.‚Äù</li>
  <li>
<strong>Random Insertion</strong>: Randomly inserts words into the document. E.g., ‚ÄúThe dog barked loudly.‚Äù -&gt; ‚ÄúThe big dog barked loudly.‚Äù</li>
  <li>
<strong>Random Deletion</strong>: Randomly deletes words from the document. E.g., ‚ÄúThe cat sat on the mat.‚Äù -&gt; ‚ÄúThe cat on the mat.‚Äù</li>
  <li>
<strong>Random Swap</strong>: Randomly swaps words in the document. E.g., ‚ÄúThe weather is very nice today.‚Äù -&gt; ‚ÄúThe weather today is very nice.‚Äù</li>
  <li>
<strong>Back Translation</strong>: Translates the document to another language and then translates it back to the original language.</li>
  <li>
<strong>Paraphrase</strong>: Generates a new sentence that has the same meaning as the original sentence.</li>
  <li>
<strong>Contextual Augmentation</strong>: Uses models that generate word embeddings based on the context within the sentence. Such as ELMo, BERT, etc. E.g., ‚ÄúThe car is fast.‚Äù -&gt; ‚ÄúThe vehicle is speedy.‚Äù</li>
  <li>
<strong>CutMix</strong>: Mixes two documents by randomly selecting a segment from one document and replacing it with a segment from another document. E.g., ‚ÄúThe sky is blue.‚Äù and ‚ÄúThe grass is green.‚Äù -&gt; ‚ÄúThe sky is green.‚Äù and ‚ÄúThe grass is blue.‚Äù</li>
  <li>
<strong>Entity Replacement</strong>: Replace named entities with similar entities from a predefined set or dictionary. E.g., ‚ÄúJohn went to Paris last summer.‚Äù -&gt; ‚ÄúMary traveled to London last summer.‚Äù</li>
  <li>
<strong>Generate Synthetic Data</strong>: Generate synthetic data using generative models. E.g., Prompt: ‚ÄúDescribe a sunny day.‚Äù, Generate: ‚ÄúThe sun shone brightly, warming the fields with golden light.‚Äù</li>
</ul>

<h2 id="fundamental-tasks-in-nlp">Fundamental Tasks in NLP</h2>

<h3 id="low-level-tasks">Low-level Tasks</h3>

<h4 id="tokenization">Tokenization</h4>

<p><strong>Definition</strong>: Splitting text into smaller units (words, subwords, or sentences).</p>

<p><strong>Examples</strong>: ‚ÄúDr. Anh Bui‚Äôs research focuses on AI.‚Äù -&gt; [‚ÄúDr.‚Äù, ‚ÄúAnh‚Äù, ‚ÄúBui‚Äù, ‚Äú‚Äòs‚Äù, ‚Äúresearch‚Äù, ‚Äúfocuses‚Äù, ‚Äúon‚Äù, ‚ÄúAI‚Äù, ‚Äú.‚Äù]</p>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Out-of-Vocabulary (OOV) words</li>
  <li>Misspellings words</li>
  <li>Tokenizing agglutinative languages (e.g., Japanese, Finnish) where words merge complex meanings</li>
  <li>Handling abbreviations (e.g., ‚ÄúU.S.A. vs USA‚Äù)</li>
  <li>Dealing with contractions (‚Äúdon‚Äôt ‚Üí do + not‚Äù)</li>
</ul>

<h4 id="part-of-speech-tagging">Part-of-Speech Tagging</h4>

<p><strong>Definition</strong>: Assigning grammatical categories (noun, verb, adjective) to words.</p>

<p><strong>Examples</strong>: ‚ÄúDr. Anh Bui‚Äôs research focuses on AI.‚Äù</p>

<ul>
  <li>‚ÄúDr.‚Äù: NNP (Proper Noun, Prepositional)</li>
  <li>‚Äú‚Äòs‚Äù: POS (Possessive)</li>
  <li>‚Äúresearch‚Äù: NN (Noun, Singular)</li>
  <li>‚Äúfocuses‚Äù: VBZ (Verb, 3rd person singular present)</li>
  <li>‚Äúon‚Äù: IN (Preposition)</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Ambiguous words (e.g., ‚Äúbank‚Äù can be a noun or a verb)</li>
  <li>Handling domain-specific terminology (e.g., medical or legal terms)</li>
</ul>

<h4 id="named-entity-recognition">Named Entity Recognition</h4>

<p><strong>Definition</strong>: Identifying and classifying named entities in text into predefined categories such as person, organization, location, etc.</p>

<p><strong>Examples</strong>: ‚ÄúDr. Anh Bui is a research fellow at Monash University, Australia.‚Äù</p>

<ul>
  <li>‚ÄúDr. Anh Bui‚Äù: B-PERSON</li>
  <li>‚ÄúMonash University‚Äù: B-ORGANIZATION</li>
  <li>‚ÄúAustralia‚Äù: B-LOCATION</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Ambiguous entities (e.g., ‚Äúapple‚Äù can be a fruit or a technology company)</li>
  <li>Handling nested entities (e.g., ‚ÄúNew York City‚Äù, ‚ÄúSan Francisco Bay Area‚Äù as a location entity)</li>
</ul>

<h4 id="syntactic-parsing">Syntactic Parsing</h4>

<p><strong>Definition</strong>: Analyzing sentence structure (dependency or constituency parsing).</p>

<p><strong>Examples</strong>: ‚ÄúThe cat sat on the mat.‚Äù</p>

<ul>
  <li>‚Äúsat‚Äù ‚Üí root</li>
  <li>‚Äúcat‚Äù ‚Üí subject</li>
  <li>‚Äúmat‚Äù ‚Üí object</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Ambiguous sentence structures (e.g., ‚ÄúThe man who the woman loves‚Äù can be parsed as ‚Äú(The man) who (the woman loves)‚Äù or ‚Äú(The man who) the woman loves‚Äù)</li>
  <li>Handling long-distance dependencies (e.g., ‚ÄúThe book that the girl gave to the boy‚Äù where ‚Äúbook‚Äù depends on ‚Äúboy‚Äù)</li>
</ul>

<h4 id="semantic-role-labeling">Semantic Role Labeling</h4>

<p><strong>Definition</strong>: Identifying roles of words in a sentence (who did what to whom).</p>

<p><strong>Examples</strong>: ‚ÄúJohn gave Mary a book.‚Äù</p>

<ul>
  <li>‚ÄúJohn‚Äù ‚Üí Giver</li>
  <li>‚ÄúMary‚Äù ‚Üí Recipient</li>
  <li>‚ÄúBook‚Äù ‚Üí Object</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Understanding implicit meaning (e.g., ‚ÄúJohn helped Mary‚Äù ‚Üí What did he do?)</li>
  <li>Handling metaphors and idioms</li>
</ul>

<h4 id="coreference-resolution">Coreference Resolution</h4>

<p><strong>Definition</strong>: Identifying which words refer to the same entity.</p>

<p><strong>Examples</strong>: ‚ÄúSarah loves her dog. She takes it for walks.‚Äù</p>

<ul>
  <li>‚ÄúShe‚Äù ‚Üí Sarah</li>
  <li>‚ÄúIt‚Äù ‚Üí dog</li>
</ul>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Resolving pronouns in long documents</li>
  <li>Handling ambiguous references (e.g., ‚ÄúJohn met Bob at the cafe. He ordered coffee.‚Äù)</li>
</ul>

<h3 id="high-levelapplication-tasks">High-level/Application Tasks</h3>

<h4 id="sentiment-analysis">Sentiment Analysis</h4>

<p><strong>Definition</strong>: Determining the emotion behind text (positive, negative, neutral).</p>

<p><strong>Examples</strong>: ‚ÄúThe service was great, but the food was terrible!‚Äù ‚Üí Mixed sentiment</p>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Sarcasm/Irony: ‚ÄúOh great, another meeting.‚Äù</li>
  <li>Context dependency: ‚ÄúNot bad‚Äù (Positive or Negative?)</li>
</ul>

<h4 id="machine-translation">Machine Translation</h4>

<p><strong>Definition</strong>: Automatically translating text between languages.</p>

<p><strong>Examples</strong>: ‚ÄúHello, how are you?‚Äù ‚Üí ‚ÄúXin ch√†o, dao n√†y b·∫°n th·∫ø n√†o?‚Äù</p>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Word alignment issues (different sentence structures in languages)</li>
  <li>Low-resource languages (limited training data)</li>
  <li>Idioms (Literal translation may not work)</li>
</ul>

<h4 id="text-summarization">Text Summarization</h4>

<p><strong>Definition</strong>: Generating a concise summary from a longer text.</p>

<p><strong>Examples</strong>: Summarizing a research paper into key takeaways.</p>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Preserving key information without losing meaning</li>
  <li>Avoiding hallucinations (LLMs generating false facts)</li>
</ul>

<h4 id="question-answering">Question Answering</h4>

<p><strong>Definition</strong>: Answering questions based on a given text or knowledge base.</p>

<p><strong>Example</strong> (Extractive QA):
Text: ‚ÄúEinstein developed the theory of relativity.‚Äù
Question: ‚ÄúWho developed the theory of relativity?‚Äù
Answer: ‚ÄúEinstein‚Äù</p>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Commonsense reasoning (e.g., ‚ÄúCan you fry ice?‚Äù)</li>
  <li>Handling unanswerable questions</li>
</ul>

<h4 id="text-generation">Text Generation</h4>

<p><strong>Definition</strong>: Automatically generating human-like text (e.g., chatbots, story generation).</p>

<p><strong>Examples</strong>: ChatGPT answering questions, generating articles.</p>

<p><strong>Challenges</strong>:</p>

<ul>
  <li>Hallucination: LLMs may generate false facts.</li>
  <li>Bias in data: Reinforces social biases present in training data.</li>
</ul>

<h2 id="evaluation">Evaluation</h2>

<p>References:</p>

<ul>
  <li>[1] <a href="https://huggingface.co/docs/transformers/en/perplexity" rel="external nofollow noopener" target="_blank">Perplexity of fixed-length models by Hugging Face</a>
</li>
  <li>[2] <a href="https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b" rel="external nofollow noopener" target="_blank">https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b</a>
</li>
  <li>[3] <a href="https://huggingface.co/spaces/evaluate-metric/bleu" rel="external nofollow noopener" target="_blank">https://huggingface.co/spaces/evaluate-metric/bleu</a>
</li>
  <li>[4] <a href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation" rel="external nofollow noopener" target="_blank">LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide</a>
</li>
</ul>

<p>Evaluating the output of generative models presents unique challenges, particularly in assessing qualities like realism in generated images or coherence in generated text. This section explores various evaluation approaches, from traditional statistical metrics to more advanced model-based methods.</p>

<p>We can broadly categorize LLM evaluation metrics into two main types:</p>

<ol>
  <li>
<strong>Statistical Metrics</strong>
    <ul>
      <li>Traditional NLP metrics like Perplexity, BLEU Score, and WER</li>
      <li>Advantages: Reliable, consistent, and computationally efficient</li>
      <li>Limitations: Often fail to capture semantic meaning and context</li>
    </ul>
  </li>
  <li>
<strong>Model-Based Metrics</strong>
    <ul>
      <li>Advanced metrics like GEVal and SelfCheckGPT</li>
      <li>Advantages: Better at capturing semantic understanding and context</li>
      <li>Limitations: Can be inconsistent and prone to hallucination</li>
    </ul>
  </li>
</ol>

<p>For example, consider the BLEU score‚Äôs limitations with word order: The sentences ‚ÄúThe guard arrived late because of the rain‚Äù and ‚ÄúThe rain arrived late because of the guard‚Äù would receive identical unigram BLEU scores, despite having very different meanings. While statistical metrics like BLEU are computationally efficient and consistent, they often miss such semantic nuances. Conversely, model-based scorers like SelfCheckGPT can better capture meaning but may suffer from inconsistency and hallucination issues.</p>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/66d400d78fa4a872b554eaec_65ae30bca9335d1c73650df0_metricsven-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/66d400d78fa4a872b554eaec_65ae30bca9335d1c73650df0_metricsven-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/66d400d78fa4a872b554eaec_65ae30bca9335d1c73650df0_metricsven-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/66d400d78fa4a872b554eaec_65ae30bca9335d1c73650df0_metricsven.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Metrics for LLM Evaluation (Image from source [1])
</div>

<p><strong>Most critical evaluation criteria:</strong></p>

<ul>
  <li>
<strong>Answer Relevancy</strong>: Determines whether an LLM output is able to address the given input in an informative and concise manner.</li>
  <li>
<strong>Prompt Alignment</strong>: Determines whether an LLM output is able to follow instructions from your prompt template.</li>
  <li>
<strong>Correctness</strong>: Determines whether an LLM output is factually correct based on some ground truth.</li>
  <li>
<strong>Hallucination</strong>: Determines whether an LLM output contains fake or made-up information.</li>
  <li>
<strong>Contextual Relevancy</strong>: Determines whether the retriever in a RAG-based LLM system is able to extract the most relevant information for your LLM as context.</li>
  <li>
<strong>Responsible Metrics</strong>: Includes metrics such as bias and toxicity, which determines whether an LLM output contains (generally) harmful and offensive content.</li>
  <li>
<strong>Task-Specific Metrics</strong>: Includes metrics such as summarization, which usually contains a custom criteria depending on the use-case.</li>
</ul>

<h3 id="perplexity">Perplexity</h3>

<p>Perplexity is a measure of how well a language model can predict the next token in a sequence. It is defined as the inverse probability of the test set, normalized by the number of tokens (a.k.a. the exponent of the cross-entropy loss).</p>

\[\text{Perplexity}(X) = \exp\left(-\frac{1}{n} \sum_{i=1}^n \log P_{\theta}(w_i \mid w_1, w_2, \ldots, w_{i-1})\right)\]

<p>Where \(P_{\theta}(w_i \mid w_1, w_2, \ldots, w_{i-1})\) is the probability of the next token in the sequence, obtained from a language model \(P_{\theta}\), and \(X = \{w_1, w_2, \ldots, w_n\}\) is a sequence of tokens in the test set.
<strong>The lower the perplexity, the better the language model. Or the better model will be able to give higher probability to the token actually appearing next.</strong></p>

<blockquote class="block-warning">
  <p><strong>Perplexity</strong></p>

  <p>Perplexity measures the uncertainty of a probability distribution.
If a random variable has more than k possible outcomes, the perplexity will still be k if the distribution is uniform over k outcomes and zero for the rest.
Thus, a random variable with a perplexity of k can be described as being ‚Äúk-ways perplexed,‚Äù meaning it has the same level of uncertainty as a fair k-sided die.</p>
</blockquote>

<p><a href="https://huggingface.co/docs/transformers/en/perplexity#example-calculating-perplexity-with-gpt-2-in--transformers" rel="external nofollow noopener" target="_blank">Example: Calculating perplexity with GPT-2 in ü§ó Transformers</a></p>

<h3 id="bleu-score">BLEU score</h3>

<p>The <a href="https://en.wikipedia.org/wiki/BLEU" rel="external nofollow noopener" target="_blank">BLEU (Bilingual Evaluation Understudy)</a> score is a metric for evaluating the quality of text generated by machine translation models by comparing it to one or more reference translations. It operates on the principle that the closer a machine-generated translation is to a professional human translation, the better it is.</p>

<h4 id="calculation-of-bleu-score">Calculation of BLEU Score</h4>

<h5 id="n-gram-precision">N-gram Precision</h5>

<p>The BLEU score calculates the precision of n-grams (contiguous sequences of ‚Äòn‚Äô words) between the candidate (machine-generated) translation and the reference translations. Commonly, unigrams (1-gram), bigrams (2-gram), trigrams (3-gram), and four-grams (4-gram) are used.</p>

<h5 id="clipped-precision">Clipped Precision</h5>

<p>To prevent the model from gaining an artificially high precision by repeating words or phrases, BLEU employs ‚Äúclipped precision.‚Äù This means that for each n-gram in the candidate translation, its count is clipped to the maximum number of times it appears in any single reference translation. For example, if the word ‚Äúthe‚Äù appears twice in a reference translation, and the candidate translation uses ‚Äúthe‚Äù four times, only two instances of ‚Äúthe‚Äù are considered for precision calculation.</p>

<p>The modified n-gram precision function is formally defined as:</p>

\[p_n(\hat{S}; S) := \frac{\sum_{i=1}^M \sum_{s\in G_n(\hat{y}^{(i)})} \min(C(s,\hat{y}^{(i)}), \max_{y\in S_i} C(s,y))}{\sum_{i=1}^M \sum_{s\in G_n(\hat{y}^{(i)})} C(s,\hat{y}^{(i)})}\]

<p>While this looks complicated, it simplifies to a straightforward case when dealing with one candidate sentence and one reference sentence:</p>

\[p_n(\{\hat{y}\};\{y\}) = \frac{\sum_{s\in G_n(\hat{y})}\min(C(s,\hat{y}), C(s,y))}{\sum_{s\in G_n(\hat{y})} C(s,\hat{y})}\]

<p>The denominator \(\sum_{s\in G_n(\hat{y})} C(s,y)\) represents the number of n-substrings in \(\hat{y}\) that appear in \(y\). Importantly, we count n-substrings, not n-grams. For example, when \(\hat{y} = aba\), \(y = abababa\), and \(n = 2\), all the 2-substrings in \(\hat{y}\) (‚Äúab‚Äù and ‚Äúba‚Äù) appear in \(y\) 3 times each, so the count is 6, not 2.</p>

<h5 id="brevity-penalty-bp">Brevity Penalty (BP)</h5>

<p>To discourage overly short candidate translations that might achieve high precision by omitting content, BLEU incorporates a brevity penalty. If the candidate translation is shorter than the reference, a penalty is applied to reduce the score.</p>

<h5 id="final-bleu-score">Final BLEU Score</h5>

<p>The BLEU score is computed by combining the geometric mean of the n-gram precisions with the brevity penalty:</p>

\[\text{BLEU} = \text{BP} \times \exp(\sum_{n=1}^N w_n \log p_n)\]

<p>Where:</p>

<ul>
  <li>BP is the brevity penalty</li>
  <li>\(w_n\) is the weight for each n-gram level (commonly uniform weights are used)</li>
  <li>\(p_n\) is the clipped precision for n-grams</li>
</ul>

<h4 id="strengths-of-bleu-score">Strengths of BLEU Score</h4>

<ul>
  <li>
<strong>Speed and Simplicity</strong>: BLEU is quick to calculate and easy to understand</li>
  <li>
<strong>Correlation with Human Judgment</strong>: It aligns well with human evaluations of translation quality</li>
  <li>
<strong>Language Independence</strong>: BLEU is language-independent, making it applicable across different languages</li>
  <li>
<strong>Multiple References</strong>: It can handle multiple reference translations, accommodating the diversity in acceptable translations</li>
  <li>
<strong>Wide Adoption</strong>: Its widespread use facilitates comparison across different models and studies</li>
</ul>

<h4 id="weaknesses-of-bleu-score">Weaknesses of BLEU Score</h4>

<ul>
  <li>
<strong>Sensitivity to Exact Matches</strong>: BLEU requires exact matches between candidate and reference translations, which may not account for synonyms or paraphrasing</li>
  <li>
<strong>Lack of Context Understanding</strong>: It doesn‚Äôt consider the overall meaning or context, focusing solely on n-gram overlap</li>
  <li>
<strong>Ignore the importance of words</strong>: With Bleu Score an incorrect word like ‚Äúto‚Äù or ‚Äúan‚Äù that is less relevant to the sentence is penalized just as heavily as a word that contributes significantly to the meaning of the sentence.</li>
  <li>
<strong>Ignore the order of words</strong>: It does not consider the order of words eg. The sentence ‚ÄúThe guard arrived late because of the rain‚Äù and ‚ÄúThe rain arrived late because of the guard‚Äù would get the same (unigram) Bleu Score even though the latter is quite different.</li>
  <li>
<strong>Brevity Penalty Limitations</strong>: While the brevity penalty discourages short translations, it may not adequately reward longer, more informative translations</li>
  <li>
<strong>Inadequate for Single Sentences</strong>: BLEU is designed for corpus-level evaluation and may not be reliable for evaluating individual sentences</li>
</ul>

<h3 id="wer-word-error-rate">WER (Word Error Rate)</h3>

<p>Speech-to-Text applications use Word Error Rate, not Bleu Score. It is because output of speech-to-text applications is a sequence of words that requires exact matches with the input speech/transcript.
The metric that is typically used for these applications is Word Error Rate (WER), or its sibling, Character Error Rate (CER). It compares the predicted output and the target transcript, word by word (or character by character) to figure out the number of differences between them.</p>

<p>The difference between prediction and target can be classified into three categories:</p>

<ul>
  <li>
<strong>Substitution</strong>: The predicted word is different from the target word.</li>
  <li>
<strong>Insertion</strong>: The predicted word is not in the target.</li>
  <li>
<strong>Deletion</strong>: The target word is not in the predicted output.</li>
</ul>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*9CWYHcb2XQ-e0EXZ-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*9CWYHcb2XQ-e0EXZ-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*9CWYHcb2XQ-e0EXZ-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*9CWYHcb2XQ-e0EXZ.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Count the Insertions, Deletions, and Substitutions between the Transcript and the Prediction (Image from source [2])
</div>

\[\text{WER} = \frac{\text{Substitutions} + \text{Insertions} + \text{Deletions}}{\text{Total Words in Transcript}}\]

<p>Drawbacks of WER:</p>

<ul>
  <li>It does not distinguish between words that are important to the meaning of the sentence and those that are not as relevant, for example, missing the word ‚Äúthe‚Äù in ‚Äúthe guard arrived late because of the rain‚Äù is the same as missing the word ‚Äúrain‚Äù in the same sentence.</li>
  <li>When comparing words, it does not consider whether two words are different in just a single character or are completely different, for example, ‚Äúarrive‚Äù and ‚Äúarrived‚Äù are considered different words.</li>
</ul>

<h3 id="rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</h3>

<p>ROUGE measures the overlap between the generated text (candidate) and one or more reference texts. It focuses on n-gram, sequence, and word-level similarity and is particularly <strong>recall-oriented</strong>, meaning it emphasizes how much of the reference text is captured by the generated text.</p>

\[\text{ROUGE-N} = \frac{\sum_{r \in R} \sum_{\text{gram}_n \in r} \min \big( \text{Count}_{C}(\text{gram}_n), \text{Count}_{r}(\text{gram}_n) \big)}{\sum_{r \in R} \sum_{\text{gram}_n \in r} \text{Count}_{r}(\text{gram}_n)}\]

<ul>
  <li>
<strong>Numerator:</strong> number of overlapping \(n\)-grams between candidate and references</li>
  <li>
<strong>Denominator:</strong> total number of \(n\)-grams in references</li>
</ul>

<p>Special cases:</p>

<ul>
  <li>
<strong>ROUGE-1</strong> ‚Üí unigram overlap</li>
  <li>
<strong>ROUGE-2</strong> ‚Üí bigram overlap</li>
</ul>

<h3 id="meteor-metric-for-evaluation-of-translation-with-explicit-ordering">METEOR (Metric for Evaluation of Translation with Explicit Ordering)</h3>

<p><strong>Motivation:</strong><br>
Traditional metrics like BLEU often rely on exact word matches and can fail to capture semantic similarity or word order quality. METEOR was proposed to better align with human judgments by combining precision, recall, and linguistic resources (e.g., stemming and synonyms). It is particularly useful when nuanced language understanding is important.</p>

<hr>

<p>Unlike purely lexical metrics like BLEU, METEOR incorporates semantic and linguistic features, aiming for a more nuanced evaluation.<br>
Some key features of METEOR:</p>

<ul>
  <li>
<strong>Semantic Matching</strong>: Uses WordNet synonyms, stemming, and paraphrases (not antonyms) to capture semantic meaning beyond exact word matches.</li>
  <li>
<strong>Sentence Level Evaluation</strong>: Evaluates translations at the sentence level, making it more sensitive to quality differences.</li>
  <li>
<strong>Explicit Ordering</strong>: Applies a penalty for disordered matches, accounting for word order in the translation.</li>
</ul>

<hr>

<h4 id="formula">Formula</h4>

<p>Harmonic mean of precision and recall (with recall weighted higher):</p>

\[F = \frac{10 P R}{9P + R}\]

<p>Where:</p>

<ul>
  <li>\(P\) = precision: how many words in the generated text are also in the reference text</li>
  <li>\(R\) = recall: how many words in the reference text are also in the generated text</li>
</ul>

<p>Penalty for disordered matches:</p>

\[Penalty = \gamma \left( \frac{c}{m} \right)^3\]

<p>Where:</p>

<ul>
  <li>\(c\) = number of contiguous matched segments</li>
  <li>\(m\) = number of matched words</li>
  <li>\(\gamma = 0.5\) (default constant)</li>
</ul>

<p>Final METEOR score:</p>

\[\text{METEOR} = F \times (1 - Penalty)\]

<hr>

<p><strong>Advantages of METEOR</strong></p>

<ul>
  <li>
<strong>Linguistic richness</strong>: Incorporates synonyms, stemming, and paraphrases.</li>
  <li>
<strong>Better correlation with human judgment</strong>: Often more reliable than metrics like BLEU.</li>
  <li>
<strong>Flexibility</strong>: Suitable for multiple languages (when appropriate linguistic resources are available).</li>
</ul>

<hr>

<p><strong>Limitations of METEOR</strong></p>

<ul>
  <li>
<strong>Computational cost</strong>: More resource-intensive than simpler metrics like BLEU.</li>
  <li>
<strong>Language dependency</strong>: Relies on linguistic resources (e.g., WordNet), which may not be equally robust across languages.</li>
  <li>
<strong>Bias toward longer outputs</strong>: Its recall orientation can favor verbose candidates.</li>
</ul>

<h3 id="levenshtein-distance-edit-distance">Levenshtein distance (Edit Distance)</h3>

<p>Levenshtein distance, also known as edit distance, is a metric used to measure the difference between two strings. It calculates the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another.</p>

<!-- $$
\text{Levenshtein}(s_1, s_2) = \min \left( \text{distance}(s_1[1:], s_2) + \text{cost of deleting } s_1[0], \text{distance}(s_1, s_2[1:]) + \text{cost of inserting } s_2[0], \text{distance}(s_1[1:], s_2[1:]) + \text{cost of substituting } s_1[0] \text{ with } s_2[0] \right)
$$ -->

<p>Where:</p>

<ul>
  <li>\(s_1\) and \(s_2\) are the two strings</li>
  <li>\(\text{distance}(s_1, s_2)\) is the Levenshtein distance between \(s_1\) and \(s_2\)</li>
  <li>\(\text{cost of deleting } s_1[0]\) is the cost of deleting the first character of \(s_1\)</li>
  <li>\(\text{cost of inserting } s_2[0]\) is the cost of inserting the first character of \(s_2\)</li>
  <li>\(\text{cost of substituting } s_1[0] \text{ with } s_2[0]\) is the cost of substituting the first character of \(s_1\) with the first character of \(s_2\)</li>
</ul>

<p><strong>Example Calculation</strong></p>

<p>Strings: A = ‚Äúkitten‚Äù, B = ‚Äúsitting‚Äù</p>

<p>Step by step calculation:</p>

<ul>
  <li>Substitute ‚Äúk‚Äù with ‚Äús‚Äù: distance(‚Äúkitten‚Äù, ‚Äúsitten‚Äù) cost = 1</li>
  <li>Substitute ‚Äúe‚Äù with ‚Äúi‚Äù: distance(‚Äúsitten‚Äù, ‚Äúsittin‚Äù) cost = 1</li>
  <li>Add ‚Äúg‚Äù to the end: distance(‚Äúsittin‚Äù, ‚Äúsitting‚Äù) cost = 1</li>
</ul>

<p>Levenshtein distance = 3</p>

<h3 id="g-eval">G-EVAL</h3>

<p>Reference:</p>

<ul>
  <li>[1] <a href="https://www.confident-ai.com/blog/g-eval-the-definitive-guide" rel="external nofollow noopener" target="_blank">https://www.confident-ai.com/blog/g-eval-the-definitive-guide</a>
</li>
</ul>

<p>G-EVAL is a framework that treats a Large Language Model (LLM) as a <strong>judge</strong> for evaluating other LLMs outputs. It allows us to define <strong>custome evaluation criteria in natural language</strong>, which the system then decomposes into structured evaluation steps. The LLM-judge evalutes outputs according to those steps, and then G-Eval produces a <strong>weighted score</strong> (leveraging token probabilities, confidence scores, etc.) as the final metric. Compared to the traditional statistical metrics like BLEU, ROUGE, G-Eval is more flexible and human-aligned alternative.</p>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://images.ctfassets.net/otwaplf7zuwf/12StS90npeMOTt9xKLt6jg/4479f3f3ca0021931750c2223fd39f0f/image-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://images.ctfassets.net/otwaplf7zuwf/12StS90npeMOTt9xKLt6jg/4479f3f3ca0021931750c2223fd39f0f/image-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://images.ctfassets.net/otwaplf7zuwf/12StS90npeMOTt9xKLt6jg/4479f3f3ca0021931750c2223fd39f0f/image-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://images.ctfassets.net/otwaplf7zuwf/12StS90npeMOTt9xKLt6jg/4479f3f3ca0021931750c2223fd39f0f/image.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    G-Eval. Image from [1].
</div>

<h4 id="how-g-eval-works">How G-Eval works</h4>

<p><strong>Criteria ‚Üí Evaluation Steps</strong></p>

<p>You write a criterion in natural language (e.g. ‚ÄúCoherence (1‚Äì5): how well does the text flow logically?‚Äù). G-Eval‚Äôs internal LLM converts that into step-by-step sub-questions or checks.</p>

<p>The criterion prompt for coherence in the paper looked like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g_eval_criteria</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Coherence (1-5) - the collective quality of all sentences. We align this dimension with
the DUC quality question of structure and coherence whereby </span><span class="sh">"</span><span class="s">the summary should be
well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>Which resulted in this final evaluation prompt after evaluation steps were generated from the above criterion</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g_eval_prompt_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
You will be given one summary written for a news article.
Your task is to rate the summary on one metric.
Please make sure you read and understand these instructions carefully. Please keep this
document open while reviewing, and refer to it as needed.

Evaluation Criteria:
Coherence (1-5) - the collective quality of all sentences. We align this dimension with
the DUC quality question of structure and coherence whereby </span><span class="sh">"</span><span class="s">the summary should be
well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.</span><span class="sh">"</span><span class="s">

Evaluation Steps:
1. Read the news article carefully and identify the main topic and key points.
2. Read the summary and compare it to the news article. Check if the summary covers the main
topic and key points of the news article, and if it presents them in a clear and logical order.
3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest
based on the Evaluation Criteria.

Example:
Source Text:

Summary:


Evaluation Form (scores ONLY):
- Coherence:
</span><span class="sh">"""</span>
</code></pre></div></div>

<p><strong>Judging via the LLM-judge</strong></p>

<p>Using those evaluation steps, the model assesses the candidate output (and perhaps reference or context) and provides intermediate judgments.</p>

<p><strong>Scoring / Aggregation with weights</strong></p>

<p>Rather than treating all judgments equally, G-Eval weights sub-judgments using model confidences (log probabilities), which gives more granularity and helps reduce bias and randomness.</p>

<p>By breaking down the judgment process and weighting according to confidence, G-Eval aims to improve consistency, fine-grained discrimination, and robustness compared to naive single-pass LLM scoring.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepeval.metrics</span> <span class="kn">import</span> <span class="n">GEval</span>
<span class="kn">from</span> <span class="n">deepeval.test_case</span> <span class="kn">import</span> <span class="n">LLMTestCaseParams</span>

<span class="n">correctness_metric</span> <span class="o">=</span> <span class="nc">GEval</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">Correctness</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">criteria</span><span class="o">=</span><span class="sh">"</span><span class="s">Determine whether the actual output is factually correct based on the expected output.</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">evaluation_params</span><span class="o">=</span><span class="p">[</span><span class="n">LLMTestCaseParams</span><span class="p">.</span><span class="n">ACTUAL_OUTPUT</span><span class="p">,</span> <span class="n">LLMTestCaseParams</span><span class="p">.</span><span class="n">EXPECTED_OUTPUT</span><span class="p">],</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="tokenization-1">Tokenization</h2>

<p>A tokenizer is an important component of any NLP model. It is responsible for converting text into tokens (e.g., words, characters, or subwords) that LLMs can work with instead of raw text.
The choice of tokenizer significantly impacts the performance of the model and the ability to generalize across different tasks.</p>

<ul>
  <li>
<strong>Shorter token sequences = faster inference and training</strong>. A good tokenizer reduces unncessary tokens and reduces the vocabulary size.</li>
  <li>
<strong>Better tokenization improves generalization</strong>. A good tokenizer will be able to handle out-of-vocabulary words (OOV) better. Words <code class="language-plaintext highlighter-rouge">"play"</code>, <code class="language-plaintext highlighter-rouge">"playing"</code>, <code class="language-plaintext highlighter-rouge">"plays"</code> should be tokenized like <code class="language-plaintext highlighter-rouge">"play"</code>, <code class="language-plaintext highlighter-rouge">"play" + "ing"</code>, <code class="language-plaintext highlighter-rouge">"play" + "s"</code> rather than three different tokens so that the model can understand the relationship between the words.</li>
  <li>
<strong>Smaller vocabularies</strong> can work for low-resource languages.</li>
  <li>
<strong>Custom tokenizers</strong> may improve domain-specific tasks, e.g., legal, medical, coding, etc, where there are specific tokens that are not found in the general tokenizer.</li>
</ul>

<p>Most common tokenization methods:</p>

<ul>
  <li>
<strong>Byte pair encoding (BPE)</strong>: A tokenization algorithm that iteratively merges the most frequent pairs of characters in a corpus of text to create a vocabulary of subword units. Used in GPT models.</li>
  <li>
<strong>WordPiece</strong>: A tokenization algorithm similar to BPE that uses a greedy algorithm to iteratively merge the most frequent subword units in a corpus of text. Used in BERT models.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="sh">"</span><span class="s">biodegradability</span><span class="sh">"</span><span class="p">))</span>  
<span class="c1"># ['bio', 'degrad', 'ability']
</span></code></pre></div></div>

<h3 id="out-of-vocabulary-oov">Out-of-Vocabulary (OOV)</h3>

<p>Reference:</p>

<ul>
  <li>[1] https://spotintelligence.com/2024/10/08/out-of-vocabulary-oov-words/</li>
</ul>

<p>When tokenizing text, new words (also called Out-of-Vocabulary (OOV) words) can be challenging. These are words that do not exist in the tokenizer‚Äôs vocabulary. There are several strategies to handle OOV words:</p>

<p><strong>Subword Tokenization</strong></p>

<p>Instead of treating words as atomic units, subword tokenization breaks words into smaller, reusable parts. This can be done by Byte Pair Encoding (BPE) or WordPiece. This helps handle new words, misspellings, and rare words effectively.</p>

<p>However, some rare words that cannot be broken down into smaller units still end up as new individual tokens.</p>

<p><strong>Character-Level Tokenization</strong></p>

<p>This approach treats each character as a token, which can handle OOV words better. However, it can lead to a very large sequence length and loss of semantic meaning, making it harder for the model to learn the context of the text.</p>

<p>However, this technique is still useful for  languages with rich morphology - where the meaning of a word is determined by changes to the word‚Äôs base form instead of relative position or additional affixes.</p>

<p><strong>Fallback to Unknown Token</strong></p>

<p>If a word is not in the tokenizer‚Äôs vocabulary, replace it with a special [UNK] token. This is the simplest approach and can handle OOV words effectively. However, it can lead to loss of information and reduced model performance.</p>

<p>Combining this technique with subword tokenization can at least provide a partial meaning of the text.</p>

<p><strong>Using Lookup Table</strong></p>

<p>Maintaining an external lookup table of OOV words and their corresponding embeddings. When an OOV word is encountered, it is replaced with the embedding from the lookup table, otherwise, it is processed by the tokenizer as normal.</p>

<p><strong>Contextual Embedding (FastText, ELMO)</strong></p>

<ul>
  <li>
    <p>FastText: FastText is an extension of Word2Vec that handles OOV words by considering subword information. Instead of learning embeddings for entire words, FastText learns embeddings for character n-grams. This allows it to generate meaningful representations for OOV words based on their constituent subwords. For example, FastText can produce an embedding for ‚Äúblockchain‚Äù, even if it wasn‚Äôt part of the training data, by combining the embeddings of ‚Äúblock‚Äù and ‚Äúchain.‚Äù</p>
  </li>
  <li>
    <p>ELMO: ELMO uses a context-sensitive approach to handle OOV words. It uses a bidirectional LSTM to generate context-specific embeddings for each word. This allows it to generate a representation for OOV words based on the context in which they appear.</p>
  </li>
</ul>

<p><strong>What is NLTK?</strong></p>

<ul>
  <li>
<strong>Natural Language Toolkit (NLTK)</strong>: A library for building programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.</li>
</ul>

<h2 id="transformer">Transformer</h2>

<p>Most modern Large Language Models (LLMs) are built on the <strong>Transformer</strong> architecture, which stacks multiple layers of <em>transformer blocks</em>. Each block uses <strong>multi-head self-attention</strong> to capture relationships between tokens, allowing the model to understand words in the context of broader sequences. This architecture is the foundation behind modern NLP and many multimodal models.</p>

<p><strong>Transformer Block</strong></p>

<p>A transformer block consists of two main components:</p>

<ol>
  <li>
<strong>Multi-Head Self-Attention</strong> ‚Äì lets each token attend to others in the sequence, capturing dependencies regardless of distance.</li>
  <li>
<strong>Feed-Forward Network (FFN)</strong> ‚Äì applies non-linear transformations to enrich token representations.</li>
</ol>

<p>Residual connections and layer normalization stabilize training and help preserve information flow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Tokenization</strong>
Before text can be processed, it must be tokenized into smaller units (tokens) such as words or subwords. These tokens are mapped to embeddings‚Äîhigh-dimensional vectors that capture semantic meaning and provide input to the transformer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Hello, how are you?</span><span class="sh">"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
</code></pre></div></div>

<p><strong>Cross-Attention and Self-Attention</strong></p>

<p>Transformers rely heavily on attention mechanisms, which compute relationships between tokens. There are two main types:</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Self-Attention</th>
      <th>Cross-Attention</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Definition</td>
      <td>Focuses on relationships between elements of the same sequence (e.g., within a sentence).</td>
      <td>Focuses on relationships between elements of one sequence and another sequence (e.g., query and context).</td>
    </tr>
    <tr>
      <td>Inputs</td>
      <td>Single sequence (e.g., the same sequence is used for queries, keys, and values).</td>
      <td>Two sequences: one provides queries, and the other provides keys and values.</td>
    </tr>
    <tr>
      <td>Purpose</td>
      <td>Captures intra-sequence dependencies, helping the model understand context within the same sequence.</td>
      <td>Captures inter-sequence dependencies, aligning information between different sequences.</td>
    </tr>
    <tr>
      <td>Key Benefit</td>
      <td>Helps the model understand contextual relationships within a sequence.</td>
      <td>Enables the model to incorporate external information from another sequence. Very important for multi-modal tasks.</td>
    </tr>
  </tbody>
</table>

<p><strong>Details of Attention Mechanism</strong></p>

<p>Below is the table of the operations and dimensions of the attention mechanisms including the original and two additional operations proposed in our paper (<a href="https://arxiv.org/abs/2403.12326" rel="external nofollow noopener" target="_blank">KPOP</a>).</p>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Original Operation</th>
      <th>Original Dim</th>
      <th>Concatenative Operation</th>
      <th>Concatenative Dim</th>
      <th>Additive Operation</th>
      <th>Additive Dim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Q</td>
      <td>W<sub>q</sub>Z</td>
      <td>b√óm<sub>z</sub>√ód</td>
      <td>W<sub>q</sub>Z</td>
      <td>b√óm<sub>z</sub>√ód</td>
      <td>W<sub>q</sub>Z</td>
      <td>b√óm<sub>z</sub>√ód</td>
    </tr>
    <tr>
      <td>K</td>
      <td>W<sub>k</sub>C</td>
      <td>b√óm<sub>c</sub>√ód</td>
      <td>W<sub>k</sub>cat(C,repeat(p,b))</td>
      <td>b√ó(m<sub>c</sub>+m<sub>p</sub>)√ód</td>
      <td>W<sub>k</sub>(C+repeat(p,b))</td>
      <td>b√óm<sub>c</sub>√ód</td>
    </tr>
    <tr>
      <td>V</td>
      <td>W<sub>v</sub>C</td>
      <td>b√óm<sub>c</sub>√ód</td>
      <td>W<sub>v</sub>cat(C,repeat(p,b))</td>
      <td>b√ó(m<sub>c</sub>+m<sub>p</sub>)√ód</td>
      <td>W<sub>v</sub>(C+repeat(p,b))</td>
      <td>b√óm<sub>c</sub>√ód</td>
    </tr>
    <tr>
      <td>A</td>
      <td>œÉ(QK<sup>T</sup>/‚àöd)</td>
      <td>b√óm<sub>z</sub>√óm<sub>c</sub>
</td>
      <td>œÉ(QK<sup>T</sup>/‚àöd)</td>
      <td>b√óm<sub>z</sub>√ó(m<sub>c</sub>+m<sub>p</sub>)</td>
      <td>œÉ(QK<sup>T</sup>/‚àöd)</td>
      <td>b√óm<sub>z</sub>√óm<sub>c</sub>
</td>
    </tr>
    <tr>
      <td>O</td>
      <td>AV</td>
      <td>b√óm<sub>z</sub>√ód</td>
      <td>AV</td>
      <td>b√óm<sub>z</sub>√ód</td>
      <td>AV</td>
      <td>b√óm<sub>z</sub>√ód</td>
    </tr>
  </tbody>
</table>

<p><em>Note: cat(), repeat(¬∑,b), œÉ() represent the concatenate (at dim=1), repeat an input b times (at dim=0) and the softmax operations (at dim=2), respectively.</em></p>

<p>In the table, Z is the input sequence (i.e., the query), C is the context sequence that should be attended to. In the self-attention, Z and C are the same.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Cross-Attention module.
        Args:
            embed_dim (int): Dimension of the input embeddings.
            num_heads (int): Number of attention heads.
            dropout (float): Dropout probability.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CrossAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="sh">"</span><span class="s">embed_dim must be divisible by num_heads</span><span class="sh">"</span>
        
        <span class="c1"># Query, Key, and Value linear projections
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        
        <span class="c1"># Output projection
</span>        <span class="n">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass for cross-attention.
        Args:
            query (torch.Tensor): Query tensor of shape (batch_size, query_len, embed_dim).
            key (torch.Tensor): Key tensor of shape (batch_size, key_len, embed_dim).
            value (torch.Tensor): Value tensor of shape (batch_size, key_len, embed_dim).
            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, key_len) or (batch_size, 1, query_len, key_len).
                                            Used to mask out invalid positions.
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, query_len, embed_dim).
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">query_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="n">key_len</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Project query, key, and value
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_proj</span><span class="p">(</span><span class="n">query</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">query_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, query_len, head_dim)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_proj</span><span class="p">(</span><span class="n">key</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">key_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>        <span class="c1"># (batch_size, num_heads, key_len, head_dim)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_proj</span><span class="p">(</span><span class="n">value</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">key_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>    <span class="c1"># (batch_size, num_heads, key_len, head_dim)
</span>
        <span class="c1"># Compute scaled dot-product attention
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, query_len, key_len)
</span>        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>  <span class="c1"># Apply mask
</span>        
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, query_len, key_len)
</span>        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Weighted sum of values
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, query_len, head_dim)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">query_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>  <span class="c1"># (batch_size, query_len, embed_dim)
</span>
        <span class="c1"># Final output projection
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, query_len, embed_dim)
</span>        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p><strong>Why Multi-Head Attention?</strong></p>

<p>Multi-head attention is a core innovation of the Transformer architecture. It enables the model to capture a richer variety of relationships within the input by combining multiple perspectives in parallel.</p>

<ul>
  <li>
<strong>Diverse Representations</strong>: Each attention <em>head</em> has its own set of learned weights, allowing it to focus on different parts of the sequence or capture different types of dependencies (e.g., short-range vs. long-range). Together, these heads provide a more comprehensive understanding of context.</li>
  <li>
<strong>Efficient Use of Dimensions</strong>: The embedding dimension is split across the heads, so each head works in a lower-dimensional subspace. This keeps individual computations efficient, while aggregating the outputs restores the full representational power of the model.</li>
</ul>

<p><strong>Positional Embedding</strong></p>

<p>The key challenge for transformers is handling the sequential nature of text, where the order of tokens matters. The Self-Attention mechanism is permutation-invariant, meaning it processes a sequence of tokens as a set, with no inherent knowledge of their order or relative positions. 
Without this positional information, the model would lose important context about word order and context, e.g., cannot distinguish between ‚Äúthe cat sat on the mat‚Äù and ‚Äúthe mat sat on the cat‚Äù.</p>

<p>To address this, transformers use <strong>positional embeddings</strong>, which are added to the token embeddings to provide the model with the positional information. The positional embeddings are learned during training and are added to the token embeddings to provide the model with the positional information.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>The positional embeddings are added to the token embeddings to provide the model with the positional information.</p>

\[E_{i} = E_{token} + E_{positional}\]

<p>Where \(E_{token}\) is the token embedding and \(E_{positional}\) is the positional embedding.</p>

<p>There are two types of positional embeddings:</p>

<ul>
  <li>
<strong>Absolute Positional Embedding</strong>: A fixed position embedding that is added to the token embeddings, such as Sinusoidal as in original Transformer paper, which uses mathematical function to generate the positional embeddings.</li>
  <li>
<strong>Learned Positional Embedding</strong>: A position embedding that is learned during training, such as in BERT, which allows more flexible and dynamic complex positional information specific to the task.</li>
</ul>

<p><strong>Relative Positional Embedding</strong>: The positional information is not added to the input embedding, but instead incorporated directly into the attention mechanism itself, by modifying the dot-product attention to include relative position information.
This enables generalization to sequence lengths longer than the training data and handle variable-length sequences.</p>

<p><strong>Rotary Positional Embedding (RoPE)</strong></p>

<h2 id="kv-caching">KV Caching</h2>

<h2 id="bert">BERT</h2>

<h3 id="tokenization-in-bert">Tokenization in BERT</h3>

<ul>
  <li>
<strong>WordPiece</strong>: A tokenization algorithm similar to BPE that uses a greedy algorithm to iteratively merge the most frequent subword units in a corpus of text.</li>
  <li>Example: The word ‚Äúuniversity‚Äù might be split into the subword units ‚Äúun‚Äù, ‚Äú##iver‚Äù, and ‚Äú##sity‚Äù. The ‚Äú##‚Äù prefix is used to indicate that a subword unit is part of a larger word. This allows the model to represent out-of-vocabulary words by combining subword units that it has seen during training. This also allows to reduce the size of the vocabulary and the number of out-of-vocabulary words.</li>
</ul>

<p>Special tokens in BERT:</p>

<ul>
  <li>
<strong>[CLS]</strong>: A special token that is added to the beginning of each input sequence in BERT. It is used to represent the classification of the entire input sequence.</li>
  <li>
<strong>[SEP]</strong>: A special token that is added between two sentences in BERT. It is used to separate the two sentences.</li>
  <li>
<strong>[MASK]</strong>: A special token that is used to mask a word in the input sequence during the pretraining of BERT.</li>
  <li>
<strong>[UNK]</strong>: A special token that is used to represent out-of-vocabulary words in BERT.</li>
  <li>
<strong>[PAD]</strong>: A special token that is used to pad input sequences to the same length in BERT.</li>
</ul>

<p><strong>Learnable position embeddings</strong>: In BERT, the position embeddings are learned during the training process, allowing the model to learn the relative positions of words in the input sequence. This is in contrast to traditional position embeddings, which are fixed and do not change during training.</p>

<p><strong>Segment embeddings</strong>: In BERT, each input sequence is associated with a segment embedding that indicates whether the input sequence is the first sentence or the second sentence in a pair of sentences. This allows the model to distinguish between the two sentences in the input sequence. There are only two values for the segment embeddings: 0 and 1.</p>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-6-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-6-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-6-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-6.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    BERT Embedding
</div>

<h3 id="masked-prediction-in-bert">Masked Prediction in BERT</h3>

<ul>
  <li>
<strong>Masked Language Model (MLM)</strong>: A pretraining objective for BERT that involves predicting the masked words in the input sequence. The masked words are replaced with the [MASK] token, and the model is trained to predict the original words in the input sequence.</li>
  <li>
<strong>Next Sentence Prediction (NSP)</strong>: A pretraining objective for BERT that involves predicting whether two sentences are consecutive in the original text. The sentences are concatenated together with the [SEP] token, and the model is trained to predict whether the two sentences are consecutive.</li>
</ul>

<h3 id="pre-training-bert">Pre-training BERT</h3>

<ul>
  <li>
<strong>Masked Language Model (MLM)</strong>: A pretraining objective for BERT that involves predicting the masked words in the input sequence. The masked words are replaced with the [MASK] token, and the model is trained to predict the original words in the input sequence.</li>
  <li>
<strong>Next Sentence Prediction (NSP)</strong>: A pretraining objective for BERT that involves predicting whether two sentences are consecutive in the original text. The sentences are concatenated together with the [SEP] token, and the model is trained to predict whether the two sentences are consecutive.</li>
</ul>

<h2 id="how-to-train-a-llm">How to train a LLM</h2>

<p>References:</p>

<ul>
  <li>[1] <a href="https://cameronrwolfe.substack.com/p/understanding-and-using-supervised" rel="external nofollow noopener" target="_blank">Understanding and Using Supervised Fine-Tuning (SFT) for Language Models</a> by Cameron Wolfe</li>
  <li>[2] <a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" rel="external nofollow noopener" target="_blank">LLM Training: RLHF and Its Alternatives</a> by Sebastian Raschka</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Different stages of training for an LLM. Image from [1].
</div>

<h3 id="unsupervised-pre-training">Unsupervised Pre-training</h3>

<p><strong>Goal:</strong></p>

<p>Unsupervised pre-training involves training a model on a large-scale, unlabeled text corpus to learn general language representations (i.e., BERT) or general language understanding (i.e., GPT), with the goal of learning the structure, syntax, semantics, and general language understanding without explicit task-specific labels.</p>

<p><strong>Process:</strong></p>

<p>Uses unsupervised objectives like:</p>

<ul>
  <li>
<strong>Causal Language Modeling (CLM)</strong>: Predict the next token in a sequence.</li>
  <li>
<strong>Masked Language Modeling (MLM)</strong>: Predict masked tokens in a sequence.</li>
  <li>
<strong>Denoising</strong>: Reconstruct corrupted text (e.g., T5‚Äôs span corruption).</li>
</ul>

<p><strong>Loss Function:</strong>
Cross-entropy loss for token prediction.</p>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Leverages massive unlabeled datasets.</li>
  <li>Provides a general-purpose foundation for language understanding.</li>
  <li>Highly scalable.</li>
</ul>

<p><strong>Limitations:</strong></p>

<p>Produces models that are task-agnostic and need further fine-tuning for specific tasks.
May generate outputs that are factual but misaligned with user intent or preferences.</p>

<h3 id="supervised-fine-tuning-sft">Supervised Fine-tuning (SFT)</h3>

<p><strong>Goal:</strong> Adapting a pretrained model to specific tasks using labeled datasets, with the aim of training the model to perform well on specific tasks (e.g., summarization, classification, translation) with supervised learning.
SFT is simple/cheap to use and a useful tool for aligning language models, which has made is popular within the open-source LLM research community and beyond [1].</p>

<p><strong>Process:</strong></p>

<p>Fine-tunes the pretrained model on task-specific datasets with labels.
Aligns the model‚Äôs output with desired task objectives.</p>

<p><strong>Loss Function:</strong>
Cross-entropy loss or task-specific loss (e.g., mean squared error for regression tasks).</p>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Tailors the model for specific tasks, improving task performance.</li>
  <li>Reduces the amount of task-specific data needed compared to training from scratch.</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Performance depends on the quality and quantity of labeled data.</li>
  <li>Fine-tuning on one task may reduce performance on others (catastrophic forgetting).</li>
</ul>

<h3 id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    SFT and RLHF. Image from [1].
</div>

<p><strong>Goal:</strong>
Fine-tuning a model using reinforcement learning where the reward signal is derived from human feedback to align the model with human preferences and values beyond task-specific objectives.</p>

<p><strong>Process:</strong></p>

<ul>
  <li>
<strong>Supervised Pretraining</strong>: Start with a pretrained and optionally fine-tuned model.</li>
  <li>
<strong>Feedback Collection</strong>: Collect human-labeled data ranking model outputs (e.g., ranking completions for prompts).</li>
  <li>
<strong>Reward Model (RM)</strong>: Train a reward model to predict rankings based on human feedback. The reward model is trained to mimic human preferences on a specific task.</li>
  <li>
<strong>Policy Optimization</strong>: Fine-tune the model (policy) using reinforcement learning (e.g., Proximal Policy Optimization, PPO) to maximize the reward from the RM.</li>
</ul>

<p>Loss Function:</p>

<ul>
  <li><strong>Combines reinforcement learning objectives (e.g., PPO loss) with supervised objectives to balance exploration and alignment.</strong></li>
</ul>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Aligns model behavior with human values, preferences, and ethical considerations.</li>
  <li>Reduces harmful or inappropriate responses.</li>
  <li>Improves usability in real-world scenarios (e.g., chatbot interactions).</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Requires expensive and time-consuming human feedback.</li>
  <li>May introduce biases based on the preferences of the feedback providers.</li>
  <li>Balancing alignment with generalization is challenging.</li>
</ul>

<p><strong>Comparison Table</strong></p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Unsupervised Pretraining</th>
      <th>Supervised Fine-Tuning</th>
      <th>RLHF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Purpose</td>
      <td>General-purpose language understanding</td>
      <td>Task-specific performance improvement</td>
      <td>Aligning outputs with human preferences</td>
    </tr>
    <tr>
      <td>Data Requirement</td>
      <td>Large-scale unlabeled text corpora</td>
      <td>Labeled datasets for specific tasks</td>
      <td>Human-labeled feedback or rankings</td>
    </tr>
    <tr>
      <td>Objective</td>
      <td>Learn language patterns and knowledge</td>
      <td>Optimize for specific task objectives</td>
      <td>Optimize for human alignment</td>
    </tr>
    <tr>
      <td>Training Cost</td>
      <td>High (large datasets, long training)</td>
      <td>Moderate (smaller labeled datasets)</td>
      <td>Very high (feedback collection + RL tuning)</td>
    </tr>
    <tr>
      <td>Model Usage</td>
      <td>Provides a base model</td>
      <td>Task-specific models</td>
      <td>Refines models for safer, more useful output</td>
    </tr>
    <tr>
      <td>Challenges</td>
      <td>Task-agnostic, needs fine-tuning</td>
      <td>Dependent on labeled data quality</td>
      <td>Expensive and subject to bias in feedback</td>
    </tr>
    <tr>
      <td>Example</td>
      <td>Training GPT, BERT, T5 from scratch</td>
      <td>Fine-tuning BERT for sentiment analysis</td>
      <td>Fine-tuning GPT with human ranking data</td>
    </tr>
  </tbody>
</table>

<h3 id="ppo-and-dpo">PPO and DPO</h3>

<p>References:</p>

<ul>
  <li><a href="https://medium.com/@bavalpreetsinghh/rlhf-ppo-vs-dpo-26b1438cf22b" rel="external nofollow noopener" target="_blank">RLHF(PPO) vs DPO</a></li>
</ul>

<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h4>

<p>The Bradley-Terry model is a probabilistic model used to rank items based on pairwise comparisons.</p>

\[p(y_1 \succ y_2 \mid x) = \frac{exp(r(x,y_1))}{exp(r(x,y_1)) + exp(r(x,y_2))}\]

<p>where \(p\) is the probability of \(y_1\) <strong>being better than</strong> \(y_2\) given \(x\) representing the true human preferences and \(r\) is the reward function.
\(y_1\) and \(y_2\) are the two items/responses being compared given \(x\) representing the input.</p>

<p>If \(r^*(x,y_1) &gt; r^*(x,y_2)\), then \(p^*(y_1 \succ y_2 \mid x) &gt; 0.5\), which means \(y_1\) is more likely to be better than \(y_2\) given \(x\).</p>

<p>To learn the reward function \(r\) from the data, we parameterize as a neural network \(r_{\phi}\) and optimize the following objective function:</p>

\[\mathcal{L}_{R}(r_{\phi}, \mathcal{D}) = - \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma (exp(r_{\phi}(x,y_w)) - exp(r_{\phi}(x,y_l))) \right]\]

<p>where \(\mathcal{D}\) is the dataset of human preferences and \(y_w\) and \(y_l\) are the winning and losing responses against the same input \(x\).
Minimizing the above objective function is equivalent to maximizing the probability of the winning response being better than the losing response given the input.</p>

<p>Note that the reward function \(r_{\phi}\) is usually initialized from the supervised fine-tuning (SFT) model same as the policy model.</p>

<p><strong>Fine-tuning the policy model</strong></p>

<p>Once we have the reward model \(r_{\phi}\), we can fine-tune the policy model \(\theta\) using the following objective function:</p>

\[\mathcal{L}_{P}(\theta, \mathcal{D}) = - \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \left[ r_{\phi}(x,y) + \beta D_{KL}(\pi_{\theta}(y \mid x) || \pi_{ref}(y \mid x)) \right]\]

<p>where \(\pi_{ref}\) is the reference model and \(\beta\) is a hyperparameter.
Minimizing the first term enforces the policy model to generate responses that are more preferred by the reward model,
while minimizing the second term ensures that the policy model does not deviate too much from the reference model.</p>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Aligns the model with human preferences.</li>
  <li>Reduces harmful or inappropriate responses.</li>
  <li>Improves usability in real-world scenarios (e.g., chatbot interactions).</li>
</ul>

<p><strong>Limitations:</strong></p>

<ul>
  <li>Requires expensive and time-consuming human feedback.</li>
  <li>May introduce biases based on the preferences of the feedback providers.</li>
  <li>Balancing alignment with generalization is challenging.</li>
</ul>

<h4 id="direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</h4>

<p>DPO simplifies PPO by directly optimizing the policy model to adhere to human preferences without the need for a reward model.</p>

<p><strong>Objective Function:</strong></p>

\[\pi_r(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp(r(x,y)/\beta)\]

<p>where \(\pi_r\) is the policy model, \(\pi_{\text{ref}}\) is the reference model, \(r\) is the reward function, and \(\beta\) is a hyperparameter.</p>

\[r(x,y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \log Z(x)\]

<p>where \(Z(x)\) is the partition function.</p>

<p>The final objective function is:</p>

\[\mathcal{L}_{DPO}(\theta, \mathcal{D}) = - \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma (\pi_{\theta}(x,y_w) - \pi_{\theta}(x,y_l)) \right]\]

<p>Where there is no need for the reward model \(r_{\phi}\) as the policy model \(\pi_{\theta}\) is directly optimized to adhere to human preferences.
Minimizing the above objective function is directly maximizing the probability of the winning response while minimizing the probability of the losing response.</p>

<h3 id="sft-vs-rlhf">SFT vs RLHF</h3>

<p>Reinforcement Learning from Human Feedback (RLHF) is a technique used to align large language models (LLMs) more closely with human preferences and expectations. While supervised fine-tuning (SFT) is a critical step in improving a model‚Äôs capabilities, it has limitations that RLHF can address. In short, RLHF is needed when there is <strong>difficulty to define appropriate objective functions</strong> that to align with something ambiguous such as human preferences. However, recently DeepSeek has proposed a new RL method called GRPO (Group Relative Policy Optimization) that can be used to train reasoning models without the need for pair of wins and losses. Read more about it in <a href="https://tuananhbui89.github.io/blog/2025/deepseek/">my DeepSeek blog post</a>.</p>

<h4 id="when-is-rlhf-needed">When is RLHF needed?</h4>

<p><strong>Ambiguity in Objectives</strong></p>

<ul>
  <li>SFT aligns the model with a dataset of ‚Äúcorrect‚Äù outputs, but human preferences often involve subjective judgment or context-specific nuance that cannot be fully captured in a static dataset.</li>
  <li>Example: Chatbots generating empathetic or polite responses where the tone and context matter significantly.</li>
</ul>

<p><strong>Unclear or Complex Evaluation Metrics</strong></p>

<ul>
  <li>For some tasks, it‚Äôs difficult to define explicit evaluation metrics, but humans can intuitively judge quality.</li>
  <li>Example: Creative writing or generating humorous content.</li>
</ul>

<p><strong>Long-Term or Multi-Step Reasoning</strong></p>

<ul>
  <li>SFT trains models to produce correct outputs based on immediate context but might fail in scenarios requiring multi-step decision-making or long-term coherence.</li>
  <li>Example: Writing a coherent multi-paragraph essay or guiding a user through a series of troubleshooting steps.</li>
</ul>

<p><strong>Avoiding Harmful Outputs</strong></p>

<ul>
  <li>Static datasets used for SFT may not include all edge cases or potential pitfalls, and RLHF can help refine the model to avoid harmful or toxic outputs based on human preferences.</li>
  <li>Example: Ensuring a conversational agent avoids generating offensive or biased content.</li>
</ul>

<p><strong>Improving User Experience</strong></p>

<ul>
  <li>SFT often focuses on correctness, but RLHF can optimize for user satisfaction, such as balancing informativeness, politeness, and conciseness.</li>
  <li>Example: Personal assistants generating concise and helpful responses tailored to user needs.</li>
</ul>

<h4 id="why-is-supervised-fine-tuning-not-enough">Why is Supervised Fine-Tuning Not Enough?</h4>

<p><strong>Static Nature of Datasets</strong></p>

<ul>
  <li>SFT relies on pre-collected datasets that might not represent all real-world scenarios or evolving user preferences.</li>
  <li>Limitation: If the dataset lacks certain examples, the model cannot generalize well.</li>
</ul>

<p><strong>Difficulty in Capturing Preferences</strong></p>

<ul>
  <li>Human preferences are often complex and not directly labeled in datasets.</li>
  <li>Limitation: A model trained on SFT might produce technically correct but undesirable outputs (e.g., overly verbose or lacking empathy).</li>
</ul>

<p><strong>Overfitting to Training Data</strong></p>

<ul>
  <li>SFT can lead to overfitting, where the model learns to replicate the training data without adapting to unseen scenarios.</li>
  <li>Limitation: This can result in poor performance on out-of-distribution examples.</li>
</ul>

<p><strong>Reward Optimization</strong></p>

<ul>
  <li>RLHF optimizes a reward function designed to capture human preferences, which allows fine-grained control over the model‚Äôs behavior.</li>
  <li>Limitation: SFT does not involve direct optimization based on human evaluations.</li>
</ul>

<h2 id="lora-and-adapters">LoRA and Adapters</h2>

<p>References:</p>

<ul>
  <li>[1] Houlsby, Neil, et al. ‚ÄúParameter-efficient transfer learning for NLP.‚Äù <a href="https://arxiv.org/abs/1902.00751" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/1902.00751</a>
</li>
  <li>[2] Hu et al. ‚ÄúLoRA: Low-Rank Adaptation of Large Language Models.‚Äù <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2106.09685</a>
</li>
</ul>

<h3 id="lora">LoRA</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    LoRA Diagram. Image from [2].
</div>

<h3 id="adapters">Adapters</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*Z2FMWTCmdkgevHr--480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*Z2FMWTCmdkgevHr--800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*Z2FMWTCmdkgevHr--1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*Z2FMWTCmdkgevHr-.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Adapter Diagram. Image from [1].
</div>

<h2 id="prompt-engineering">Prompt Engineering</h2>

<p>References:</p>

<ul>
  <li>[1] <a href="https://www.promptingguide.ai/" rel="external nofollow noopener" target="_blank">https://www.promptingguide.ai</a>
</li>
  <li>[2] <a href="https://learnprompting.org/docs/advanced/introduction" rel="external nofollow noopener" target="_blank">https://learnprompting.org/docs/advanced/introduction</a>
</li>
</ul>

<h3 id="zero-shot-and-few-shot-learning">Zero-shot and Few-shot Learning</h3>

<h3 id="chain-of-thought">Chain of Thought</h3>

<p><strong>What is Chain of Thought?</strong> Chain of Thought (CoT) is a technique that allows LLMs to reason step by step. It can be done by adding intermediate reasoning steps in addition to the final answer to guide the model to break down a complex problem into smaller steps (Few-shot CoT). It is also possible to use Zero-shot CoT by adding a prompt ‚ÄúLet‚Äôs think step by step‚Äù to the input.</p>

<p>There are several advanced CoT methods such as Auto-CoT or Tree of Thoughts (ToT), addressing the limitations of CoT from different perspectives.
For example, when applying CoT prompting with demonstrations, the process involves <strong>hand-crafting effective and diverse examples</strong>, which is time-consuming and not scalable.
Auto-CoT proposes an automatic way to generate reasoning chains for demonstrations by leveraging other LLMs with Zero-Shot-CoT (‚ÄúLet‚Äôs think step by step‚Äù).
However, the generated chains are not guaranteed to be optimal and may contain errors.
To address this, Auto-CoT proposes a two-stage process aiming to generate diverse reasoning chains for each question.</p>

<ul>
  <li>Stage 1: partition questions into clusters</li>
  <li>Stage 2: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fauto-cot.642d9bad&amp;w=3840&amp;q=75-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fauto-cot.642d9bad&amp;w=3840&amp;q=75-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fauto-cot.642d9bad&amp;w=3840&amp;q=75-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fauto-cot.642d9bad.png&amp;w=3840&amp;q=75" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Auto-CoT. Image from [1].
</div>

<h3 id="meta-prompting">Meta Prompting</h3>

<p>A.k.a. Advanced Zero-Shot-CoT. Provide the <strong>format and pattern</strong> of problems and solutions rather than a specific content/example as in CoT. Uses syntax as a guiding template for the expected response or solution.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Meta Prompting. Image from [1].
</div>

<h3 id="self-consistency">Self-Consistency</h3>

<p>Idea: Sample multiple reasoning chains and select the most consistent one as the final answer.</p>

<p>Prompt with CoT examples:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,
there will be 21 trees. How many trees did the grove workers plant today?
A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.
So, they must have planted 21 - 15 = 6 trees. The answer is 6.

Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.

Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and Leah's sister had 42. That means there were originally 32 + 42 = 74
chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.

Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops
did Jason give to Denny?
A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of
lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.

Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does
he have now?
A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so
in total he has 7 + 2 = 9 toys. The answer is 9.

Q: There were nine computers in the server room. Five more computers were installed each day, from
monday to thursday. How many computers are now in the server room?
A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 <span class="err">*</span> 5 =
20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.
The answer is 29.

Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many
golf balls did he have at the end of wednesday?
A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On
Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.

Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.

Q: When I was 6 my sister was half my age. Now I'm 70 how old is my sister?
A:
</code></pre></div></div>

<p>Output 1:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.
</code></pre></div></div>

<p>Output 2:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.
</code></pre></div></div>

<p>Output 3:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70/2 = 35. The answer is 35.
</code></pre></div></div>

<p><strong>Question: How to get multiple outputs?</strong></p>

<p><strong>Introduce Randomness</strong>: Use randomness in the generation process to obtain diverse outputs. This can be done by:</p>

<ul>
  <li>Temperature Settings: Set a higher temperature (e.g., 0.7 or above) to encourage more variability in responses.</li>
  <li>Top-k or Top-p Sampling: Adjust these parameters to allow exploration of a broader range of plausible outputs.</li>
</ul>

<p><strong>Generate Multiple Outputs</strong>: Generate multiple outputs by sampling multiple times from the model using the same prompt. For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">openai</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Explain the significance of photosynthesis in plants.</span><span class="sh">"</span>
<span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>  <span class="c1"># Generate 10 outputs
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">Completion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">engine</span><span class="o">=</span><span class="sh">"</span><span class="s">text-davinci-003</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
        <span class="n">n</span><span class="o">=</span><span class="mi">1</span>  <span class="c1"># Single output per call
</span>    <span class="p">)</span>
    <span class="n">responses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span>
</code></pre></div></div>

<p><strong>Question: How to aggregate and analyze outputs?</strong></p>

<p>Review the generated outputs and identify consistent patterns or common elements. This can be done manually or using automated techniques such as:</p>

<ul>
  <li>
<strong>Majority Voting</strong>: Identify the most frequently occurring response.</li>
  <li>
<strong>Semantic Similarity</strong>: Use clustering or similarity scoring to group similar responses.</li>
</ul>

<h3 id="generated-knowledge-prompting">Generated Knowledge Prompting</h3>

<p>Idea: Use LLM to generate knowledge and then incorporate that knowledge into the prompt to help the model make more accurate predictions.</p>

<p>Examples:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: Greece is larger than mexico.
Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.
Input: Glasses always fog up.
Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.
Input: A fish is capable of thinking.
Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of 'higher' vertebrates including non-human primates. Fish's long-term memories help them keep track of complex social relationships.
Input: A common effect of smoking lots of cigarettes in one's lifetime is a higher than normal chance of getting lung cancer.
Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.
Input: A rock is the same size as a pebble.
Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).
Input: Part of golf is trying to get a higher point total than others.
Knowledge:
</code></pre></div></div>

<p>Knowledge 1:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.
</code></pre></div></div>

<p>Knowledge 2:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.
</code></pre></div></div>

<p><strong>The next step is to integrate the knowledge and get a prediction</strong></p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Question: Part of golf is trying to get a higher point total than others. Yes or No?
Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.
Explain and Answer: 
</code></pre></div></div>

<p><strong>Note</strong>: As mentioned in [1], the answer with Knowledge 1 is with a very high confidence while that with Knowledge 2 is very low one. Interestingly, Knowledge 2 is directly answering the question (The player with the lowest score wins the game.) while Knowledge 1 is not.</p>

<h3 id="prompt-chaining">Prompt Chaining</h3>

<p>Idea: Break down a complex task into multiple simpler tasks and use LLM to solve each task. Each step involves a specific prompt and the output of the previous step is used as the input for the next step.</p>

<p><strong>Example Use Case: Writing an Essay</strong></p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prompt 1: Brainstorm Topics
<span class="p">
-</span> "List five unique topics for an essay about the benefits of renewable energy."
<span class="p">-</span> Output: ["Solar energy in urban areas", "Wind energy for rural development", etc.]

Prompt 2: Develop an Outline
<span class="p">
-</span> Input: "Create a detailed outline for an essay on 'Solar energy in urban areas.'"
<span class="p">-</span> Output: Introduction, Benefits of Solar Energy, Implementation Challenges, Conclusion.

Prompt 3: Write the Introduction
<span class="p">
-</span> Input: "Write an engaging introduction for an essay on 'Solar energy in urban areas.'"
<span class="p">-</span> Output: A well-crafted opening paragraph.

Prompt 4: Complete Sections
<span class="p">
-</span> Repeat the process for other sections using tailored prompts.
</code></pre></div></div>

<h3 id="tree-of-thoughts">Tree of Thoughts</h3>

<p>Idea: Unlike Self-Consistency CoT when each reasoning path is independent and the final answer is voting/aggregating at the final step, Tree of Thoughts (ToT) explores multiple reasoning paths at each step and selects the most promising step to continue.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e&amp;w=1200&amp;q=75-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e&amp;w=1200&amp;q=75-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e&amp;w=1200&amp;q=75-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&amp;w=1200&amp;q=75" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Tree of Thoughts. Image from [1].
</div>

<p>Examples (from ToT repos: <a href="https://github.com/princeton-nlp/tree-of-thought-llm/blob/master/src/tot/prompts/text.py" rel="external nofollow noopener" target="_blank">https://github.com/princeton-nlp/tree-of-thought-llm/blob/master/src/tot/prompts/</a>).
You can also find more examples in this blog post <a href="https://learnprompting.org/docs/advanced/decomposition/tree_of_thoughts" rel="external nofollow noopener" target="_blank">https://learnprompting.org/docs/advanced/decomposition/tree_of_thoughts</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">standard_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input}
</span><span class="sh">'''</span>

<span class="n">cot_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input}

Make a plan then write. Your output should be of the following format:

Plan:
Your plan here.

Passage:
Your passage here.
</span><span class="sh">'''</span>


<span class="n">vote_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">Given an instruction and several choices, decide which choice is most promising. Analyze each choice in detail, then conclude in the last line </span><span class="sh">"</span><span class="s">The best choice is {s}</span><span class="sh">"</span><span class="s">, where s the integer id of the choice.
</span><span class="sh">'''</span>

<span class="n">compare_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">Briefly analyze the coherency of the following two passages. Conclude in the last line </span><span class="sh">"</span><span class="s">The more coherent passage is 1</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">The more coherent passage is 2</span><span class="sh">"</span><span class="s">, or </span><span class="sh">"</span><span class="s">The two passages are similarly coherent</span><span class="sh">"</span><span class="s">.
</span><span class="sh">'''</span>

<span class="n">score_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">Analyze the following passage, then at the last line conclude </span><span class="sh">"</span><span class="s">Thus the coherency score is {s}</span><span class="sh">"</span><span class="s">, where s is an integer from 1 to 10.
</span><span class="sh">'''</span>
</code></pre></div></div>

<h3 id="mixture-of-reasoning-experts">Mixture of Reasoning Experts</h3>

<p>MoRE leverages a pool of specialized experts, where each expert is optimized for a distinct reasoning type, such as:</p>

<ul>
  <li>Factual reasoning (e.g., fact-based questions).</li>
  <li>Multihop reasoning (e.g., questions that require multiple steps of reasoning).</li>
  <li>Mathematical reasoning (e.g., solving math word problems).</li>
  <li>Commonsense reasoning (e.g., questions requiring implicit knowledge).</li>
</ul>

<p>MoRE uses an answer selector to choose the best response based on predictions from the specialized experts. If the system detects that none of the answers are reliable, it can abstain from answering. Another key feature of MoRE is its ability to abstain from answering when it‚Äôs unsure, improving the system‚Äôs reliability.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://learnprompting.org/docs/assets/advanced/more_2.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://learnprompting.org/docs/assets/advanced/more_2.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://learnprompting.org/docs/assets/advanced/more_2.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://learnprompting.org/docs/assets/advanced/more_2.svg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Experts in Mixture-of-Reasoning Experts. Image from [2].
</div>

<h3 id="how-to-create-a-prompt">How to create a prompt</h3>

<p>To me, the most important part of prompt engineering is to understand the task, the data, and know which prompt techniques to apply, but not how to create a prompt because it is just a piece of text.
However, there are some tools that can help you create a prompt, providing you several common templates.
For example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PipelinePromptTemplate</span><span class="p">,</span> <span class="n">PromptTemplate</span>

<span class="n">full_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">{introduction}

{example}

{start}</span><span class="sh">"""</span>
<span class="n">full_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">full_template</span><span class="p">)</span>

<span class="n">introduction_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are impersonating {person}.</span><span class="sh">"""</span>
<span class="n">introduction_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">introduction_template</span><span class="p">)</span>

<span class="n">example_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Here</span><span class="sh">'</span><span class="s">s an example of an interaction:

Q: {example_q}
A: {example_a}</span><span class="sh">"""</span>
<span class="n">example_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">example_template</span><span class="p">)</span>

<span class="n">start_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Now, do this for real!

Q: {input}
A:</span><span class="sh">"""</span>
<span class="n">start_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">start_template</span><span class="p">)</span>

<span class="n">input_prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">introduction</span><span class="sh">"</span><span class="p">,</span> <span class="n">introduction_prompt</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">example</span><span class="sh">"</span><span class="p">,</span> <span class="n">example_prompt</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">start</span><span class="sh">"</span><span class="p">,</span> <span class="n">start_prompt</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">pipeline_prompt</span> <span class="o">=</span> <span class="nc">PipelinePromptTemplate</span><span class="p">(</span>
    <span class="n">final_prompt</span><span class="o">=</span><span class="n">full_prompt</span><span class="p">,</span> <span class="n">pipeline_prompts</span><span class="o">=</span><span class="n">input_prompts</span>
<span class="p">)</span>

<span class="n">pipeline_prompt</span><span class="p">.</span><span class="n">input_variables</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span>
    <span class="n">pipeline_prompt</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
        <span class="n">person</span><span class="o">=</span><span class="sh">"</span><span class="s">Elon Musk</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">example_q</span><span class="o">=</span><span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="s">s your favorite car?</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">example_a</span><span class="o">=</span><span class="sh">"</span><span class="s">Tesla</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="s">s your favorite social media site?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">You</span> <span class="n">are</span> <span class="n">impersonating</span> <span class="n">Elon</span> <span class="n">Musk</span><span class="p">.</span>

<span class="n">Here</span><span class="sh">'</span><span class="s">s an example of an interaction:

Q: What</span><span class="sh">'</span><span class="n">s</span> <span class="n">your</span> <span class="n">favorite</span> <span class="n">car</span><span class="err">?</span>
<span class="n">A</span><span class="p">:</span> <span class="n">Tesla</span>

<span class="n">Now</span><span class="p">,</span> <span class="n">do</span> <span class="n">this</span> <span class="k">for</span> <span class="n">real</span><span class="err">!</span>

<span class="n">Q</span><span class="p">:</span> <span class="n">What</span><span class="sh">'</span><span class="s">s your favorite social media site?
A:
</span></code></pre></div></div>

<h2 id="code-and-frameworks-for-llms">Code and Frameworks for LLMs</h2>

<h3 id="langchain">LangChain</h3>

<h4 id="prompt-templates">Prompt Templates</h4>

<p>Purpose:</p>

<ul>
  <li>
<strong>Standardize prompts</strong>: Ensure consistency across different prompts.</li>
  <li>
<strong>Parameterize prompts</strong>: Allow for easy modification of prompts.</li>
  <li>
<strong>Chain prompts</strong>: Combine multiple prompts into a single prompt.</li>
</ul>

<p>Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Translate the following text to French: {text}</span><span class="sh">"</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sh">"</span><span class="s">Hello, how are you?</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="chains">Chains</h4>

<p>Purpose:</p>

<ul>
  <li>Combine multiple steps or functions into a cohesive pipeline</li>
</ul>

<p>Key Functions:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">LLMChain</code>: Simplest chain, combining a prompt with an LLM.</li>
  <li>
<code class="language-plaintext highlighter-rouge">SequentialChain</code>: Execute multiple chains sequentially, passing outputs as inputs.</li>
  <li>
<code class="language-plaintext highlighter-rouge">RouterChain</code>: Routes user input to specific chains based on conditions</li>
</ul>

<p>Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="sh">"</span><span class="s">What is a good name for a company that makes {product}?</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">product</span><span class="sh">"</span><span class="p">])</span>
<span class="n">chain</span> <span class="o">=</span> <span class="nc">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chain</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">product</span><span class="o">=</span><span class="sh">"</span><span class="s">colorful socks</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="memory">Memory</h4>

<p>Purpose:</p>

<ul>
  <li>
<strong>Store and retrieve information</strong>: Allow chains to remember previous outputs.</li>
  <li>
<strong>Chain of Thought</strong>: Help chains reason through complex problems.</li>
</ul>

<p>Key Functions:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">ConversationBufferMemory</code>: Stores entire conversation history..</li>
  <li>
<code class="language-plaintext highlighter-rouge">ConversationSummaryMemory</code>: Summarizes past interactions for compact memory.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferMemory</span>

<span class="n">memory</span> <span class="o">=</span> <span class="nc">ConversationBufferMemory</span><span class="p">()</span>
<span class="n">memory</span><span class="p">.</span><span class="nf">save_context</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">},</span> <span class="p">{</span><span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi there!</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="n">memory</span><span class="p">.</span><span class="nf">load_memory_variables</span><span class="p">({}))</span>
</code></pre></div></div>

<h4 id="agents">Agents</h4>

<p>Purpose:</p>

<ul>
  <li>
<strong>Provide additional tools</strong>: Use tools to perform tasks that are not directly supported by the LLM.</li>
</ul>

<p>Key Functions:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">initialize_agent</code>: Initialize an agent with a set of tools and an LLM. There are several predefined tools like <code class="language-plaintext highlighter-rouge">Wikipedia</code>, <code class="language-plaintext highlighter-rouge">Search</code>, and <code class="language-plaintext highlighter-rouge">Calculator</code>.</li>
  <li>
<code class="language-plaintext highlighter-rouge">run</code>: Execute the agent with a given input.</li>
</ul>

<p>Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.agents</span> <span class="kn">import</span> <span class="n">initialize_agent</span><span class="p">,</span> <span class="n">Tool</span>
<span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">Tool</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">Calculator</span><span class="sh">"</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">eval</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Calculates basic arithmetic.</span><span class="sh">"</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">agent</span> <span class="o">=</span> <span class="nf">initialize_agent</span><span class="p">(</span><span class="n">tools</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">agent</span><span class="o">=</span><span class="sh">"</span><span class="s">zero-shot-react-description</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="sh">"</span><span class="s">What is 5 plus 7?</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="tools">Tools</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://python.langchain.com/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://python.langchain.com/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://python.langchain.com/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://python.langchain.com/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Tool calling. Image from LangChain.
</div>

<p>Purpose:</p>

<ul>
  <li>Extend functionality by integrating APIs or utilities</li>
  <li>Tools can be passed to chat models that support tool calling allowing the model to request the execution of a specific function with specific inputs.</li>
</ul>

<p><strong>Create tools using the @tool decorator</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>

<span class="nd">@tool</span>
<span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
   <span class="sh">"""</span><span class="s">Multiply two numbers.</span><span class="sh">"""</span>
   <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>

<span class="nf">print</span><span class="p">(</span><span class="n">multiply</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">}))</span>
</code></pre></div></div>

<p><strong>RunnableConfig</strong></p>

<p>You can use the <code class="language-plaintext highlighter-rouge">RunnableConfig</code> object to pass custom run time values to tools.</p>

<p>If you need to access the <code class="language-plaintext highlighter-rouge">RunnableConfig</code> object from within a tool. This can be done by using the <code class="language-plaintext highlighter-rouge">RunnableConfig</code> annotation in the tool‚Äôs function signature.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>

<span class="nd">@tool</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">some_func</span><span class="p">(...,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">...:</span>
    <span class="sh">"""</span><span class="s">Tool that does something.</span><span class="sh">"""</span>
    <span class="c1"># do something with config
</span>    <span class="bp">...</span>

<span class="k">await</span> <span class="n">some_func</span><span class="p">.</span><span class="nf">ainvoke</span><span class="p">(...,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">configurable</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">value</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">some_value</span><span class="sh">"</span><span class="p">}})</span>
</code></pre></div></div>

<p><strong>Search for tools</strong></p>

<p>You can search for tools that are available in LangChain in the <a href="https://python.langchain.com/docs/integrations/tools/#search" rel="external nofollow noopener" target="_blank">LangChain documentation</a>.
Some free tools are:</p>

<ul>
  <li>
<a href="https://python.langchain.com/docs/integrations/tools/ddg/" rel="external nofollow noopener" target="_blank">DuckDuckGo Search</a> that allows you to search the web and return URL, snippet, and title.</li>
  <li>
<a href="https://python.langchain.com/docs/integrations/tools/wikipedia/" rel="external nofollow noopener" target="_blank">Wikipedia</a> that allows you to search Wikipedia and return a summary of the page.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.tools</span> <span class="kn">import</span> <span class="n">WikipediaQueryRun</span>
<span class="kn">from</span> <span class="n">langchain_community.utilities</span> <span class="kn">import</span> <span class="n">WikipediaAPIWrapper</span>

<span class="n">wikipedia</span> <span class="o">=</span> <span class="nc">WikipediaQueryRun</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="nc">WikipediaAPIWrapper</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="n">wikipedia</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">LangChain</span><span class="sh">"</span><span class="p">}))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.tools</span> <span class="kn">import</span> <span class="n">DuckDuckGoSearchResults</span>

<span class="n">duckduckgo</span> <span class="o">=</span> <span class="nc">DuckDuckGoSearchResults</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">duckduckgo</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">LangChain</span><span class="sh">"</span><span class="p">}))</span>
</code></pre></div></div>

<h3 id="llamaindex">LlamaIndex</h3>

<h2 id="fine-tuning-llms">Fine-tuning LLMs</h2>

<p>References:</p>

<ul>
  <li>[1] <a href="https://www.datacamp.com/tutorial/fine-tuning-llama-2" rel="external nofollow noopener" target="_blank">https://www.datacamp.com/tutorial/fine-tuning-llama-2</a>
</li>
</ul>

<p>In the following snippet, we will fine-tune a Llama 2 model using QLoRA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">BitsAndBytesConfig</span><span class="p">,</span>
    <span class="n">TrainingArguments</span><span class="p">,</span>
    <span class="n">pipeline</span><span class="p">,</span>
    <span class="n">logging</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span>
<span class="kn">from</span> <span class="n">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span>
</code></pre></div></div>

<p>Loading Llama 2 model with 4-bit quantization:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compute_dtype</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sh">"</span><span class="s">float16</span><span class="sh">"</span><span class="p">)</span>

<span class="n">quant_config</span> <span class="o">=</span> <span class="nc">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="sh">"</span><span class="s">nf4</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">base_model</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="p">{</span><span class="sh">""</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">pretraining_tp</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>Setting up the PEFT LoRA configuration. The <code class="language-plaintext highlighter-rouge">lora_alpha</code> parameter controls the scaling of the LoRA weights, while <code class="language-plaintext highlighter-rouge">r</code> is the rank of the low-rank adaptation matrices. Parameters for LoraConfig can be found in Hugging Face documentation <a href="https://huggingface.co/docs/peft/main/en/conceptual_guides/lora" rel="external nofollow noopener" target="_blank">link</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">peft_params</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="sh">"</span><span class="s">CAUSAL_LM</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Setting up the Trainer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span> <span class="o">=</span> <span class="nc">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">peft_config</span><span class="o">=</span><span class="n">peft_params</span><span class="p">,</span>
    <span class="n">dataset_text_field</span><span class="o">=</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_params</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="rag">RAG</h2>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">The Illustrated Transformer</a></li>
  <li><a href="https://nlp.seas.harvard.edu/annotated-transformer/" rel="external nofollow noopener" target="_blank">The Annotated Transformer</a></li>
  <li><a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention is All You Need</a></li>
  <li><a href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models" rel="external nofollow noopener" target="_blank">Hands-on LLMs</a></li>
</ul>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">The Foundations and Frontiers of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cs336-lec14/">CS336 Lecture 14 - Data - Part 2</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
