<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Foundations of Machine Learning | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="How to break into $100k+ salary roles">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/ml-foundation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Foundations of Machine Learning</h1>
    <p class="post-meta">January 14, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#machine-learning-paradigms">Machine Learning Paradigms</a>
<ul>
<li class="toc-entry toc-h3"><a href="#supervised-learning">Supervised learning</a></li>
<li class="toc-entry toc-h3">
<a href="#unsupervised-learning">Unsupervised learning</a>
<ul>
<li class="toc-entry toc-h4"><a href="#self-supervised-learning">Self-supervised learning</a></li>
<li class="toc-entry toc-h4"><a href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
<li class="toc-entry toc-h3"><a href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#machine-learning-settings">Machine Learning Settings</a>
<ul>
<li class="toc-entry toc-h3"><a href="#representation-learning">Representation learning</a></li>
<li class="toc-entry toc-h3"><a href="#continual-learning">Continual learning</a></li>
<li class="toc-entry toc-h3"><a href="#meta-learning">Meta-learning</a></li>
<li class="toc-entry toc-h3"><a href="#transfer-learning">Transfer learning</a></li>
<li class="toc-entry toc-h3"><a href="#multi-task-learning">Multi-task learning</a></li>
<li class="toc-entry toc-h3">
<a href="#domain-adaptation">Domain adaptation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#unsupervised-domain-adaptation">Unsupervised domain adaptation</a></li>
<li class="toc-entry toc-h4"><a href="#supervised-domain-adaptation">Supervised domain adaptation</a></li>
<li class="toc-entry toc-h4"><a href="#challenges">Challenges</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#domain-generalization">Domain generalization</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#optimization">Optimization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#gradient-descent">Gradient descent</a></li>
<li class="toc-entry toc-h3"><a href="#newtons-method">Newton’s method</a></li>
<li class="toc-entry toc-h3"><a href="#fisher-information">Fisher information</a></li>
<li class="toc-entry toc-h3"><a href="#hessian-matrix">Hessian matrix</a></li>
<li class="toc-entry toc-h3"><a href="#stochastic-gradient-descent">Stochastic gradient descent</a></li>
<li class="toc-entry toc-h3"><a href="#adam-rmsprop-adagrad">Adam, RMSProp, AdaGrad</a></li>
<li class="toc-entry toc-h3"><a href="#bayesian-optimization">Bayesian Optimization</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#bias-variance">Bias-Variance</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-bias-variance-tradeoff">The bias-variance tradeoff</a></li>
<li class="toc-entry toc-h3">
<a href="#overcoming-overfitting">Overcoming overfitting</a>
<ul>
<li class="toc-entry toc-h4"><a href="#l1-vs-l2-regularization">L1 vs L2 regularization</a></li>
<li class="toc-entry toc-h4"><a href="#dropout">Dropout</a></li>
<li class="toc-entry toc-h4"><a href="#early-stopping">Early stopping</a></li>
<li class="toc-entry toc-h4"><a href="#batch-normalizationlayer-normalization">Batch normalization/layer normalization</a></li>
<li class="toc-entry toc-h4"><a href="#data-augmentation">Data augmentation</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#cross-validation">Cross-validation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#generalization">Generalization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#out-of-distribution-generalization">Out-of-distribution generalization</a></li>
<li class="toc-entry toc-h3"><a href="#adversarial-generalizationtraining">Adversarial generalization/training</a></li>
<li class="toc-entry toc-h3"><a href="#sharpness-aware-minimization">Sharpness-aware minimization</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#evaluation">Evaluation</a>
<ul>
<li class="toc-entry toc-h3">
<a href="#metrics">Metrics</a>
<ul>
<li class="toc-entry toc-h4"><a href="#type-i-and-type-ii-error">Type I and Type II error</a></li>
<li class="toc-entry toc-h4"><a href="#roc-curve">ROC curve</a></li>
<li class="toc-entry toc-h4"><a href="#precision-recall-curve">Precision-recall curve</a></li>
<li class="toc-entry toc-h4"><a href="#f1-score">F1 score</a></li>
<li class="toc-entry toc-h4"><a href="#auc">AUC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#generative-models">Generative models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#generative-adversarial-networks-gans">Generative adversarial networks (GANs)</a></li>
<li class="toc-entry toc-h3"><a href="#variational-autoencoders-vaes">Variational autoencoders (VAEs)</a></li>
<li class="toc-entry toc-h3"><a href="#flow-matching">Flow matching</a></li>
<li class="toc-entry toc-h3"><a href="#diffusion-models">Diffusion models</a></li>
</ul>
</li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <h2 id="machine-learning-paradigms">Machine Learning Paradigms</h2>

<h3 id="supervised-learning">Supervised learning</h3>

<p>Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning that the input data is paired with the correct output. The model learns to make predictions based on this labeled data.</p>

<p>Mathematically, given a training set of \(N\) samples:</p>

\[\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\]

<p>where \(x_i \in \mathbb{R}^d\) is the input feature vector with \(d\) dimensions. The corresponding label \(y_i\) can be either a continuous value \(y_i \in \mathbb{R}\) for regression problems, or a categorical value \(y_i \in \{0, 1, \ldots, K-1\}\) for classification problems.</p>

<p>the goal of supervised learning is to learn a function \(f_{\theta}\) parameterized by \(\theta\) that maps input features \(x\) to output labels \(y\), i.e., \(\hat{y} \triangleq f_{\theta}(x) \approx y\).</p>

<p>It can be done by minimizing the loss function - which measures the discrepancy between the predicted output \(\hat{y}\) and the true output \(y\) over the training set, i.e., \(\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{l}(y_i, f_{\theta}(x_i))\).</p>

<p>Common loss functions:</p>

<ul>
  <li>Mean squared error (MSE) for regression problems:</li>
</ul>

\[\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} (y_i - f_{\theta}(x_i))^2\]

<ul>
  <li>Cross-entropy loss for classification problems:</li>
</ul>

\[\mathcal{L}(\theta) = - \frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log \hat{y}_{ik}\]

<p>where \(y_{ik}\) is the one-hot encoded label for the \(i\)-th sample and the \(k\)-th class, and \(\hat{y}_{ik}\) is the predicted probability of the \(i\)-th sample for the \(k\)-th class.</p>

<p>More advanced topics which are discussed later in this post:</p>

<ul>
  <li>Advanced loss functions</li>
  <li>Regularization</li>
  <li>Bias-variance tradeoff</li>
  <li>Out-of-distribution generalization</li>
  <li>Adversarial generalization/training</li>
  <li>Sharpness-aware minimization</li>
</ul>

<h3 id="unsupervised-learning">Unsupervised learning</h3>

<p>Unsupervised learning is a machine learning paradigm where a model learns patterns, structures, or representations from unlabeled data without explicit supervision.</p>

<p>Unlike supervised learning, there is no unified mathematical formulation for unsupervised learning, instead, it is a collection of different techniques/problems that can be grouped into following categories:</p>

<p><strong>Dimensionality reduction/representation learning</strong>: Finding a low-dimensional representation of the data that captures the most important information, e.g., to learn a mapping \(f: \mathbb{R}^d \rightarrow \mathbb{R}^k\) where \(k \ll d\).
Common methods include Principal Component Analysis (PCA), t-SNE, and Autoencoders.</p>

<p><strong>Clustering</strong>: Finding groups of similar features, e.g., assigning cluster \(c_i\) to each sample \(x_i\), where \(c_i \in \{1, 2, \ldots, K\}\).
Common methods include K-means, Gaussian Mixture Models (GMM), and Hierarchical Clustering.</p>

<p><strong>Density estimation</strong>: Estimating the probability distribution \(p(x)\) of the data.
Common methods include Parametric models (e.g., Gaussian Mixture Models (GMM), Mixture of Experts (MoE)), Non-parametric models (e.g., Kernel Density Estimation (KDE), Histogram), and Variational Autoencoders (VAEs).</p>

<blockquote>
  <p>Parametric methods are the ones that assume the data follows a known distribution (e.g., Gaussian, MoG, etc.) and characterize by a fixed number of parameters that do not grow with the amount of data.</p>

  <p>Non-parametric methods are the ones that do not make any assumptions about the distribution of the data. The number of parameters can grow with more data. It provides greater flexibility but requires more data/computational expense.</p>
</blockquote>

<p><strong>Gaussian Mixture Model</strong> (GMM) models the data as a weighted sum of \(K\) Gaussian distributions:</p>

\[p(x) = \sum_{k=1}^{K} \alpha_k \mathcal{N}(x; \mu_k, \Sigma_k)\]

<p>where \(\alpha_k\) is the mixing coefficient, \(\mu_k\) is the mean, and \(\Sigma_k\) is the covariance matrix of the \(k\)-th Gaussian distribution.
It can be learned using Expectation-Maximization (EM) algorithm, including the E-step and M-step:</p>

<p>E-step: compute the probability that each sample \(x_i\) belongs to each cluster \(k\):</p>

\[p(c_i = k \mid x_i) = \frac{\alpha_k \mathcal{N}(x_i; \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \alpha_j \mathcal{N}(x_i; \mu_j, \Sigma_j)}\]

<p>M-step: update the parameters of the Gaussian distributions to maximize the likelihood of the data:</p>

\[\alpha_k = \frac{1}{N} \sum_{i=1}^{N} p(c_i = k \mid x_i)\]

\[\mu_k = \frac{\sum_{i=1}^{N} p(c_i = k \mid x_i) x_i}{\sum_{i=1}^{N} p(c_i = k \mid x_i)}\]

\[\Sigma_k = \frac{\sum_{i=1}^{N} p(c_i = k \mid x_i) (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} p(c_i = k \mid x_i)}\]

<p>And repeat the E-step and M-step until convergence.</p>

<p><strong>Kernel Density Estimation</strong> (KDE) uses a kernel function to estimate the probability density function of the data.</p>

\[p(x) = \frac{1}{Nh} \sum_{i=1}^{N} K \left( \frac{x - x_i}{h} \right)\]

<p>where \(K\) is the kernel function and \(h\) is the bandwidth to control the smoothness of the estimated density. Common kernel functions include Gaussian, Epanechnikov, and Uniform kernels.</p>

<h4 id="self-supervised-learning">Self-supervised learning</h4>

<h4 id="clustering">Clustering</h4>

<h3 id="semi-supervised-learning">Semi-supervised learning</h3>

<h3 id="reinforcement-learning">Reinforcement learning</h3>

<h2 id="machine-learning-settings">Machine Learning Settings</h2>

<h3 id="representation-learning">Representation learning</h3>

<h3 id="continual-learning">Continual learning</h3>

<h3 id="meta-learning">Meta-learning</h3>

<h3 id="transfer-learning">Transfer learning</h3>

<h3 id="multi-task-learning">Multi-task learning</h3>

<h3 id="domain-adaptation">Domain adaptation</h3>

<h4 id="unsupervised-domain-adaptation">Unsupervised domain adaptation</h4>

<p>Given a set of \(N \leq 1\) source domains \(\{ \mathcal{D}_{S_i} \}_{i=1}^{N}\) each of which is a collection of data-label pairs of domain \(S_i\), i.e., \(\mathcal{D}_{S_i} = \{ (x_{j}, y_{j}) \}_{j=1}^{N_{S_i}}\), \(y_i \in [K] := \{1, 2, \ldots, K\}\), (the set of classes \(K\) is the same for all domains), and one unlabeled target domain \(\mathcal{D}_T=\{x_j\}_{j=1}^{N_T}\), the goal of unsupervised domain adaptation is to learn a classifier \(f_{\theta}\) that can generalize well to the target domain \(\mathcal{D}_T\).</p>

<p>Example:</p>

<ul>
  <li>Source domain: Images of faces from different countries, each country can be considered as a domain with different geographic features.</li>
  <li>Target domain: Images of faces from a specific country that does not have the same geographic features as the source domains and no labels. It can be a rare race or ethnicity.</li>
</ul>

<h4 id="supervised-domain-adaptation">Supervised domain adaptation</h4>

<p>The setting is similar to unsupervised domain adaptation, but the target domain has labels.</p>

<h4 id="challenges">Challenges</h4>

<h3 id="domain-generalization">Domain generalization</h3>

<h2 id="optimization">Optimization</h2>

<h3 id="gradient-descent">Gradient descent</h3>

<h3 id="newtons-method">Newton’s method</h3>

<h3 id="fisher-information">Fisher information</h3>

<h3 id="hessian-matrix">Hessian matrix</h3>

<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>

<h3 id="adam-rmsprop-adagrad">Adam, RMSProp, AdaGrad</h3>

<h3 id="bayesian-optimization">Bayesian Optimization</h3>

<h2 id="bias-variance">Bias-Variance</h2>

<h3 id="the-bias-variance-tradeoff">The bias-variance tradeoff</h3>

<h3 id="overcoming-overfitting">Overcoming overfitting</h3>

<h4 id="l1-vs-l2-regularization">L1 vs L2 regularization</h4>

<h4 id="dropout">Dropout</h4>

<h4 id="early-stopping">Early stopping</h4>

<h4 id="batch-normalizationlayer-normalization">Batch normalization/layer normalization</h4>

<h4 id="data-augmentation">Data augmentation</h4>

<h3 id="cross-validation">Cross-validation</h3>

<h2 id="generalization">Generalization</h2>

<h3 id="out-of-distribution-generalization">Out-of-distribution generalization</h3>

<h3 id="adversarial-generalizationtraining">Adversarial generalization/training</h3>

<h3 id="sharpness-aware-minimization">Sharpness-aware minimization</h3>

<h2 id="evaluation">Evaluation</h2>

<h3 id="metrics">Metrics</h3>

<h4 id="type-i-and-type-ii-error">Type I and Type II error</h4>

<h4 id="roc-curve">ROC curve</h4>

<h4 id="precision-recall-curve">Precision-recall curve</h4>

<h4 id="f1-score">F1 score</h4>

<h4 id="auc">AUC</h4>

<h2 id="generative-models">Generative models</h2>

<h3 id="generative-adversarial-networks-gans">Generative adversarial networks (GANs)</h3>

<h3 id="variational-autoencoders-vaes">Variational autoencoders (VAEs)</h3>

<h3 id="flow-matching">Flow matching</h3>

<h3 id="diffusion-models">Diffusion models</h3>

<p><strong>Gradient descent</strong></p>

<p><strong>Closed form solution of linear regression</strong></p>

\[W = (X^T X)^{-1} X^T y\]

<p><strong>What is logistic regression?</strong></p>

<p>Logistic regression is a statistical model that uses a logistic function to model the probability of a binary response based on one or more predictor variables. It is used for binary classification problems.</p>

<p>Mathematically, the logistic function is defined as:</p>

\[P(Y=1 \mid X) = \text{sigmoid} (W X + b)\]

<p>Where:</p>

<ul>
  <li>\(P(Y=1 \mid X)\) is the probability of the response variable Y being 1 given the input features X.</li>
  <li>\(\text{sigmoid} (z) = \frac{1}{1 + e^{-z}}\) is the sigmoid function, which maps any real-valued number to the range [0, 1].</li>
  <li>\(W\) is the weight matrix and \(b\) is the bias vector. \(X\) has dimension \(n \times m\), where \(n\) is the number of features and \(m\) is the number of instances. \(W\) has dimension \(1 \times n\) and \(b\) has dimension \(1 \times m\).</li>
</ul>

<p><strong>What is the difference between supervised and unsupervised learning?</strong></p>

<p>Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning that the input data is paired with the correct output. The model learns to make predictions based on this labeled data. Unsupervised learning, on the other hand, is a type of machine learning where the model is trained on an unlabeled dataset, meaning that the input data is not paired with the correct output. The model learns to find patterns and structure in the data without explicit guidance.</p>

<p><strong>What is overfitting and how can it be prevented?</strong></p>

<p>Overfitting occurs when a model learns the training data too well, to the point that it performs poorly on new, unseen data. Overfitting can be prevented by using techniques such as cross-validation, regularization, and early stopping.</p>

<p><strong>What is the bias-variance tradeoff?</strong></p>

<p>The bias-variance tradeoff is a fundamental concept in machine learning that refers to the balance between bias and variance in a model. Bias refers to the error introduced by approximating a real-world problem with a simplified model, while variance refers to the error introduced by the model’s sensitivity to small fluctuations in the training data. The tradeoff arises because reducing bias typically increases variance, and vice versa.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Bias-variance tradeoff
</div>

<p>It is worth noting that in the image above, the variance is the spread of the model’s predictions around the mean, not the error on the validation set as we usually see in DL tutorials. The generalization error is the sum of the bias, variance, and irreducible error.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-4-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-4-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-4-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-4.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Validation error
</div>

<p>Mathematically, the generalization error can be decomposed into three components:</p>

\[\text{Generalization error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible error}\]

\[\text{Bias} = \mathbb{E}[\hat{f}(x)] - f(x)\]

\[\text{Variance} = \mathbb{E}[\hat{f}(x) - \mathbb{E}[\hat{f}(x)]]^2\]

<p>Reference: https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54</p>

<p><strong>What is the difference between classification and regression?</strong></p>

<p>Classification is a type of supervised learning where the goal is to predict a discrete class label, such as “spam” or “not spam.” Regression, on the other hand, is a type of supervised learning where the goal is to predict a continuous value, such as a person’s age or the price of a house.</p>

<p><strong>What is the difference between precision and recall?</strong></p>

<p>Precision is the ratio of true positive predictions to the total number of positive predictions, while recall is the ratio of true positive predictions to the total number of actual positive instances. Precision measures the accuracy of the positive predictions, while recall measures the ability of the model to find all the positive instances.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-1.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Precision and recall
</div>

<p><strong>What is ROC curve?</strong></p>

<p>The ROC curve is a graphical representation of the tradeoff between the true positive rate (TPR) and the false positive rate (FPR) for a binary classification model. It is used to evaluate the performance of the model and to choose the optimal threshold for making predictions. Each point of the ROC curve represents a different threshold, and the area under the curve (AUC) is a measure of the model’s performance. The optimal threshold is the one that maximizes the TPR and minimizes the FPR. The best model is the one that has the highest AUC.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-2.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    ROC curve
</div>

<p><strong>What is the difference between bagging and boosting?</strong></p>

<p>Bagging and boosting are two ensemble learning techniques that combine multiple models to improve the overall performance. Bagging, or bootstrap aggregating, involves training multiple models on different subsets of the training data and then combining their predictions. Boosting, on the other hand, involves training multiple models sequentially, with each model focusing on the instances that were misclassified by the previous models. The main difference between bagging and boosting is the way the models are combined and the focus of each model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-3.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Bagging and Boosting
</div>

<p>Reference: https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422</p>

<p><strong>What is the difference between a generative model and a discriminative model?</strong></p>

<p>A generative model is a type of model that learns the joint probability distribution of the input features and the output labels, while a discriminative model is a type of model that learns the conditional probability distribution of the output labels given the input features. In other words, a generative model learns to generate new data, while a discriminative model learns to discriminate between different classes.</p>

<p>Mathematical explanation:</p>

<ul>
  <li>Generative model: \(P(X, Y)\) where \(X\) is the input features and \(Y\) is the output labels. \(P(X, Y)\) is the joint probability distribution of X and Y.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Discriminative model: $$P(Y</td>
          <td>X)\(where\)X\(is the input features and\)Y\(is the output labels.\)P(Y</td>
          <td>X)$$ is the conditional probability distribution of Y given X.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Example: Naive Bayes is a generative model, while logistic regression is a discriminative model.</li>
</ul>

<p><strong>What is the difference between L1 and L2 regularization?</strong></p>

<p>L1 and L2 regularization are two common techniques used to prevent overfitting in machine learning models. L1 regularization adds a penalty to the model’s loss function based on the absolute value of the model’s weights, while L2 regularization adds a penalty based on the squared value of the model’s weights. The main difference between L1 and L2 regularization is the type of penalty they impose on the model’s weights and the effect on the model’s sparsity.</p>

<p>L1 regularization:</p>

<ul>
  <li>Adds a penalty based on the absolute value of the model’s weights.</li>
  <li>Encourages sparsity in the model’s weights, meaning that many of the weights will be set to zero.</li>
  <li>Useful for feature selection and reducing the number of features in the model.</li>
  <li>Example: Lasso regression.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Loss function: $$\mathcal{L}(\theta) + \lambda \sum_{i=1}^{n}</td>
          <td>\theta_i</td>
          <td>$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Gradient: \(\nabla_{\theta} \mathcal{L}(\theta) + \lambda \text{sign}(\theta)\)</li>
  <li>Update rule: \(\theta_{t+1} = \theta_t - \alpha \nabla_{\theta} \mathcal{L}(\theta) - \alpha \lambda \text{sign}(\theta)\)</li>
</ul>

<p>L2 regularization:</p>

<ul>
  <li>Adds a penalty based on the squared value of the model’s weights.</li>
  <li>Encourages small weights in the model, but does not force them to be exactly zero.</li>
  <li>Useful for preventing overfitting and improving the generalization of the model.</li>
  <li>Example: Ridge regression.</li>
  <li>Loss function: \(\mathcal{L}(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2\)</li>
  <li>Gradient: \(\nabla_{\theta} \mathcal{L}(\theta) + 2 \lambda \theta\)</li>
  <li>Update rule: \(\theta_{t+1} = \theta_t - \alpha \nabla_{\theta} \mathcal{L}(\theta) - \alpha 2 \lambda \theta\)</li>
  <li>Note: The update rule for L2 regularization is also known as weight decay.</li>
</ul>

<p><strong>What is the difference between a hyperparameter and a parameter?</strong></p>

<p>A hyperparameter is a configuration setting for a machine learning model that is set before the model is trained, while a parameter is a variable that is learned by the model during training. Hyperparameters are used to control the learning process and the structure of the model, while parameters are used to make predictions based on the input data.</p>

<p>Examples of hyperparameters:</p>

<ul>
  <li>Learning rate</li>
  <li>Number of hidden layers</li>
  <li>Number of neurons in each layer</li>
  <li>Regularization strength</li>
  <li>Batch size</li>
</ul>

<p><strong>What is non-parametric machine learning?</strong></p>

<p>Non-parametric machine learning refers to a class of machine learning models that do not make strong assumptions about the functional form of the underlying data distribution. Instead of estimating a fixed number of parameters, non-parametric models use the data to determine the number of parameters needed to represent the data. Non-parametric models are often more flexible and can capture complex patterns in the data, but they may require more data and computational resources.</p>

<p>On the other hand, parametric machine learning refers to a class of machine learning models that make strong assumptions about the functional form of the underlying data distribution. “A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.” (Artificial Intelligence: A Modern Approach, page 737).</p>

<p>Examples of non-parametric machine learning models:</p>

<ul>
  <li>k-nearest neighbors (KNN)</li>
  <li>Decision trees</li>
  <li>Support vector machines (SVM)</li>
</ul>

<p>Examples of parametric machine learning models:</p>

<ul>
  <li>Linear regression</li>
  <li>Neural networks</li>
</ul>

<p><strong>What is k-nearest neighbors (KNN)?</strong></p>

<p>K-nearest neighbors (KNN) is a simple and intuitive machine learning algorithm that is used for both classification and regression tasks. In KNN, the output of a new instance is predicted based on the majority class of its k-nearest neighbors. The distance between instances is typically measured using Euclidean distance, but other distance metrics can also be used.</p>

<p>Pseudocode for KNN:</p>

<ol>
  <li>For each instance in the training data, calculate the distance between the new instance and the training instance.</li>
  <li>Select the k-nearest neighbors based on the distance metric.</li>
  <li>For classification tasks, predict the majority class of the k-nearest neighbors. For regression tasks, predict the average value of the k-nearest neighbors.</li>
  <li>Return the predicted output.</li>
</ol>

<p>The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning.</p>

<p><strong>What is k-means clustering?</strong></p>

<p>K-means clustering is a popular unsupervised machine learning algorithm that is used to partition a dataset into k clusters. The algorithm works by iteratively assigning instances to the nearest cluster center and updating the cluster centers based on the mean of the instances in each cluster. The goal of k-means clustering is to minimize the sum of squared distances between instances and their respective cluster centers.</p>

<p>Pseudocode for k-means clustering:</p>

<ol>
  <li>Initialize k cluster centers randomly or using a heuristic.</li>
  <li>Assign each instance to the nearest cluster center.</li>
  <li>Update the cluster centers based on the mean of the instances in each cluster.</li>
  <li>Repeat steps 2 and 3 until convergence.</li>
  <li>Return the final cluster assignments and cluster centers.</li>
</ol>

<p>K-means clustering is sensitive to the initial cluster centers and can converge to a local minimum. To mitigate this issue, the algorithm is often run multiple times with different initializations, and the best clustering is selected based on a predefined criterion.</p>

<p>Applications of k-means clustering include customer segmentation, image compression, and anomaly detection.</p>

<p><strong>What is Bayes’ theorem?</strong></p>

<p>Bayes’ theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It is named after the Reverend Thomas Bayes, who first formulated the theorem. Bayes’ theorem is used to update the probability of an event based on new evidence or information.</p>

<p>Mathematically, Bayes’ theorem is expressed as:</p>

\[P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}\]

<p>Where:</p>

<ul>
  <li>\(P(A \mid B)\) is the probability of event A given event B, also known as the posterior probability. This is the probability of event A after considering new evidence.</li>
  <li>\(P(B \mid A)\) is the probability of event B given event A, also known as the likelihood. This is the probability of event B given that event A has occurred.</li>
  <li>\(P(A)\) is the prior probability of event A, which is the probability of event A before considering any new evidence. This is the initial belief about the probability of event A.</li>
  <li>\(P(B)\) is the prior probability of event B, also known as the marginal likelihood.</li>
</ul>

<p><strong>What is the Naive Bayes classifier?</strong></p>

<p>The Naive Bayes classifier is a simple and efficient machine learning algorithm that is based on Bayes’ theorem and the assumption of conditional independence between features. Despite its simplicity, the Naive Bayes classifier is often used as a baseline model for text classification and other tasks.</p>

<p>The Naive Bayes classifier is particularly well-suited for text classification tasks, such as spam detection and sentiment analysis, where the input features are typically word frequencies or presence/absence of words.</p>

<p>Mathematically, the Naive Bayes classifier predicts the class label of an instance based on the maximum a posteriori (MAP) estimation:</p>

\[\hat{y} = \underset{y \in \mathcal{Y}}{\text{argmax}} P(y|X)\]

<p>Where:</p>

<ul>
  <li>\(\hat{y}\) is the predicted class label.</li>
  <li>\(\mathcal{Y}\) is the set of possible class labels.</li>
  <li>\(X\) is the input features.</li>
  <li>\(P(y \mid X)\) is the posterior probability of class label y given the input features X. The posterior probability \(P(y \mid X)\) is calculated using Bayes’ theorem and the assumption of conditional independence, i.e., \(P(y \mid X) = \frac{P(X \mid y)P(y)}{P(X)}\). The denominator \(P(X)\) is constant for all class labels and can be ignored for the purpose of classification.</li>
</ul>

<p><strong>What is Type I and Type II error?</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-5-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-5-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-5-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-5.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Type I and Type II error
</div>

<p>Null hypothesis (H0) is a statement that there is no relationship between two measured phenomena, or no association among groups. It is the default assumption that there is no effect or no difference. The alternative hypothesis (H1) is the statement that there is a relationship between two measured phenomena, or an association among groups. It is the opposite of the null hypothesis.</p>

<p>In the example of Innocent and Guilty, the null hypothesis is “Innocent” and the alternative hypothesis is “Guilty”. Type I error is the incorrect rejection of the null hypothesis (false positive), while Type II error is the failure to reject the null hypothesis when it is false (false negative).</p>

<p><strong>Taylor expansion</strong></p>

<p><strong>Newton’s method</strong></p>

<p><strong>Fisher information</strong></p>

<p><strong>Hessian matrix</strong></p>

<p>How to obtain the Hessian matrix</p>

<p>And its importance in optimization</p>

<ul>
  <li>It can help us to understand the local curvature of the loss function.</li>
  <li>It can help us to point out which weights are more important to a specific data point (machine unlearning) (ref <a href="https://arxiv.org/pdf/1703.04730" rel="external nofollow noopener" target="_blank">Understanding Black-box Predictions via Influence Functions</a>)</li>
  <li>It can help us to understand which data points are more important to the model (Influence functions)</li>
</ul>

\[\theta^{*}_{-z} - \theta^* = - H^{-1} \nabla_{\theta} \mathcal{L}(\theta, z)\]

<p>where $\theta^<em>$ is the optimal solution, $\theta^{</em>}_{-z}$ is the optimal solution without the $z$-th data point, 
and $H$ is the Hessian matrix, which is averaged over all the data points (including $z$).</p>

<p>How to obtain the Hessian matrix.</p>

<ul>
  <li>Using Gauss-Newton-Barlett estimator.</li>
</ul>

<p><strong>Machine Unlearning of Features and Labels</strong></p>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/fairness-irt/">Comprehensive Algorithm Portfolio Evaluation using Item Response Theory</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/sharpness/">Connection between Flatness and Generalization</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/watermark-diffusion/">Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
