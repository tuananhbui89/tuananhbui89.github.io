<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Foundations of Machine Learning | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="How to break into $100k+ salary roles">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/ml-foundation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh¬†</span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Foundations of Machine Learning</h1>
    <p class="post-meta">January 14, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
      ¬† ¬∑ ¬†
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a> ¬†
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#machine-learning-paradigms">Machine Learning Paradigms</a>
<ul>
<li class="toc-entry toc-h3"><a href="#supervised-learning">Supervised learning</a></li>
<li class="toc-entry toc-h3">
<a href="#unsupervised-learning">Unsupervised learning</a>
<ul>
<li class="toc-entry toc-h4"><a href="#self-supervised-learning">Self-supervised learning</a></li>
<li class="toc-entry toc-h4"><a href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
<li class="toc-entry toc-h3"><a href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#machine-learning-settings">Machine Learning Settings</a>
<ul>
<li class="toc-entry toc-h3"><a href="#representation-learning">Representation learning</a></li>
<li class="toc-entry toc-h3"><a href="#continual-learning">Continual learning</a></li>
<li class="toc-entry toc-h3"><a href="#meta-learning">Meta-learning</a></li>
<li class="toc-entry toc-h3"><a href="#transfer-learning">Transfer learning</a></li>
<li class="toc-entry toc-h3"><a href="#multi-task-learning">Multi-task learning</a></li>
<li class="toc-entry toc-h3">
<a href="#domain-adaptation">Domain adaptation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#unsupervised-domain-adaptation">Unsupervised domain adaptation</a></li>
<li class="toc-entry toc-h4"><a href="#supervised-domain-adaptation">Supervised domain adaptation</a></li>
<li class="toc-entry toc-h4"><a href="#challenges">Challenges</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#domain-generalization">Domain generalization</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#optimization">Optimization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#gradient-descent">Gradient descent</a></li>
<li class="toc-entry toc-h3"><a href="#newtons-method">Newton‚Äôs method</a></li>
<li class="toc-entry toc-h3"><a href="#fisher-information">Fisher information</a></li>
<li class="toc-entry toc-h3"><a href="#hessian-matrix">Hessian matrix</a></li>
<li class="toc-entry toc-h3"><a href="#stochastic-gradient-descent">Stochastic gradient descent</a></li>
<li class="toc-entry toc-h3"><a href="#adam-rmsprop-adagrad">Adam, RMSProp, AdaGrad</a></li>
<li class="toc-entry toc-h3"><a href="#bayesian-optimization">Bayesian Optimization</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#bias-variance">Bias-Variance</a></li>
<li class="toc-entry toc-h2">
<a href="#bagging-and-boosting">Bagging and Boosting</a>
<ul>
<li class="toc-entry toc-h3"><a href="#bagging-bootstrap-aggregating">Bagging (Bootstrap Aggregating)</a></li>
<li class="toc-entry toc-h3"><a href="#boosting">Boosting</a></li>
<li class="toc-entry toc-h3"><a href="#xgboost">XGBoost</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</a></li>
<li class="toc-entry toc-h2"><a href="#k-means-clustering">K-Means Clustering</a></li>
<li class="toc-entry toc-h2"><a href="#logistic-regression">Logistic regression</a></li>
<li class="toc-entry toc-h2"><a href="#bayesian-theorem">Bayesian theorem</a></li>
<li class="toc-entry toc-h2"><a href="#curse-of-dimensionality">Curse of dimensionality</a></li>
<li class="toc-entry toc-h2">
<a href="#overcoming-overfitting">Overcoming overfitting</a>
<ul>
<li class="toc-entry toc-h3"><a href="#l1-vs-l2-regularization">L1 vs L2 regularization</a></li>
<li class="toc-entry toc-h3"><a href="#dropout">Dropout</a></li>
<li class="toc-entry toc-h3"><a href="#early-stopping">Early stopping</a></li>
<li class="toc-entry toc-h3">
<a href="#batch-normalizationlayer-normalization">Batch normalization/layer normalization</a>
<ul>
<li class="toc-entry toc-h4"><a href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-entry toc-h4"><a href="#internal-covariate-shift">Internal Covariate Shift</a></li>
<li class="toc-entry toc-h4"><a href="#layer-normalization">Layer Normalization</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#data-augmentation">Data augmentation</a></li>
<li class="toc-entry toc-h3"><a href="#cross-validation">Cross-validation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#generalization">Generalization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#out-of-distribution-generalization">Out-of-distribution generalization</a></li>
<li class="toc-entry toc-h3"><a href="#adversarial-generalizationtraining">Adversarial generalization/training</a></li>
<li class="toc-entry toc-h3"><a href="#sharpness-aware-minimization">Sharpness-aware minimization</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#evaluation">Evaluation</a>
<ul>
<li class="toc-entry toc-h3">
<a href="#metrics">Metrics</a>
<ul>
<li class="toc-entry toc-h4"><a href="#type-i-and-type-ii-error">Type I and Type II error</a></li>
<li class="toc-entry toc-h4"><a href="#roc-curve">ROC curve</a></li>
<li class="toc-entry toc-h4"><a href="#precision-recall-curve">Precision-recall curve</a></li>
<li class="toc-entry toc-h4"><a href="#f1-score">F1 score</a></li>
<li class="toc-entry toc-h4"><a href="#auc">AUC</a></li>
</ul>
</li>
</ul>
</li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <p>(Work in progress I will gradually add more content when having more time:D Please stay tuned :D)</p>

<h2 id="machine-learning-paradigms">Machine Learning Paradigms</h2>

<h3 id="supervised-learning">Supervised learning</h3>

<p>Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning that the input data is paired with the correct output. The model learns to make predictions based on this labeled data.</p>

<p>Mathematically, given a training set of \(N\) samples:</p>

\[\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\]

<p>where \(x_i \in \mathbb{R}^d\) is the input feature vector with \(d\) dimensions. The corresponding label \(y_i\) can be either a continuous value \(y_i \in \mathbb{R}\) for regression problems, or a categorical value \(y_i \in \{0, 1, \ldots, K-1\}\) for classification problems.</p>

<p>the goal of supervised learning is to learn a function \(f_{\theta}\) parameterized by \(\theta\) that maps input features \(x\) to output labels \(y\), i.e., \(\hat{y} \triangleq f_{\theta}(x) \approx y\).</p>

<p>It can be done by minimizing the loss function - which measures the discrepancy between the predicted output \(\hat{y}\) and the true output \(y\) over the training set, i.e., \(\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{l}(y_i, f_{\theta}(x_i))\).</p>

<p>Common loss functions:</p>

<ul>
  <li>Mean squared error (MSE) for regression problems:</li>
</ul>

\[\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} (y_i - f_{\theta}(x_i))^2\]

<ul>
  <li>Cross-entropy loss for classification problems:</li>
</ul>

\[\mathcal{L}(\theta) = - \frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log \hat{y}_{ik}\]

<p>where \(y_{ik}\) is the one-hot encoded label for the \(i\)-th sample and the \(k\)-th class, and \(\hat{y}_{ik}\) is the predicted probability of the \(i\)-th sample for the \(k\)-th class.</p>

<p>More advanced topics which are discussed later in this post:</p>

<ul>
  <li>Advanced loss functions</li>
  <li>Regularization</li>
  <li>Bias-variance tradeoff</li>
  <li>Out-of-distribution generalization</li>
  <li>Adversarial generalization/training</li>
  <li>Sharpness-aware minimization</li>
</ul>

<h3 id="unsupervised-learning">Unsupervised learning</h3>

<p>Unsupervised learning is a machine learning paradigm where a model learns patterns, structures, or representations from unlabeled data without explicit supervision.</p>

<p>Unlike supervised learning, there is no unified mathematical formulation for unsupervised learning, instead, it is a collection of different techniques/problems that can be grouped into following categories:</p>

<p><strong>Dimensionality reduction/representation learning</strong>: Finding a low-dimensional representation of the data that captures the most important information, e.g., to learn a mapping \(f: \mathbb{R}^d \rightarrow \mathbb{R}^k\) where \(k \ll d\).
Common methods include Principal Component Analysis (PCA), t-SNE, and Autoencoders.</p>

<p><strong>Clustering</strong>: Finding groups of similar features, e.g., assigning cluster \(c_i\) to each sample \(x_i\), where \(c_i \in \{1, 2, \ldots, K\}\).
Common methods include K-means, Gaussian Mixture Models (GMM), and Hierarchical Clustering.</p>

<p><strong>Density estimation</strong>: Estimating the probability distribution \(p(x)\) of the data.
Common methods include Parametric models (e.g., Gaussian Mixture Models (GMM), Mixture of Experts (MoE)), Non-parametric models (e.g., Kernel Density Estimation (KDE), Histogram), and Variational Autoencoders (VAEs).</p>

<blockquote>
  <p>Parametric methods are the ones that assume the data follows a known distribution (e.g., Gaussian, MoG, etc.) and characterize by a fixed number of parameters that do not grow with the amount of data.</p>

  <p>Non-parametric methods are the ones that do not make any assumptions about the distribution of the data. The number of parameters can grow with more data. It provides greater flexibility but requires more data/computational expense.</p>
</blockquote>

<p><strong>Gaussian Mixture Model</strong> (GMM) models the data as a weighted sum of \(K\) Gaussian distributions:</p>

\[p(x) = \sum_{k=1}^{K} \alpha_k \mathcal{N}(x; \mu_k, \Sigma_k)\]

<p>where \(\alpha_k\) is the mixing coefficient, \(\mu_k\) is the mean, and \(\Sigma_k\) is the covariance matrix of the \(k\)-th Gaussian distribution.
It can be learned using Expectation-Maximization (EM) algorithm, including the E-step and M-step:</p>

<p>E-step: compute the probability that each sample \(x_i\) belongs to each cluster \(k\):</p>

\[p(c_i = k \mid x_i) = \frac{\alpha_k \mathcal{N}(x_i; \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \alpha_j \mathcal{N}(x_i; \mu_j, \Sigma_j)}\]

<p>M-step: update the parameters of the Gaussian distributions to maximize the likelihood of the data:</p>

\[\alpha_k = \frac{1}{N} \sum_{i=1}^{N} p(c_i = k \mid x_i)\]

\[\mu_k = \frac{\sum_{i=1}^{N} p(c_i = k \mid x_i) x_i}{\sum_{i=1}^{N} p(c_i = k \mid x_i)}\]

\[\Sigma_k = \frac{\sum_{i=1}^{N} p(c_i = k \mid x_i) (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} p(c_i = k \mid x_i)}\]

<p>And repeat the E-step and M-step until convergence.</p>

<p><strong>Kernel Density Estimation</strong> (KDE) uses a kernel function to estimate the probability density function of the data.</p>

\[p(x) = \frac{1}{Nh} \sum_{i=1}^{N} K \left( \frac{x - x_i}{h} \right)\]

<p>where \(K\) is the kernel function and \(h\) is the bandwidth to control the smoothness of the estimated density. Common kernel functions include Gaussian, Epanechnikov, and Uniform kernels.</p>

<h4 id="self-supervised-learning">Self-supervised learning</h4>

<h4 id="clustering">Clustering</h4>

<h3 id="semi-supervised-learning">Semi-supervised learning</h3>

<h3 id="reinforcement-learning">Reinforcement learning</h3>

<h2 id="machine-learning-settings">Machine Learning Settings</h2>

<h3 id="representation-learning">Representation learning</h3>

<h3 id="continual-learning">Continual learning</h3>

<h3 id="meta-learning">Meta-learning</h3>

<h3 id="transfer-learning">Transfer learning</h3>

<h3 id="multi-task-learning">Multi-task learning</h3>

<h3 id="domain-adaptation">Domain adaptation</h3>

<h4 id="unsupervised-domain-adaptation">Unsupervised domain adaptation</h4>

<p>Given a set of \(N \leq 1\) source domains \(\{ \mathcal{D}_{S_i} \}_{i=1}^{N}\) each of which is a collection of data-label pairs of domain \(S_i\), i.e., \(\mathcal{D}_{S_i} = \{ (x_{j}, y_{j}) \}_{j=1}^{N_{S_i}}\), \(y_i \in [K] := \{1, 2, \ldots, K\}\), (the set of classes \(K\) is the same for all domains), and one unlabeled target domain \(\mathcal{D}_T=\{x_j\}_{j=1}^{N_T}\), the goal of unsupervised domain adaptation is to learn a classifier \(f_{\theta}\) that can generalize well to the target domain \(\mathcal{D}_T\).</p>

<p>Example:</p>

<ul>
  <li>Source domain: Images of faces from different countries, each country can be considered as a domain with different geographic features.</li>
  <li>Target domain: Images of faces from a specific country that does not have the same geographic features as the source domains and no labels. It can be a rare race or ethnicity.</li>
</ul>

<h4 id="supervised-domain-adaptation">Supervised domain adaptation</h4>

<p>The setting is similar to unsupervised domain adaptation, but the target domain has labels.</p>

<h4 id="challenges">Challenges</h4>

<h3 id="domain-generalization">Domain generalization</h3>

<h2 id="optimization">Optimization</h2>

<h3 id="gradient-descent">Gradient descent</h3>

<h3 id="newtons-method">Newton‚Äôs method</h3>

<h3 id="fisher-information">Fisher information</h3>

<h3 id="hessian-matrix">Hessian matrix</h3>

<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>

<h3 id="adam-rmsprop-adagrad">Adam, RMSProp, AdaGrad</h3>

<h3 id="bayesian-optimization">Bayesian Optimization</h3>

<h2 id="bias-variance">Bias-Variance</h2>

<p>Bias ‚Äì Error due to <strong>overly simplistic</strong> assumptions in the model.</p>

<ul>
  <li>High bias models underfit the data because they fail to capture important patterns.</li>
  <li>Example: A linear regression model trying to fit a complex, nonlinear dataset.</li>
</ul>

<p>Variance ‚Äì Error due to <strong>excessive sensitivity</strong> to training data.</p>

<ul>
  <li>High variance models overfit the data by learning noise along with patterns.</li>
  <li>Example: A deep neural network memorizing training data instead of generalizing.</li>
</ul>

<p>It is a worth noting that bias or variance is measured on the validation set, not the training set.</p>

\[\text{Bias} = \mathbb{E}[\hat{f}(x)] - f(x)\]

<ul>
  <li>high bias means large errors due to incorrect assumptions or model is too simple</li>
  <li>low bias means small errors, and close to the ground truth</li>
</ul>

\[\text{Variance} = \mathbb{E}[\hat{f}(x) - \mathbb{E}[\hat{f}(x)]]^2\]

<ul>
  <li>high variance means the model makes different predictions each time after training on different datasets.</li>
  <li>low variance means similar predictions on different datasets</li>
</ul>

<p>Tradeoff</p>

<ul>
  <li>A model with high bias and low variance is simple but inaccurate (underfitting).</li>
  <li>A model with low bias and high variance is flexible but unreliable (overfitting).</li>
  <li>The goal is to find a balance‚Äîan optimal point where bias and variance are minimized to achieve good generalization.</li>
</ul>

<p>How to Control Bias-Variance Tradeoff?</p>

<ul>
  <li>Increase model complexity to reduce bias (e.g., moving from linear to polynomial models).</li>
  <li>Reduce complexity to lower variance (e.g., regularization, pruning in decision trees).</li>
  <li>Use more training data to help models generalize better.</li>
  <li>Use ensemble methods like bagging and boosting to balance bias and variance.</li>
</ul>

<h2 id="bagging-and-boosting">Bagging and Boosting</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-3.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Bagging and Boosting
</div>

<p>Reference: https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422</p>

<h3 id="bagging-bootstrap-aggregating">Bagging (Bootstrap Aggregating)</h3>

<p>üí° Goal: Reduce <strong>variance</strong> and <strong>prevent overfitting</strong> by <strong>training multiple models in parallel and averaging their predictions</strong>.</p>

<p>How it works?</p>

<ul>
  <li>Bootstrap Sampling: Random subsets of training data (with replacement) are drawn.</li>
  <li>Train Weak Learners: Independent models (often decision trees) are trained on these subsets.</li>
  <li>Aggregate Predictions:
    <ul>
      <li>For regression: Take the average of predictions.</li>
      <li>For classification: Use majority voting.</li>
    </ul>
  </li>
</ul>

<h3 id="boosting">Boosting</h3>

<p>üí° Goal: Reduce bias and improve weak models by training them sequentially, where each new model <strong>focuses on the errors of the previous one</strong>.</p>

<p>How it works?</p>

<ul>
  <li>Train a Weak Model (e.g., a shallow decision tree).</li>
  <li>Identify Errors: Assign higher weights to misclassified instances.</li>
  <li>Train the Next Model: Focus more on difficult cases.</li>
  <li>Repeat: Continue training models sequentially, combining them for the final prediction.</li>
</ul>

<p><strong>Popular Boosting Algorithms</strong></p>

<p>üî• AdaBoost (Adaptive Boosting)</p>

<ul>
  <li>Adjusts sample weights to focus on hard-to-classify instances.</li>
  <li>Final prediction is a weighted sum of weak models.</li>
</ul>

<p>üî• Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost)</p>

<ul>
  <li>Models are trained sequentially to minimize the residual errors of previous models.</li>
  <li>Uses gradient descent to optimize performance.</li>
</ul>

<p>üî• Boosted Trees (e.g., XGBoost)</p>

<ul>
  <li>A more efficient version of gradient boosting that includes regularization.</li>
  <li>Popular for Kaggle competitions and real-world ML tasks.</li>
</ul>

<h3 id="xgboost">XGBoost</h3>

<h2 id="k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</h2>

<p>1Ô∏è‚É£ Choose a value for K (number of neighbors).
2Ô∏è‚É£ Compute the distance between the new data point and all existing points in the dataset (e.g., Euclidean distance).
3Ô∏è‚É£ Select the K closest neighbors.
4Ô∏è‚É£ Make a prediction:</p>

<ul>
  <li>Classification: Assign the most common class among neighbors (majority voting).</li>
  <li>Regression: Take the average of the target values of the K neighbors.</li>
</ul>

<p><strong>Choose K</strong>:</p>

<ul>
  <li>Small K (e.g., K=1 or 3):
    <ul>
      <li>‚úÖ Captures details but can overfit (high variance).</li>
    </ul>
  </li>
  <li>Large K (e.g., K=10 or 20):
    <ul>
      <li>‚úÖ More generalization but may underfit (high bias).</li>
    </ul>
  </li>
  <li>Common practice: Choose K using cross-validation (often ‚àöN, where N is the dataset size).</li>
</ul>

<h2 id="k-means-clustering">K-Means Clustering</h2>

<p>K-means clustering is a popular unsupervised machine learning algorithm that is used to partition a dataset into k clusters. The algorithm works by iteratively assigning instances to the nearest cluster center and updating the cluster centers based on the mean of the instances in each cluster. The goal of k-means clustering is to minimize the sum of squared distances between instances and their respective cluster centers.</p>

<p>Pseudocode for k-means clustering:</p>

<ol>
  <li>Initialize k cluster centers randomly or using a heuristic.</li>
  <li>Assign each instance to the nearest cluster center.</li>
  <li>Update the cluster centers based on the mean of the instances in each cluster.</li>
  <li>Repeat steps 2 and 3 until convergence.</li>
  <li>Return the final cluster assignments and cluster centers.</li>
</ol>

<p>K-means clustering is sensitive to the initial cluster centers and can converge to a local minimum. To mitigate this issue, the algorithm is often run multiple times with different initializations, and the best clustering is selected based on a predefined criterion.</p>

<p>Applications of k-means clustering include customer segmentation, image compression, and anomaly detection.</p>

<h2 id="logistic-regression">Logistic regression</h2>

<p>Sigmoid function:</p>

\[\sigma(z) = \frac{1}{1 + e^{-z}}\]

<p>where \(z = wX + b\) is the linear combination of the input features and the model parameters.
The nice thing about the sigmoid function is that it maps any real-valued number to the range [0, 1], which is very useful for binary classification.</p>

<p>Cost function:</p>

\[\mathcal{L}(\theta) = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]\]

<p>where \(\hat{y}_i = \sigma(z_i)\) is the predicted probability of the \(i\)-th sample.</p>

<p>Gradient descent:</p>

\[\nabla_{\theta} \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)X_i\]

<h2 id="bayesian-theorem">Bayesian theorem</h2>

<p>Bayesian theorem:</p>

\[P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}\]

<p>Where \(A\) and \(B\) are events, e.g., A: having a cancer, B: having a positive test result, and \(P(A \mid B)\) is the probability of \(A\) given \(B\).
Bayesian theorem can be used to update our belief based on new evidence (given \(B\))</p>

<ul>
  <li>\(P(A \mid B)\) is the posterior probability of \(A\) given \(B\).</li>
  <li>\(P(B \mid A)\) is the likelihood of \(B\) given \(A\). How likely a patient with cancer (\(A\)) has a positive test result (\(B\)).</li>
  <li>\(P(A)\) is the prior probability of \(A\). The probability of having cancer over all the population.</li>
  <li>\(P(B)\) is the marginal probability of \(B\). The probability of having a positive test result over all the population, including patients with and without cancer.</li>
</ul>

<p>The marginal probability \(P(B)\) can be computed by summing over all the possible values of \(A\) and usually very expensive to compute because it requires summing over all the possible values of \(A\).</p>

\[P(B) = \sum_{A} P(B \mid A)P(A)\]

<p><strong>What is the Naive Bayes classifier?</strong></p>

<p>The Naive Bayes classifier is a simple and efficient machine learning algorithm that is based on Bayes‚Äô theorem and the assumption of conditional independence between features. Despite its simplicity, the Naive Bayes classifier is often used as a baseline model for text classification and other tasks.</p>

<p>The Naive Bayes classifier is particularly well-suited for text classification tasks, such as spam detection and sentiment analysis, where the input features are typically word frequencies or presence/absence of words.</p>

<p>Mathematically, the Naive Bayes classifier predicts the class label of an instance based on the maximum a posteriori (MAP) estimation:</p>

\[\hat{y} = \underset{y \in \mathcal{Y}}{\text{argmax}} P(y|X)\]

<p>Where:</p>

<ul>
  <li>\(\hat{y}\) is the predicted class label.</li>
  <li>\(\mathcal{Y}\) is the set of possible class labels.</li>
  <li>\(X\) is the input features.</li>
  <li>\(P(y \mid X)\) is the posterior probability of class label y given the input features X. The posterior probability \(P(y \mid X)\) is calculated using Bayes‚Äô theorem and the assumption of conditional independence, i.e., \(P(y \mid X) = \frac{P(X \mid y)P(y)}{P(X)}\). The denominator \(P(X)\) is constant for all class labels and can be ignored for the purpose of classification.</li>
  <li>
    <h2 id="curse-of-dimensionality">Curse of dimensionality</h2>
  </li>
</ul>

<p>The curse of dimensionality refers to the challenges and problems that arise when working with high-dimensional data. As the number of features (dimensions) increases, data becomes sparse, <strong>distances become less meaningful</strong>, and models struggle to generalize well.</p>

<p>And because distance becomes less meaningful, algorithms that rely on distance such as KNN, SVN, or K-means become less effective.</p>

<p>And because data becomes sparse (but the true pattern is still in lower dimensions), machine learning models need more data to fill in the space, otherwise they will overfit the data.</p>

<h2 id="overcoming-overfitting">Overcoming overfitting</h2>

<h3 id="l1-vs-l2-regularization">L1 vs L2 regularization</h3>

<p>Regularization helps prevent overfitting by adding a penalty to the loss function, <strong>discouraging overly complex models</strong> (large weights or many features used).</p>

<p>L1 regularization:</p>

\[\mathcal{L}(\theta) + \lambda \sum_{i=1}^{n} |\theta_i|\]

<p>L2 regularization:</p>

\[\mathcal{L}(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2\]

<p>L1 regularization <strong>encourages sparsity</strong> in the model‚Äôs weights (some weights will be exactly 0), while L2 regularization **encourages small weights **(no weight will be exactly 0 but very small).</p>

<p>Because L1 regularization encourages sparsity, it can be used for <strong>feature selection</strong> (eliminate some features that are not important - set their weights to 0). But it can lead to unstable solutions when features are highly correlated/entangled.</p>

<p><strong>Elastic net regularization</strong> is a combination of L1 and L2 regularization.</p>

\[\mathcal{L}(\theta) + \lambda_1 \sum_{i=1}^{n} |\theta_i| + \lambda_2 \sum_{i=1}^{n} \theta_i^2\]

<h3 id="dropout">Dropout</h3>

<p>Dropout is a regularization technique used in neural networks to prevent overfitting by randomly ‚Äúdropping‚Äù (deactivating) neurons during training. This forces the network to learn more robust and generalizable features rather than relying on specific neurons.</p>

<p>üîπ Why? In deep networks, some neurons may become too dependent on others, leading to overfitting. Works well in fully connected layers, less useful in convolutional layers (which have spatial dependencies and less neurons than fully connected layers).</p>

<p>üîπ How? During training, for each mini-batch, some neurons are randomly dropped out with a probability \(p\) (e.g., 0.5). <em>Important thing that most people forget</em>: the remaining neurons need to be scaled up by \(\frac{1}{1-p}\) to keep the expected output the same.</p>

<h3 id="early-stopping">Early stopping</h3>

<p>Early stopping is a regularization technique that stops training when the model‚Äôs <strong>performance on the validation set starts to degrade</strong> (model starts to overfit or memorize the training data - therefore, the performance on the validation set starts to decrease).</p>

<h3 id="batch-normalizationlayer-normalization">Batch normalization/layer normalization</h3>

<h4 id="batch-normalization">Batch Normalization</h4>

<p>Batch Normalization normalizes activations <strong>across the batch</strong> dimension for each feature.</p>

\[\mu = \frac{1}{m} \sum_{i=1}^{m} x_i\]

\[\sigma = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2}\]

\[\hat{x}_i = \frac{x_i - \mu}{\sigma}\]

\[x_i = \hat{x}_i \cdot \sigma + \mu\]

<p>This transformation ensures stable activations, reducing internal covariate shift.</p>

<p>However, batch statistics vary across mini-batches, making it problematic for small batch sizes or RNNs.
Therefore, batch normalization is commonly used in CNNs and Fully Connected Networks (FCNs).
In inference, we use the running mean and variance to normalize the activations.</p>

<h4 id="internal-covariate-shift">Internal Covariate Shift</h4>

<p>Internal Covariate Shift occurs when the distribution of activations (features) changes during training due to updates in previous layers. This makes optimization harder because each layer must constantly adapt to new distributions of inputs.
It causes slow convergence and unstable training.</p>

<p>It differs from the <strong>covariate shift</strong> problem, which is the <strong>change in the distribution of the input features</strong> (e.g., training on sunny images but testing on rainy ones).
Internal Covariate Shift <strong>happens inside a deep network</strong> during training.</p>

<h4 id="layer-normalization">Layer Normalization</h4>

<p>Layer Normalization normalizes activations <strong>across features</strong> for each individual data point (sample-wise normalization).</p>

<p>Key Properties:</p>

<ul>
  <li>Works sample-wise (statistics are computed for each input independently).</li>
  <li>Works well for RNNs and small batch sizes, unlike BatchNorm.</li>
  <li>More stable training in NLP and reinforcement learning tasks.</li>
  <li>Less effective than BatchNorm for CNNs, where spatial dependencies are important.</li>
</ul>

<h3 id="data-augmentation">Data augmentation</h3>

<h3 id="cross-validation">Cross-validation</h3>

<h2 id="generalization">Generalization</h2>

<h3 id="out-of-distribution-generalization">Out-of-distribution generalization</h3>

<h3 id="adversarial-generalizationtraining">Adversarial generalization/training</h3>

<h3 id="sharpness-aware-minimization">Sharpness-aware minimization</h3>

<h2 id="evaluation">Evaluation</h2>

<h3 id="metrics">Metrics</h3>

<h4 id="type-i-and-type-ii-error">Type I and Type II error</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-5-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-5-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-5-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-5.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Type I and Type II error
</div>

<p>Null hypothesis (H0) is a statement that there is no relationship between two measured phenomena, or no association among groups. It is the default assumption that there is no effect or no difference. The alternative hypothesis (H1) is the statement that there is a relationship between two measured phenomena, or an association among groups. It is the opposite of the null hypothesis.</p>

<p>In the example of Innocent and Guilty, the null hypothesis is ‚ÄúInnocent‚Äù and the alternative hypothesis is ‚ÄúGuilty‚Äù. Type I error is the incorrect rejection of the null hypothesis (false positive), while Type II error is the failure to reject the null hypothesis when it is false (false negative).</p>

<h4 id="roc-curve">ROC curve</h4>

<h4 id="precision-recall-curve">Precision-recall curve</h4>

<h4 id="f1-score">F1 score</h4>

<h4 id="auc">AUC</h4>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">Foundation of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
