<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Karpathy Series - Building Makemore | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/karpathy-lec02/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Karpathy Series - Building Makemore</h1>
    <p class="post-meta">December 15, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#makemore-is-a-character-level-generator-that-learns-to-produce-strings-similar-to-a-provided-dataset">MakeMore is a character-level generator that learns to produce strings similar to a provided dataset.</a></li>
<li class="toc-entry toc-h1"><a href="#load-the-raw-dataset-into-a-list-of-strings-and-compute-basic-statistics-before-modeling">Load the raw dataset into a list of strings and compute basic statistics before modeling.</a></li>
<li class="toc-entry toc-h1"><a href="#a-character-level-language-model-predicts-the-next-character-in-a-sequence-and-can-be-implemented-with-models-ranging-from-bigrams-to-transformers">A character-level language model predicts the next character in a sequence and can be implemented with models ranging from bigrams to transformers.</a></li>
<li class="toc-entry toc-h1"><a href="#bigrams-are-extracted-by-sliding-a-two-character-window-over-sequences-augmented-with-explicit-start-and-end-tokens">Bigrams are extracted by sliding a two-character window over sequences augmented with explicit start and end tokens.</a></li>
<li class="toc-entry toc-h1"><a href="#accumulate-bigram-frequencies-in-a-dictionary-and-then-represent-them-as-a-2d-count-matrix-for-efficient-computation">Accumulate bigram frequencies in a dictionary and then represent them as a 2D count matrix for efficient computation.</a></li>
<li class="toc-entry toc-h1"><a href="#build-character-to-index-and-index-to-character-lookup-tables-populate-the-count-matrix-and-visualize-bigram-statistics">Build character-to-index and index-to-character lookup tables, populate the count matrix, and visualize bigram statistics.</a></li>
<li class="toc-entry toc-h1"><a href="#special-startend-token-placement-can-create-wasted-zero-rowscolumns-use-a-single-sentinel-and-offset-indices-to-eliminate-redundancy">Special start/end token placement can create wasted zero rows/columns; use a single sentinel and offset indices to eliminate redundancy.</a></li>
<li class="toc-entry toc-h1"><a href="#sample-sequences-by-normalizing-a-row-of-counts-into-probabilities-and-drawing-with-torchmultinomial-using-a-seeded-generator-for-deterministic-randomness">Sample sequences by normalizing a row of counts into probabilities and drawing with torch.multinomial using a seeded generator for deterministic randomness.</a></li>
<li class="toc-entry toc-h1"><a href="#iterative-sampling-from-a-bigram-model-produces-limited-quality-outputs-because-context-is-restricted-to-one-previous-character-uniform-baselines-illustrate-the-learned-signal">Iterative sampling from a bigram model produces limited-quality outputs because context is restricted to one previous character; uniform baselines illustrate the learned signal.</a></li>
<li class="toc-entry toc-h1"><a href="#precompute-a-probability-matrix-p-by-normalizing-the-counts-tensor-once-and-use-it-for-fast-sampling-instead-of-repeated-per-step-normalization">Precompute a probability matrix P by normalizing the counts tensor once and use it for fast sampling instead of repeated per-step normalization.</a></li>
<li class="toc-entry toc-h1"><a href="#respect-broadcasting-semantics-and-prefer-keepdim-to-preserve-intended-shapes-use-in-place-ops-to-reduce-allocations">Respect broadcasting semantics and prefer keepdim to preserve intended shapes; use in-place ops to reduce allocations.</a></li>
<li class="toc-entry toc-h1"><a href="#measure-model-quality-with-the-likelihood-and-its-log-transforms-using-negative-log-likelihood-as-a-loss-to-minimize">Measure model quality with the likelihood and its log transforms, using negative log-likelihood as a loss to minimize.</a></li>
<li class="toc-entry toc-h1"><a href="#prevent-infinite-loss-from-zero-probability-events-by-applying-additive-smoothing-to-counts-before-normalization">Prevent infinite loss from zero-probability events by applying additive smoothing to counts before normalization.</a></li>
<li class="toc-entry toc-h1"><a href="#cast-bigram-prediction-as-a-supervised-neural-network-task-by-compiling-input-target-integer-pairs-from-all-bigrams">Cast bigram prediction as a supervised neural network task by compiling input-target integer pairs from all bigrams.</a></li>
<li class="toc-entry toc-h1"><a href="#use-one-hot-encoding-to-convert-integer-inputs-to-float-vectors-and-implement-the-linear-layer-as-a-weight-matrix-evaluated-by-matrix-multiplication">Use one-hot encoding to convert integer inputs to float vectors and implement the linear layer as a weight matrix evaluated by matrix multiplication.</a></li>
<li class="toc-entry toc-h1"><a href="#interpret-linear-outputs-as-logits-apply-softmax-to-obtain-probabilities-and-compute-nll-loss-by-indexing-predicted-probabilities-at-target-positions">Interpret linear outputs as logits, apply softmax to obtain probabilities, and compute NLL loss by indexing predicted probabilities at target positions.</a></li>
<li class="toc-entry toc-h1"><a href="#compute-gradients-via-automatic-differentiation-and-update-parameters-with-gradient-descent-to-minimize-the-nll-loss">Compute gradients via automatic differentiation and update parameters with gradient descent to minimize the NLL loss.</a></li>
<li class="toc-entry toc-h1"><a href="#gradient-based-training-on-the-full-bigram-dataset-converges-to-a-parameter-matrix-that-is-equivalent-to-the-count-based-table-initialization-and-regularization-connect-to-smoothing-and-the-neural-model-can-be-used-for-sampling">Gradient-based training on the full bigram dataset converges to a parameter matrix that is equivalent to the count-based table; initialization and regularization connect to smoothing, and the neural model can be used for sampling.</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/PaCmpygFfXo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="makemore-is-a-character-level-generator-that-learns-to-produce-strings-similar-to-a-provided-dataset">MakeMore is a character-level generator that learns to produce strings similar to a provided dataset.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-01-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-01-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-01-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-01-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>MakeMore is a <strong>character-level language generator</strong> that learns from a corpus of example strings and produces new strings that resemble the training data in <strong>style and structure</strong>.<br></p>

<p>The example dataset is a large names file of roughly <strong>32,000</strong> entries, and training on this data yields novel, name-like outputs.<br></p>

<p>Because the model operates at the <strong>character level</strong>, every training example is a line treated as a sequence of characters, and repeated sampling from the trained model generates unique yet name-like tokens.<br></p>

<p>Use cases include creative generation — for example, inventing candidate <strong>names</strong> or other short strings that follow learned character patterns.<br></p>

<hr>

<h1 id="load-the-raw-dataset-into-a-list-of-strings-and-compute-basic-statistics-before-modeling">Load the raw dataset into a list of strings and compute basic statistics before modeling.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-05-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-05-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-05-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-05-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The raw text file is opened and read into a single string and then split into individual lines to form a list of word strings.<br></p>

<p>Basic exploratory statistics to compute and record include:</p>
<ul>
  <li>
<strong>Total number of words</strong> (≈32,000)</li>
  <li><strong>Shortest word length</strong></li>
  <li>
<strong>Longest word length</strong><br>
</li>
</ul>

<p>These statistics guide downstream decisions such as <strong>vocabulary size</strong> and <strong>sequence length handling</strong>.<br></p>

<p>Inspecting the first few list elements also reveals potential <strong>frequency ordering</strong>, which can affect sampling priors and preprocessing assumptions.<br></p>

<p>Recording these metadata ensures reproducible preprocessing and informs later choices for <strong>tokenization</strong> and <strong>model capacity</strong>.<br></p>

<hr>

<h1 id="a-character-level-language-model-predicts-the-next-character-in-a-sequence-and-can-be-implemented-with-models-ranging-from-bigrams-to-transformers">A character-level language model predicts the next character in a sequence and can be implemented with models ranging from bigrams to transformers.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-09-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-09-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-09-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-09-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A <strong>character-level language model</strong> treats each string as a sequence of characters and learns the conditional distribution of the next character given a context.<br></p>

<p>Implementations range from simple statistical models (e.g., <strong>bigrams</strong>) to multilayer perceptrons, <strong>recurrent neural networks (RNNs)</strong>, and modern <strong>transformers</strong>.<br></p>

<p>Key distinctions across architectures:</p>
<ul>
  <li>Amount of preceding <strong>context</strong> they condition on</li>
  <li>Parameterization of the <strong>conditional distribution</strong><br>
</li>
</ul>

<p>Building progressively from bigrams up to a character-level transformer clarifies how <strong>complexity</strong> and <strong>expressivity</strong> increase while preserving the same prediction and training objectives.<br></p>

<p>Character-level models are a compact setting to explore language-modeling mechanics before scaling to word-level or multimodal architectures.<br></p>

<hr>

<h1 id="bigrams-are-extracted-by-sliding-a-two-character-window-over-sequences-augmented-with-explicit-start-and-end-tokens">Bigrams are extracted by sliding a two-character window over sequences augmented with explicit start and end tokens.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-14-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-14-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-14-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-14-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A <strong>bigram dataset</strong> is formed by sliding a window of two consecutive characters through each augmented sequence.<br></p>

<p>Augmentation is done by <strong>prepending a start token</strong> and <strong>appending an end token</strong> so that boundary events (start→first, last→end) become explicit training examples.<br></p>

<p>In code, the Python <strong>zip trick</strong> (zipping a string with its offset-by-one copy) produces consecutive character pairs for each word efficiently.<br></p>

<p>Including the special tokens ensures the model learns <strong>initial</strong> and <strong>terminal</strong> character statistics.<br></p>

<p>Each original string therefore yields multiple training examples equal to its length plus one (for the final end token), providing many local-context examples for statistical estimation.<br></p>

<p>This bigram representation is the foundation for both counting-based statistics and neural formulations.<br></p>

<hr>

<h1 id="accumulate-bigram-frequencies-in-a-dictionary-and-then-represent-them-as-a-2d-count-matrix-for-efficient-computation">Accumulate bigram frequencies in a dictionary and then represent them as a 2D count matrix for efficient computation.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-20-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-20-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-20-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-20-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Bigram occurrences</strong> can be counted in a dictionary keyed by (char1, char2) tuples with integer counts incremented for every observed pair; this gives a simple maximum-likelihood estimate of conditional counts.<br></p>

<p>For computational efficiency and vectorized operations, convert the dictionary counts into a two-dimensional numeric array (matrix):</p>
<ul>
  <li>Rows index the <strong>first character</strong>
</li>
  <li>Columns index the <strong>second character</strong><br>
</li>
</ul>

<p>A numeric tensor representation (e.g., a <strong>PyTorch tensor</strong>) allows efficient indexing, arithmetic, and downstream normalization into probability distributions.<br></p>

<p>Using an <strong>integer data type</strong> for counts preserves exactness prior to conversion to floating point for probabilistic computations.<br></p>

<hr>

<h1 id="build-character-to-index-and-index-to-character-lookup-tables-populate-the-count-matrix-and-visualize-bigram-statistics">Build character-to-index and index-to-character lookup tables, populate the count matrix, and visualize bigram statistics.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-27-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-27-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-27-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-27-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Construct an <strong>ordered vocabulary</strong> by concatenating the dataset and taking the sorted set of characters, then enumerate this list to create:</p>
<ul>
  <li>
<strong>s2i</strong> (string-to-index) mapping</li>
  <li>
<strong>i2s</strong> (index-to-string) inverse for reconstruction<br>
</li>
</ul>

<p>Use these mappings to translate character bigrams to integer indices and increment the corresponding entries in the 2D count matrix.<br></p>

<p>Visualization (for example with <strong>matplotlib</strong>) of the 2D matrix provides diagnostics that expose frequent and rare transitions and highlights anomalies or structural patterns in the dataset.<br></p>

<p>When plotting, convert tensor elements to native Python numbers (e.g., using <strong>.item()</strong>) as framework-specific scalar objects may not be directly plottable.<br></p>

<hr>

<h1 id="special-startend-token-placement-can-create-wasted-zero-rowscolumns-use-a-single-sentinel-and-offset-indices-to-eliminate-redundancy">Special start/end token placement can create wasted zero rows/columns; use a single sentinel and offset indices to eliminate redundancy.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-33-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-33-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-33-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-33-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>When start and end tokens are represented as distinct indices and placed only at sequence boundaries, entire rows or columns of the count matrix become identically zero because those transitions are impossible by construction.<br></p>

<p>A compact alternative is to replace the two separate tokens with a single <strong>sentinel</strong> (for example, a dot) at <strong>index 0</strong> and offset regular characters by +1. This yields an N×N matrix (for the names example, <strong>27×27</strong>) with no structurally guaranteed empty rows/columns.<br></p>

<p>Benefits of this compacting:</p>
<ul>
  <li>Reduces wasted storage</li>
  <li>Simplifies indexing logic for sampling and training<br>
</li>
</ul>

<p>Ensure the sentinel is handled consistently across preprocessing and sampling to avoid subtle out-of-range or impossible-transition bugs.<br></p>

<hr>

<h1 id="sample-sequences-by-normalizing-a-row-of-counts-into-probabilities-and-drawing-with-torchmultinomial-using-a-seeded-generator-for-deterministic-randomness">Sample sequences by normalizing a row of counts into probabilities and drawing with torch.multinomial using a seeded generator for deterministic randomness.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-39-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-39-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-39-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-39-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Sampling the next character from the bigram model proceeds as follows:</p>
<ol>
  <li>Select the <strong>row of counts</strong> corresponding to the current character.</li>
  <li>Convert that integer vector to floating point and normalize by its sum to create a <strong>probability distribution</strong>.</li>
  <li>Sample an index according to that distribution.<br>
</li>
</ol>

<p>In <strong>PyTorch</strong>, <strong>torch.multinomial</strong> performs multinomial draws and accepts a <strong>generator</strong> object seeded for deterministic reproducibility so repeated runs produce identical samples when required.<br></p>

<p>Set <strong>replacement</strong> appropriately (usually True) to allow repeated draws of the same index within a sampling batch.<br></p>

<p>Map sampled indices back to characters via <strong>i2s</strong> to yield the generated sequence; termination is signaled by sampling the sentinel index.<br></p>

<hr>

<h1 id="iterative-sampling-from-a-bigram-model-produces-limited-quality-outputs-because-context-is-restricted-to-one-previous-character-uniform-baselines-illustrate-the-learned-signal">Iterative sampling from a bigram model produces limited-quality outputs because context is restricted to one previous character; uniform baselines illustrate the learned signal.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-46-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-46-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-46-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-46-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Repeated sampling by starting at the sentinel and drawing successive characters until the sentinel appears yields outputs that are superficially name-like but often nonsensical — reflecting the intrinsic limitation of <strong>bigram models</strong>, which condition only on the immediately preceding character.<br></p>

<p>A <strong>uniform sampling baseline</strong> (equal probability for each character) demonstrates how much empirical bigram probabilities have learned structure: the trained bigram model typically outperforms uniform, but still produces poor global coherence because <strong>longer-range dependencies</strong> are ignored.<br></p>

<p>This comparison highlights the need for larger-context models (n-grams with n&gt;2, RNNs, transformers) to capture <strong>morphological</strong> and <strong>phonotactic</strong> patterns beyond local adjacency.<br></p>

<p>Empirical inspection of generated samples is a practical, qualitative diagnostic for model adequacy.<br></p>

<hr>

<h1 id="precompute-a-probability-matrix-p-by-normalizing-the-counts-tensor-once-and-use-it-for-fast-sampling-instead-of-repeated-per-step-normalization">Precompute a probability matrix P by normalizing the counts tensor once and use it for fast sampling instead of repeated per-step normalization.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/00-54-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/00-54-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/00-54-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/00-54-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Repeated per-step conversion of count rows to probabilities is inefficient and duplicates work.<br></p>

<p>Instead:</p>
<ol>
  <li>Compute a floating-point copy of the entire count matrix.</li>
  <li>Sum each row to produce a column vector of row totals.</li>
  <li>Divide the count matrix by that column vector to obtain a full <strong>P</strong> matrix of row-wise conditional probability distributions.<br>
</li>
</ol>

<p>Performing this normalization once up front enables constant-time row lookup during sampling and avoids repeated allocations and numeric conversions.<br></p>

<p>Use <strong>torch.sum</strong> with the appropriate dimension and <strong>keepdim=True</strong> to preserve the singleton dimension required for correct broadcasting during division.<br></p>

<p>Storing <strong>P</strong> as a float tensor trades some memory for much faster sampling and vectorized evaluation.<br></p>

<hr>

<h1 id="respect-broadcasting-semantics-and-prefer-keepdim-to-preserve-intended-shapes-use-in-place-ops-to-reduce-allocations">Respect broadcasting semantics and prefer keepdim to preserve intended shapes; use in-place ops to reduce allocations.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-03-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-03-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-03-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-03-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Broadcasting rules require alignment of tensor shapes starting from trailing dimensions; preserving singleton dimensions via <strong>keepdim=True</strong> prevents silent creation of undesired row/column orientation when a summed vector is used in elementwise operations.<br></p>

<p>If a summed vector loses its singleton dimension, it may broadcast as a row rather than a column, yielding <strong>column-wise normalization</strong> instead of the intended row-wise normalization and producing incorrect probability matrices — a subtle, framework-specific bug.<br></p>

<p>In-place operations (where supported) reduce memory churn and can be faster than creating new tensors; however, they must be used with an understanding of <strong>autograd</strong> semantics to avoid interfering with gradient computation.<br></p>

<p>Careful unit tests or shape assertions are recommended when performing broadcasted normalization to avoid hard-to-find errors.<br></p>

<hr>

<h1 id="measure-model-quality-with-the-likelihood-and-its-log-transforms-using-negative-log-likelihood-as-a-loss-to-minimize">Measure model quality with the likelihood and its log transforms, using negative log-likelihood as a loss to minimize.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-11-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-11-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-11-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-11-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The <strong>likelihood</strong> of the dataset under the model is the product of predicted probabilities for every observed bigram; the <strong>log-likelihood</strong> is the sum of the log probabilities, which is numerically more stable and additive across examples.<br></p>

<p>Define <strong>negative log-likelihood (NLL)</strong> as the negative of the log-likelihood. NLL is a convenient loss because:</p>
<ul>
  <li>It maps perfect predictions to <strong>zero</strong>
</li>
  <li>It penalizes improbable predictions with larger positive values<br>
</li>
</ul>

<p>Averaging the NLL across examples yields an interpretable per-example loss metric (e.g., measured in bits or nats depending on log base) that can be minimized during training.<br></p>

<p>Using the log makes multiplication of many small probabilities tractable and converts maximum-likelihood estimation into a standard minimization objective.<br></p>

<hr>

<h1 id="prevent-infinite-loss-from-zero-probability-events-by-applying-additive-smoothing-to-counts-before-normalization">Prevent infinite loss from zero-probability events by applying additive smoothing to counts before normalization.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-19-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-19-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-19-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-19-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Direct maximum-likelihood counts may assign <strong>zero probability</strong> to unseen bigrams, which produces log(0) = -infinity and makes the NLL ill-posed on those examples.<br></p>

<p><strong>Additive smoothing (Laplace smoothing)</strong> adds a small positive pseudo-count (commonly <strong>1</strong>) to every count cell before normalization, ensuring every bigram has non-zero probability and finite log-loss.<br></p>

<p>Varying the smoothing constant interpolates between the empirical distribution (small smoothing) and a uniform prior (very strong smoothing); the smoothing choice trades <strong>bias</strong> for numeric robustness.<br></p>

<p>Smoothing is especially important when evaluating or generating sequences that may contain low-frequency or previously unseen transitions.<br></p>

<hr>

<h1 id="cast-bigram-prediction-as-a-supervised-neural-network-task-by-compiling-input-target-integer-pairs-from-all-bigrams">Cast bigram prediction as a supervised neural network task by compiling input-target integer pairs from all bigrams.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-28-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-28-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-28-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-28-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The supervised training dataset consists of:</p>
<ul>
  <li>
<strong>Input indices</strong> (the first character of each bigram)</li>
  <li>
<strong>Target indices</strong> (the second character)<br>
</li>
</ul>

<p>Collect these across all augmented sequences so that each bigram becomes a separate training example.<br></p>

<p>Convert the collected Python lists into framework tensors using constructors that preserve intended integer dtypes (for PyTorch, use <strong>torch.tensor</strong> with integer dtype semantics).<br></p>

<p>This formulation enables standard neural training pipelines: batched forward passes, differentiable losses computed on predicted distributions, and backpropagation to optimize parameters.<br></p>

<p>Treating bigram prediction as a <strong>classification task</strong> over the token vocabulary frames the problem for gradient-based optimization.<br></p>

<hr>

<h1 id="use-one-hot-encoding-to-convert-integer-inputs-to-float-vectors-and-implement-the-linear-layer-as-a-weight-matrix-evaluated-by-matrix-multiplication">Use one-hot encoding to convert integer inputs to float vectors and implement the linear layer as a weight matrix evaluated by matrix multiplication.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-37-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-37-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-37-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-37-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Integer token indices are transformed into <strong>one-hot vectors</strong> of length equal to the vocabulary size so a linear layer (matrix multiply plus optional bias) can process them as real-valued inputs.<br></p>

<p>Note: One-hot encoding APIs often return an integer dtype by default, so an explicit cast to <strong>float</strong> is typically required before feeding vectors into a neural layer.<br></p>

<p>The linear layer weight matrix <strong>W</strong> with shape (<strong>vocab_size</strong>, <strong>hidden_or_output_dim</strong>) can be multiplied with the one-hot vectors; when inputs are one-hot, multiplication is equivalent to selecting the corresponding <strong>row of W</strong>, so the matrix representation generalizes the lookup table.<br></p>

<p>Initializing weights with small random values (e.g., a normal distribution) provides <strong>symmetry breaking</strong> for gradient-based learning.<br></p>

<hr>

<h1 id="interpret-linear-outputs-as-logits-apply-softmax-to-obtain-probabilities-and-compute-nll-loss-by-indexing-predicted-probabilities-at-target-positions">Interpret linear outputs as logits, apply softmax to obtain probabilities, and compute NLL loss by indexing predicted probabilities at target positions.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-46-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-46-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-46-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-46-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A neural net linear layer outputs real-valued <strong>logits</strong> for every class which are exponentiated and normalized (softmax) to produce a valid probability distribution over next tokens.<br></p>

<p>The <strong>softmax</strong> operation is the differentiable composition of exp and row-wise normalization and maps arbitrary logits to non-negative numbers that sum to one.<br></p>

<p>For loss calculation:</p>
<ul>
  <li>Select the probability assigned to the true <strong>target class</strong> for each example by indexing the probability matrix at row indices (batch positions) and column indices (targets).</li>
  <li>Convert these to log probabilities and average the negative logs to produce the <strong>NLL</strong>.<br>
</li>
</ul>

<p>This vectorized indexing approach enables efficient batch loss computation without explicit Python loops.<br></p>

<hr>

<h1 id="compute-gradients-via-automatic-differentiation-and-update-parameters-with-gradient-descent-to-minimize-the-nll-loss">Compute gradients via automatic differentiation and update parameters with gradient descent to minimize the NLL loss.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-53-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-53-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-53-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-53-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Automatic differentiation frameworks build a computational graph during the forward pass and populate gradient tensors after invoking <strong>backward()</strong> on the scalar loss, making derivatives of the NLL with respect to model parameters available automatically.<br></p>

<p>Before backward propagation, parameter gradients are typically zeroed or set to <strong>None</strong> to avoid accumulation from previous iterations; after <strong>loss.backward()</strong>, each parameter’s <strong>.grad</strong> field contains its gradient.<br></p>

<p>Parameter updates are implemented as in-place operations on parameter data, for example:</p>
<ul>
  <li>w.data -= learning_rate * w.grad<br>
</li>
</ul>

<p>Gradient clearing is performed before the next forward-backward cycle. Iterating this loop reduces the average NLL and shapes logits so that exponentiated and normalized outputs match empirical bigram frequencies.<br></p>

<hr>

<h1 id="gradient-based-training-on-the-full-bigram-dataset-converges-to-a-parameter-matrix-that-is-equivalent-to-the-count-based-table-initialization-and-regularization-connect-to-smoothing-and-the-neural-model-can-be-used-for-sampling">Gradient-based training on the full bigram dataset converges to a parameter matrix that is equivalent to the count-based table; initialization and regularization connect to smoothing, and the neural model can be used for sampling.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec02/01-56-51-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec02/01-56-51-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec02/01-56-51-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec02/01-56-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>When the neural linear model is trained with NLL on the entire bigram dataset, the learned weight matrix (after exponentiation) reproduces the relative frequencies captured by the explicit count table — gradient-based optimization recovers the <strong>maximum-likelihood</strong> solution that counting produced.<br></p>

<p>Practical notes:</p>
<ul>
  <li>Initializing the weight matrix to <strong>zeros</strong> yields uniform logits and therefore uniform probabilities, which is equivalent to very strong additive smoothing.</li>
  <li>Adding an <strong>L2 penalty</strong> on weights acts as a regularizer that biases weights toward zero and increases distributional smoothness.<br>
</li>
</ul>

<p>The trained neural model supports deterministic or seeded probabilistic sampling by computing logits for the current token, applying softmax to obtain <strong>P</strong>, and drawing the next token from that distribution.<br></p>

<p>This neural formulation generalizes straightforwardly to larger contexts and more complex architectures where explicit table counting becomes infeasible.<br></p>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">The Foundations and Frontiers of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/karpathy-lec03/">Karpathy Series - Bulding Makemore Part 2 - MLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/karpathy-lec12/">Karpathy Series - Let's build GPT from scratch</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
