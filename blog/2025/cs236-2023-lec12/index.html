<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Stanford CS236 - Deep Generative Models I 2023 I Lecture 12 - Energy Based Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec12/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Stanford CS236 - Deep Generative Models I 2023 I Lecture 12 - Energy Based Models</h1>
    <p class="post-meta">December 18, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#energy-based-models-ebms-define-probability-densities-via-an-unnormalized-energy-function-and-a-parameter-dependent-partition-function">Energy-based models (EBMs) define probability densities via an unnormalized energy function and a parameter-dependent partition function</a></li>
<li class="toc-entry toc-h1"><a href="#partition-function-intractability-makes-direct-likelihood-evaluation-and-maximum-likelihood-training-difficult-but-relative-probability-comparisons-are-feasible">Partition function intractability makes direct likelihood evaluation and maximum likelihood training difficult, but relative probability comparisons are feasible</a></li>
<li class="toc-entry toc-h1"><a href="#contrastive-divergence-estimates-likelihood-gradients-by-contrasting-training-data-with-samples-from-the-model">Contrastive Divergence estimates likelihood gradients by contrasting training data with samples from the model</a></li>
<li class="toc-entry toc-h1"><a href="#markov-chain-monte-carlo-mcmc-methods-enable-sampling-from-ebms-via-local-proposals-and-acceptance-rules-that-satisfy-detailed-balance">Markov chain Monte Carlo (MCMC) methods enable sampling from EBMs via local proposals and acceptance rules that satisfy detailed balance</a></li>
<li class="toc-entry toc-h1"><a href="#langevin-dynamics-and-noisy-gradient-ascent-are-efficient-mcmc-proposals-for-continuous-ebms-using-score-information">Langevin dynamics and noisy gradient ascent are efficient MCMC proposals for continuous EBMs using score information</a></li>
<li class="toc-entry toc-h1"><a href="#sampling-and-contrastive-training-costs-make-naive-likelihood-based-training-of-ebms-impractical-at-scale">Sampling and contrastive-training costs make naive likelihood-based training of EBMs impractical at scale</a></li>
<li class="toc-entry toc-h1"><a href="#the-score-function-is-the-gradient-of-the-log-density-with-respect-to-the-input-and-does-not-depend-on-the-partition-function">The score function is the gradient of the log-density with respect to the input and does not depend on the partition function</a></li>
<li class="toc-entry toc-h1"><a href="#score-matching-and-the-fisher-divergence-compare-distributions-by-their-score-vector-fields-and-yield-a-partition-free-training-objective">Score matching and the Fisher divergence compare distributions by their score vector fields and yield a partition-free training objective</a></li>
<li class="toc-entry toc-h1"><a href="#integration-by-parts-and-its-multivariate-analogue-converts-the-fisher-divergence-into-a-computable-loss-involving-model-derivatives-only">Integration by parts (and its multivariate analogue) converts the Fisher divergence into a computable loss involving model derivatives only</a></li>
<li class="toc-entry toc-h1"><a href="#practical-computation-of-the-score-matching-loss-requires-approximations-for-second-derivatives-and-scalable-estimators-such-as-sliced-or-denoising-score-matching">Practical computation of the score-matching loss requires approximations for second derivatives and scalable estimators such as sliced or denoising score matching</a></li>
<li class="toc-entry toc-h1"><a href="#noise-contrastive-estimation-nce-trains-a-classifier-to-distinguish-data-from-a-known-noise-distribution-yielding-density-ratio-estimates">Noise-contrastive estimation (NCE) trains a classifier to distinguish data from a known noise distribution, yielding density-ratio estimates</a></li>
<li class="toc-entry toc-h1"><a href="#an-energy-based-discriminator-with-a-learnable-partition-constant-turns-nce-into-a-trainable-ebm-without-inner-loop-sampling">An energy-based discriminator with a learnable partition constant turns NCE into a trainable EBM without inner-loop sampling</a></li>
<li class="toc-entry toc-h1"><a href="#adapting-the-noise-distribution-via-parametric-flows-yields-flow-contrastive-estimation-that-jointly-trains-a-tractable-generator-and-an-ebm-like-discriminator">Adapting the noise distribution via parametric flows yields flow-contrastive estimation that jointly trains a tractable generator and an EBM-like discriminator</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Nci1Bepcy0g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="energy-based-models-ebms-define-probability-densities-via-an-unnormalized-energy-function-and-a-parameter-dependent-partition-function">Energy-based models (EBMs) define probability densities via an unnormalized energy function and a parameter-dependent partition function</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-00-52-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-00-52-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-00-52-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-00-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Energy-based models represent probability densities using an <strong>energy function f_theta(x)</strong> so that the model density is<br>
<strong>p_theta(x) = exp(f_theta(x)) / Z(theta)</strong>, where the <strong>partition function Z(theta)</strong> normalizes the unnormalized probability.<br></p>

<ul>
  <li>The <strong>energy function</strong> can be any parameterized function (commonly a neural network), which makes EBMs a highly <strong>flexible family of distributions</strong>.<br>
</li>
  <li>The partition function <strong>Z(theta)</strong> equals the integral or sum of exp(f_theta(x)) over the sample space and <strong>depends on the model parameters</strong>, so likelihood values are meaningful only relative to Z(theta).<br>
</li>
  <li>Because <strong>Z(theta) couples all possible x values</strong>, evaluating exact normalized likelihoods typically becomes <strong>computationally intractable in high-dimensional settings</strong>.<br>
</li>
</ul>

<hr>

<h1 id="partition-function-intractability-makes-direct-likelihood-evaluation-and-maximum-likelihood-training-difficult-but-relative-probability-comparisons-are-feasible">Partition function intractability makes direct likelihood evaluation and maximum likelihood training difficult, but relative probability comparisons are feasible</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-02-53-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-02-53-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-02-53-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-02-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The partition function <strong>Z(theta)</strong> is generally <strong>intractable</strong> to compute for multivariate or high-dimensional x because it requires summing or integrating the unnormalized probability across an exponentially large space.<br></p>

<ul>
  <li>Direct probability comparisons are feasible via ratios because <strong>Z(theta)</strong> cancels: <strong>p_theta(x) / p_theta(x’)</strong>, enabling relative-likelihood comparisons useful for many sampling procedures.<br>
</li>
  <li>
<strong>Maximum likelihood training</strong> is challenging because the log-likelihood gradient contains two theta-dependent terms:
    <ol>
      <li>The gradient of the <strong>energy at the data</strong> (depends on f_theta at data points).<br>
</li>
      <li>The <strong>gradient of log Z(theta)</strong>, which requires knowing how parameter changes reweight the entire space.<br>
</li>
    </ol>
  </li>
  <li>Consequently, <strong>practical training requires approximations or methods that avoid direct evaluation of Z(theta)</strong>.<br>
</li>
</ul>

<hr>

<h1 id="contrastive-divergence-estimates-likelihood-gradients-by-contrasting-training-data-with-samples-from-the-model">Contrastive Divergence estimates likelihood gradients by contrasting training data with samples from the model</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-05-06-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-05-06-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-05-06-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-05-06.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Contrastive Divergence (CD)</strong> approximates the log-likelihood gradient by comparing energy gradients at data samples and at samples drawn from the current model, turning the intractable partition-function gradient into a <strong>sample-based estimate</strong>.<br></p>

<ol>
  <li>Start with a minibatch of real data samples.</li>
  <li>Generate short-run samples from the current model (e.g., a few MCMC/Langevin steps) starting from the data or other initializations.</li>
  <li>Compute the gradient difference: increase unnormalized probability (decrease energy) for real data and decrease it for the synthetic samples.<br>
</li>
</ol>

<ul>
  <li>The intuitive update is: <strong>raise mass on data, lower mass on generated samples</strong>, approximating the effect of the partition-function term.<br>
</li>
  <li>This requires the ability to <strong>generate samples from the model</strong>; when those samples approximate p_theta well, the CD gradient approximates the true maximum-likelihood gradient.<br>
</li>
  <li>Practically, CD shifts complexity from evaluating <strong>Z(theta)</strong> to <strong>generating representative model samples</strong>.<br>
</li>
</ul>

<hr>

<h1 id="markov-chain-monte-carlo-mcmc-methods-enable-sampling-from-ebms-via-local-proposals-and-acceptance-rules-that-satisfy-detailed-balance">Markov chain Monte Carlo (MCMC) methods enable sampling from EBMs via local proposals and acceptance rules that satisfy detailed balance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-09-12-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-09-12-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-09-12-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-09-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Sampling from an EBM is typically done by constructing a Markov chain whose transition operator satisfies <strong>detailed balance</strong> with respect to <strong>p_theta</strong>.<br></p>

<ul>
  <li>A common recipe:
    <ol>
      <li>Propose a local perturbation x’ from the current state x.</li>
      <li>Accept or reject x’ based on the ratio of unnormalized probabilities (the Metropolis–Hastings acceptance rule).<br>
</li>
    </ol>
  </li>
  <li>The acceptance rule enforces that transitions occur with probabilities making <strong>p_theta</strong> a fixed point of the Markov operator, so under mild conditions repeated application converges to <strong>p_theta</strong> regardless of initialization.<br>
</li>
  <li>Intuitively, <strong>MCMC</strong> is stochastic local search (or stochastic hill-climbing): uphill moves are accepted deterministically, downhill moves accepted with probability proportional to the unnormalized-probability ratio, preserving the correct stationary distribution.<br>
</li>
  <li>In high-dimensional spaces, however, <strong>mixing can be extremely slow</strong>, and many steps may be required to obtain high-quality independent samples.<br>
</li>
</ul>

<hr>

<h1 id="langevin-dynamics-and-noisy-gradient-ascent-are-efficient-mcmc-proposals-for-continuous-ebms-using-score-information">Langevin dynamics and noisy gradient ascent are efficient MCMC proposals for continuous EBMs using score information</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-15-39-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-15-39-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-15-39-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-15-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Langevin dynamics</strong> perform MCMC in continuous state spaces by combining gradient ascent on the log unnormalized density (the <strong>score</strong>) with injected Gaussian noise.<br></p>

<ul>
  <li>Each update has the form: <strong>x_{t+1} = x_t + (epsilon^2 / 2) * grad_x log p_theta(x_t) + epsilon * Normal(0, I)</strong>, so the step size <strong>epsilon</strong> controls the signal-to-noise trade-off and must be scaled relative to the gradient magnitude to ensure convergence.<br>
</li>
  <li>Variants:
    <ul>
      <li>
<strong>MALA</strong> (Metropolis-adjusted Langevin algorithm) adds an MH accept/reject step to correct discretization bias.</li>
      <li>
<strong>Unadjusted Langevin</strong> always accepts; both converge to p_theta in the limit of small step size and many iterations under technical conditions.<br>
</li>
    </ul>
  </li>
  <li>Using gradient information (the score) usually yields <strong>much faster practical mixing</strong> than naive local proposals, but each step is computationally costly because it requires evaluating <strong>grad_x f_theta(x)</strong> via backprop.<br>
</li>
</ul>

<hr>

<h1 id="sampling-and-contrastive-training-costs-make-naive-likelihood-based-training-of-ebms-impractical-at-scale">Sampling and contrastive-training costs make naive likelihood-based training of EBMs impractical at scale</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-22-01-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-22-01-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-22-01-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-22-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Training EBMs with sampling-based inner loops (e.g., <strong>CD</strong>, <strong>MCMC</strong>, <strong>Langevin</strong>) requires generating fresh model samples repeatedly during optimization, which multiplies the cost of forward and backward passes through the energy network by the number of sampling steps.<br></p>

<ul>
  <li>If thousands or tens of thousands of sampling steps are needed per sample to reach high-probability modes, including such sampling during each training update becomes <strong>computationally prohibitive</strong>.<br>
</li>
  <li>Therefore, <strong>alternative training objectives that avoid model sampling during training</strong> are desirable for efficient, scalable EBM training.</li>
  <li>These alternatives typically exploit quantities that <strong>do not depend on the partition function</strong>.<br>
</li>
</ul>

<hr>

<h1 id="the-score-function-is-the-gradient-of-the-log-density-with-respect-to-the-input-and-does-not-depend-on-the-partition-function">The score function is the gradient of the log-density with respect to the input and does not depend on the partition function</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-27-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-27-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-27-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-27-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The <strong>score function s_theta(x) = grad_x log p_theta(x)</strong> equals <strong>grad_x f_theta(x)</strong> because the partition function <strong>Z(theta)</strong> does not depend on x and drops out when differentiating with respect to the input.<br></p>

<ul>
  <li>Consequences:
    <ul>
      <li>The <strong>score</strong> is directly computable from the energy network without evaluating <strong>Z(theta)</strong>.</li>
      <li>It provides a vector field that points in the direction of steepest increase of log-density at every x.</li>
      <li>For simple parametric families (e.g., Gaussians) the score has closed-form dependence on x and parameters; for neural-network energies it is available via automatic differentiation.<br>
</li>
    </ul>
  </li>
  <li>Using the <strong>score</strong> rather than the log-density itself opens a pathway to construct training losses that <strong>avoid partition-function evaluation</strong>.<br>
</li>
</ul>

<hr>

<h1 id="score-matching-and-the-fisher-divergence-compare-distributions-by-their-score-vector-fields-and-yield-a-partition-free-training-objective">Score matching and the Fisher divergence compare distributions by their score vector fields and yield a partition-free training objective</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-32-48-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-32-48-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-32-48-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-32-48.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The <strong>Fisher divergence</strong> measures discrepancy between two densities p and q by the expected squared L2 norm of the difference between their score functions:<br>
<strong>E_{x~p}[ || grad_x log p(x) - grad_x log q(x) ||^2 ]</strong>, which vanishes iff p = q under mild conditions.<br></p>

<ul>
  <li>Because it depends only on <strong>score fields</strong>, the Fisher divergence does <strong>not require evaluating normalization constants</strong>, making it well suited to EBMs whose scores are available from the energy gradient.</li>
  <li>Minimizing the Fisher divergence between the data distribution p_data and a model p_theta corresponds to <strong>matching their score fields</strong> and provides an alternative to KL-based maximum likelihood that is free of explicit partition-function dependence.</li>
  <li>Conceptually, this reframes density matching as aligning a conserved vector field (the <strong>score</strong>) rather than matching scalar likelihood values.<br>
</li>
</ul>

<hr>

<h1 id="integration-by-parts-and-its-multivariate-analogue-converts-the-fisher-divergence-into-a-computable-loss-involving-model-derivatives-only">Integration by parts (and its multivariate analogue) converts the Fisher divergence into a computable loss involving model derivatives only</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-43-18-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-43-18-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-43-18-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-43-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Although the Fisher divergence contains the unknown data score term, <strong>integration by parts</strong> (univariate) or the <strong>divergence theorem</strong> (multivariate) can move derivatives from the unknown data density onto the model score, eliminating explicit dependence on <strong>grad_x log p_data</strong> under mild boundary conditions.<br></p>

<ul>
  <li>In one dimension the manipulation yields an objective composed of:
    <ul>
      <li>The squared model score, and</li>
      <li>The derivative of the model score — both computable for a parametric energy model.<br>
</li>
    </ul>
  </li>
  <li>In multiple dimensions the transformed objective involves:
    <ul>
      <li>The squared norm of the model score, plus</li>
      <li>The trace of the model score’s Jacobian (equivalently, the <strong>Hessian of log p_theta</strong>).<br>
</li>
    </ul>
  </li>
  <li>Up to a theta-independent constant, the result is an expectation over data samples of terms depending only on the model and its derivatives, enabling empirical estimation by sample averages and stochastic optimization using only training data and energy-network derivatives.<br>
</li>
</ul>

<hr>

<h1 id="practical-computation-of-the-score-matching-loss-requires-approximations-for-second-derivatives-and-scalable-estimators-such-as-sliced-or-denoising-score-matching">Practical computation of the score-matching loss requires approximations for second derivatives and scalable estimators such as sliced or denoising score matching</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-52-30-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-52-30-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-52-30-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-52-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Direct evaluation of the multivariate objective requires computing the <strong>trace of the Hessian of log p_theta(x)</strong> (or equivalent second-order input derivatives), which is expensive in high dimensions because it naively requires many backward passes.<br></p>

<ul>
  <li>Scalable alternatives:
    <ul>
      <li>
<strong>Hutchinson’s estimator</strong> (randomized trace estimation) projects Hessian action onto random vectors to approximate the trace.</li>
      <li>
<strong>Sliced score matching</strong> approximates the multivariate trace via random one-dimensional projections.</li>
      <li>
<strong>Denoising score matching</strong> recovers score information by training to denoise noisy inputs, avoiding explicit second-order computation.<br>
</li>
    </ul>
  </li>
  <li>Interpreting the resulting loss shows it encourages:
    <ul>
      <li>Data points to be <strong>stationary points</strong> of the model log-density (small gradients), and</li>
      <li>Those points to be <strong>local maxima rather than minima</strong> (controlled by second-derivative terms), aligning model mass with data modes.<br>
</li>
    </ul>
  </li>
  <li>These approximations retain the <strong>partition-free advantage</strong> while enabling practical optimization of score-based objectives at scale.<br>
</li>
</ul>

<hr>

<h1 id="noise-contrastive-estimation-nce-trains-a-classifier-to-distinguish-data-from-a-known-noise-distribution-yielding-density-ratio-estimates">Noise-contrastive estimation (NCE) trains a classifier to distinguish data from a known noise distribution, yielding density-ratio estimates</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/00-59-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/00-59-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/00-59-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/00-59-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Noise-contrastive estimation (NCE)</strong> reframes density estimation as a supervised binary classification problem: a discriminator learns to distinguish real data from samples drawn from a chosen <strong>noise distribution p_n(x)</strong>.<br></p>

<ul>
  <li>The optimal classifier recovers the density ratio <strong>p_data(x) / (p_data(x) + p_n(x))</strong>, so because <strong>p_n(x)</strong> is chosen to be tractable to sample from and to evaluate, the classifier’s outputs provide information about p_data relative to p_n.</li>
  <li>NCE turns unsupervised density estimation into <strong>likelihood-free discriminative training</strong> that does not require sampling from the parametric model being trained.</li>
  <li>The choice of <strong>noise distribution</strong> is critical: when p_n is similar to p_data the classifier must learn subtle structure and yields stronger learning signals.<br>
</li>
</ul>

<hr>

<h1 id="an-energy-based-discriminator-with-a-learnable-partition-constant-turns-nce-into-a-trainable-ebm-without-inner-loop-sampling">An energy-based discriminator with a learnable partition constant turns NCE into a trainable EBM without inner-loop sampling</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/01-07-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/01-07-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/01-07-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/01-07-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>NCE can be specialized to EBMs by parameterizing the classifier’s model likelihood term with <strong>p_theta(x) = exp(f_theta(x)) / Z</strong> and treating <strong>log Z</strong> as an additional free scalar parameter <strong>z</strong> that is optimized jointly with theta.<br></p>

<ul>
  <li>Under the cross-entropy objective, optimizing (theta, z) to make the classifier distinguish data from noise pushes <strong>p_theta</strong> toward <strong>p_data</strong> in the infinite-data, perfect-optimization limit, and the learned <strong>z</strong> converges to the true log partition function in that limit.</li>
  <li>Training proceeds by:
    <ol>
      <li>Sampling minibatches of real data and noise samples from <strong>p_n</strong>.</li>
      <li>Evaluating the discriminator probability via the energy and the noise density.</li>
      <li>Applying stochastic gradient updates to (theta, z).<br>
</li>
    </ol>
  </li>
  <li>This approach fits EBMs <strong>without generating samples from p_theta</strong>. In finite-data or imperfect-optimization regimes the learned <strong>z</strong> may not equal the true partition function, so the energy and normalization are approximate, but NCE remains a practical, likelihood-free fitting method.<br>
</li>
</ul>

<hr>

<h1 id="adapting-the-noise-distribution-via-parametric-flows-yields-flow-contrastive-estimation-that-jointly-trains-a-tractable-generator-and-an-ebm-like-discriminator">Adapting the noise distribution via parametric flows yields flow-contrastive estimation that jointly trains a tractable generator and an EBM-like discriminator</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec12/01-17-03-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec12/01-17-03-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec12/01-17-03-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec12/01-17-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A refinement is to parameterize the noise distribution <strong>p_n(x; phi)</strong> with a tractable <strong>flow model</strong> that can both generate samples efficiently and evaluate likelihoods, and to update phi adversarially to make the discriminator’s task harder.<br></p>

<ul>
  <li>In <strong>flow-contrastive estimation</strong>:
    <ul>
      <li>The discriminator (an energy plus normalization scalar) and the flow-based noise model are trained jointly in a <strong>minimax-style</strong> scheme.</li>
      <li>The flow model is optimized to approximate <strong>p_data</strong> and confuse the discriminator, while discriminator updates push the energy toward <strong>p_data</strong>.<br>
</li>
    </ul>
  </li>
  <li>This yields two learned objects—a <strong>flow generator</strong> and an <strong>energy function</strong>—and in practice the flow can provide high-quality samples while the energy captures discriminative density structure.</li>
  <li>The approach bridges <strong>score-based</strong>, <strong>contrastive</strong>, and <strong>adversarial</strong> paradigms but requires careful tuning due to the adversarial component. Empirically, learning the noise distribution often improves sample quality compared to a fixed noise distribution, while theoretical guarantees revert to the infinite-data, perfect-optimization limits.<br>
</li>
</ul>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
