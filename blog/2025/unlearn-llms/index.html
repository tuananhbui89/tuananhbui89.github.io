<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Unlearning LLMs | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/unlearn-llms/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Unlearning LLMs</h1>
    <p class="post-meta">March 14, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#setting">Setting</a></li>
<li class="toc-entry toc-h2"><a href="#representation-misdirection-for-unlearning-rmu">Representation Misdirection for Unlearning (RMU)</a></li>
<li class="toc-entry toc-h2"><a href="#adaptive-rmu-on-effects-of-steering-latent-representation-for-large-language-model-unlearning---aaai-2025">Adaptive RMU (On Effects of Steering Latent Representation for Large Language Model Unlearning - AAAI 2025)</a></li>
<li class="toc-entry toc-h2">
<a href="#llm-unlearning-via-loss-adjustment-with-only-forget-data-iclr-2025">LLM Unlearning via Loss Adjustment with Only Forget Data (ICLR 2025)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#motivation---eliminating-the-need-for-a-retain-set">Motivation - Eliminating the need for a retain set</a></li>
<li class="toc-entry toc-h3"><a href="#loss-adjustments-via-f-divergence-maximization">Loss-Adjustments via f-divergence Maximization</a></li>
<li class="toc-entry toc-h3"><a href="#connection-with-dpo">Connection with DPO</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#a-closer-look-at-machine-unlearning-for-large-language-models-iclr-2025">A Closer Look at Machine Unlearning for Large Language Models (ICLR 2025)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#untargeted-vs-targeted-unlearning">Untargeted vs Targeted Unlearning</a></li>
<li class="toc-entry toc-h3"><a href="#maximizing-entropy-for-untargeted-unlearning">Maximizing Entropy for Untargeted Unlearning</a></li>
<li class="toc-entry toc-h3"><a href="#mitigate-excessive-ignorance-of-targeted-unlearning">Mitigate Excessive Ignorance of Targeted Unlearning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#muse---machine-unlearning-six-way-evaluation-for-language-models-iclr-2025">MUSE - Machine Unlearning Six-Way Evaluation for Language Models (ICLR 2025)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#metrics-for-each-aspect">Metrics for each aspect</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <h2 id="introduction">Introduction</h2>

<p><strong>Why unlearning LLMs? From [1]</strong></p>

<ul>
  <li>
<strong>Reducing the risk of malicious use</strong>: to decrease the potential for LLMs to empower malicious use cases such as developing biological, cyber, or chemical weapons. Unlearning aims to <strong>remove hazardous knowledge</strong> from these LLMs, which is a <strong>prerequisite</strong> for the deployment of LLMs in the real world.</li>
  <li>
<strong>Enhancing inherent safety</strong>: By directly removing hazardous knowledge, unlearning can lead to inherently safer models. Even if the LLMs are jailbroken by adversaries, they would lack the necessary knowledge to assist adversaries in launching attacks.</li>
  <li>
<strong>Counteracting adversarial attacks</strong>: Current safeguards like refusal training [2] can be bypassed through adversarial attacks, and hazardous information can be reintroduced through finetuning. Similar to the case of enhancing inherent safety, unlearning especially when applied before model serving, can act as a countermeasure by removing the knowledge that these attacks or finetuning might exploit or reveal.</li>
</ul>

<blockquote class="block-tip">
  <p><strong>Refusal training</strong>:</p>

  <p>Refusal training is a method that trains a model to decline or refuse to generate responses to prompts that are associated with malicious use cases.</p>

  <p>This is typically achieved through (1) Supervised fine-tuning (SFT) with a dataset of pair harmful prompts with appropriate refusal responses, (2) RLHF with human preference data, (3) Adversarial training aiming to be more robust against adversarial prompts designed to bypass safety mechanisms.
However, this method is still be circumvented by aversaries, such as posing harmful prompts in the <strong><a href="https://arxiv.org/pdf/2407.11969" rel="external nofollow noopener" target="_blank">past tense</a></strong>.</p>
</blockquote>

<h2 id="setting">Setting</h2>

<ul>
  <li>Forget set: \(\mathcal{D}_f\) (used in fine-tuning) This is a dataset of examples representing the knowledge that the unlearning process aims to remove from the language model, e.g., the WMDP benchmark [1] which contains hazardous knowledge in biosecurity and cybersecurity.</li>
  <li>Retain set: \(\mathcal{D}_r\) (used in fine-tuning) This is a dataset of examples representing general, benign knowledge that the unlearning process should aim to preserve. Recent works [3] try to remove the need of a retain set by using a “flat” loss adjustment approach which adjusts the loss function using only the forget data.</li>
  <li>Testing set: \(\mathcal{D}_t\) (used in evaluation) to evaluate two aspects: (1) unlearning performance - Question-Answering (QA) accuracy on WMDP benchmark, and (2) retaining performance - other benchmarks like MMLU and MT-Bench.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-14-10-30-41-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-14-10-30-41.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Figure from [1]
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-10-51-49-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-10-51-49.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>

<p>A commonly used form of unlearning consists of the following optimization problem:</p>

\[\mathcal{L} = \min_{\theta} \underbrace{\mathbb{E}_{x^f \in \mathcal{D}_f} \ell(y^f|h_{\theta}^{(l)}(x^f))}_{\text{forget loss}} + \alpha \underbrace{\mathbb{E}_{x^r \in \mathcal{D}_r} \ell(y^r|h_{\theta}^{(l)}(x^r))}_{\text{retain loss}}\]

<p>where \(\theta\) is the model parameters of an autoregressive LLM \(f_{\theta}\), \(\ell\) is the loss function, \(y^f\) and \(y^r\) are the target representations (e.g., representations of next token) for the forget and retain sets, and \(\alpha\) is a hyperparameter.</p>

<p>It is worth noting that \(h_{\theta}^{(l)}\) is the <strong>average</strong> output hidden state of all tokens (with the size of sequence length) in forget-sample \(x^f \in \mathcal{D}_f\). Similarly, \(h_{\theta}^{(l)}\) is the <strong>average</strong> output hidden state of all tokens in retain-sample \(x^r \in \mathcal{D}_r\). Using this average representation to represent the entire forget-sample is one of current limitations that requires further investigation.</p>

<p><strong>Intepretation</strong>: Minimizing the forget loss is to steer the model representation of forget-samples to a target random representation, while minimizing the retain loss is to keep the model representation of retain-samples unchanged.</p>

<!-- ### Challenges

**Overlap between forget set and retain set**

**Expressiveness of token-level representation for the entire set/document** -->

<!-- ### Milestones - Approaches -->

<h2 id="representation-misdirection-for-unlearning-rmu">Representation Misdirection for Unlearning (RMU)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a method to unlearn LLMs by steering the representation of forget samples towards a target random representation while keeping the representation of retain samples unchanged.</p>

<p><strong>Approach</strong></p>

<p>RMU [1] aims to steer model representation of forget samples (e.g., malicious use cases - that sampled from the forget set) in the intermediate layer towards a target random representation while keeping the representation of retain samples (e.g., benign use cases - that sampled from the retain set) unchanged.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-14-10-34-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-14-10-34-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of RMU from [1]
</div>

<p>More specifically, given a forget set \(\mathcal{D}_f\) and a retain set \(\mathcal{D}_r\), and a frozen model \(h_{\theta^{\text{frozen}}}\) with parameters \(\theta^{\text{frozen}}\), RMU steers the latent representation of forget-tokens to a predetermined random representation \(y^f = cu\), where \(u\) is a random unit vector each element is sampled from Uniform distribution \(U(0,1)\), \(c \in \mathbb{R}^+\) is a coefficient, and regularizes the latent representation of retain-tokens back to the frozen model’s representation. The loss of RMU is</p>

\[\mathcal{L} = \mathbb{E}_{x^f \in \mathcal{D}_f} \|h_{\theta^{\text{rm}}}^{(l)}(x^f) - cu\|^2 + \alpha\mathbb{E}_{x^r \in \mathcal{D}_r} \|h_{\theta^{\text{rm}}}^{(l)}(x^r) - h_{\theta^{\text{frozen}}}^{(l)}(x^r)\|^2,\]

<p>where \(\theta^{\text{rm}}\) is the parameters of the model to be optimized, and \(\alpha\) is a hyperparameter.</p>

<h2 id="adaptive-rmu-on-effects-of-steering-latent-representation-for-large-language-model-unlearning---aaai-2025">Adaptive RMU (On Effects of Steering Latent Representation for Large Language Model Unlearning - AAAI 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a <strong>layer-adaptive</strong> RMU method, which uses a different \(c\) for each layer.</p>

<p><strong>Key Observation</strong></p>

<p>This paper points out an interesing phenomenon that the performance of RMU is sensitive to the choice of coefficient \(c\) in the above loss function.</p>

<p>More specifically, the random unit vector \(u\) and the representation of forget samples \(\hat{h}_{\theta^{\text{rm}}}^{(l)}(x^f)\) are more aligned as \(c\) increases, suggesting the better unlearning performance.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-16-21-49-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-16-21-49.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The distribution of the representation of forget samples with different $c$ from [4]. The blue histogram becomes more Gaussian as $c$ increases.
</div>

<p>However, using a fixed \(c\) across all layers is not ideal, since the performance of RMU is observed to be <strong>layer-dependent</strong>.
More specifically, as discussed in Section 4.3 in [4], within early layers, the \(l^2\) norm of the representation of forget samples is relatively smaller than the coefficient \(c\) and during the unlearning process, that norm exponentially grows and appproaches \(c\), thereby facilitating the convergence of the unlearning process. However, in later layer, the \(l^2\) norm of the representation of forget samples is initially larger than \(c\) and remains unchanged during unlearning, making the unlearning process less effective.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-16-34-02-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-16-34-02.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The norm of the representation of forget samples in different layers from [4].
</div>

<p>Inspired by the above observation, this paper proposes a <strong>layer-adaptive</strong> RMU method, which uses a different \(c\) for each layer.</p>

\[\mathcal{L}^{\text{adaptive}} = \underbrace{\mathbb{E}_{x_F \in \mathcal{D}_{\text{forget}}} \|h_{\theta^{\text{unlearn}}}^{(l)}(x_F) - \beta\|h_{\theta^{\text{frozen}}}^{(l)}(x_F)\|u\|_2^2}_{\text{adaptive forget loss}} + \underbrace{\alpha \mathbb{E}_{x_R \in \mathcal{D}_{\text{retain}}} \|h_{\theta^{\text{unlearn}}}^{(l)}(x_R) - h_{\theta^{\text{frozen}}}^{(l)}(x_R)\|_2^2}_{\text{retain loss}}\]

<p>where \(\beta \|h_{\theta^{\text{frozen}}}^{(l)}(x_F)\|\) is the <strong>adaptive scaling coefficient</strong> for the forget loss which is computed by the norm of the representation of forget samples in the corresponding layer of the frozen model.</p>

<p>However, it is worth noting that intuitively, the higher \(c\) leads to a more alignment between forget representation and the random unit vector \(u\), which suggests the better unlearning performance - more randomness of the output with the forget prompt. However, it also leads to a worse retaining performance.</p>

<h2 id="llm-unlearning-via-loss-adjustment-with-only-forget-data-iclr-2025">LLM Unlearning via Loss Adjustment with Only Forget Data (ICLR 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper proposes a method to unlearn LLMs by adjusting the loss function using only the forget data.</p>

<h3 id="motivation---eliminating-the-need-for-a-retain-set">Motivation - Eliminating the need for a retain set</h3>

<p>Previous unlearning methods typically require a retain set or a reference model to maintain the performance of the unlearned model on the retain set.
The limitation of this requirement (as stated in [3]) is that it is may lead to a trade-off between model utility and forget performance (why?).
Furthermore, fine-tuning using both retain data and forget data would require a careful design of a data mixing strategy to avoid information leakage from the retain set to the forget set.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-15-17-17-10-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-15-17-17-10.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison between unlearning methods from [3]
</div>

<h3 id="loss-adjustments-via-f-divergence-maximization">Loss-Adjustments via f-divergence Maximization</h3>

<p>For each learning batch, we assume that we only have access to a set of forget samples \((x_f, y_f) \in D_f\). Instead of directly adopting gradient ascent over these forget samples, we propose to maximize the divergence between exemplary and bad generations of forget data. Key steps are summarized as below.</p>

<p><strong>Step 1</strong>: Equip example/template responses \(y_e\) for each forget sample \(x_f\). Together we denote the paired samples as \(D_e = \{(x_f^j, y_e^j)\}_{j\in[N]}\).</p>

<ul>
  <li>
    <p>This could be done by leveraging open-source LLMs such as Llama 3.1 [25] or self-defining the responses according to our wish, etc. The designated unlearning response could be a reject-based answer such as “I don’t know” (denoted as “IDK”) or an irrelevant answer devoid of the unlearning target-related information.</p>
  </li>
  <li>
    <p><strong>Motivation</strong>: Step 1 generates example responses for LLM fine-tuning and provides better instructions on what LLM should respond given the forget data. Besides, certain existing methods make LLM generate hallucinated responses after unlearning, which further illustrates the importance of example responses for LLM unlearning.</p>
  </li>
</ul>

<p><strong>Step 2</strong>: Loss adjustments w.r.t. the sample pairs \((x_f, y_e, y_f)\) through:</p>

\[L(x_f, y_e, y_f; \theta) = \lambda_e \cdot L_e(x_f, y_e; \theta) - \lambda_f \cdot L_f(x_f, y_f; \theta),\]

<p>where \(L_e, L_f\) are losses designed for the data sample \((x_f, y_e)\) and \((x_f, y_f)\), respectively.</p>

<ul>
  <li>
<strong>Motivation</strong>: Step 2 encourages the LLM to forget the forget data with bad responses, meanwhile, learn to generate good responses on relevant forget data [such as template answers].</li>
</ul>

<p><strong>Step 3</strong>: How to decide on the values of \(\lambda_e\) and \(\lambda_f\)?</p>

<p>We leverage f-divergence to illustrate the appropriate balancing between \(L_e(x_f, y_e; \theta)\) and \(L_f(x_f, y_f; \theta)\). Assume \(x_f, y_e\) is generated by the random variable \(X_f, Y_e\) jointly following the distribution \(\mathcal{D}_e\). Similarly, \(x_f, y_f\) is given by \(X_f, Y_f\) and \((X_f, Y_f) \sim \mathcal{D}_f\). Step 2 shares similar insights as if we are maximizing the divergence between \(\mathcal{D}_e\) and \(\mathcal{D}_f\). Our theoretical purpose is to obtain the model that maximizes the f-divergence between \(\mathcal{D}_e\) and \(\mathcal{D}_f\), defined as \(f_{div}(\mathcal{D}_e\|\mathcal{D}_f)\).</p>

<p><strong>The variational form f-divergence</strong>: Instead of optimizing the \(f_{div}\) term directly, we resolve to the variational form of it. Due to the Fenchel duality, we would have:</p>

\[f_{div}(\mathcal{D}_e\|\mathcal{D}_f) = \sup_g [\mathbb{E}_{z_e\sim\mathcal{D}_e} [g(z_e)] - \mathbb{E}_{z_f\sim\mathcal{D}_f} [f^*(g(z_f))]] := \sup_g \text{VA}(\theta, g),\]

<p>we define \(f^*\) as the conjugate function of the f-divergence function. For simplicity, we define \(\text{VA}(\theta, g^*) := \sup_g \text{VA}(\theta, g)\), where \(g^*\) is the optimal variational function.</p>

<h3 id="connection-with-dpo">Connection with DPO</h3>

<p><a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#ppo-and-dpo">Direct Preference Optimization</a> (DPO) is a method to align LLMs with human preferences, however, unlike PPO which uses a reward model, DPO simplifies PPO by directly optimizing the policy model to adhere to human preferences without the need for a reward model. In the context of unlearning, DPO can be used to unlearn the LLM by directly optimizing the original model to align forget prompt with the template response.</p>

<p>Given a dataset \(D = \{(x_f^j, y_e^j, y_f^j)\}_{j\in[N]}\), where \(y_e\) and \(y_f\) are preferred template and original forget responses to the forget prompt \(x_f\), DPO fine-tunes original model \(\theta_o\) using \(D\) to better align it with good answer preferences, which minimizes:</p>

\[L_{\text{DPO},\beta}(\theta) = -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta\log \frac{\pi_\theta(y_e \mid x_f)}{\pi_{\text{ref}}(y_e \mid x_f)} - \beta\log \frac{\pi_\theta(y_f \mid x_f)}{\pi_{\text{ref}}(y_f \mid x_f)}\right)\right]\]

\[= -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i}) - \log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i}))-M_{\text{ref}}\right)\right],\]

<p>where, \(\sigma(t) = \frac{1}{1+e^{-t}}\) is the sigmoid function, \(\beta &gt; 0\) is the inverse temperature, \(\pi_\theta := \prod_{i=1}^{\mid y \mid} h_\theta(x, y_{&lt;i})\) is the predicted probability of the response \(y\) to prompt \(x\) given by LLM \(\theta\), \(\pi_{\text{ref}}\) is the predicted probability given by reference model, and \(M_{\text{ref}} := \beta(\log \prod_{i=1}^{\mid y_e \mid}h_{\theta_o}(x_f, y_{e,i}) - \log \prod_{i=1}^{\mid y_f \mid}h_{\theta_o}(x_f, y_{f,i}))\).</p>

<p><strong>Intepretation</strong>: minimizing \(L_{\text{DPO},\beta}(\theta)\) is equivalent to maximizing \(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i})\) - which is the log probability of the template response - while minimizing \(\log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i})\) - that of the forget response.
the \(M_{\text{ref}}\) term is a constant w.r.t. \(\theta\) which is the gap between two log probabilities of the template and forget responses given by the reference model - so that the optimal should maintain the same gap as the reference model (IMO: should add max-margin loss here).</p>

<p><strong>FLAT</strong></p>

<p>As for FLAT, we calculate the average probability of all correctly generated tokens and employ a novel re-weighting mechanism that assigns different importance to each term using distinct activate functions for both the example and forget loss terms, which minimizes:</p>

\[L_{\text{FLAT}}(\theta) = -\mathbb{E}_D\left[g^*\left(\frac{1}{\mid y_e \mid} \sum_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_{e,&lt;i})\right) - f^*(g^*\left(\frac{1}{\mid y_f \mid} \sum_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_{f,&lt;i})\right))\right].\]

<p>Here, \(f^*(\cdot), g^*(f^*(\cdot))\) are the activate functions that assign appropriate weights to each loss term. The detailed derivation is in Appendix B.2 in the paper. Specifically, DPO relies on a reference model to guide the unlearning process, whereas FLAT only uses a sample pair dataset containing both exemplar and forget responses. Besides, FLAT differs from DPO in three critical aspects: the re-weighting activation function, whether to sum or average the token losses, and whether to apply the logarithm to the output probability.</p>

<p>An example (from Appendix B.2 in the paper) is as follows:</p>

<p>Here, \(v\) is the vocabulary size, \(y_{e,i,k}\) is the \(k\)-th element of vector representing the \(i\)-th token in the good response \(y_e\), \(y_{f,i,k}\) is the \(k\)-th element of vector representing the \(i\)-th token in the forget response \(y_f\). Additionally, \(h_\theta(x_f, y_{e,&lt;i})_k\) and \(h_\theta(x_f, y_{f,&lt;i})_k\) denote the \(k\)-th entry of the probability distribution for the correctly generated token.</p>

<p>For <strong>KL f-divergence</strong>, \(f^*(u) = e^{u-1}, g^*(v) = v\), hence, \(g^*(\mathbb{P}(x_f, y_e; \theta)) - f^*(g^*(\mathbb{P}(x_f, y_f; \theta))) = \mathbb{P}(x_f, y_e; \theta) - e^{\mathbb{P}(x_f, y_f; \theta)-1}\). We have:</p>

\[L_{\text{FLAT}}(\theta) = -\mathbb{E}_D\left[\frac{\sum_{i=1}^{ \mid y_e \mid} h_\theta(x_f, y_{e,&lt;i})}{\mid y_e \mid} - e^{\frac{\sum_{i=1}^{ \mid y_f \mid} h_\theta(x_f, y_{f,&lt;i})}{\mid y_f \mid}-1}\right].\]

<h2 id="a-closer-look-at-machine-unlearning-for-large-language-models-iclr-2025">A Closer Look at Machine Unlearning for Large Language Models (ICLR 2025)</h2>

<p><strong>Summary</strong></p>

<p>This paper investigates the effectiveness of unlearning LLMs for targeted and untargeted use cases.
It proposes a Maximizing Entropy (ME) objective for untargeted unlearning and Answer-Preservation (AP) objective for targeted unlearning.</p>

<h3 id="untargeted-vs-targeted-unlearning">Untargeted vs Targeted Unlearning</h3>

<p>Targeted unlearning hopes to make a specified template response to the questions in the forget set, while untargeted unlearning only requires not leaking the contents of the forget set.
Mathematically, the loss function for untargeted unlearning is (borrowing the notation of DPOfrom [3])</p>

\[= -\frac{2}{\beta}\mathbb{E}_D\left[\log \sigma\left(\beta(\log \prod_{i=1}^{\mid y_e \mid}h_\theta(x_f, y_e,_{&lt;i}) - \log \prod_{i=1}^{\mid y_f \mid}h_\theta(x_f, y_f,_{&lt;i}))-M_{\text{ref}}\right)\right],\]

<p>Where \(y_e\) is the template response and \(y_f\) is the forget response. Targeted unlearning aims to make the model response to the forget set to be close to the template response, hence, maximizing the probability of the template response. On the other hand, untargeted unlearning only requires the model response to the forget set to be far from the forget response, hence, minimizing the probability of the forget response.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-16-15-58-48-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-16-15-58-48.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Illustration of untargeted and targeted unlearning from [5]
</div>

<h3 id="maximizing-entropy-for-untargeted-unlearning">Maximizing Entropy for Untargeted Unlearning</h3>

<p>The paper states that “the core of most untargeted unlearning methods is to adapt a gradient ascent direction that maximizes the prediction loss over the forget set - may have several challenges”.</p>

<ul>
  <li>
<strong>The behavior of the ideal retain model is unpredictable</strong>: The cost to retrain the model from scratch is extremely expensive in the case of LLMs. More importantly, gradient ascent on the forget set when retraining may lead to unpredicatable behavior of the model.</li>
  <li>
<strong>Potential hallucinations in the surrogate retain model</strong>: An alternative approach for the expensive retraining is to use a surrogate model - which is a base model such as Llama 2, fine-tuned on a small fictitious dataset \(\mathcal{D}^f = \{ \mathcal{D}^f_F,  \mathcal{D}^f_R \}\), where \(\mathcal{D}^f_F\) and \(\mathcal{D}^f_R\) are the forget and retain sets, respectively. However, this approach may lead to hallucinations, where the model generates responses that are not present in the retain set.</li>
</ul>

<p>Idea: <em>Align the prediction behavior of the unlearned model on the forget set with that of a <strong>randomly initialized</strong> model</em>.</p>

<ul>
  <li>The randomly initialized model is data-independent and does not contain any knowledge about the forget set, avoids the leakage of relevant information.</li>
  <li>The behavior of the randomly initialized model is random guessing - maximizing the entropy of the output distribution.</li>
</ul>

\[\mathcal{L}_{\text{ME}}(\mathcal{D}_F; \theta) = \mathbb{E}_{(x,y)\sim\mathcal{D}_F}\left[\frac{1}{T}\sum_{t=1}^T \text{KL}(P_t\|\mathcal{U}_{[K]})\right],\]

<p>where \(P_t = p(x'_t \mid x'_{&lt;t}; \theta)\) is the predicted probability for the \(t\)-th token in \(x' = x \circ y\) and \(\mathcal{U}_{[K]}\) is a uniform distribution over the vocabulary of size \(K\), where each value is \(1/K\).</p>

<p>Minimizing above loss is equivalent to Maximizing Entropy (ME) of predicted distribution for each next token. <strong>The greater the entropy, the higher the uncertainty of the prediction, indicating that the model behaves closer to a randomly initialized model for random guessing</strong>. This objective also avoids catastrophic collapse caused by the unbounded forget loss (Zhang et al., 2024a; Ji et al., 2024).</p>

<h3 id="mitigate-excessive-ignorance-of-targeted-unlearning">Mitigate Excessive Ignorance of Targeted Unlearning</h3>

<p><strong>Over Ignorance Issue</strong>: Refuse to answer most questions in the retain set - False Positive. It is due to (based on their argument) that \((\mathcal{X}_F, \mathcal{Y}_F) \approxeq (\mathcal{X}_R, \mathcal{Y}_R)\), therefore, increasing \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_F)\) will also increase \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_R)\).</p>

<p>Their experiment to support the above argument is as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/unlearnllms/2025-03-17-12-50-45-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/unlearnllms/2025-03-17-12-50-45.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    The correlation between performances of unlearned models on the forget set and the retain set from [5].
</div>

<p><strong>Answer-Preservation (AP)</strong>:</p>

<p>Intuitively, given a question in the retain set, the regularization loss for targeted unlearning should satisfy two objectives:</p>

<ul>
  <li>Reduce the probability of the rejection template.</li>
  <li>Maintain the probability of the original answer.</li>
</ul>

<p>Thus, the authors propose the Answer Preservation (AP) loss as follows:</p>

\[\mathcal{L}_{\text{AP}}(\mathcal{D}_R, \mathcal{D}_{\text{IDK}}; \theta) = -\frac{1}{\beta}\mathbb{E}_{(x,y)\sim\mathcal{D}_R,y'\sim\mathcal{D}_{\text{IDK}}}\left[\log \sigma\left(-\beta\log \frac{p(y' \mid x; \theta)}{p(y \mid x; \theta)}\right)\right],\]

<p>where \(\sigma(\cdot)\) is the sigmoid function, \(\beta\) is a hyper-parameter.</p>

<p>The gradient of AP loss w.r.t. the model parameters is:</p>

\[\nabla_\theta\mathcal{L}_{\text{AP}}(\theta) = \mathbb{E}_{\mathcal{D}_R,\mathcal{D}_{\text{IDK}}}[W_\theta(x, y, y')\nabla_\theta (\log p(y' \mid x; \theta) - \log p(y \mid x; \theta))].\]

<p>The \(W_\theta(x, y, y') = 1/(1 + (\frac{p(y \mid x; \theta)}{p(y' \mid x; \theta)})^\beta)\) can be regarded as an adaptive gradient weight.</p>

<p>Given a question \(x\) in \(\mathcal{D}_R\), in the early stage of unlearning process, where \(p(y \mid x; \theta) \gg p(y' \mid x; \theta)\), we have \(W_\theta(x, y, y') \ll 1\).</p>

<p>As the unlearning proceeds, either a decrease in \(p(y \mid x; \theta)\) or an increase in \(p(y' \mid x; \theta)\) will result in a larger \(W_\theta(x, y, y')\), thereby providing stronger regularization. The gradient of AP loss consists of two terms in addition to the adaptive weight. The first term is equivalent to GA on the rejection template, which satisfies the first objective. The second term is equivalent to GD on the original answer, which satisfies the second objective.</p>

<p>My interpretation of the above equation is as follows:</p>

<ul>
  <li>Reminder that in gradient descent, we follow the negative gradient direction to update the model parameters, i.e., \(\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}\)</li>
  <li>In the first term, we follow the negative direction of \(\nabla_\theta (\log p(y' \mid x; \theta)\) - which aims to reduce the probability of \(P(\mathcal{Y}_{IDK} \mid \mathcal{X}_R)\)</li>
  <li>In the second term, we follow the direction of \(\nabla_\theta (\log p(y \mid x; \theta)\) - which aims to increase the probability of original answer \(P(y \mid \mathcal{X}_R)\)</li>
</ul>

<h2 id="muse---machine-unlearning-six-way-evaluation-for-language-models-iclr-2025">MUSE - Machine Unlearning Six-Way Evaluation for Language Models (ICLR 2025)</h2>

<p>Link to the paper: <a href="https://openreview.net/pdf?id=TArmA033BU" rel="external nofollow noopener" target="_blank">OpenReview</a></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-10-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-10-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Six-way evaluation of machine unlearning
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-18-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-18-28.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Comparison with a previous benchmark TOFU.
</div>

<p>This paper present a new benchmark for evaluating the quality of machine unlearning, which considers six aspects:</p>

<ul>
  <li>No verbatim memorization: The model should not exactly replicate any details from the forget set.</li>
  <li>No knowledge memorization: The model should be incapable of responding to questions about the forget set.</li>
  <li>No privacy leakage: It should be impossible to detect that the model was ever trained on the forget set.</li>
  <li>Utility preservation: The model should maintain high performance on the tasks it was trained for except for the forget set.</li>
  <li>Scalability: The method should be able to handle large forget set sizes.</li>
  <li>Substantiality: The method should be able to handle a large number of forget queries - continuous unlearning setting.</li>
</ul>

<h3 id="metrics-for-each-aspect">Metrics for each aspect</h3>

<p><strong>Verbatim Memorization</strong>
To measure verbatim memorization, the paper uses the <a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE</a> score between the model’s continuation and the actual continuation in the forget set:</p>

\[\text{VerbMem}(f, \mathcal{D}) := \frac{1}{|\mathcal{D}_{\text{forget}}|} \sum_{x \in \mathcal{D}_{\text{forget}}} \text{ROUGE}(f(x_{[:l]}), x_{[l+1:]})\]

<p>ROUGE measures the overlap between the generated text (candidate) and one or more reference texts. It focuses on n-gram, sequence, and word-level similarity and is particularly recall-oriented, meaning it emphasizes how much of the reference text is captured by the generated text.</p>

\[\text{ROUGE} = \frac{\sum_{i=1}^n \text{Count}_{i}}{\sum_{i=1}^n \text{Count}_{i}}\]

<p>Where:</p>

<ul>
  <li>\(n\) is the number of reference texts</li>
  <li>\(\text{Count}_{i}\) is the number of n-grams in the generated text that are also in the \(i\)-th reference text</li>
</ul>

<p><strong>Knowledge Memorization</strong></p>

<p>To measure knowledge memorization, the paper uses the <a href="https://tuananhbui89.github.io/blog/2025/nlp-foundation/#rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE</a> score between the model’s continuation and the actual continuation in the forget set:</p>

\[\text{KnowMem}(f, \mathcal{D}_{\text{forget}}) := \frac{1}{|\mathcal{D}_{\text{forget}}|} \sum_{(q,a)\in\mathcal{D}_{\text{forget}}} \text{ROUGE}(f(q), a)\]

<p><strong>Privacy Leakage</strong></p>

<p>To measure privacy leakage, the paper uses (Min-K Prob) [6] a start-of-the-art MIA method to compute the standard AUC-ROC score of discriminating members \(\mathcal{D}_{\text{forget}}\) and non-members \(\mathcal{D}_{\text{holdout}}\) by using the logits of the last layer of the model.</p>

\[\text{PrivLeak} := \frac{\text{AUC}(f_{\text{unlearn}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}}) - \text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})}{\text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})}\]

<p>The good unlearning method should have \(\text{AUC}(f_{\text{unlearn}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}}) \approx \text{AUC}(f_{\text{retrain}}; \mathcal{D}_{\text{forget}}, \mathcal{D}_{\text{holdout}})\) which means the unlearned model is indistinguishable from the original model. In contrast, the bad unlearning method will get a large positive or negative difference.</p>

<p><strong>Min-K Prob [6]</strong></p>

<p>The Min-K Prob is based on the hypothesis that a non-member example is more likely to include a few outlier words with low probability, while a member example is less likely to include such words.</p>

<p>Consider a sequence of tokens in a sentence, denoted as \(x = x_1, x_2, ..., x_N\), the log-likelihood of a token, \(x_i\), given its preceding tokens is calculated as \(\log p(x_i \mid x_1, ..., x_{i-1})\). We then select the \(k\%\) of tokens from \(x\) with the minimum token probability to form a set, Min-K%(x), and compute the average log-likelihood of the tokens in this set:</p>

\[\text{MIN-K\% PROB}(x) = \frac{1}{E} \sum_{x_i \in \text{Min-K\%}(x)} \log p(x_i \midx_1, ..., x_{i-1}).\]

<p>where \(E\) is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining data simply by thresholding this MIN-K% PROB result.</p>

<p><strong>Why this new benchmark is important?</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/thoughts/2025-03-18-13-29-24-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/thoughts/2025-03-18-13-29-24.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    It seems that unlearning methods are not good at scalability - when increasing the forget size and substantiality - when number of forget queries increase.
</div>

<p>This benchmark provides another perspective - from model developer - who want to keep the model utility after unlearning. The current benchmark is more focused on the data owner’s expectation - forgetting the data - but not the model utility. With this benchmark, we can see that current unlearning methods are not good at these metrics:</p>

<ul>
  <li>“Unlearning significantly degrades model utility”</li>
  <li>“Unlearning methods scale poorly with forget set sizes”</li>
  <li>“Unlearning methods cannot sustainably accommodate sequential unlearning requests”</li>
</ul>

<p><strong>Membership Inference Attack</strong></p>

<p>A Membership Inference Attack (MIA) is a type of privacy attack where an adversary aims to determine whether a specific data sample was used in training a machine learning model. This poses a serious privacy risk, especially for models trained on sensitive datasets (e.g., medical or financial data).</p>

<h2 id="references">References</h2>

<p>[1] <a href="https://arxiv.org/abs/2403.03218" rel="external nofollow noopener" target="_blank">Li, Nathaniel, et al. “The wmdp benchmark: Measuring and reducing malicious use with unlearning.” arXiv preprint arXiv:2403.03218 (2024).</a></p>

<p>[2] <a href="https://arxiv.org/pdf/2407.11969" rel="external nofollow noopener" target="_blank">Andriushchenko, Maksym, and Nicolas Flammarion. “Does Refusal Training in LLMs Generalize to the Past Tense?.” arXiv preprint arXiv:2407.11969 (2024).</a></p>

<p>[3] <a href="https://arxiv.org/pdf/2410.11143" rel="external nofollow noopener" target="_blank">Wang, Yaxuan, et al. “LLM Unlearning via Loss Adjustment with Only Forget Data.” arXiv preprint arXiv:2410.11143 (2024).</a></p>

<p>[4] <a href="https://arxiv.org/pdf/2408.06223" rel="external nofollow noopener" target="_blank">Huu-Tien, Dang, et al. “On effects of steering latent representation for large language model unlearning.” arXiv preprint arXiv:2408.06223 (2024).</a></p>

<p>[5] <a href="https://arxiv.org/pdf/2410.08109" rel="external nofollow noopener" target="_blank">Yuan, Xiaojian, et al. “A Closer Look at Machine Unlearning for Large Language Models.” arXiv preprint arXiv:2410.08109 (2024).</a></p>

<p>[6] <a href="https://arxiv.org/pdf/2310.16789" rel="external nofollow noopener" target="_blank">Weijia Shi, et al. “Detecting Pretraining Data from Large Language Models.” arXiv preprint arXiv:2310.16789 (2023).</a></p>

<!-- mkdir -p assets/img/unlearnllms/ -->
<!-- mv _posts/2025-03-17-*.png assets/img/unlearnllms/ -->

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/fairness-irt/">Comprehensive Algorithm Portfolio Evaluation using Item Response Theory</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/f4t/">About me</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
