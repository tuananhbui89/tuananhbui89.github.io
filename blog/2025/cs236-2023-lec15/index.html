<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Stanford CS236 - Deep Generative Models I 2023 I Lecture 15 - Evaluation of Generative Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec15/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Stanford CS236 - Deep Generative Models I 2023 I Lecture 15 - Evaluation of Generative Models</h1>
    <p class="post-meta">December 18, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#lecture-overview-focus-on-evaluating-generative-models">Lecture overview: focus on evaluating generative models</a></li>
<li class="toc-entry toc-h1"><a href="#evaluative-comparison-requires-defining-what-better-means-for-a-task">Evaluative comparison requires defining what ‘‘better’’ means for a task</a></li>
<li class="toc-entry toc-h1"><a href="#evaluating-generative-models-is-harder-than-discriminative-models-because-the-task-is-ill-defined">Evaluating generative models is harder than discriminative models because the task is ill-defined</a></li>
<li class="toc-entry toc-h1"><a href="#likelihood-average-log-likelihood-is-the-canonical-metric-for-density-estimation-and-relates-to-compression">Likelihood (average log-likelihood) is the canonical metric for density estimation and relates to compression</a></li>
<li class="toc-entry toc-h1"><a href="#compression-objective-captures-structure-but-has-limitations-for-downstream-importance-of-bits">Compression objective captures structure but has limitations for downstream importance of bits</a></li>
<li class="toc-entry toc-h1"><a href="#models-without-tractable-likelihoods-gans-ebms-complicate-likelihood-based-evaluation">Models without tractable likelihoods (GANs, EBMs) complicate likelihood-based evaluation</a></li>
<li class="toc-entry toc-h1"><a href="#kernel-density-estimation-kde-approximates-densities-from-samples-via-smoothing-kernels-but-fails-in-high-dimensions">Kernel density estimation (KDE) approximates densities from samples via smoothing kernels but fails in high dimensions</a></li>
<li class="toc-entry toc-h1"><a href="#estimating-likelihoods-for-latent-variable-models-requires-importance-sampling-and-can-have-high-variance">Estimating likelihoods for latent-variable models requires importance sampling and can have high variance</a></li>
<li class="toc-entry toc-h1"><a href="#human-evaluation-remains-the-gold-standard-for-sample-quality-but-is-costly-and-has-caveats">Human evaluation remains the gold standard for sample quality but is costly and has caveats</a></li>
<li class="toc-entry toc-h1"><a href="#inception-score-measures-sample-sharpness-and-label-diversity-using-a-pretrained-classifier">Inception Score measures sample sharpness and label diversity using a pretrained classifier</a></li>
<li class="toc-entry toc-h1"><a href="#fr%C3%A9chet-inception-distance-fid-compares-pretrained-feature-distributions-between-real-and-generated-data">Fréchet Inception Distance (FID) compares pretrained feature distributions between real and generated data</a></li>
<li class="toc-entry toc-h1"><a href="#kernel-based-two-sample-tests-mmdkid-operate-in-feature-space-and-offer-principled-comparisons">Kernel-based two-sample tests (MMD/KID) operate in feature space and offer principled comparisons</a></li>
<li class="toc-entry toc-h1"><a href="#holistic-evaluation-for-conditional-generative-tasks-text-to-image-requires-multiple-specialized-metrics">Holistic evaluation for conditional generative tasks (text-to-image) requires multiple specialized metrics</a></li>
<li class="toc-entry toc-h1"><a href="#representation-quality-is-evaluated-via-downstream-tasks-such-as-clustering-reconstruction-and-supervised-transfer">Representation quality is evaluated via downstream tasks such as clustering, reconstruction, and supervised transfer</a></li>
<li class="toc-entry toc-h1"><a href="#disentanglement-aims-for-interpretable-latent-factors-but-is-provably-unidentifiable-without-supervision">Disentanglement aims for interpretable latent factors but is provably unidentifiable without supervision</a></li>
<li class="toc-entry toc-h1"><a href="#pretrained-language-models-can-be-adapted-to-downstream-tasks-via-prompting-or-fine-tuning">Pretrained language models can be adapted to downstream tasks via prompting or fine-tuning</a></li>
<li class="toc-entry toc-h1"><a href="#prompting-is-less-natural-for-non-sequential-modalities-but-models-can-be-adapted-using-preference-data-and-fine-tuning">Prompting is less natural for non-sequential modalities but models can be adapted using preference data and fine-tuning</a></li>
<li class="toc-entry toc-h1"><a href="#prompting-versus-fine-tuning-trade-offs-in-cost-accessibility-and-performance">Prompting versus fine-tuning: trade-offs in cost, accessibility, and performance</a></li>
<li class="toc-entry toc-h1"><a href="#evaluation-of-generative-models-remains-an-open-research-area-requiring-multiple-complementary-metrics">Evaluation of generative models remains an open research area requiring multiple complementary metrics</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/MJt_ahtO-to" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="lecture-overview-focus-on-evaluating-generative-models">Lecture overview: focus on evaluating generative models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-01-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-01-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-01-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-01-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment introduces the lecture objective: to survey methods for evaluating <strong>generative models</strong>, and to emphasize that <strong>evaluation is a challenging open problem with no single consensus metric</strong>.<br></p>

<p>Evaluation is framed as essential for:</p>
<ul>
  <li>
<strong>Comparing model families</strong> — e.g., <strong>autoregressive models</strong>, <strong>flows</strong>, <strong>latent-variable models</strong>, <strong>energy-based models (EBMs)</strong>, <strong>GANs</strong>, and <strong>score-based models</strong>.<br>
</li>
  <li>
<strong>Guiding model and objective selection</strong> in both research and engineering contexts.<br>
</li>
</ul>

<p>Motivations and connections:</p>
<ul>
  <li>Evaluation helps decide which model is appropriate for a particular <strong>dataset</strong> and <strong>downstream use</strong>.<br>
</li>
  <li>
<strong>Quantitative assessment</strong> is critical for scientific <strong>progress</strong> and <strong>reproducibility</strong> in generative modeling.<br>
</li>
</ul>

<p>Preview of evaluation goals:</p>
<ul>
  <li>Different goals require different metrics, for example:
    <ul>
      <li><strong>Density estimation</strong></li>
      <li><strong>Sample quality / perceptual realism</strong></li>
      <li>
<strong>Representation utility</strong> (for downstream tasks)<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="evaluative-comparison-requires-defining-what-better-means-for-a-task">Evaluative comparison requires defining what ‘‘better’’ means for a task</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-03-51-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-03-51-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-03-51-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-03-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment defines <strong>evaluation</strong> as the process of determining whether <strong>Model A is better than Model B</strong>, and it emphasizes that this comparison requires a clear notion of the <strong>task or objective</strong>.<br></p>

<p>Key points:</p>
<ul>
  <li>Multiple <strong>model families</strong> and <strong>training objectives</strong> exist, so selection must be driven by the property that matters most for the intended use case:
    <ul>
      <li><strong>Density estimation</strong></li>
      <li><strong>Sampling quality</strong></li>
      <li>
<strong>Representation learning</strong><br>
</li>
    </ul>
  </li>
  <li>Practical research motivation:
    <ul>
      <li>Open-source models enable iterative improvements, which in turn require <strong>objective evaluation criteria</strong> to establish real progress.<br>
</li>
    </ul>
  </li>
</ul>

<p>Setup for the lecture:</p>
<ul>
  <li>Different <strong>evaluation metrics</strong> are appropriate depending on the <strong>end goal</strong>; the rest of the lecture explores which metrics align with which goals.<br>
</li>
</ul>

<hr>

<h1 id="evaluating-generative-models-is-harder-than-discriminative-models-because-the-task-is-ill-defined">Evaluating generative models is harder than discriminative models because the task is ill-defined</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-06-52-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-06-52-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-06-52-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-06-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment contrasts evaluation for <strong>discriminative models</strong> with evaluation for <strong>generative models</strong>.<br></p>

<p>For discriminative models (e.g., classifiers):</p>
<ul>
  <li>Tasks and metrics (like <strong>accuracy</strong>) are typically well defined.<br>
</li>
  <li>There is usually a clear <strong>loss function</strong> and a well-specified <strong>test distribution</strong>, enabling straightforward comparison on held-out data.<br>
</li>
</ul>

<p>For generative models:</p>
<ul>
  <li>The underlying task is often <strong>ambiguous</strong> because many objectives are possible:
    <ul>
      <li><strong>Likelihood</strong></li>
      <li><strong>Sampling fidelity / perceptual quality</strong></li>
      <li><strong>Compression</strong></li>
      <li><strong>Representation learning</strong></li>
      <li>
<strong>Downstream task performance</strong><br>
</li>
    </ul>
  </li>
</ul>

<p>Takeaway:</p>
<ul>
  <li>Choosing an evaluation metric requires specifying the <strong>downstream or operational use</strong> of the generative model, since different metrics capture different aspects of performance.<br>
</li>
</ul>

<hr>

<h1 id="likelihood-average-log-likelihood-is-the-canonical-metric-for-density-estimation-and-relates-to-compression">Likelihood (average log-likelihood) is the canonical metric for density estimation and relates to compression</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-11-20-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-11-20-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-11-20-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-11-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>When the goal is <strong>accurate density estimation</strong>, <strong>average log-likelihood on held-out data</strong> (equivalently minimizing <strong>Kullback–Leibler divergence</strong>) is a principled metric.<br></p>

<p>Practical evaluation procedure:</p>
<ol>
  <li>
<strong>Split the data</strong> into train / validation / test sets.<br>
</li>
  <li>
<strong>Select hyperparameters</strong> using validation data to avoid overfitting.<br>
</li>
  <li>
<strong>Report average log-likelihood</strong> on test data as the measure of model fit.<br>
</li>
</ol>

<p>Connection to compression:</p>
<ul>
  <li>
<strong>Maximum likelihood</strong> is directly tied to <strong>lossless compression</strong> under <strong>Shannon information theory</strong>: better likelihoods imply shorter expected code lengths.<br>
</li>
  <li>Practical schemes like <strong>arithmetic coding</strong> can approach the theoretical limits implied by likelihood estimates.<br>
</li>
</ul>

<hr>

<h1 id="compression-objective-captures-structure-but-has-limitations-for-downstream-importance-of-bits">Compression objective captures structure but has limitations for downstream importance of bits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-16-26-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-16-26-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-16-26-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-16-26.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment argues why <strong>likelihood-based compression</strong> is useful and also highlights its limitations.<br></p>

<p>Why it helps:</p>
<ul>
  <li>Compression forces models to <strong>identify redundancy and structure</strong> in data, which often corresponds to learning salient relationships.<br>
</li>
  <li>Historical motivations (e.g., the <strong>Hutter Prize</strong>) link <strong>compression</strong> to measures of intelligence and modeling skill.<br>
</li>
  <li>Empirical comparisons (e.g., human next-character prediction vs. modern language models’ <strong>bits-per-character</strong>) illustrate how compression quantifies learned regularities.<br>
</li>
</ul>

<p>Crucial limitations:</p>
<ul>
  <li>
<strong>Not all bits are equally important</strong> for downstream tasks — compression treats every bit the same.<br>
</li>
  <li>Compression <strong>treats all prediction errors equally</strong>, regardless of semantic impact (life-critical vs. cosmetic features).<br>
</li>
  <li>Therefore, <strong>likelihood alone</strong> may not prioritize task-relevant semantics, motivating alternative evaluation criteria when the goal is not pure density estimation.<br>
</li>
</ul>

<hr>

<h1 id="models-without-tractable-likelihoods-gans-ebms-complicate-likelihood-based-evaluation">Models without tractable likelihoods (GANs, EBMs) complicate likelihood-based evaluation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-20-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-20-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-20-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-20-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment explains a practical problem: many popular generative model classes do <strong>not provide tractable exact likelihoods</strong>, which complicates direct comparison via average log-likelihood or compression measures.<br></p>

<p>Relevant examples:</p>
<ul>
  <li><strong>GANs</strong></li>
  <li>Some <strong>EBMs</strong>
</li>
  <li>Certain <strong>VAEs</strong>, depending on tractability of the marginal likelihood<br>
</li>
</ul>

<p>Practical notes:</p>
<ul>
  <li>For <strong>VAEs</strong>, the <strong>ELBO</strong> provides a lower bound on log-likelihood, but <strong>translating ELBO differences into comparable likelihoods across model families is problematic</strong>.<br>
</li>
  <li>Practitioners therefore rely on <strong>alternative approximations</strong> or <strong>two-sample methods</strong> when exact likelihoods are unavailable.<br>
</li>
  <li>
<strong>Kernel density estimation</strong> and other sample-based density approximations are pragmatic options, but they have significant limitations (discussed next).<br>
</li>
</ul>

<hr>

<h1 id="kernel-density-estimation-kde-approximates-densities-from-samples-via-smoothing-kernels-but-fails-in-high-dimensions">Kernel density estimation (KDE) approximates densities from samples via smoothing kernels but fails in high dimensions</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-26-53-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-26-53-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-26-53-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-26-53.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment describes <strong>histogram-based</strong> and <strong>kernel density estimators (KDEs)</strong> as sample-based approaches to approximate unknown model densities when only samples are available.<br></p>

<p>KDE mechanism (stepwise):</p>
<ol>
  <li>Place a <strong>kernel</strong> (e.g., Gaussian) centered at each sample.<br>
</li>
  <li>
<strong>Sum the kernels</strong> across samples and divide by the sample count to obtain the density estimate.<br>
</li>
  <li>Control smoothness via the <strong>bandwidth parameter σ</strong> (larger σ = smoother estimate).<br>
</li>
</ol>

<p>Practical aspects:</p>
<ul>
  <li>
<strong>Kernel choice</strong> (Gaussian, Epanechnikov, etc.) matters but bandwidth selection is often more important.<br>
</li>
  <li>
<strong>Bandwidth selection</strong> is typically done by cross-validation to trade off bias and variance.<br>
</li>
  <li>There is a tension between <strong>undersmoothing</strong> (noisy estimate) and <strong>oversmoothing</strong> (loss of detail).<br>
</li>
</ul>

<p>Major limitation:</p>
<ul>
  <li>
<strong>Curse of dimensionality</strong> — KDE requires exponentially many samples as dimensionality grows (e.g., images), making KDE impractical for typical modern generative-modeling domains.<br>
</li>
</ul>

<hr>

<h1 id="estimating-likelihoods-for-latent-variable-models-requires-importance-sampling-and-can-have-high-variance">Estimating likelihoods for latent-variable models requires importance sampling and can have high variance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-32-40-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-32-40-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-32-40-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-32-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment treats <strong>latent-variable models</strong> and the challenge of estimating the marginal likelihood p(x) = ∫ p(x|z) p(z) dz.<br></p>

<p>Key points:</p>
<ul>
  <li>
<strong>Naive Monte Carlo</strong> estimation (sampling z from the prior) can have <strong>high variance</strong> when the prior and posterior differ substantially.<br>
</li>
  <li>
<strong>Importance sampling</strong> can reduce variance by sampling from a proposal distribution closer to the posterior.<br>
</li>
  <li>
<strong>Annealed / bridged strategies</strong> (e.g., <strong>sequential importance sampling</strong>, <strong>annealed importance sampling</strong>) interpolate between the prior and posterior to obtain more accurate estimates.<br>
</li>
</ul>

<p>Practical implications:</p>
<ul>
  <li>
<strong>Naive sampling from the prior</strong> often yields poor likelihood estimates.<br>
</li>
  <li>
<strong>Specialized estimators</strong> can substantially improve accuracy when likelihood evaluation is required for model comparison in latent-variable settings.<br>
</li>
</ul>

<hr>

<h1 id="human-evaluation-remains-the-gold-standard-for-sample-quality-but-is-costly-and-has-caveats">Human evaluation remains the gold standard for sample quality but is costly and has caveats</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-37-10-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-37-10-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-37-10-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-37-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment advocates <strong>human perceptual studies</strong> as the most direct way to assess <strong>sample quality</strong> and <strong>visual realism</strong>.<br></p>

<p>Common protocols:</p>
<ul>
  <li>Annotators <strong>compare real and generated samples</strong> or <strong>rate sample realism</strong> on a scale.<br>
</li>
  <li>
<strong>Psychological-style evaluations</strong> measure how long it takes humans to distinguish real from fake images — faster distinction implies lower sample quality.<br>
</li>
  <li>Measure <strong>deception rate</strong> when annotators have unlimited time (higher deception = higher perceptual realism).<br>
</li>
</ul>

<p>Practical limitations:</p>
<ul>
  <li>Human evaluations are <strong>expensive</strong> and <strong>difficult to scale</strong> during model development.<br>
</li>
  <li>Results are <strong>sensitive to task wording</strong> and experimental design, making reproducibility challenging.<br>
</li>
  <li>Human studies can <strong>fail to reveal memorization</strong> (a model that memorizes training images may still fool annotators but lacks generalization).<br>
</li>
</ul>

<hr>

<h1 id="inception-score-measures-sample-sharpness-and-label-diversity-using-a-pretrained-classifier">Inception Score measures sample sharpness and label diversity using a pretrained classifier</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-43-25-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-43-25-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-43-25-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-43-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment defines the <strong>Inception Score (IS)</strong> as an automated metric for labeled image domains that leverages a <strong>pretrained classifier</strong> to assess two aspects of generated samples: <strong>sharpness</strong> and <strong>diversity</strong>.<br></p>

<p>How IS works (intuition):</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>
<strong>Sharpness</strong>: low conditional entropy **p(y</td>
          <td>x)** indicates the classifier is confident about the class of a generated image.<br>
</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
<strong>Diversity</strong>: high entropy of the marginal <strong>p(y)</strong> across generated samples indicates coverage of many classes.<br>
</li>
  <li>The IS summarizes these by computing an exponentiated <strong>KL divergence</strong> between the conditional and marginal label distributions.<br>
</li>
</ul>

<p>Limitations:</p>
<ul>
  <li>IS inspects <strong>only generated samples</strong>, not how they compare to real data.<br>
</li>
  <li>It <strong>depends on the chosen classifier and label set</strong>, so results vary with the network used.<br>
</li>
  <li>IS can be <strong>gamed</strong> (e.g., produce images that maximize classifier confidence without capturing intra-class variability).<br>
</li>
</ul>

<hr>

<h1 id="fréchet-inception-distance-fid-compares-pretrained-feature-distributions-between-real-and-generated-data">Fréchet Inception Distance (FID) compares pretrained feature distributions between real and generated data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-50-55-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-50-55-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-50-55-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-50-55.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment describes the <strong>Fréchet Inception Distance (FID)</strong>, a widely used metric that compares the distribution of <strong>pretrained-network features</strong> on real versus generated images.<br></p>

<p>Procedure:</p>
<ol>
  <li>Compute activation vectors (e.g., <strong>Inception features</strong>) for many real and generated samples.<br>
</li>
  <li>Fit <strong>multivariate Gaussians</strong> to each feature set — estimate mean and covariance.<br>
</li>
  <li>Compute the <strong>Fréchet (Wasserstein-2) distance</strong> between these Gaussians; the distance has a closed-form expression in terms of means and covariances.<br>
</li>
</ol>

<p>Intuition and trade-offs:</p>
<ul>
  <li>
<strong>Smaller FID</strong> indicates generated features more closely match real data features, emphasizing <strong>higher-level perceptual features</strong> rather than raw pixels.<br>
</li>
  <li>
<strong>Kernel-based alternatives</strong> (e.g., <strong>MMD / KID</strong>) perform two-sample tests in feature space and can offer stronger statistical guarantees at higher computational cost.<br>
</li>
</ul>

<hr>

<h1 id="kernel-based-two-sample-tests-mmdkid-operate-in-feature-space-and-offer-principled-comparisons">Kernel-based two-sample tests (MMD/KID) operate in feature space and offer principled comparisons</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/00-56-43-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/00-56-43-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/00-56-43-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/00-56-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment explains <strong>maximum mean discrepancy (MMD)</strong> and the <strong>kernel inception distance (KID)</strong> as kernel-based two-sample statistics.<br></p>

<p>Computation (intuition):</p>
<ul>
  <li>Compare distributions by averaging pairwise <strong>kernel similarities</strong>:
    <ul>
      <li>Average kernel value among <strong>real–real</strong> pairs</li>
      <li>Average among <strong>fake–fake</strong> pairs</li>
      <li>Average among <strong>real–fake</strong> pairs</li>
    </ul>
  </li>
  <li>Combine these averages into a statistic that is <strong>zero iff the distributions match</strong> (for <strong>characteristic kernels</strong>).<br>
</li>
</ul>

<p>Practical notes:</p>
<ul>
  <li>Using <strong>pretrained-network features</strong> (e.g., Inception activations) as the kernel input emphasizes <strong>perceptual similarity</strong> rather than raw-pixel proximity.<br>
</li>
  <li>Trade-offs:
    <ul>
      <li>
<strong>MMD/KID are more principled</strong>, but are <strong>computationally heavier</strong> (quadratic in sample count).<br>
</li>
      <li>They require <strong>careful kernel choice and scaling</strong> to be effective.<br>
</li>
    </ul>
  </li>
</ul>

<hr>

<h1 id="holistic-evaluation-for-conditional-generative-tasks-text-to-image-requires-multiple-specialized-metrics">Holistic evaluation for conditional generative tasks (text-to-image) requires multiple specialized metrics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-02-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-02-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-02-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-02-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment observes that <strong>conditional generative tasks</strong> (e.g., text-to-image) require <strong>multifaceted evaluation</strong> beyond general sample quality because models must also respect the conditioning input and meet additional desiderata.<br></p>

<p>Recommended combined metrics:</p>
<ul>
  <li>
<strong>Perceptual quality</strong> (e.g., FID / IS)<br>
</li>
  <li>
<strong>Caption–image alignment</strong> (automated metrics and human alignment checks)<br>
</li>
  <li>
<strong>Robustness to prompt variations</strong><br>
</li>
  <li>
<strong>Originality</strong> and <strong>aesthetic measures</strong><br>
</li>
  <li>
<strong>Bias / toxicity assessments</strong> and safety-related checks<br>
</li>
</ul>

<p>Practice:</p>
<ul>
  <li>Holistic benchmark efforts aggregate many automated metrics and <strong>human studies</strong> to enable comprehensive comparisons.<br>
</li>
  <li>
<strong>No single metric suffices</strong> for complex conditional tasks — evaluations must be multidimensional.<br>
</li>
</ul>

<hr>

<h1 id="representation-quality-is-evaluated-via-downstream-tasks-such-as-clustering-reconstruction-and-supervised-transfer">Representation quality is evaluated via downstream tasks such as clustering, reconstruction, and supervised transfer</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-07-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment discusses evaluating <strong>learned representations</strong> from generative models by measuring <strong>utility on downstream tasks</strong>.<br></p>

<p>Common evaluation paradigms:</p>
<ul>
  <li>Map data to <strong>latent features</strong>, then:
    <ul>
      <li>Apply simple <strong>classifiers</strong> to measure supervised performance (few-shot / linear-probe accuracy).<br>
</li>
      <li>Apply <strong>clustering</strong> (e.g., <strong>k-means</strong>) and report cluster-quality measures (completeness, homogeneity, <strong>V-measure</strong>).<br>
</li>
    </ul>
  </li>
  <li>Measure <strong>reconstruction fidelity</strong> for lossy-compression objectives using <strong>MSE / PSNR / SSIM</strong>.<br>
</li>
  <li>Quantify <strong>compression ratios</strong> (bits or dimensionality reduction) versus reconstruction quality.<br>
</li>
</ul>

<p>Key point:</p>
<ul>
  <li>The <strong>appropriate metric depends on the intended downstream use</strong> (classification, compression, clustering); representation comparisons are therefore necessarily <strong>task-dependent</strong>.<br>
</li>
</ul>

<hr>

<h1 id="disentanglement-aims-for-interpretable-latent-factors-but-is-provably-unidentifiable-without-supervision">Disentanglement aims for interpretable latent factors but is provably unidentifiable without supervision</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-07-22-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-07-22.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment defines <strong>disentanglement</strong> and explains theoretical and practical limitations.<br></p>

<p>Definition and measurement:</p>
<ul>
  <li>
<strong>Disentanglement</strong>: latent variables correspond to independent, interpretable generative factors (e.g., pose, lighting, object identity).<br>
</li>
  <li>Practical metrics often use <strong>linear classifier accuracy</strong> to predict known factors from latent representations.<br>
</li>
</ul>

<p>Theoretical limitation:</p>
<ul>
  <li>
<strong>Fully unsupervised disentanglement is generally unidentifiable</strong> — without inductive biases or supervision, the mapping from data to factors is not unique, so provable recovery of true factors from unlabeled data alone is impossible.<br>
</li>
</ul>

<p>Practical implication:</p>
<ul>
  <li>Empirical methods sometimes produce disentangled representations, but success <strong>lacks general theoretical guarantees</strong>; reliable disentanglement typically requires <strong>supervision or structural constraints</strong>.<br>
</li>
</ul>

<hr>

<h1 id="pretrained-language-models-can-be-adapted-to-downstream-tasks-via-prompting-or-fine-tuning">Pretrained language models can be adapted to downstream tasks via prompting or fine-tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-10-34-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-10-34-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-10-34-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-10-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment explains how <strong>autoregressive language models</strong> trained by maximum likelihood can be adapted for downstream tasks via <strong>prompting</strong> or <strong>fine-tuning</strong>.<br></p>

<p>Two adaptation strategies:</p>
<ol>
  <li>
<strong>Prompting</strong>:
    <ul>
      <li>Craft a natural-language context that converts the task into <strong>next-token prediction</strong> (no parameter updates).<br>
</li>
      <li>Example pattern for sentiment classification: <strong>instruction + few examples + target</strong> (i.e., few-shot prompt).<br>
</li>
      <li>Advantages: requires no model updates and works with <strong>black-box API access</strong>.<br>
</li>
    </ul>
  </li>
  <li>
<strong>Fine-tuning</strong>:
    <ul>
      <li>Modify model weights on labeled task examples to optimize task-specific performance.<br>
</li>
      <li>Advantages: usually yields <strong>higher task accuracy</strong> but requires compute and maintenance.<br>
</li>
    </ul>
  </li>
</ol>

<p>Operational trade-offs:</p>
<ul>
  <li>Strong pretrained models often show useful <strong>few-shot / zero-shot</strong> behavior via prompting.<br>
</li>
  <li>
<strong>Fine-tuning</strong> typically improves accuracy at the cost of compute, storage, and operational complexity.<br>
</li>
</ul>

<hr>

<h1 id="prompting-is-less-natural-for-non-sequential-modalities-but-models-can-be-adapted-using-preference-data-and-fine-tuning">Prompting is less natural for non-sequential modalities but models can be adapted using preference data and fine-tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-15-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-15-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-15-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-15-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment considers extending the <strong>prompting paradigm beyond text</strong> and highlights challenges when model APIs emit <strong>images</strong> rather than discrete tokens.<br></p>

<p>Adaptation strategies for image or multimodal models:</p>
<ul>
  <li>Collect <strong>preference</strong> or <strong>preference-pair</strong> data (which image is preferred for a caption).<br>
</li>
  <li>Fine-tune models to optimize for <strong>human preferences</strong> using preference learning or <strong>reinforcement learning from human feedback (RLHF)</strong> to improve aesthetics, reduce toxicity, or correct biases.<br>
</li>
</ul>

<p>Research and trade-offs:</p>
<ul>
  <li>Ongoing work aims to transfer <strong>instruction-style adaptation</strong> to vision and multimodal models.<br>
</li>
  <li>Practical trade-offs include the need for <strong>labeled preference data</strong> and the additional training effort required to implement preference-based fine-tuning.<br>
</li>
</ul>

<hr>

<h1 id="prompting-versus-fine-tuning-trade-offs-in-cost-accessibility-and-performance">Prompting versus fine-tuning: trade-offs in cost, accessibility, and performance</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-18-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-18-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-18-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-18-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This segment compares <strong>prompting</strong> and <strong>fine-tuning</strong> as adaptation strategies for large pretrained models.<br></p>

<p>Summary of trade-offs:</p>
<ul>
  <li>
<strong>Prompting</strong>
    <ul>
      <li>Highly accessible: <strong>no retraining</strong>, works with <strong>black-box API</strong> access.<br>
</li>
      <li>Inexpensive in development time and easy to iterate.<br>
</li>
      <li>May provide strong few-shot/zero-shot capabilities, but typically limited compared to fine-tuning.<br>
</li>
    </ul>
  </li>
  <li>
<strong>Fine-tuning</strong>
    <ul>
      <li>Typically yields <strong>superior performance</strong> for many tasks.<br>
</li>
      <li>Requires <strong>compute resources</strong>, access to model weights or permissive APIs, and expertise in training.<br>
</li>
      <li>Better suited for repeated or large-scale deployments where higher accuracy justifies the cost.<br>
</li>
    </ul>
  </li>
</ul>

<p>Recommendation:</p>
<ul>
  <li>Select the strategy according to <strong>resource constraints</strong>, <strong>privacy requirements</strong>, desired <strong>performance</strong>, and deployment scale; both approaches are actively used depending on context.<br>
</li>
</ul>

<hr>

<h1 id="evaluation-of-generative-models-remains-an-open-research-area-requiring-multiple-complementary-metrics">Evaluation of generative models remains an open research area requiring multiple complementary metrics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec15/01-20-13-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec15/01-20-13-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec15/01-20-13-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec15/01-20-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>This closing segment summarizes the core takeaways about evaluation in generative modeling.<br></p>

<p>Core messages:</p>
<ul>
  <li>
<strong>Evaluation is an open problem</strong> with many partial solutions; there is no one-size-fits-all metric.<br>
</li>
  <li>Metrics must be chosen to reflect the <strong>downstream objective</strong>:
    <ul>
      <li>
<strong>Likelihood</strong> for density estimation<br>
</li>
      <li>
<strong>Human / perceptual metrics</strong> for sample quality<br>
</li>
      <li>
<strong>Task-driven metrics</strong> for representation utility<br>
</li>
    </ul>
  </li>
  <li>
<strong>Large-scale benchmarks</strong> that aggregate diverse tasks and metrics are useful, but they do not resolve how to weight or prioritize different metrics.<br>
</li>
</ul>

<p>Practical recommendations:</p>
<ul>
  <li>Use a <strong>mixture of human and automated evaluations</strong>.<br>
</li>
  <li>Practice <strong>careful experimental design</strong> to ensure reproducibility.<br>
</li>
  <li>Continue research into <strong>new evaluation methodologies</strong> tailored to specific generative modeling applications.<br>
</li>
</ul>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
