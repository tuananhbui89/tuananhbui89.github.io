<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS336 Lecture 13 - Data | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary of CS336 Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/cs336-lec13/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">CS336 Lecture 13 - Data</h1>
    <p class="post-meta">December 8, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#data-is-the-primary-determinant-of-language-model-behavior">Data is the primary determinant of language model behavior</a></li>
<li class="toc-entry toc-h1"><a href="#pre-training-mid-training-and-post-training-form-distinct-stages-of-model-training">Pre-training, mid-training, and post-training form distinct stages of model training</a></li>
<li class="toc-entry toc-h1"><a href="#example-training-mixes-illustrate-the-premidpost-pipeline-and-token-scales">Example training mixes illustrate the pre/mid/post pipeline and token scales</a></li>
<li class="toc-entry toc-h1"><a href="#there-is-no-simple-formalism-for-dataset-selection-curated-practices-are-inductive-and-empirical">There is no simple formalism for dataset selection; curated practices are inductive and empirical</a></li>
<li class="toc-entry toc-h1"><a href="#bert-demonstrated-early-pre-training-on-books-and-wikipedia-and-revealed-issues-like-data-poisoning-risk">BERT demonstrated early pre-training on books and Wikipedia and revealed issues like data poisoning risk</a></li>
<li class="toc-entry toc-h1"><a href="#webtext-used-link-popularity-heuristics-to-extract-higher-quality-web-pages">WebText used link popularity heuristics to extract higher-quality web pages</a></li>
<li class="toc-entry toc-h1"><a href="#common-crawl-provides-large-monthly-web-snapshots-but-requires-extensive-processing">Common Crawl provides large monthly web snapshots but requires extensive processing</a></li>
<li class="toc-entry toc-h1"><a href="#common-crawl-contains-offensive-and-copyrighted-material-and-respects-robotstxt-policies-variably">Common Crawl contains offensive and copyrighted material and respects robots.txt policies variably</a></li>
<li class="toc-entry toc-h1"><a href="#two-principal-approaches-to-web-filtering-are-model-based-scoring-and-rule-based-heuristics">Two principal approaches to web filtering are model-based scoring and rule-based heuristics</a></li>
<li class="toc-entry toc-h1"><a href="#gpt-3-used-a-mixture-of-common-crawl-filtered-web-text-books-and-wikipedia-with-a-learned-quality-classifier">GPT-3 used a mixture of Common Crawl, filtered web text, books, and Wikipedia with a learned quality classifier</a></li>
<li class="toc-entry toc-h1"><a href="#the-pile-aggregated-diverse-high-quality-domains-to-maximize-open-source-training-material">The Pile aggregated diverse high-quality domains to maximize open-source training material</a></li>
<li class="toc-entry toc-h1"><a href="#stack-exchange-and-github-provide-structurally-rich-application-relevant-corpora-but-require-careful-licensing-and-processing">Stack Exchange and GitHub provide structurally rich, application-relevant corpora but require careful licensing and processing</a></li>
<li class="toc-entry toc-h1"><a href="#gopher-and-other-research-era-large-datasets-emphasized-curated-filters-and-manual-heuristics">Gopher and other research-era large datasets emphasized curated filters and manual heuristics</a></li>
<li class="toc-entry toc-h1"><a href="#llama-redpajama-reproductions-and-repav2-illustrate-dataset-reproduction-link-aware-quality-and-large-scale-signal-computation">LLaMA, RedPajama reproductions, and RepAv2 illustrate dataset reproduction, link-aware quality, and large-scale signal computation</a></li>
<li class="toc-entry toc-h1"><a href="#refinedweb-and-fineweb-provide-minimally-processed-web-datasets-intended-for-further-curation">RefinedWeb and FineWeb provide minimally processed web datasets intended for further curation</a></li>
<li class="toc-entry toc-h1"><a href="#datacomp-defined-a-standard-benchmarking-and-competition-framework-and-demonstrated-aggressive-model-based-filtering-gains">DataComp defined a standard benchmarking and competition framework and demonstrated aggressive model-based filtering gains</a></li>
<li class="toc-entry toc-h1"><a href="#nemoneatroncc-ensembles-model-based-scorers-and-rewrites-to-increase-usable-token-volume-while-preserving-quality">NeMo/NeatronCC ensembles model-based scorers and rewrites to increase usable token volume while preserving quality</a></li>
<li class="toc-entry toc-h1"><a href="#copyright-law-and-licensing-are-central-constraints-for-dataset-collection-and-sharing">Copyright law and licensing are central constraints for dataset collection and sharing</a></li>
<li class="toc-entry toc-h1"><a href="#training-models-on-copyrighted-content-raises-complex-legal-and-technical-issues-including-memorization-and-transformative-use-arguments">Training models on copyrighted content raises complex legal and technical issues including memorization and transformative use arguments</a></li>
<li class="toc-entry toc-h1"><a href="#mid-training-and-post-training-use-targeted-corpora-and-synthetic-data-to-instill-capabilities-such-as-long-context-and-instruction-following">Mid-training and post-training use targeted corpora and synthetic data to instill capabilities such as long context and instruction following</a></li>
<li class="toc-entry toc-h1"><a href="#dataset-engineering-is-heuristic-consequential-and-a-primary-axis-for-improving-language-models">Dataset engineering is heuristic, consequential, and a primary axis for improving language models</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/WePxmeXU1xg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="data-is-the-primary-determinant-of-language-model-behavior">Data is the primary determinant of language model behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-01-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-01-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-01-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-01-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Data quality, composition, and curation</strong> materially determine the capabilities and risks of <strong>foundation models</strong>.<br></p>

<p>High-level system choices like <strong>architecture</strong>, <strong>optimization</strong>, and <strong>tokenization</strong> matter, but the empirical performance and <strong>emergent behaviors</strong> of large language models are primarily driven by what they are trained on:<br></p>
<ul>
  <li>
<strong>data sources</strong> (web crawls, books, code, papers, etc.)<br>
</li>
  <li>
<strong>filtering heuristics</strong> (rule-based or model-based)<br>
</li>
  <li>
<strong>deduplication</strong> strategies<br>
</li>
  <li>
<strong>downstream instructional tuning</strong> (instruction data, RLHF)<br>
</li>
</ul>

<p>Commercial disclosure about training data is often limited for competitive or legal reasons, which complicates <strong>reproducibility</strong> and independent evaluation.<br></p>

<p>Organizing and staffing data efforts is a distinct, highly scalable operational function separate from architecture development, enabling specialized teams to produce targeted corpora for capabilities like <strong>multilinguality</strong>, <strong>code</strong>, or <strong>multimodal learning</strong>.<br></p>

<hr>

<h1 id="pre-training-mid-training-and-post-training-form-distinct-stages-of-model-training">Pre-training, mid-training, and post-training form distinct stages of model training</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-03-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-03-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-03-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-03-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Model training</strong> is commonly organized into three phases:<br></p>
<ol>
  <li>
<strong>Pre‑training</strong> — large, raw corpora to learn general language priors.<br>
</li>
  <li>
<strong>Mid‑training</strong> — smaller, curated corpora to develop targeted capabilities (e.g., math, code, long‑context).<br>
</li>
  <li>
<strong>Post‑training</strong> — instruction tuning and reinforcement methods to align models for safe, useful interaction.<br>
</li>
</ol>

<p>Checkpoints after pre‑ and mid‑training are often called <strong>base models</strong>; models after post‑training are <strong>production</strong> or <strong>instruction‑tuned</strong> checkpoints.<br></p>

<p>In practice, these boundaries blur: organizations use multiple iterative stages and mixtures of data to reach desired behaviors.<br></p>

<p>Each stage imposes different curation and filtering trade-offs because early stages prioritize <strong>scale</strong>, while later stages emphasize <strong>higher signal‑to‑noise</strong> and <strong>specific task formats</strong>.<br></p>

<hr>

<h1 id="example-training-mixes-illustrate-the-premidpost-pipeline-and-token-scales">Example training mixes illustrate the pre/mid/post pipeline and token scales</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-04-44-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-04-44-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-04-44-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-04-44.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Released open‑source examples show a common pipeline:<br></p>
<ul>
  <li>Massive pre‑training mixes dominated by <strong>web crawls</strong> plus books, code, academic papers, math, and <strong>Wikipedia</strong>.<br>
</li>
  <li>Mid‑training on <strong>filtered subsets</strong> and <strong>synthetic data</strong> to focus capabilities.<br>
</li>
  <li>Post‑training on <strong>chat</strong> and <strong>instruction‑like</strong> data for alignment.<br>
</li>
</ul>

<p>Token counts in public reproductions typically scale down across stages:<br></p>
<ul>
  <li>Trillions of tokens in <strong>pre‑training</strong><br>
</li>
  <li>Tens–hundreds of billions in <strong>mid‑training</strong><br>
</li>
  <li>Billions in <strong>post‑training</strong><br>
</li>
</ul>

<p>This illustrates a progressive focusing from <strong>scale → quality</strong>.<br></p>

<p><strong>Synthetic generation</strong> is increasingly used at mid and post stages to produce targeted instruction or reasoning traces, and dataset composition choices plus token budgets materially affect which capabilities improve at each stage.<br></p>

<hr>

<h1 id="there-is-no-simple-formalism-for-dataset-selection-curated-practices-are-inductive-and-empirical">There is no simple formalism for dataset selection; curated practices are inductive and empirical</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-05-57-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-05-57-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-05-57-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-05-57.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>There is no established theory prescribing exact dataset composition for optimal model behavior; choices remain <strong>empirical, heuristic, and context‑dependent</strong>.<br></p>

<p>Data preparation is therefore an <strong>inductive process</strong>: study historical dataset designs, observe how they map to capabilities, and form principled heuristics for:<br></p>
<ul>
  <li>
<strong>filtering</strong><br>
</li>
  <li>
<strong>deduplication</strong><br>
</li>
  <li>
<strong>augmentation</strong><br>
</li>
</ul>

<p>Practical dataset engineering combines domain knowledge (e.g., including math or code sources) with scalable collection and quality pipelines.<br></p>

<p>The lack of formal principles motivates reproducibility efforts, ablation studies, and benchmarks that compare filtering strategies and data mixes.<br></p>

<hr>

<h1 id="bert-demonstrated-early-pre-training-on-books-and-wikipedia-and-revealed-issues-like-data-poisoning-risk">BERT demonstrated early pre-training on books and Wikipedia and revealed issues like data poisoning risk</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-09-15-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-09-15-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-09-15-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-09-15.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Early transformer pre‑training relied on curated, high‑quality corpora such as <strong>books</strong> and <strong>Wikipedia</strong> to learn robust language representations and long‑context structure.<br></p>

<p>Public book collections (e.g., Smashwords‑derived corpora, Project Gutenberg) and Wikipedia provide coherent long‑form and factual content but do <strong>not</strong> cover informal or opinionated genres like recipes or personal blogs.<br></p>

<p>Public datasets that mirror live services can be vulnerable to <strong>data poisoning</strong>, where adversarial edits are timed to appear in dumps—showing that training data from live services contains manipulation risks and requires pipeline‑level safeguards.<br></p>

<p>Adversarial or erroneous content in foundational sources can produce undesirable model behaviors if not detected and removed.<br></p>

<hr>

<h1 id="webtext-used-link-popularity-heuristics-to-extract-higher-quality-web-pages">WebText used link popularity heuristics to extract higher-quality web pages</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-12-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-12-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-12-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-12-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>WebText</strong> (the GPT‑2 corpus) used a pragmatic heuristic to get a diverse but higher‑quality subset of the web: extract pages linked from <strong>Reddit</strong> posts that exceeded a small karma threshold.<br></p>

<p>This produced a compact set of higher‑signal documents (≈8 million pages and tens of gigabytes of text in the original work) compared to undifferentiated web crawls.<br></p>

<p>The approach highlights the value of using <strong>social signals</strong> or other external proxies to select content likely to be high quality and human‑valued.<br></p>

<p>However, proprietary crawls and heuristics used by many developers are often not fully disclosed, limiting reproducibility.<br></p>

<hr>

<h1 id="common-crawl-provides-large-monthly-web-snapshots-but-requires-extensive-processing">Common Crawl provides large monthly web snapshots but requires extensive processing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-15-05-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-15-05-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-15-05-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-15-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Common Crawl</strong> is an open, monthly web crawl that provides raw HTTP responses and WARC archives for billions of pages—an academic approximation of the web.<br></p>

<p>Raw data requires heavy preprocessing before training, including:<br></p>
<ul>
  <li>
<strong>HTML→text conversion</strong> (WET files or custom parsers)<br>
</li>
  <li>
<strong>deduplication</strong><br>
</li>
  <li>
<strong>language identification</strong><br>
</li>
  <li>
<strong>filtering</strong> for boilerplate, dynamic URLs, and near‑duplicates<br>
</li>
</ul>

<p>The HTML‑to‑text toolchain materially affects downstream model quality: different extractors yield measurable performance differences in ablations.<br></p>

<p>Common Crawl is deliberately conservative in crawling behavior and is not a comprehensive mirror of the full web, so downstream datasets often supplement it with targeted crawls or third‑party sources.<br></p>

<hr>

<h1 id="common-crawl-contains-offensive-and-copyrighted-material-and-respects-robotstxt-policies-variably">Common Crawl contains offensive and copyrighted material and respects robots.txt policies variably</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-19-25-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-19-25-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-19-25-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-19-25.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Common Crawl does not perform aggressive semantic filtering by default, so raw dumps include <strong>offensive, toxic, and copyrighted content</strong>, and handling of illegal or disallowed pages is limited.<br></p>

<ul>
  <li>Web hosts can request exclusion via <strong>robots.txt</strong> or other crawler rules, but adherence varies and is partly heuristic.<br>
</li>
  <li>Major model developers often run bespoke crawlers beyond Common Crawl.<br>
</li>
  <li>Media (images, etc.) may appear in raw responses but are frequently ignored by text pipelines unless explicit media crawlers are used.<br>
</li>
</ul>

<p>The prevalence of copyrighted material complicates legal usage; teams typically address this with licensing agreements or fair‑use arguments, while content owners may restrict downstream uses via <strong>terms of service</strong>.<br></p>

<hr>

<h1 id="two-principal-approaches-to-web-filtering-are-model-based-scoring-and-rule-based-heuristics">Two principal approaches to web filtering are model-based scoring and rule-based heuristics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-24-34-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-24-34-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-24-34-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-24-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Filtering large web crawls into high‑quality corpora has been implemented via two main paradigms:<br></p>
<ul>
  <li>
<strong>Model‑based approaches</strong> (e.g., CCNet) train a classifier or language model to score documents against curated positive examples (Wikipedia, books). These enable language‑aware quality judgments but can concentrate content toward the positive example distribution.<br>
</li>
  <li>
<strong>Rule‑based heuristics</strong> (e.g., C4) apply deterministic filters (punctuation counts, sentence thresholds, blacklists). They preserve broader linguistic diversity but can admit well‑formed spam or boilerplate.<br>
</li>
</ul>

<p>Both paradigms have trade‑offs: model‑based selection amplifies similarity to positives; rule‑based selection preserves diversity at the cost of admitting noise.<br></p>

<hr>

<h1 id="gpt-3-used-a-mixture-of-common-crawl-filtered-web-text-books-and-wikipedia-with-a-learned-quality-classifier">GPT-3 used a mixture of Common Crawl, filtered web text, books, and Wikipedia with a learned quality classifier</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-28-16-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-28-16-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-28-16-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-28-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The <strong>GPT‑3 era</strong> dataset combined Common Crawl processed into <strong>WebText2</strong>, curated book corpora (Books1/Books2), and <strong>Wikipedia</strong>, then applied a quality classifier to surface high‑signal documents.<br></p>

<p>Key aspects of the classifier‑based pipeline:<br></p>
<ul>
  <li>Select positive examples representative of desired quality.<br>
</li>
  <li>Train a classifier to find similar material across a much larger pool.<br>
</li>
  <li>Produce a focused ~400B‑token training mix by amplifying scarce high‑quality sources.<br>
</li>
</ul>

<p>This demonstrates the utility of <strong>supervised quality scoring</strong> to extract high‑signal content from massive raw pools, but proprietary training mixes and classifier details are often incompletely disclosed, limiting reproducibility and independent evaluation.<br></p>

<hr>

<h1 id="the-pile-aggregated-diverse-high-quality-domains-to-maximize-open-source-training-material">The Pile aggregated diverse high-quality domains to maximize open-source training material</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-32-50-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-32-50-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-32-50-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-32-50.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>The Pile</strong> is an open, community‑assembled dataset aggregating multiple high‑quality domains: Common Crawl subsets, OpenWebText, arXiv, PubMed Central, Stack Exchange, GitHub, Project Gutenberg, and curated book collections.<br></p>

<p>Goals and practices:<br></p>
<ul>
  <li>Maximize <strong>domain diversity</strong> and include permissively licensed material where possible.<br>
</li>
  <li>Perform <strong>licensing checks</strong>, <strong>deduplication</strong>, and conversion choices to preserve code and long‑form attributes.<br>
</li>
</ul>

<p>The Pile shows how curated open datasets can replicate—and sometimes exceed—proprietary mixes when volunteers and institutions coordinate collection and processing.<br></p>

<hr>

<h1 id="stack-exchange-and-github-provide-structurally-rich-application-relevant-corpora-but-require-careful-licensing-and-processing">Stack Exchange and GitHub provide structurally rich, application-relevant corpora but require careful licensing and processing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-39-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-39-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-39-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-39-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Stack Exchange</strong> dumps and <strong>GitHub</strong> repositories contribute QA‑style conversational data and code, respectively—both valuable for instruction‑following and reasoning behaviors.<br></p>

<p>Important considerations:<br></p>
<ul>
  <li>These sources include metadata (votes, comments, commit history) that support fine‑grained filtering and high‑signal selection.<br>
</li>
  <li>They often carry <strong>licensing constraints</strong> and commercial‑use caveats (license‑aware inclusion policies are required).<br>
</li>
  <li>Converting repo snapshots into tokenizable training data needs careful handling of non‑code files, deduplication, and licensing; permissive licenses facilitate open redistribution (e.g., The Stack).<br>
</li>
</ul>

<p>Random sampling reveals large heterogeneity in quality and content distribution, so preprocessing choices strongly affect final dataset composition.<br></p>

<hr>

<h1 id="gopher-and-other-research-era-large-datasets-emphasized-curated-filters-and-manual-heuristics">Gopher and other research-era large datasets emphasized curated filters and manual heuristics</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-44-18-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-44-18-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-44-18-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-44-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Research models such as DeepMind’s <strong>Gopher</strong> collected massive text sources and applied language‑specific filters, rule‑based heuristics, and toxicity checks to improve signal‑to‑noise in the pre‑training pool.<br></p>

<p>Early decisions favored manual rule‑based filters because model understanding was limited, and conservative filters reduced the risk of excluding marginalized or non‑standard linguistic forms.<br></p>

<p>Massive curated pools were often larger than required by training runs, enabling selective training on the best subsets and iterative dataset design.<br></p>

<p>Design choices in these datasets influenced later <strong>model‑based filtering</strong> approaches and the community trade‑offs between <strong>coverage</strong> and <strong>quality</strong>.<br></p>

<hr>

<h1 id="llama-redpajama-reproductions-and-repav2-illustrate-dataset-reproduction-link-aware-quality-and-large-scale-signal-computation">LLaMA, RedPajama reproductions, and RepAv2 illustrate dataset reproduction, link-aware quality, and large-scale signal computation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-47-43-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-47-43-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-47-43-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-47-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>LLaMA</strong> combined Common Crawl processed with CCNet‑style filters, <strong>C4</strong>, permissively licensed GitHub, Wikipedia, and multiple book sources to produce a multi‑trillion‑token training mix; the exact proprietary mix was not released but has been materially reproduced (e.g., RedPajama).<br></p>

<p>Variants like <strong>RepAv2</strong> process multiple Common Crawl snapshots and compute many quality signals at scale to produce candidate corpora for research into filtering strategies.<br></p>

<p>Alternative signals include <strong>link‑structure classifiers</strong> that predict whether pages are cited by Wikipedia, leveraging web link graphs as proxies for quality.<br></p>

<p>These reproductions highlight challenges of exact replication and the benefits of publishing both data and filtering code for the research community.<br></p>

<hr>

<h1 id="refinedweb-and-fineweb-provide-minimally-processed-web-datasets-intended-for-further-curation">RefinedWeb and FineWeb provide minimally processed web datasets intended for further curation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-52-03-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-52-03-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-52-03-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-52-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>RefinedWeb</strong> and <strong>FineWeb</strong> produce lightly filtered, deduplicated, and extract‑cleaned Common Crawl derivatives that preserve broad coverage while removing obvious noise.<br></p>

<p>Typical priorities:<br></p>
<ul>
  <li>High‑quality <strong>HTML→text extraction</strong><br>
</li>
  <li>
<strong>Fuzzy deduplication</strong><br>
</li>
  <li>Conservative rule‑based removal of low‑signal documents<br>
</li>
</ul>

<p>These datasets yield multi‑trillion‑token pools with released subsets intended as researcher‑friendly starting points for subsequent model‑based filtering or task‑specific selection, preserving linguistic diversity that Wikipedia‑centric filters might eliminate.<br></p>

<hr>

<h1 id="datacomp-defined-a-standard-benchmarking-and-competition-framework-and-demonstrated-aggressive-model-based-filtering-gains">DataComp defined a standard benchmarking and competition framework and demonstrated aggressive model-based filtering gains</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/00-57-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/00-57-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/00-57-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/00-57-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>DataComp</strong> produced a standardized infrastructure to process all Common Crawl dumps into a massive DCM pool, then applied a staged recipe to filter down to a DCM baseline using a learned quality classifier plus rule‑based filters.<br></p>

<p>Key elements:<br></p>
<ul>
  <li>Label positive examples (e.g., instruction‑like data from Open Hermes, ELI5).<br>
</li>
  <li>Train a fast classifier to score candidates.<br>
</li>
  <li>Aggressively filter the large raw pool to a small high‑quality subset.<br>
</li>
</ul>

<p>Aggressive model‑in‑the‑loop filtering delivered substantial downstream gains on language modeling benchmarks and catalyzed reproducible comparisons of filtering strategies through open artifacts and baselines.<br></p>

<hr>

<h1 id="nemoneatroncc-ensembles-model-based-scorers-and-rewrites-to-increase-usable-token-volume-while-preserving-quality">NeMo/NeatronCC ensembles model-based scorers and rewrites to increase usable token volume while preserving quality</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-02-29-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-02-29-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-02-29-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-02-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>NeMo/NeatronCC</strong> addressed the scale‑quality trade‑off by combining multiple scoring models, bucket‑based sampling, and token‑preserving extraction to recover more usable tokens from Common Crawl while maintaining benchmark performance.<br></p>

<p>Techniques used:<br></p>
<ul>
  <li>Use large models to score documents for educational value.<br>
</li>
  <li>Ensemble scores with DCM classifiers and sample from score buckets to preserve coverage.<br>
</li>
  <li>In some cases, use models to <strong>rewrite</strong> lower‑quality texts into higher‑quality forms or generate task‑like input‑output pairs from high‑quality documents.<br>
</li>
</ul>

<p>This increased the available high‑quality token pool to several trillion tokens and produced subsets that outperformed aggressively pruned baselines on standard benchmarks—illustrating ensemble and rewrite techniques as practical mechanisms to expand training budgets without sacrificing curated signal.<br></p>

<hr>

<h1 id="copyright-law-and-licensing-are-central-constraints-for-dataset-collection-and-sharing">Copyright law and licensing are central constraints for dataset collection and sharing</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-07-39-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-07-39-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-07-39-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-07-39.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Copyright</strong> protects original expressive works fixed in a tangible medium, and most internet text is copyrighted even without registration.<br></p>

<p>Legal pathways for dataset use include:<br></p>
<ul>
  <li>
<strong>Licenses</strong> from rights holders<br>
</li>
  <li>
<strong>Fair use</strong> defenses (assessed by purpose, nature, amount used, and market effect)<br>
</li>
</ul>

<p>Platform licensing agreements and <strong>terms of service</strong> (YouTube, Reddit, GitHub, etc.) can restrict automated crawling and downstream model training even when content is publicly accessible.<br></p>

<p>For research and production datasets, explicit licensing choices or risk assessments about fair use and market effects are essential to determine lawful inclusion and redistribution strategies.<br></p>

<hr>

<h1 id="training-models-on-copyrighted-content-raises-complex-legal-and-technical-issues-including-memorization-and-transformative-use-arguments">Training models on copyrighted content raises complex legal and technical issues including memorization and transformative use arguments</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-13-45-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-13-45-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-13-45-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-13-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Using copyrighted material for training can implicate reproduction rights because training copies data into storage and models can sometimes <strong>memorize and regurgitate</strong> verbatim excerpts.<br></p>

<p>Common legal and technical considerations:<br></p>
<ul>
  <li>Defenses include arguing training is <strong>transformative</strong>—extracting abstract patterns rather than reproducing expression—but the legal status is unsettled and fact‑specific.<br>
</li>
  <li>Platform <strong>terms of service</strong> may independently prohibit automated extraction, so datasets must account for contractual constraints as well as copyright law.<br>
</li>
  <li>Technical mitigations: <strong>deduplication</strong>, removal of verbatim copyrighted passages, and auditing to limit extractable memorized content.<br>
</li>
</ul>

<hr>

<h1 id="mid-training-and-post-training-use-targeted-corpora-and-synthetic-data-to-instill-capabilities-such-as-long-context-and-instruction-following">Mid-training and post-training use targeted corpora and synthetic data to instill capabilities such as long context and instruction following</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-18-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Mid‑training</strong> often focuses on capability extension, for example training on longer documents (books, mathematical text) to enable <strong>long‑context modeling</strong>, since attention complexity makes long‑context training expensive and better deferred until model capacity is adequate.<br></p>

<p>Instruction tuning and post‑training exploit task‑structured datasets by converting benchmarks into unified prompt‑response formats (e.g., <strong>FLAN</strong>, <strong>SuperNaturalInstructions</strong>) or by synthesizing instruction data using existing models (<strong>self‑instruct</strong>, <strong>Alpaca</strong>, <strong>Vicuna</strong>).<br></p>

<p>Notes on synthetic pipelines and human alignment:<br></p>
<ul>
  <li>Synthetic generation can produce large amounts of conversational or reasoning‑trace data by prompting stronger models, but generator license terms may restrict their use.<br>
</li>
  <li>
<strong>Human annotation</strong> and <strong>RLHF</strong> remain important for alignment, though they are more expensive and slower than synthetic methods.<br>
</li>
</ul>

<hr>

<h1 id="dataset-engineering-is-heuristic-consequential-and-a-primary-axis-for-improving-language-models">Dataset engineering is heuristic, consequential, and a primary axis for improving language models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec13/01-18-13-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec13/01-18-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Effective language model development requires an end‑to‑end data pipeline that includes:<br></p>
<ol>
  <li>Acquiring live service snapshots.<br>
</li>
  <li>Converting raw responses to tokenizable text.<br>
</li>
  <li>Language identification.<br>
</li>
  <li>Deduplication.<br>
</li>
  <li>Quality filtering and license checking.<br>
</li>
  <li>Constructing staged mixes for <strong>pre‑</strong>, <strong>mid‑</strong>, and <strong>post‑training</strong>.<br>
</li>
</ol>

<p>Small implementation choices—HTML extractor, deduplication thresholds, positive example selection for classifiers, sampling from score buckets—have measurable downstream impacts on model behavior and benchmarks.<br></p>

<p>Because many decisions remain heuristic, there are substantial opportunities for systematic research on:<br></p>
<ul>
  <li>Principled filtering<br>
</li>
  <li>Fairness‑aware curation<br>
</li>
  <li>Copyright‑safe pipelines<br>
</li>
  <li>Reproducible dataset benchmarks<br>
</li>
</ul>

<p>Investing in dataset engineering and open reproducible pipelines often yields outsized returns relative to marginal architectural changes when scale and compute are sufficient.<br></p>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
