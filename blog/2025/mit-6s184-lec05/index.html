<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>MIT 6.S184 - Lecture 5 - Diffusion for Robotics | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/mit-6s184-lec05/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</h1>
    <p class="post-meta">December 18, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#challenges-of-applying-diffusion-and-flow-models-to-robotics-differ-from-image-generation">Challenges of applying diffusion and flow models to robotics differ from image generation</a></li>
<li class="toc-entry toc-h1"><a href="#diffusion-models-for-robotics-condition-on-robot-observations-and-denoise-future-trajectory-waypoints">Diffusion models for robotics condition on robot observations and denoise future trajectory waypoints</a></li>
<li class="toc-entry toc-h1"><a href="#receding-horizon-diffusion-planning-produces-coherent-short-horizon-trajectories">Receding-horizon diffusion planning produces coherent short-horizon trajectories</a></li>
<li class="toc-entry toc-h1"><a href="#teleoperation-and-diverse-human-demonstrations-are-the-primary-data-sources-for-robot-behavior-learning">Teleoperation and diverse human demonstrations are the primary data sources for robot behavior learning</a></li>
<li class="toc-entry toc-h1"><a href="#robot-morphology-and-camera-configuration-critically-affect-learned-policy-robustness">Robot morphology and camera configuration critically affect learned policy robustness</a></li>
<li class="toc-entry toc-h1"><a href="#generalist-models-use-language-or-goal-conditioning-to-scale-beyond-single-behaviors-and-inference-is-executed-asynchronously-with-a-low-level-safety-controller">Generalist models use language or goal conditioning to scale beyond single behaviors and inference is executed asynchronously with a low-level safety controller</a></li>
<li class="toc-entry toc-h1"><a href="#architectural-upgrades-improve-performance-shared-encoders-transformers-and-diffusion-heads">Architectural upgrades improve performance: shared encoders, transformers, and diffusion heads</a></li>
<li class="toc-entry toc-h1"><a href="#single-behavior-training-produces-many-concrete-manipulation-capabilities-but-faces-memory-and-horizon-limitations">Single-behavior training produces many concrete manipulation capabilities but faces memory and horizon limitations</a></li>
<li class="toc-entry toc-h1"><a href="#action-representation-as-commanded-end-effector-pose-with-impedance-control-makes-policies-force-aware">Action representation as commanded end-effector pose with impedance control makes policies force-aware</a></li>
<li class="toc-entry toc-h1"><a href="#closed-loop-wiping-example-demonstrates-robust-recovery-and-the-practical-utility-of-behavior-cloning">Closed-loop wiping example demonstrates robust recovery and the practical utility of behavior cloning</a></li>
<li class="toc-entry toc-h1"><a href="#simulation-and-tactile-sensing-augment-real-world-training-and-evaluation-pipelines">Simulation and tactile sensing augment real-world training and evaluation pipelines</a></li>
<li class="toc-entry toc-h1"><a href="#out-of-distribution-shifts-produce-brittleness-with-characteristic-failure-modes">Out-of-distribution shifts produce brittleness with characteristic failure modes</a></li>
<li class="toc-entry toc-h1"><a href="#data-curation-practicesshort-memory-multimodality-pauses-observability-and-intentional-imperfectionare-crucial-for-robustness">Data curation practices—short memory, multimodality, pauses, observability, and intentional imperfection—are crucial for robustness</a></li>
<li class="toc-entry toc-h1"><a href="#scaling-to-multitask-generalist-models-leverages-diverse-datasets-and-pretrained-visual-language-models">Scaling to multitask generalist models leverages diverse datasets and pretrained visual-language models</a></li>
<li class="toc-entry toc-h1"><a href="#generalist-versus-specialist-tradeoffs-and-emergent-cooperative-behaviors">Generalist versus specialist tradeoffs and emergent cooperative behaviors</a></li>
<li class="toc-entry toc-h1"><a href="#denoising-dynamics-are-implementation-specific-and-are-decoupled-from-predicted-action-trajectories-using-alternative-training-objectives">Denoising dynamics are implementation-specific and are decoupled from predicted action trajectories using alternative training objectives</a></li>
<li class="toc-entry toc-h1"><a href="#long-term-research-goal-build-general-purpose-scientifically-understood-robotic-systems-and-scale-research-teams-and-collaborations">Long-term research goal: build general-purpose, scientifically-understood robotic systems and scale research teams and collaborations</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/7tsCN2hRBMg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="challenges-of-applying-diffusion-and-flow-models-to-robotics-differ-from-image-generation">Challenges of applying diffusion and flow models to robotics differ from image generation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-02-32-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-02-32-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-02-32-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-02-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Applying <strong>diffusion and flow models</strong> to robotics introduces distinct practical constraints compared to image generation because robotics involves much <strong>smaller datasets</strong> and <strong>closed-loop</strong> performance requirements.<br></p>

<p>Robotic models must run in a physical <strong>feedback loop</strong>, which places strict <strong>latency</strong> and <strong>reliability</strong> requirements on inference and makes <strong>distribution shift</strong> during deployment a central concern.<br></p>

<p>The combination of limited data and the need for robust closed-loop behavior changes many design decisions, including:</p>
<ul>
  <li>
<strong>Model size</strong> and capacity trade-offs</li>
  <li>
<strong>Conditioning modalities</strong> (what sensors and signals the model consumes)</li>
  <li>
<strong>Evaluation strategies</strong> that emphasize real-time safety and robustness<br>
</li>
</ul>

<p>Effective deployment therefore requires adapting generative modeling techniques to <strong>real-time control constraints</strong> and the realities of <strong>physical interaction</strong>.<br></p>

<hr>

<h1 id="diffusion-models-for-robotics-condition-on-robot-observations-and-denoise-future-trajectory-waypoints">Diffusion models for robotics condition on robot observations and denoise future trajectory waypoints</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-04-51-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-04-51-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-04-51-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-04-51.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Diffusion-based robotic policies</strong> treat the prediction problem analogously to image diffusion, but the target is a <strong>temporally ordered sequence of control waypoints</strong> rather than pixels.<br></p>

<p>Key aspects:</p>
<ul>
  <li>The model <strong>conditions</strong> on robot observations (camera images, proprioception such as pose and joint states).<br>
</li>
  <li>It <strong>denoises</strong> a noisy sequence of future waypoints to produce a coherent <strong>short-horizon plan</strong>—typically predicting at rates like <strong>10 Hz</strong> for windows around <strong>two seconds</strong>.<br>
</li>
</ul>

<p>Inference is performed online in a <strong>receding-horizon loop</strong>:</p>
<ol>
  <li>Predict a short window of waypoints (the plan).<br>
</li>
  <li>Execute a subset of actions from that window while inference runs again.<br>
</li>
  <li>Refresh the plan to remain reactive to new observations.<br>
</li>
</ol>

<p>This formulation emphasizes <strong>low-latency inference</strong> (ideally under one second) and <strong>dense waypoint prediction</strong> rather than long open-loop rollouts.<br></p>

<hr>

<h1 id="receding-horizon-diffusion-planning-produces-coherent-short-horizon-trajectories">Receding-horizon diffusion planning produces coherent short-horizon trajectories</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-06-42-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-06-42-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-06-42-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-06-42.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Visualizing diffusion outputs in simple 2D tasks clarifies the approach:<br></p>

<ul>
  <li>The <strong>noising process</strong> starts with random waypoint trajectories. Denoising produces smooth, task-specific trajectories that reach the goal.<br>
</li>
  <li>By predicting a <strong>dense sequence of future poses</strong> and repeatedly re-planning, the system achieves <strong>closed-loop behavior</strong> that can recover from intermediate perturbations and unexpected environment changes.<br>
</li>
  <li>The <strong>receding-horizon</strong> nature ensures plans are short enough to be reactive yet long enough to encode meaningful motion structure, enabling complex manipulations with relatively simple conditioning and modest data.<br>
</li>
</ul>

<p>Practical success also requires engineering <strong>tips and tricks</strong> around data curation, sensor placement and inference scheduling to maximize robustness.<br></p>

<hr>

<h1 id="teleoperation-and-diverse-human-demonstrations-are-the-primary-data-sources-for-robot-behavior-learning">Teleoperation and diverse human demonstrations are the primary data sources for robot behavior learning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-08-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-08-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-08-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-08-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Supervised learning</strong> of behavior-diffusion policies uses human demonstrations as the core dataset, typically collected via teleoperation interfaces:<br></p>

<ul>
  <li>High-bandwidth <strong>haptic devices</strong><br>
</li>
  <li>
<strong>VR teaching rigs</strong><br>
</li>
  <li>Six-degree-of-freedom mice<br>
</li>
  <li>Handheld instrumented grippers<br>
</li>
</ul>

<p>Typical data-collection patterns:</p>
<ul>
  <li>A single behavior: <strong>a few hundred demonstrations</strong> collected over a few hours of teleoperation.<br>
</li>
  <li>Yields <strong>tens of thousands</strong> of time-indexed samples at <strong>10 Hz</strong> for training.<br>
</li>
  <li>Different collection modalities are combined into a unified dataset for single-behavior training.<br>
</li>
</ul>

<p>Careful teleop design (including <strong>haptic feedback</strong>) improves demonstration quality. This <strong>data-centric</strong> approach emphasizes instrumented, repeatable teaching workflows as the primary lever for performance improvements in single-task settings.<br></p>

<hr>

<h1 id="robot-morphology-and-camera-configuration-critically-affect-learned-policy-robustness">Robot morphology and camera configuration critically affect learned policy robustness</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-10-04-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-10-04-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-10-04-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-10-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Physical station design</strong> (camera count, viewpoints, wrist-mounted cameras) strongly influences policy generalization because policies easily overfit to static camera configurations when data is limited.<br></p>

<p>Practical recommendations:</p>
<ul>
  <li>Add <strong>wrist cameras</strong> that move with the gripper to provide natural image randomization and shared visual features.<br>
</li>
  <li>
<strong>Share the same image encoder</strong> across wrist and scene cameras to enable cross-view feature transfer.<br>
</li>
  <li>Design hardware and sensor placement to <strong>increase visual diversity</strong> in training data, reducing sensitivity to small shifts in camera position or scene appearance.<br>
</li>
</ul>

<p>In short, hardware design and sensor placement are as important as model architecture for robust deployment.<br></p>

<hr>

<h1 id="generalist-models-use-language-or-goal-conditioning-to-scale-beyond-single-behaviors-and-inference-is-executed-asynchronously-with-a-low-level-safety-controller">Generalist models use language or goal conditioning to scale beyond single behaviors and inference is executed asynchronously with a low-level safety controller</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-12-11-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-12-11-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-12-11-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-12-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Scaling from one behavior to many typically uses <strong>conditioning modalities</strong> such as <strong>language prompts</strong> or <strong>goal images</strong> to specify the desired behavior for a generalist model.<br></p>

<p>Execution architecture on real robots:</p>
<ul>
  <li>A <strong>buffer</strong> of low-level commands is maintained and executed by a <strong>high-frequency controller</strong> (impedance or position controller) that enforces safety constraints.<br>
</li>
  <li>The diffusion policy runs <strong>asynchronously</strong> to refill the buffer and replan.<br>
</li>
</ul>

<p>Typical practice:</p>
<ol>
  <li>Predict a horizon of actions (e.g., <strong>16–32 steps</strong>).<br>
</li>
  <li>Execute a portion of them. <br>
</li>
  <li>Re-invoke inference and overwrite the buffer, enabling graceful handling of variable compute latency and continuous reactivity.<br>
</li>
</ol>

<p>This combines <strong>high-level learned planning</strong> with <strong>classical control primitives</strong> to ensure safety and smooth execution.<br></p>

<hr>

<h1 id="architectural-upgrades-improve-performance-shared-encoders-transformers-and-diffusion-heads">Architectural upgrades improve performance: shared encoders, transformers, and diffusion heads</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-13-34-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-13-34-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-13-34-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-13-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Early diffusion policy work has evolved to incorporate modern ML practices:</p>
<ul>
  <li>
<strong>Shared visual encoders</strong> across multiple cameras (instead of independent encoders).<br>
</li>
  <li>
<strong>Transformer-based backbones</strong> for richer spatiotemporal modeling.<br>
</li>
  <li>
<strong>Attention-based conditioning</strong> between observations and action tokens.<br>
</li>
</ul>

<p>Benefits:</p>
<ul>
  <li>Increased <strong>cross-view feature sharing</strong> and improved <strong>sample efficiency</strong> on small robotics datasets.<br>
</li>
  <li>In some cases, a Transformer head resembles a <strong>diffusion transformer</strong> that models the denoising process in action-space.<br>
</li>
</ul>

<p>These upgrades keep the core diffusion objective but substantially boost empirical performance and form a practical baseline for training new robotic behaviors from scratch.<br></p>

<hr>

<h1 id="single-behavior-training-produces-many-concrete-manipulation-capabilities-but-faces-memory-and-horizon-limitations">Single-behavior training produces many concrete manipulation capabilities but faces memory and horizon limitations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-15-23-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-15-23-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-15-23-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-15-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Training one behavior at a time with the diffusion recipe yields a wide variety of capabilities—cloth folding, wiping, deformable-object handling and extended multi-step skills—often from a few hundred demonstrations.<br></p>

<p>Observed limitations and patterns:</p>
<ul>
  <li>Learned policies typically have <strong>limited implicit memory</strong> (on the order of about <strong>one second</strong> of history).<br>
</li>
  <li>Policies become brittle when asked to remember or reason across much longer horizons; this is more a <strong>data</strong> limitation than an architectural one in the from-scratch regime.<br>
</li>
  <li>
<strong>Long behaviors</strong> are often realized as implicit <strong>mode-switching</strong> between many short sub-behaviors rather than explicit long-term memory.<br>
</li>
</ul>

<p>Practical workflow: iterative prototyping, adding demonstrations and careful curriculum design often convert short successful snippets into richer, longer-running skills.<br></p>

<hr>

<h1 id="action-representation-as-commanded-end-effector-pose-with-impedance-control-makes-policies-force-aware">Action representation as commanded end-effector pose with impedance control makes policies force-aware</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-17-30-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-17-30-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-17-30-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-17-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The most common action representation combines <strong>commanded end-effector poses</strong> (six-DOF targets) with an underlying <strong>impedance-control mode</strong> that maps position commands to commanded forces.<br></p>

<p>Why this works:</p>
<ul>
  <li>
<strong>Impedance control</strong> treats a position setpoint as a target converted to forces by controller gains, so contact implicitly controls applied force magnitude.<br>
</li>
  <li>Providing a short history of previous commands and visual feedback enables the policy to <strong>implicitly infer contact forces</strong> from visual-proprioceptive deltas, enabling force-aware manipulation without explicit force sensing.<br>
</li>
</ul>

<p>This representation decouples <strong>high-level learned planning</strong> from <strong>low-level safety and torque-limiting control</strong>, while enabling compliant interactions.<br></p>

<hr>

<h1 id="closed-loop-wiping-example-demonstrates-robust-recovery-and-the-practical-utility-of-behavior-cloning">Closed-loop wiping example demonstrates robust recovery and the practical utility of behavior cloning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-19-03-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-19-03-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-19-03-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-19-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Closed-loop learned wiping policies repeatedly adjust actions until the visual evidence of dirt is removed, demonstrating an emergent <strong>recovery behavior</strong> that is robust in the trained environment.<br></p>

<p>Empirical lessons:</p>
<ul>
  <li>Although <strong>behavior cloning</strong> was criticized for replaying demonstrations, careful data curation and scaling of supervised regimes produce substantial generalization and reactive capability.<br>
</li>
  <li>Simple supervised recipes, <strong>receding-horizon diffusion planning</strong>, and good teleoperation data produce surprisingly capable closed-loop manipulators for many everyday tasks.<br>
</li>
</ul>

<p>Key caveat: these behaviors are primarily reliable within the <strong>training lab distribution</strong> unless explicitly hardened via data variation.<br></p>

<hr>

<h1 id="simulation-and-tactile-sensing-augment-real-world-training-and-evaluation-pipelines">Simulation and tactile sensing augment real-world training and evaluation pipelines</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-20-58-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-20-58-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-20-58-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-20-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Because physical evaluation is expensive and robots are limited resources, large-scale evaluation and some data generation are performed in <strong>simulation</strong> using realistic environments and physics engines (e.g., <strong>Drake</strong>).<br></p>

<p>Simulation benefits:</p>
<ul>
  <li>Enables <strong>hundreds of thousands of rollouts</strong> for benchmarking.<br>
</li>
  <li>Supports internal <strong>multi-task benchmarks</strong> and rapid automated evaluation across dozens of behaviors, accelerating development and hyperparameter search.<br>
</li>
</ul>

<p>Sensor modalities and conditioning:</p>
<ul>
  <li>Non-visual sensors (tactile membranes, gel or air-filled with embedded cameras) produce <strong>dense latent observations</strong> that the same visual encoder machinery can condition on.<br>
</li>
  <li>This enables <strong>touch-aware</strong> and contact-sensitive behaviors (e.g., tightening bottle caps) without major code changes.<br>
</li>
</ul>

<p>Combining sim and multiple sensor modalities increases training fidelity and enables richer conditioning beyond cameras.<br></p>

<hr>

<h1 id="out-of-distribution-shifts-produce-brittleness-with-characteristic-failure-modes">Out-of-distribution shifts produce brittleness with characteristic failure modes</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-23-13-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-23-13-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-23-13-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-23-13.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Policies trained from scratch on limited datasets can be <strong>brittle</strong> under modest distribution shifts (changed camera types, altered optics, scratched parts, background changes).<br></p>

<p>Symptoms of shift:</p>
<ul>
  <li>Incorrect grasp placements, jerky or hesitant motions, <strong>mode collapse</strong> where the policy gets stuck in a single mode.<br>
</li>
  <li>Extreme shifts can cause random or flailing motions and catastrophic failures; intermediate shifts often show degraded or halting sequences before failure.<br>
</li>
</ul>

<p>Practical remedies:</p>
<ul>
  <li>Augment training with <strong>targeted demonstrations</strong> that show recovery behaviors.<br>
</li>
  <li>Add <strong>visual diversity</strong> during data collection.<br>
</li>
  <li>Iteratively patch failures by collecting additional teleop data demonstrating desired recovery transitions.<br>
</li>
</ul>

<hr>

<h1 id="data-curation-practicesshort-memory-multimodality-pauses-observability-and-intentional-imperfectionare-crucial-for-robustness">Data curation practices—short memory, multimodality, pauses, observability, and intentional imperfection—are crucial for robustness</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-27-11-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-27-11-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-27-11-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-27-11.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Data quality and collection protocols</strong> strongly determine policy robustness.<br></p>

<p>Guidelines:</p>
<ul>
  <li>Short implicit memory (the <strong>“goldfish” effect</strong>) means demonstrations should <strong>avoid long idle pauses</strong>, which teach the policy to remain still.<br>
</li>
  <li>
<strong>Long-horizon multimodal decisions</strong> require explicit demonstration of alternative modes and recovery transitions to avoid indecision.<br>
</li>
  <li>
<strong>Observability alignment</strong>: operators should teach through the robot’s sensors to avoid leaked information the robot cannot see.<br>
</li>
  <li>Include controlled variability and small mistakes so the policy learns recovery behaviors, but avoid so many errors that the policy adopts suboptimal strategies.<br>
</li>
</ul>

<p>Because robustness via added variation is laborious, <strong>scaling solutions</strong> are required to avoid per-task manual patching.<br></p>

<hr>

<h1 id="scaling-to-multitask-generalist-models-leverages-diverse-datasets-and-pretrained-visual-language-models">Scaling to multitask generalist models leverages diverse datasets and pretrained visual-language models</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-33-52-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-33-52-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-33-52-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-33-52.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>To move beyond single-behavior policies, the prevailing approach is to train larger <strong>multitask models</strong> conditioned by language or goal signals and to leverage diverse data sources:</p>
<ul>
  <li>Other robots’ logs and video datasets<br>
</li>
  <li>
<strong>Pretrained visual-language models (VLMs)</strong> as foundational encoders<br>
</li>
</ul>

<p>Practical pattern:</p>
<ul>
  <li>Use pretrained VLMs for rich multimodal representations, combine them with action heads via cross-attention or conditioning, then <strong>fine-tune</strong> on action-labeled robot data to improve zero-shot or few-shot generalization.<br>
</li>
</ul>

<p>Main limitation: scarcity of large-scale robot action datasets, so transfer from large visual-language pretraining and integration of video/action datasets are central to current scaling strategies. Fine-tuning remains important to align generalist models to specific robot stations or tasks.<br></p>

<hr>

<h1 id="generalist-versus-specialist-tradeoffs-and-emergent-cooperative-behaviors">Generalist versus specialist tradeoffs and emergent cooperative behaviors</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-38-45-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-38-45-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-38-45-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-38-45.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Generalist models</strong> trained on broad multimodal datasets can exhibit <strong>compositional generalization</strong>—combining primitives to solve novel tasks—and sometimes show emergent recovery behaviors not seen in from-scratch specialists.<br></p>

<p>Practical observations:</p>
<ul>
  <li>Generalists can be idiosyncratic, less steerable, or not perfectly cooperative out-of-the-box.<br>
</li>
  <li>
<strong>Fine-tuning</strong> on task-specific data is often required for reliably controlled, demo-quality deployments.<br>
</li>
</ul>

<p>Strategy: use generalists as a strong foundation for transfer, then build specialists via fine-tuning when peak reliability is required. Engineering effort focuses on improving <strong>steerability</strong>, <strong>data efficiency</strong>, and fine-tuning recipes to preserve pretraining benefits.<br></p>

<hr>

<h1 id="denoising-dynamics-are-implementation-specific-and-are-decoupled-from-predicted-action-trajectories-using-alternative-training-objectives">Denoising dynamics are implementation-specific and are decoupled from predicted action trajectories using alternative training objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-40-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-40-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-40-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-40-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The <strong>denoising path</strong> taken during diffusion or flow-matching optimization is an implementation detail distinct from the action trajectories the policy outputs to the robot.<br></p>

<p>Practical design principle:</p>
<ul>
  <li>Decouple <strong>optimization dynamics</strong> (diffusion vs flow matching) from the <strong>action-space prediction target</strong> (e.g., waypoint sequences).<br>
</li>
  <li>Treat denoising or matching dynamics as a <strong>black-box sampler</strong> for the learned action distribution.<br>
</li>
</ul>

<p>This separation helps ensure idiosyncrasies of the denoising optimizer do not alter the semantics of executed robot behaviors.<br></p>

<hr>

<h1 id="long-term-research-goal-build-general-purpose-scientifically-understood-robotic-systems-and-scale-research-teams-and-collaborations">Long-term research goal: build general-purpose, scientifically-understood robotic systems and scale research teams and collaborations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-42-47-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-42-47-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mit-6.s184/frames/lec05/00-42-47-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mit-6.s184/frames/lec05/00-42-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The strategic aim is to develop scientific understanding and engineering practices for training <strong>general-purpose robotic systems</strong> using AI, emphasizing <strong>transferability</strong>, <strong>reproducibility</strong> and <strong>open research</strong>.<br></p>

<p>Required components:</p>
<ul>
  <li>Large multimodal pretraining and pretrained backbones<br>
</li>
  <li>Improved evaluation infrastructure and extensive <strong>simulation benchmarks</strong><br>
</li>
  <li>Practical data-collection pipelines and instrumented teaching workflows<br>
</li>
  <li>Collaborative academic-industrial efforts to scale expertise and datasets<br>
</li>
</ul>

<p>Building robust generalists that can be fine-tuned into specialists and creating community benchmarking resources will accelerate progress toward capable deployed robotic systems. Organizations engaged in this research often seek collaborations and talent to expand experimental and evaluation capacity.<br></p>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec01/">MIT 6.S184 - Lecture 1 - Generative AI with SDEs</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
