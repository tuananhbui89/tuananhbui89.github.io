<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Stanford CS236 - Deep Generative Models I 2023 I Lecture 14 - Score Based Models | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/cs236-2023-lec14/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Stanford CS236 - Deep Generative Models I 2023 I Lecture 14 - Score Based Models</h1>
    <p class="post-meta">December 18, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#score-based-models-represent-probability-distributions-via-neural-network-parameterized-score-vector-fields">Score-based models represent probability distributions via neural network parameterized score vector fields</a></li>
<li class="toc-entry toc-h1"><a href="#denoising-score-matching-trains-the-score-model-on-noise-perturbed-data-rather-than-the-clean-data-distribution">Denoising score matching trains the score model on noise-perturbed data rather than the clean data distribution</a></li>
<li class="toc-entry toc-h1"><a href="#slice-score-matching-uses-random-one-dimensional-projections-to-make-score-estimation-scalable-while-targeting-the-true-data-score">Slice score matching uses random one-dimensional projections to make score estimation scalable while targeting the true data score</a></li>
<li class="toc-entry toc-h1"><a href="#langevin-dynamics-uses-the-score-field-for-sampling-but-fails-in-practice-on-high-dimensional-manifold-supported-data">Langevin dynamics uses the score field for sampling but fails in practice on high-dimensional manifold-supported data</a></li>
<li class="toc-entry toc-h1"><a href="#adding-noise-to-data-remedies-manifold-and-low-density-problems-by-giving-the-perturbed-distribution-full-support">Adding noise to data remedies manifold and low-density problems by giving the perturbed distribution full support</a></li>
<li class="toc-entry toc-h1"><a href="#the-data-manifold-concept-explains-why-noise-magnitude-matters-and-motivates-a-tradeoff-between-estimation-accuracy-and-target-mismatch">The data manifold concept explains why noise magnitude matters and motivates a tradeoff between estimation accuracy and target mismatch</a></li>
<li class="toc-entry toc-h1"><a href="#diffusion--score-based-models-jointly-learn-scores-at-multiple-noise-levels-and-sample-by-annealing-from-high-to-low-noise">Diffusion / score-based models jointly learn scores at multiple noise levels and sample by annealing from high to low noise</a></li>
<li class="toc-entry toc-h1"><a href="#a-single-noise-conditional-neural-network-amortizes-estimation-across-many-noise-levels-and-balances-model-capacity-and-efficiency">A single noise-conditional neural network amortizes estimation across many noise levels and balances model capacity and efficiency</a></li>
<li class="toc-entry toc-h1"><a href="#training-uses-a-weighted-mixture-of-denoising-objectives-across-noise-scales-and-often-parameterizes-outputs-as-noise-predictors">Training uses a weighted mixture of denoising objectives across noise scales and often parameterizes outputs as noise predictors</a></li>
<li class="toc-entry toc-h1"><a href="#training-is-implemented-by-stochastic-gradient-descent-with-per-sample-noise-level-selection-and-amortized-denoising-tasks">Training is implemented by stochastic gradient descent with per-sample noise-level selection and amortized denoising tasks</a></li>
<li class="toc-entry toc-h1"><a href="#sampling-uses-annealed-langevin-dynamics-or-numerical-solvers-of-reverse-time-sdes-with-tradeoffs-between-steps-and-sample-quality">Sampling uses annealed Langevin dynamics or numerical solvers of reverse-time SDEs, with tradeoffs between steps and sample quality</a></li>
<li class="toc-entry toc-h1"><a href="#the-continuous-time-diffusion-perspective-formulates-forward-corruptions-as-an-sde-and-sampling-as-solving-a-reverse-time-sde-conditioned-on-scores">The continuous-time diffusion perspective formulates forward corruptions as an SDE and sampling as solving a reverse-time SDE conditioned on scores</a></li>
<li class="toc-entry toc-h1"><a href="#diffusion-models-connect-to-deterministic-odes-and-continuous-normalizing-flows-and-exhibit-strong-empirical-performance-with-memorization-caveats">Diffusion models connect to deterministic ODEs and continuous normalizing flows, and exhibit strong empirical performance with memorization caveats</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/E69Lp_T9nVg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="score-based-models-represent-probability-distributions-via-neural-network-parameterized-score-vector-fields">Score-based models represent probability distributions via neural network parameterized score vector fields</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-01-12-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-01-12-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-01-12-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-01-12.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Score-based models parameterize the gradient of the log-density—the <strong>score</strong>—as a vector-valued neural network that maps each point in data space to the local gradient of log probability. <br></p>

<ul>
  <li>The model output is a vector field <strong>s_theta(x)</strong> intended to approximate <strong>∇_x log p_data(x)</strong>. <br>
</li>
  <li>Training seeks to make that vector field match the true score field. <br>
</li>
  <li>
<strong>Score matching</strong> provides a principled loss for fitting this vector field by minimizing an expectation derived via integration by parts. <br>
</li>
  <li>The direct objective, however, involves the <strong>trace of a Jacobian</strong>, which is computationally prohibitive in high dimensions. <br>
</li>
</ul>

<p>Consequently, naive score matching is impractical for image-scale problems without scalable approximations or alternative formulations. <br></p>

<hr>

<h1 id="denoising-score-matching-trains-the-score-model-on-noise-perturbed-data-rather-than-the-clean-data-distribution">Denoising score matching trains the score model on noise-perturbed data rather than the clean data distribution</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-03-41-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-03-41-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-03-41-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-03-41.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Denoising score matching (DSM)</strong> learns the score of a noise-perturbed data distribution q_sigma(x_t | x) obtained by adding noise (typically Gaussian) to a clean data point x. <br></p>

<ul>
  <li>Instead of estimating <strong>∇ log p_data(x)</strong> directly, DSM trains the network to predict the score of the <strong>corrupted distribution</strong>, <strong>∇_x log q_sigma(x_t)</strong>. <br>
</li>
  <li>This can be implemented as a simple regression objective (e.g., <strong>L2 loss</strong>) between the network output and the tractable score of the perturbation kernel. <br>
</li>
  <li>For Gaussian perturbations the score has a <strong>closed-form expression</strong> proportional to the difference between the corrupted input and the clean mean, yielding an efficient per-sample training target and removing the need to compute Jacobian traces. <br>
</li>
</ul>

<p>DSM is computationally scalable, compatible with common neural architectures, and admits a natural <strong>denoising interpretation</strong>: the network learns to recover the added noise or the clean signal as a function of noise level. <br></p>

<hr>

<h1 id="slice-score-matching-uses-random-one-dimensional-projections-to-make-score-estimation-scalable-while-targeting-the-true-data-score">Slice score matching uses random one-dimensional projections to make score estimation scalable while targeting the true data score</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-06-07-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-06-07-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-06-07-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-06-07.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Slice score matching</strong> projects the vector-valued score at each point along randomly sampled directions v and matches scalar projections rather than full gradients. <br></p>

<ul>
  <li>At each sample the method draws a random direction <strong>v</strong> and computes the projected scalar score ⟨v, ∇_x log p(x)⟩. <br>
</li>
  <li>The model is trained to match that scalar via an objective that can be rewritten to depend only on the model using integration by parts. <br>
</li>
  <li>This replaces full Jacobian computations with <strong>directional derivatives</strong>, which are far cheaper to compute. <br>
</li>
</ul>

<p>Compared to DSM: slice score matching directly targets the score of the <strong>clean data distribution</strong> (no corruption), but it still requires derivative computations and is somewhat slower in practice. <br></p>

<p>When the true-data score is well defined, slice score matching yields consistency with that score. <br></p>

<hr>

<h1 id="langevin-dynamics-uses-the-score-field-for-sampling-but-fails-in-practice-on-high-dimensional-manifold-supported-data">Langevin dynamics uses the score field for sampling but fails in practice on high-dimensional manifold-supported data</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-08-09-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-08-09-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-08-09-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-08-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Langevin dynamics</strong> integrates noisy gradient ascent on log-density using the learned score field: x_{t+1} = x_t + α s_theta(x_t) + √(2α) ξ. <br></p>

<ul>
  <li>The scheme moves particles toward high-probability regions by combining score-driven updates with injected noise. <br>
</li>
  <li>It assumes well-defined scores away from training samples, but natural-image data typically lies on or near a <strong>low-dimensional manifold</strong> in pixel space where the score can be ill-defined or explode off the manifold. <br>
</li>
  <li>Learned scores are most accurate near high-density training regions and unreliable in low-density regions. <br>
</li>
  <li>As a result, naive Langevin chains can get lost, mix poorly between modes, and fail to converge to realistic samples. <br>
</li>
</ul>

<p>These limitations motivate strategies to make score estimation more robust off the data manifold and to improve mixing during sampling. <br></p>

<hr>

<h1 id="adding-noise-to-data-remedies-manifold-and-low-density-problems-by-giving-the-perturbed-distribution-full-support">Adding noise to data remedies manifold and low-density problems by giving the perturbed distribution full support</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-11-05-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-11-05-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-11-05-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-11-05.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Convolving the data distribution with <strong>isotropic noise</strong> (e.g., Gaussian) produces a family of noise-perturbed densities that have <strong>full support</strong> in ambient space, eliminating singular manifold support and producing well-defined, bounded scores everywhere. <br></p>

<ul>
  <li>Estimating scores for these perturbed densities is empirically easier and yields <strong>smoother loss landscapes</strong>: small Gaussian noise regularizes the score estimation problem and improves optimization stability. <br>
</li>
  <li>The tradeoff is that the learned score corresponds to the <strong>perturbed (noisy) distribution</strong>, so following that score naively produces noisy samples rather than clean data. <br>
</li>
</ul>

<p>This motivates either methods to <strong>denoise samples</strong> after sampling or learning scores across <strong>multiple noise scales</strong> so sampling can transition from very noisy to nearly clean distributions. <br></p>

<hr>

<h1 id="the-data-manifold-concept-explains-why-noise-magnitude-matters-and-motivates-a-tradeoff-between-estimation-accuracy-and-target-mismatch">The data manifold concept explains why noise magnitude matters and motivates a tradeoff between estimation accuracy and target mismatch</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-16-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-16-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-16-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-16-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Real data typically occupy a <strong>low-dimensional embedded manifold</strong> in high-dimensional observation space. <br></p>

<ul>
  <li>Many pixel combinations thus have essentially zero probability under <strong>p_data</strong>, and <strong>∇ log p_data</strong> can be ill-behaved off the manifold. <br>
</li>
  <li>Adding a small amount of noise smooths the distribution and eases estimation near the manifold, but: <br>
    <ul>
      <li>Very small noise does not resolve low-density estimation far from data. <br>
</li>
      <li>Very large noise destroys the signal needed to recover clean samples. <br>
</li>
    </ul>
  </li>
</ul>

<p>There is an inherent tradeoff: increasing noise improves global score estimation and mixing but moves the learned target away from <strong>p_data</strong>; decreasing noise yields a score closer to the true data score but is harder to estimate and leads to poor mixing. <br></p>

<p>This tension motivates methods that jointly consider <strong>multiple noise magnitudes</strong> rather than a single perturbation scale. <br></p>

<hr>

<h1 id="diffusion--score-based-models-jointly-learn-scores-at-multiple-noise-levels-and-sample-by-annealing-from-high-to-low-noise">Diffusion / score-based models jointly learn scores at multiple noise levels and sample by annealing from high to low noise</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-22-36-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-22-36-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-22-36-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-22-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Diffusion / score-based models</strong> estimate score fields for a sequence of noise scales σ_L, …, σ_1 that interpolate between heavy corruption and near-clean data. <br></p>

<ul>
  <li>Sampling starts from essentially pure noise and then sequentially applies a sampling procedure (e.g., <strong>annealed Langevin dynamics</strong>) that: <br>
    <ol>
      <li>Uses the score for a <strong>large-noise</strong> level to obtain reasonably mixed initial particles. <br>
</li>
      <li>Progressively switches to scores for <strong>smaller noise</strong> levels to introduce finer structure. <br>
</li>
    </ol>
  </li>
  <li>The multiscale strategy yields accurate directional information at all stages: <strong>coarse scales</strong> guide global structure and mixing, while <strong>fine scales</strong> refine details, enabling generation of nearly clean samples despite estimation challenges at any single noise level. <br>
</li>
</ul>

<p>Practical implementations discretize a continuum of noise levels (often ~1000 steps) and amortize computation by <strong>conditioning one neural network</strong> on the noise level. <br></p>

<hr>

<h1 id="a-single-noise-conditional-neural-network-amortizes-estimation-across-many-noise-levels-and-balances-model-capacity-and-efficiency">A single noise-conditional neural network amortizes estimation across many noise levels and balances model capacity and efficiency</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-28-59-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-28-59-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-28-59-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-28-59.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Training a separate score network for each noise scale is computationally costly, so practice uses a single <strong>noise-conditional network</strong> <strong>s_theta(x, σ)</strong> that takes the noise scale σ (or time index) as an additional input and jointly approximates scores for all desired corruptions. <br></p>

<ul>
  <li>This network shares computation and parameters across scales, amortizing learning. <br>
</li>
  <li>Implementation choices include <strong>embedding σ</strong> and concatenating or injecting it via adaptive layers. <br>
</li>
  <li>The resulting vector fields are not required to be <strong>conservative</strong> (exact gradients of an energy), although conservative parameterizations are possible; empirically, free-form vector fields perform well. <br>
</li>
</ul>

<p>Key hyperparameters: the number of discrete noise levels, maximum and minimum magnitudes, and the <strong>interpolation schedule</strong> (commonly geometric), which control overlap between successive noise shells and the success of annealed sampling. <br></p>

<hr>

<h1 id="training-uses-a-weighted-mixture-of-denoising-objectives-across-noise-scales-and-often-parameterizes-outputs-as-noise-predictors">Training uses a weighted mixture of denoising objectives across noise scales and often parameterizes outputs as noise predictors</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-33-23-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-33-23-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-33-23-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-33-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The training objective sums or integrates <strong>denoising score-matching</strong> losses over the chosen noise scales and typically weights each term with a function <strong>λ(σ)</strong> that balances contributions by noise magnitude and numerical conditioning. <br></p>

<ul>
  <li>For Gaussian perturbations a convenient parameterization is to predict the <strong>added noise ε</strong> (using <strong>ε_theta</strong>) from the corrupted input x_t; this <strong>noise-prediction</strong> parameterization is algebraically equivalent to predicting the scaled score and often improves numerical stability. <br>
</li>
  <li>The loss is implemented by: <br>
    <ol>
      <li>Sampling a mini-batch of clean data. <br>
</li>
      <li>Sampling σ (or an index) per example and generating noisy inputs x_t = x + σ ε. <br>
</li>
      <li>Minimizing the weighted squared error between predicted and true noise (or between predicted and true score). <br>
</li>
    </ol>
  </li>
  <li>Proper choice of <strong>λ(σ)</strong> or scaling factors ensures no single noise level dominates training and yields balanced performance across scales. <br>
</li>
</ul>

<hr>

<h1 id="training-is-implemented-by-stochastic-gradient-descent-with-per-sample-noise-level-selection-and-amortized-denoising-tasks">Training is implemented by stochastic gradient descent with per-sample noise-level selection and amortized denoising tasks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-38-01-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-38-01-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-38-01-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-38-01.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Each training iteration proceeds as follows: <br></p>

<ol>
  <li>Sample a batch of data points. <br>
</li>
  <li>For each point, independently sample a noise scale σ (commonly uniformly or according to a prescribed schedule). <br>
</li>
  <li>Draw Gaussian noise and form the corrupted input x_t = x + σ ε. <br>
</li>
  <li>Evaluate the network <strong>s_theta(x_t, σ)</strong> or <strong>ε_theta(x_t, σ)</strong> and compute the per-sample denoising regression loss weighted by <strong>λ(σ)</strong>. <br>
</li>
  <li>Backpropagate gradients and update parameters with standard optimizers (SGD/Adam). <br>
</li>
</ol>

<p>This single-model, multi-task setup amortizes the cost of solving many denoising tasks and yields a model usable at inference across all noise scales. <br>
In practice, practitioners discretize σ levels (e.g., 1000) and may optionally ensemble or use multiple networks for incremental gains if compute permits. <br></p>

<hr>

<h1 id="sampling-uses-annealed-langevin-dynamics-or-numerical-solvers-of-reverse-time-sdes-with-tradeoffs-between-steps-and-sample-quality">Sampling uses annealed Langevin dynamics or numerical solvers of reverse-time SDEs, with tradeoffs between steps and sample quality</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/00-48-30-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/00-48-30-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/00-48-30-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/00-48-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Sampling from the model begins from samples of a <strong>high-noise prior</strong> (essentially Gaussian) and iteratively reduces noise by running a stochastic procedure that uses <strong>s_theta(x, σ)</strong> to push particles toward higher-density regions while injecting appropriate noise at each step. <br></p>

<ul>
  <li>
<strong>Annealed Langevin dynamics</strong> applies multiple Langevin updates at each discrete σ, using the corresponding conditional score and optionally decreasing step sizes. More steps yield higher quality but increase inference cost because each step requires a full model evaluation. <br>
</li>
  <li>Viewing the noise sequence as a discretization of a continuous diffusion process leads to <strong>reverse-time SDEs</strong> whose numerical integration (predictor-corrector methods) provides principled solvers and can be combined with Langevin correctors for improved mixing. <br>
</li>
</ul>

<p>The practical tradeoff is <strong>compute versus fidelity</strong>: state-of-the-art models often use thousands of network evaluations to generate very high-quality images, which is far more expensive at inference than alternative generator architectures but produces superior results and stable training behavior. <br></p>

<hr>

<h1 id="the-continuous-time-diffusion-perspective-formulates-forward-corruptions-as-an-sde-and-sampling-as-solving-a-reverse-time-sde-conditioned-on-scores">The continuous-time diffusion perspective formulates forward corruptions as an SDE and sampling as solving a reverse-time SDE conditioned on scores</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/01-04-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/01-04-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/01-04-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/01-04-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A continuum of noise levels is naturally modeled as a stochastic process {x_t}_{t∈[0,T]} obtained by an <strong>SDE</strong> that gradually corrodes data into noise; the marginal at each time t corresponds to the data distribution convolved with the appropriate Gaussian. <br></p>

<ul>
  <li>Reversing time yields a <strong>reverse-time SDE</strong> whose drift term depends on the score <strong>∇_x log p_t(x)</strong>, so accurate score estimates across t allow construction of an exact generative reverse-time dynamics. <br>
</li>
  <li>Replacing the true score by <strong>s_theta(x, t)</strong> yields a tractable reverse SDE that can be numerically integrated with standard SDE solvers; discretization recovers annealed sampling procedures. <br>
</li>
  <li>This SDE viewpoint clarifies connections to classical diffusion theory and enables use of advanced numerical techniques (higher-order solvers, predictor-corrector steps) to trade off step count and sample quality. <br>
</li>
</ul>

<hr>

<h1 id="diffusion-models-connect-to-deterministic-odes-and-continuous-normalizing-flows-and-exhibit-strong-empirical-performance-with-memorization-caveats">Diffusion models connect to deterministic ODEs and continuous normalizing flows, and exhibit strong empirical performance with memorization caveats</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs236-2023/frames/lec14/01-18-24-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs236-2023/frames/lec14/01-18-24-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs236-2023/frames/lec14/01-18-24-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs236-2023/frames/lec14/01-18-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Under particular constructions the stochastic forward process admits an equivalent deterministic <strong>ODE</strong> that matches the forward marginals; integrating the ODE backwards yields a continuous-time <strong>normalizing flow</strong> that is invertible and maps noise to data with a tractable Jacobian flow. <br></p>

<ul>
  <li>This connection means diffusion models can be interpreted as very deep invertible flows trained with score-based losses rather than maximum likelihood, which also enables computation of likelihoods in certain formulations. <br>
</li>
  <li>Empirically, diffusion models achieve <strong>state-of-the-art image synthesis quality</strong>, are more stable to train than adversarial models, and scale well with compute at inference time—though they require many model evaluations to sample. <br>
</li>
  <li>Practical concerns include rare <strong>memorization</strong> of training images (detectable with nearest-neighbor checks and loss monitoring) and frequent failure modes (e.g., hands/fingers) that improve with more data and model capacity. <br>
</li>
</ul>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
