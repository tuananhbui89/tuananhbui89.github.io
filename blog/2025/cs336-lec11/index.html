<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS336 Lecture 11 - Scaling laws | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary of CS336 Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/cs336-lec11/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">CS336 Lecture 11 - Scaling laws</h1>
    <p class="post-meta">December 8, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#this-lecture-presents-case-studies-of-scaling-laws-in-modern-large-language-model-development-and-a-mathematical-deep-dive-into-maximum-update-parameterization-mup">This lecture presents case studies of scaling laws in modern large language model development and a mathematical deep dive into Maximum Update Parameterization (MUP).</a></li>
<li class="toc-entry toc-h1"><a href="#cerebras-gpt-applies-maximum-update-parametrization-mup-to-stabilize-hyperparameter-behavior-and-improve-predictability-of-scaling-curves">Cerebras-GPT applies Maximum Update Parametrization (MUP) to stabilize hyperparameter behavior and improve predictability of scaling curves.</a></li>
<li class="toc-entry toc-h1"><a href="#minicpm-combines-mup-with-a-warmup-stable-decay-wsd-learning-rate-schedule-to-enable-single-run-chin-chilla-style-data-scaling-experiments-and-efficient-proxy-model-hyperparameter-selection">MiniCPM combines MUP with a Warmup-Stable-Decay (WSD) learning-rate schedule to enable single-run Chin chilla-style data-scaling experiments and efficient proxy-model hyperparameter selection.</a></li>
<li class="toc-entry toc-h1"><a href="#deepseek-performs-explicit-grid-searches-for-optimal-batch-size-and-learning-rate-across-compute-scales-fits-scaling-laws-to-these-optima-and-extrapolates-performance-to-large-target-models-using-wsd-schedules-and-isoflops-analysis">DeepSeek performs explicit grid searches for optimal batch size and learning rate across compute scales, fits scaling laws to these optima, and extrapolates performance to large target models using WSD schedules and isoflops analysis.</a></li>
<li class="toc-entry toc-h1"><a href="#recent-releases-llama-3-hunuan-1-minimax-01-reapply-isoflopschinchilla-style-analyses-and-report-varying-optimal-token-to-parameter-ratios-and-architectural-trade-offs-for-linearhybrid-attentions">Recent releases (Llama 3, Hunuan 1, Minimax-01) reapply isoflops/Chinchilla-style analyses and report varying optimal token-to-parameter ratios and architectural trade-offs for linear/hybrid attentions.</a></li>
<li class="toc-entry toc-h1"><a href="#common-practical-scaling-ingredients-are-stable-parameterizations-eg-mup-wsd-schedules-for-efficient-data-scaling-isoflopschinchilla-analyses-and-fixed-aspect-ratio-scaling-of-model-dimensions">Common practical scaling ingredients are stable parameterizations (e.g., MUP), WSD schedules for efficient data-scaling, isoflops/Chinchilla analyses, and fixed aspect-ratio scaling of model dimensions.</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/OSYuUqGBQxw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="this-lecture-presents-case-studies-of-scaling-laws-in-modern-large-language-model-development-and-a-mathematical-deep-dive-into-maximum-update-parameterization-mup">This lecture presents case studies of scaling laws in modern large language model development and a mathematical deep dive into Maximum Update Parameterization (MUP).</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-02-36-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-02-36-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-02-36-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-02-36.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The lecture frames two primary goals: to examine detailed case studies of how contemporary LLM builders apply <strong>scaling laws</strong> in practice, and to present a mathematical treatment of <strong>Maximum Update Parametrization (MUP)</strong> that makes hyperparameters <strong>scale-invariant</strong>.<br></p>

<p>It highlights common practitioner concerns about scaling-law use, including whether <strong>log–log curve fits</strong> reliably predict optimal <strong>token/model trade-offs</strong> and hyperparameters such as <strong>learning rate</strong>.<br></p>

<p>Motivation and approach:<br></p>
<ul>
  <li>Study both published <strong>scaling-law</strong> analyses and less-documented industrial practices to derive best practices for <strong>model sizing</strong>, <strong>data budgets</strong>, and <strong>stable training parameterizations</strong>.<br>
</li>
  <li>Use case studies (Cerebras-GPT, MiniCPM, DeepSeek) plus a formal derivation and empirical evaluation of <strong>MUP</strong> to link theory and practice.<br>
</li>
</ul>

<p>The section sets expectations for subsequent material: practical case studies, concrete recipes, and a mathematical foundation for <strong>MUP</strong> with empirical validation.<br></p>

<hr>

<h1 id="cerebras-gpt-applies-maximum-update-parametrization-mup-to-stabilize-hyperparameter-behavior-and-improve-predictability-of-scaling-curves">Cerebras-GPT applies Maximum Update Parametrization (MUP) to stabilize hyperparameter behavior and improve predictability of scaling curves.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-08-04-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-08-04-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-08-04-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-08-04.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Cerebras-GPT</strong> is a family of models spanning ~<strong>0.1B to 13B</strong> parameters trained with <strong>Chinchilla-style compute/data ratios</strong> and using <strong>MUP</strong> to reduce sensitivity of optimal learning rates across scale.<br></p>

<p>Key practical points:<br></p>
<ul>
  <li>
<strong>MUP initialization:</strong> nearly all non-embedding parameters scaled proportional to <strong>1 / layer width</strong>.<br>
</li>
  <li>
<strong>Per-layer LR scaling:</strong> apply learning-rate multipliers inversely proportional to width, producing much smoother adherence to predicted <strong>scaling-law</strong> fits.<br>
</li>
  <li>
<strong>Validation strategy:</strong> perform extensive small-proxy hyperparameter sweeps (down to <strong>40M</strong> parameter models), then scale those hyperparameters upward under <strong>MUP</strong>, showing reduced oscillation around predicted scaling points compared with standard parameterization.<br>
</li>
  <li>
<strong>Reproducibility artifacts:</strong> detailed appendix tables enumerate initialization differences and per-layer learning-rate multipliers between standard parameterization and <strong>MUP</strong>, enabling straightforward implementation.<br>
</li>
</ul>

<hr>

<h1 id="minicpm-combines-mup-with-a-warmup-stable-decay-wsd-learning-rate-schedule-to-enable-single-run-chin-chilla-style-data-scaling-experiments-and-efficient-proxy-model-hyperparameter-selection">MiniCPM combines MUP with a Warmup-Stable-Decay (WSD) learning-rate schedule to enable single-run Chin chilla-style data-scaling experiments and efficient proxy-model hyperparameter selection.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-21-16-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-21-16-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-21-16-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-21-16.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>MiniCPM</strong> aims to use substantial compute to produce highly optimized <strong>small models</strong>, leveraging <strong>MUP</strong> to keep optimal hyperparameters stable when scaling width and using a <strong>WSD (Warmup–Stable–Decay)</strong> learning-rate schedule to make data-scaling reproducible from single long runs.<br></p>

<p>WSD schedule (three phases):<br></p>
<ol>
  <li>
<strong>Short warmup</strong> to ramp to the target LR.<br>
</li>
  <li>
<strong>Long stable plateau</strong> at the target LR to concentrate most of the token budget.<br>
</li>
  <li>
<strong>Rapid decay</strong> to a small termination LR to finish and stabilize weights.<br>
</li>
</ol>

<p>Practical features and analyses:<br></p>
<ul>
  <li>Checkpoint <strong>rewinding</strong> and selective decay-phase application allow emulating training to different token counts without retraining from scratch.<br>
</li>
  <li>Measure <strong>critical batch size scaling</strong> versus <strong>terminal loss</strong> (an isoflops-style analysis) to choose batch sizes.<br>
</li>
  <li>Use two fitting methods for <strong>Chinchilla-style</strong> token-to-parameter trade-offs:
    <ul>
      <li>Take the <strong>lower envelope</strong> of training curves.<br>
</li>
      <li>Jointly fit a <strong>two-variable power-law</strong> scaling law.<br>
</li>
    </ul>
  </li>
  <li>Report unusually high token-to-parameter ratios compared to original Chinchilla estimates, while cautioning that these ratios are sensitive to <strong>architecture</strong> and <strong>data quality</strong>, illustrating both the power and fragility of curve-fitting approaches.<br>
</li>
</ul>

<hr>

<h1 id="deepseek-performs-explicit-grid-searches-for-optimal-batch-size-and-learning-rate-across-compute-scales-fits-scaling-laws-to-these-optima-and-extrapolates-performance-to-large-target-models-using-wsd-schedules-and-isoflops-analysis">DeepSeek performs explicit grid searches for optimal batch size and learning rate across compute scales, fits scaling laws to these optima, and extrapolates performance to large target models using WSD schedules and isoflops analysis.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-35-47-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-35-47-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-35-47-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-35-47.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>DeepSeek</strong> obtains optimal hyperparameter estimates by running grids of <strong>batch-size</strong> and <strong>learning-rate</strong> sweeps at multiple compute scales, fitting scaling relationships to the optima, and extrapolating those fits to set hyperparameters for <strong>7B</strong> and <strong>67B</strong> models.<br></p>

<p>Practical methodology:<br></p>
<ul>
  <li>Adopt <strong>WSD-style</strong> schedules with multiple decay phases to permit cheap Chinchilla-style experiments and concentrate compute on the effective cooldown that yields most terminal-loss improvements.<br>
</li>
  <li>Replicate <strong>isoflops/Chinchilla</strong> analyses (fit quadratics to training curves, take the lower envelope) and demonstrate that <strong>extrapolated scaling laws</strong> can accurately predict held-out larger-model performance when experiments are well executed.<br>
Caveats:<br>
</li>
  <li>
<strong>Hyperparameter-scaling relations</strong> are noisier than isoflops trends, so careful experimental design and validation are required when extrapolating <strong>learning-rate</strong> and <strong>batch-size</strong> schedules.<br>
</li>
</ul>

<hr>

<h1 id="recent-releases-llama-3-hunuan-1-minimax-01-reapply-isoflopschinchilla-style-analyses-and-report-varying-optimal-token-to-parameter-ratios-and-architectural-trade-offs-for-linearhybrid-attentions">Recent releases (Llama 3, Hunuan 1, Minimax-01) reapply isoflops/Chinchilla-style analyses and report varying optimal token-to-parameter ratios and architectural trade-offs for linear/hybrid attentions.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-43-18-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-43-18-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-43-18-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-43-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Several recent releases revisit <strong>Chinchilla-style isoflops</strong> fits and demonstrate variability in optimal token-to-parameter ratios across projects:<br></p>

<ul>
  <li>
<strong>Llama 3</strong>: redoes isoflops fits and reports an optimal token-to-parameter ratio around <strong>~39:1</strong>, higher than the original <strong>~20:1</strong> Chinchilla heuristic—possible causes include architecture improvements and better data quality.<br>
</li>
  <li>
<strong>Hunuan 1</strong>: reports even larger ratios (e.g., <strong>~96:1</strong>) under its training regime, showing strong dependence on architecture, data, and methodology.<br>
</li>
  <li>
<strong>Minimax-01</strong>: investigates <strong>linear (lightning) attention</strong> and hybrid architectures and finds that under isoflops analysis, linear/hybrid attention families can achieve <strong>similar scaling performance</strong> to full softmax attention, supporting their use for long-context, linear-time models.<br>
</li>
</ul>

<p>Across these releases, <strong>isoflops-style quadratic fits</strong> and <strong>lower-envelope</strong> methods remain reliable tools for inferring compute-to-token/model trade-offs, even though absolute ratio estimates vary by project.<br></p>

<hr>

<h1 id="common-practical-scaling-ingredients-are-stable-parameterizations-eg-mup-wsd-schedules-for-efficient-data-scaling-isoflopschinchilla-analyses-and-fixed-aspect-ratio-scaling-of-model-dimensions">Common practical scaling ingredients are stable parameterizations (e.g., MUP), WSD schedules for efficient data-scaling, isoflops/Chinchilla analyses, and fixed aspect-ratio scaling of model dimensions.</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs336-2025/frames/lec11/00-48-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs336-2025/frames/lec11/00-48-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs336-2025/frames/lec11/00-48-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs336-2025/frames/lec11/00-48-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Production-grade scaling workflows typically combine several reproducible elements to reduce expensive per-scale hyperparameter searches and enable controlled extrapolation from small proxies:<br></p>

<ul>
  <li>A <strong>stable parameterization or initialization</strong> (e.g., <strong>MUP</strong>) to reduce hyperparameter drift across width or depth.<br>
</li>
  <li>An <strong>LR schedule</strong> such as <strong>Warmup–Stable–Decay (WSD)</strong> to permit checkpoint-based data-scaling and reuse.<br>
</li>
  <li>
<strong>Isoflops/Chinchilla analyses</strong> to set token/model trade-offs and estimate critical batch sizes.<br>
</li>
  <li>A <strong>fixed aspect-ratio</strong> approach when increasing total parameter counts to keep architecture proportions consistent.<br>
</li>
</ul>

<p>These elements enable small-proxy sweeps and controlled extrapolation to target compute budgets, reducing overall experimental cost. The combination is not universal but recurs across prominent open studies and high-performance model recipes.<br></p>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cs336-lec09/">CS336 Lecture 9 - Scaling laws</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">The Foundations and Frontiers of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cs336-lec01/">CS336 Lecture 1 - Overview and Tokenization</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cs336-lec07/">CS336 Lecture 7 - Parallelism</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
