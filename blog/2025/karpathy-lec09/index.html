<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Karpathy Series - How I use LLMs | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="AI Summary Lecture">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2025/karpathy-lec09/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Karpathy Series - How I use LLMs</h1>
    <p class="post-meta">December 15, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/tutorial">
          <i class="fas fa-hashtag fa-sm"></i> tutorial</a>  
          <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#introduction-and-video-objectives">Introduction and video objectives</a></li>
<li class="toc-entry toc-h1"><a href="#chatgpt-and-the-expanding-llm-ecosystem">ChatGPT and the expanding LLM ecosystem</a></li>
<li class="toc-entry toc-h1"><a href="#where-to-discover-and-compare-llms">Where to discover and compare LLMs</a></li>
<li class="toc-entry toc-h1"><a href="#basic-interaction-model-and-tokenization">Basic interaction model and tokenization</a></li>
<li class="toc-entry toc-h1"><a href="#conversation-format-and-context-window-behavior">Conversation format and context window behavior</a></li>
<li class="toc-entry toc-h1"><a href="#pre-training-and-post-training-as-sources-of-model-behavior">Pre-training and post-training as sources of model behavior</a></li>
<li class="toc-entry toc-h1"><a href="#how-to-conceptualize-chatgpts-identity-and-limits">How to conceptualize ChatGPT’s identity and limits</a></li>
<li class="toc-entry toc-h1"><a href="#practical-knowledge-queries-and-verification">Practical knowledge queries and verification</a></li>
<li class="toc-entry toc-h1"><a href="#context-management-start-new-chats-and-token-costs">Context management: start new chats and token costs</a></li>
<li class="toc-entry toc-h1"><a href="#model-selection-and-subscription-tiers">Model selection and subscription tiers</a></li>
<li class="toc-entry toc-h1"><a href="#using-multiple-providers-and-an-llm-council">Using multiple providers and an ‘LLM council’</a></li>
<li class="toc-entry toc-h1"><a href="#thinking-models-and-reinforcement-learning-tuning">Thinking models and reinforcement learning tuning</a></li>
<li class="toc-entry toc-h1"><a href="#tool-use-concept-integrating-internet-search">Tool use concept: integrating internet search</a></li>
<li class="toc-entry toc-h1"><a href="#search-integration-across-providers-and-behavior">Search integration across providers and behavior</a></li>
<li class="toc-entry toc-h1"><a href="#practical-search-use-cases-and-examples">Practical search use cases and examples</a></li>
<li class="toc-entry toc-h1"><a href="#deep-research-long-form-citation-rich-automated-research">Deep research: long-form, citation-rich automated research</a></li>
<li class="toc-entry toc-h1"><a href="#deep-research-outputs-caveats-and-examples">Deep research outputs, caveats, and examples</a></li>
<li class="toc-entry toc-h1"><a href="#uploading-documents-and-adding-specific-sources-to-context">Uploading documents and adding specific sources to context</a></li>
<li class="toc-entry toc-h1"><a href="#reading-and-studying-books-or-papers-with-an-llm">Reading and studying books or papers with an LLM</a></li>
<li class="toc-entry toc-h1"><a href="#python-interpreter-and-programmatic-tool-use">Python interpreter and programmatic tool use</a></li>
<li class="toc-entry toc-h1"><a href="#advanced-data-analysis-and-plotting-with-llms">Advanced Data Analysis and plotting with LLMs</a></li>
<li class="toc-entry toc-h1"><a href="#cloud-artifacts-llm-generated-interactive-apps-and-diagrams">Cloud artifacts: LLM-generated interactive apps and diagrams</a></li>
<li class="toc-entry toc-h1"><a href="#code-centric-development-tools-and-vibe-coding">Code-centric development tools and ‘vibe coding’</a></li>
<li class="toc-entry toc-h1"><a href="#multimodality-speech-inputoutput-and-advanced-voice">Multimodality: speech input/output and advanced voice</a></li>
<li class="toc-entry toc-h1"><a href="#notebooklm-and-on-demand-podcast-generation">NotebookLM and on-demand podcast generation</a></li>
<li class="toc-entry toc-h1"><a href="#image-modalities-ocr-image-understanding-and-generation">Image modalities: OCR, image understanding, and generation</a></li>
<li class="toc-entry toc-h1"><a href="#video-understanding-via-mobile-camera-and-demo-use-cases">Video understanding via mobile camera and demo use cases</a></li>
<li class="toc-entry toc-h1"><a href="#ai-video-generation-landscape">AI video generation landscape</a></li>
<li class="toc-entry toc-h1"><a href="#memory-features-for-personalized-experiences">Memory features for personalized experiences</a></li>
<li class="toc-entry toc-h1"><a href="#custom-instructions-and-persistent-persona-tuning">Custom instructions and persistent persona tuning</a></li>
<li class="toc-entry toc-h1"><a href="#custom-gpts-and-saved-prompts-for-repeatable-tasks">Custom GPTs and saved prompts for repeatable tasks</a></li>
<li class="toc-entry toc-h1"><a href="#summary-practical-takeaways-and-system-level-considerations">Summary: practical takeaways and system-level considerations</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <div style="display: flex; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/EWvNQjAaOHw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
    </iframe>
</div>

<h1 id="introduction-and-video-objectives">Introduction and video objectives</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-00-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-00-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-00-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-00-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>The presenter introduces a <strong>practical guide to using large language models (LLMs)</strong>, with the explicit objective to demonstrate <strong>settings, examples, and workflows</strong> for both everyday and professional use.<br></p>

<p>This session is framed as a continuation of an earlier <strong>foundational lecture</strong>, but it deliberately focuses on <strong>applied techniques</strong> rather than training theory.<br></p>

<p>What the segment promises to cover:</p>
<ul>
  <li>A survey of <strong>multiple LLM providers and interfaces</strong>.</li>
  <li>Live demonstrations of <strong>how the presenter personally uses these tools</strong>.</li>
  <li>Concrete <strong>demos and configuration tips</strong> viewers can replicate.<br>
</li>
</ul>

<p>The introduction therefore establishes scope and motivates viewers to follow the practical demonstrations for actionable guidance.<br></p>

<hr>

<h1 id="chatgpt-and-the-expanding-llm-ecosystem">ChatGPT and the expanding LLM ecosystem</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-01-20-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-01-20-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-01-20-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-01-20.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>ChatGPT</strong> is presented as the seminal <strong>text-chat interface</strong> that popularized public interaction with LLMs and remains feature-rich thanks to longevity.<br></p>

<p>At the same time, the ecosystem now includes many competing offerings from <strong>Big Tech and startups</strong>. The speaker names prominent alternatives:</p>
<ul>
  <li>
<strong>Gemini</strong>, <strong>Claude</strong>, <strong>Grok</strong>, <strong>Mistral</strong>, <strong>DeepSeek</strong>, and <strong>Llama-like services</strong>.<br>
</li>
</ul>

<p>Key point: different vendors expose <strong>distinct features and user experiences</strong>. The speaker recommends:</p>
<ul>
  <li>Track model progress with <strong>leaderboards and evaluation dashboards</strong> to monitor comparative capabilities and performance.</li>
  <li>Treat <strong>ChatGPT</strong> as a reasonable representative starting point, but be open to <strong>specialized providers</strong> when a particular feature or capability is required.<br>
</li>
</ul>

<hr>

<h1 id="where-to-discover-and-compare-llms">Where to discover and compare LLMs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-02-28-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-02-28-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-02-28-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-02-28.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Multiple public resources aggregate <strong>model performance and features</strong>, enabling side-by-side comparisons across providers and tasks.<br></p>

<p>Recommended comparators:</p>
<ul>
  <li><strong>Chatbot Arena</strong></li>
  <li>
<strong>Scale AI’s evaluation pages</strong><br>
</li>
</ul>

<p>What these leaderboards provide:</p>
<ul>
  <li>
<strong>Relative strengths</strong>, ELO-like scores, and <strong>task-specific benchmarks</strong> (reasoning, coding, general knowledge).</li>
  <li>Up-to-date evaluations that complement <strong>vendor documentation and product pages</strong>.<br>
</li>
</ul>

<p>Why use them: vendor marketing alone rarely reveals <strong>empirical differences</strong>, so external comparators help make informed model-selection decisions.<br></p>

<hr>

<h1 id="basic-interaction-model-and-tokenization">Basic interaction model and tokenization</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-04-43-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-04-43-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-04-43-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-04-43.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A language-model interaction is fundamentally <strong>text-in, text-out</strong>; under the UI this maps to a <strong>linear token sequence</strong>.<br></p>

<p>Demonstrated with a tokenizer tool:</p>
<ul>
  <li>Inputs and responses are split into <strong>tokens</strong> from a fixed vocabulary (hundreds of thousands of tokens).</li>
  <li>Tokens are represented by <strong>token IDs</strong>, the atomic units the model processes.<br>
</li>
</ul>

<p>Why tokenization matters:</p>
<ul>
  <li>
<strong>Prompts, responses, and conversational context</strong> all occupy the same one-dimensional token stream.</li>
  <li>Tokens count against <strong>context/window limits</strong>, so every chat bubble contributes to that single sequence the model continues.<br>
</li>
</ul>

<hr>

<h1 id="conversation-format-and-context-window-behavior">Conversation format and context window behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-07-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-07-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-07-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-07-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Conversational UIs</strong> wrap user messages and model replies into a structured chat format that is serialized into the token stream, including <strong>role markers</strong> and <strong>message boundaries</strong>.<br></p>

<p>Important operational details:</p>
<ul>
  <li>The <strong>context window</strong> (working memory) contains all tokens visible to the model.</li>
  <li>
<strong>New-chat</strong> resets the context window to zero tokens — effectively starting a fresh working memory.</li>
  <li>The model and user alternate by appending tokens until the model emits a <strong>termination token</strong>, at which point control returns to the user.</li>
  <li>Each turn builds the <strong>shared context</strong>; managing that context window is equivalent to managing the model’s working memory and directly impacts what the model can reference and reason about.<br>
</li>
</ul>

<hr>

<h1 id="pre-training-and-post-training-as-sources-of-model-behavior">Pre-training and post-training as sources of model behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-09-40-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-09-40-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-09-40-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-09-40.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>LLMs acquire general knowledge through a costly <strong>pre-training</strong> stage and then adopt an assistant persona through <strong>post-training</strong> on curated conversational datasets.<br></p>

<p>Clarifying the stages:</p>
<ol>
  <li>
<strong>Pre-training</strong>: compresses large corpora into network parameters — a lossy, probabilistic compressed representation of the internet inside the model. This yields a <strong>large but stale knowledge base</strong> with a clear <strong>knowledge cutoff date</strong>.</li>
  <li>
<strong>Post-training</strong>: includes <strong>supervised fine-tuning on dialogues</strong> and <strong>reinforcement learning from human feedback (RLHF)</strong>, which modify model behavior toward an <strong>assistant style</strong>.<br>
</li>
</ol>

<p>Why this distinction matters: it explains how models can be <strong>knowledgeable yet out-of-date</strong>, and why <strong>persona/response style</strong> is separate from stored knowledge.<br></p>

<hr>

<h1 id="how-to-conceptualize-chatgpts-identity-and-limits">How to conceptualize ChatGPT’s identity and limits</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-12-17-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-12-17-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-12-17-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-12-17.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Think of <strong>ChatGPT</strong> as a large, self-contained artifact: its knowledge is a <strong>compressed snapshot of internet content</strong>, and its conversational behavior is shaped by <strong>human-curated fine-tuning</strong>.<br></p>

<p>Consequences of this framing:</p>
<ul>
  <li>
<strong>Probabilistic recall</strong>: frequently occurring internet items are remembered better.</li>
  <li>
<strong>Vagueness about recent events</strong>: due to the knowledge cutoff.</li>
  <li>
<strong>Style driven by post-training labelers</strong>.<br>
</li>
</ul>

<p>Practical expectations: the model is <strong>fallible</strong>, may <strong>hallucinate</strong>, and cannot access fresh data unless explicitly connected to tools. Treat the base model as a <strong>static knowledge artifact</strong> unless tool integrations are present.<br></p>

<hr>

<h1 id="practical-knowledge-queries-and-verification">Practical knowledge queries and verification</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-14-46-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-14-46-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-14-46-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-14-46.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Simple, common factual queries that are unlikely to have changed recently are good candidates to ask the <strong>base model directly</strong>, but answers should be verified when accuracy matters.<br></p>

<p>Recommended workflow:</p>
<ol>
  <li>
<strong>Model-first</strong>: use the model for quick orientation (e.g., approximate facts like caffeine content in an Americano).</li>
  <li>
<strong>Targeted verification</strong>: cross-check with primary or authoritative sources for moderately important or safety-sensitive matters (e.g., medication ingredients).<br>
</li>
</ol>

<p>Rationale: frequent facts are well represented in pre-training, but critical decisions require <strong>external confirmation</strong>.<br></p>

<hr>

<h1 id="context-management-start-new-chats-and-token-costs">Context management: start new chats and token costs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-17-10-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-17-10-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-17-10-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-17-10.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Start <strong>new chats</strong> whenever switching topics because the context window is a <strong>finite, expensive resource</strong> that affects performance and cost.<br></p>

<p>Operational guidance:</p>
<ul>
  <li>Large accumulated token windows can <strong>distract the model</strong> with irrelevant prior context.</li>
  <li>They also slightly <strong>increase computation cost per token</strong> when sampling new tokens.</li>
  <li>Treat tokens in the context window as <strong>precious working memory</strong>: keep them concise and relevant, and reset the conversation when prior tokens are no longer beneficial.<br>
</li>
</ul>

<p>Benefit: effective <strong>context hygiene</strong> improves response quality and reduces unnecessary expense and latency.<br></p>

<hr>

<h1 id="model-selection-and-subscription-tiers">Model selection and subscription tiers</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-19-03-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-19-03-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-19-03-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-19-03.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Models differ in <strong>size, capability, and pricing tiers</strong>; larger models generally provide better knowledge, creativity, and reliability but at higher compute cost.<br></p>

<p>Typical provider tiers:</p>
<ul>
  <li>
<strong>Free tier</strong>: reduced-capacity models.</li>
  <li>
<strong>Middle tier</strong>: limited access to flagship models.</li>
  <li>
<strong>Professional tier</strong>: priority or unlimited access to the largest variants.<br>
</li>
</ul>

<p>Practical advice:</p>
<ul>
  <li>Be deliberate about <strong>tradeoffs between cost and capability</strong> based on use case (prototyping vs professional work).</li>
  <li>Paid plans can be cost-effective for heavy users (e.g., software engineers).</li>
  <li>
<strong>Always verify which model is actually active</strong> in your session—UIs can be ambiguous about the selected model.<br>
</li>
</ul>

<hr>

<h1 id="using-multiple-providers-and-an-llm-council">Using multiple providers and an ‘LLM council’</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-21-31-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-21-31-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-21-31-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-21-31.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Relying on multiple LLM providers and models — an <strong>“LLM council”</strong> — surfaces diverse perspectives and comparative suggestions for ideation and decision support.<br></p>

<p>How to use an LLM council:</p>
<ul>
  <li>Poll several models to aggregate <strong>complementary strengths</strong> (search integration, deep-research features, reasoning biases).</li>
  <li>Experiment across vendors and <strong>pay selectively</strong> for higher tiers when needed.</li>
  <li>Use the council for <strong>non-critical tasks</strong> (travel planning, ideation) where multiple viewpoints add value and increase confidence.<br>
</li>
</ul>

<hr>

<h1 id="thinking-models-and-reinforcement-learning-tuning">Thinking models and reinforcement learning tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-26-58-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-26-58-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-26-58-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-26-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>A class of models tuned with <strong>reinforcement learning</strong> can develop internal, chain-of-thought-like strategies that materially improve performance on complex problems (math, coding, deep reasoning).<br></p>

<p>Key characteristics:</p>
<ul>
  <li>
<strong>RLHF</strong> helps models iterate and discover problem-solving heuristics difficult to encode with supervision alone.</li>
  <li>
<strong>Thinking models</strong> often take longer to respond (they may emit many intermediate tokens) but can yield higher accuracy on deep tasks.</li>
  <li>They are <strong>less helpful for simple prompts</strong> and should be used selectively for difficult logical, mathematical, or programmatic problems.<br>
</li>
</ul>

<hr>

<h1 id="tool-use-concept-integrating-internet-search">Tool use concept: integrating internet search</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-33-09-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-33-09-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-33-09-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-33-09.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Tool use</strong> extends a closed model by letting it emit a special search (or action) that the host application executes, then inserting retrieved content into the model’s context window.<br></p>

<p>What this enables:</p>
<ul>
  <li>Access to <strong>fresh or niche information</strong> not present in pre-trained parameters.</li>
  <li>The application inserts retrieved page text as tokens into the same context stream so the model can <strong>reference, synthesize, and cite</strong> those sources.<br>
</li>
</ul>

<p>When to use tools: whenever <strong>recency or niche specificity</strong> is required — prefer tool-mediated search over relying solely on frozen model knowledge.<br></p>

<hr>

<h1 id="search-integration-across-providers-and-behavior">Search integration across providers and behavior</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-36-54-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-36-54-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-36-54-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-36-54.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Provider apps integrate search differently: some <strong>auto-detect</strong> the need to search, some expose <strong>explicit search buttons</strong>, and some tiers or models <strong>lack real-time search</strong> entirely.<br></p>

<p>Practical demo observations:</p>
<ul>
  <li>Some models will <strong>automatically switch to search mode</strong> for clearly recent queries.</li>
  <li>Others <strong>require user enablement</strong> of search.</li>
  <li>Availability can vary by <strong>subscription tier</strong>.<br>
</li>
</ul>

<p>Result behavior: when search is available the system returns <strong>citations</strong> for verification; when absent the model either <strong>declines</strong> or answers based on stale knowledge with a <strong>knowledge-cutoff disclaimer</strong>. Choose the right provider or UI control for time-sensitive questions.<br></p>

<hr>

<h1 id="practical-search-use-cases-and-examples">Practical search use cases and examples</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-40-18-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-40-18-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-40-18-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-40-18.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Common, practical searches where tool use is especially helpful include:</p>
<ul>
  <li>Market open days and stock drivers</li>
  <li>Filming locations and cast lists</li>
  <li>Product feature availability and launch rumors</li>
  <li>Recent news context and social-media trends<br>
</li>
</ul>

<p>Why: these queries are <strong>time-sensitive, esoteric, or trending</strong>, and LLM+search pipelines can aggregate top links and produce succinct summaries with citations. For quick situational awareness, use a <strong>search-capable app</strong> (e.g., Perplexity) for concise, cited answers derived from multiple web sources.<br></p>

<hr>

<h1 id="deep-research-long-form-citation-rich-automated-research">Deep research: long-form, citation-rich automated research</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-45-32-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-45-32-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-45-32-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-45-32.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Deep research</strong> combines iterative internet search with extended internal thinking and synthesis to produce long-form, citation-backed reports over many minutes.<br></p>

<p>Typical workflow:</p>
<ol>
  <li>Issue many searches and read primary literature/web sources.</li>
  <li>Synthesize findings into a structured report with proposed mechanisms, safety considerations, and references.</li>
  <li>Return a first-pass literature-review–style draft.<br>
</li>
</ol>

<p>Tradeoffs: deep research is <strong>expensive and time-consuming</strong> but valuable for complex questions that require evidence aggregation. Treat outputs as <strong>first drafts</strong> useful for scoping and citation discovery, and always validate key claims against original papers.<br></p>

<hr>

<h1 id="deep-research-outputs-caveats-and-examples">Deep research outputs, caveats, and examples</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-49-58-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-49-58-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-49-58-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-49-58.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Deep-research outputs include detailed reports and citation lists but remain prone to <strong>hallucination and extraction errors</strong>, so validate by inspecting cited sources.<br></p>

<p>Observations from demos:</p>
<ul>
  <li>
<strong>ChatGPT’s</strong> deep research tends to be thorough and long; other providers often produce briefer reports.</li>
  <li>All systems can misrepresent or omit critical items (major labs, funding details, etc.).<br>
</li>
</ul>

<p>Practical use: compile candidate references and summaries with deep research, then <strong>drill into primary literature</strong> to confirm accuracy and context (examples: supplement analyses, privacy comparisons, longevity studies).<br></p>

<hr>

<h1 id="uploading-documents-and-adding-specific-sources-to-context">Uploading documents and adding specific sources to context</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-52-24-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-52-24-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-52-24-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-52-24.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>LLMs can accept <strong>direct document uploads</strong> (PDFs, web pages, text) which the application converts to text and injects into the context window for precise reasoning over user-provided material.<br></p>

<p>Use cases: reading technical papers, proprietary reports, or any documents where <strong>exact quoting and targeted Q&amp;A</strong> are required.<br></p>

<p>Implementation notes:</p>
<ul>
  <li>Uploaded documents may be preprocessed (images dropped or OCR-converted).</li>
  <li>Converted text becomes part of the working memory for subsequent queries, providing a <strong>controlled alternative to web search</strong> for adding up-to-date, targeted content.<br>
</li>
</ul>

<hr>

<h1 id="reading-and-studying-books-or-papers-with-an-llm">Reading and studying books or papers with an LLM</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/00-56-23-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/00-56-23-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/00-56-23-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/00-56-23.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Using an LLM as a <strong>reading partner</strong> accelerates comprehension by loading chapters or papers into context and asking progressive, clarifying questions.<br></p>

<p>Practical pattern:</p>
<ol>
  <li>Load chapter-sized extracts or papers.</li>
  <li>Request summaries and clarifications.</li>
  <li>Iteratively query the model to unpack dense passages.<br>
</li>
</ol>

<p>Benefits: improves retention and cross-disciplinary comprehension, making older or unfamiliar domain material more accessible. Current tooling can be clunky (manual copy-paste), but the collaborative-reading pattern is powerful and widely applicable.<br></p>

<hr>

<h1 id="python-interpreter-and-programmatic-tool-use">Python interpreter and programmatic tool use</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-01-34-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-01-34-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-01-34-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-01-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Integrating a <strong>programming environment</strong> (e.g., a Python REPL) as a model tool lets the model emit code, request execution, and then consume computed results — dramatically improving correctness for numeric computation, data processing, and debugging.<br></p>

<p>How it works:</p>
<ul>
  <li>The model writes code and the host runs it.</li>
  <li>Execution outputs are injected back into the conversation for the model to reference.<br>
</li>
</ul>

<p>Why this matters: avoids in-head hallucinated calculations and yields exact outputs. Not all providers expose interpreters — models without execution tools may attempt to compute internally and produce plausible but incorrect results. For accuracy, prefer environments with a <strong>trusted interpreter tool</strong>.<br></p>

<hr>

<h1 id="advanced-data-analysis-and-plotting-with-llms">Advanced Data Analysis and plotting with LLMs</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-06-35-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-06-35-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-06-35-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-06-35.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Advanced Data Analysis (ADA)</strong> features combine model tool use and program execution to ingest tabular data, produce runnable analysis code, and render plots and trend extrapolations interactively.<br></p>

<p>What ADA does:</p>
<ul>
  <li>The model writes plotting/statistics code, runs it, and returns figures and numeric extrapolations.</li>
  <li>It behaves like a junior data analyst that writes and executes scripts on demand.<br>
</li>
</ul>

<p>Caveats: users must <strong>scrutinize generated code and assumptions</strong>. The assistant can make implicit substitutions or produce inconsistent summaries. ADA is powerful but requires technical oversight to validate transformations, imputations, and statistical assumptions.<br></p>

<hr>

<h1 id="cloud-artifacts-llm-generated-interactive-apps-and-diagrams">Cloud artifacts: LLM-generated interactive apps and diagrams</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-10-29-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-10-29-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-10-29-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-10-29.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Some LLMs can generate <strong>client-side interactive artifacts</strong>—small web apps or diagrams—by producing front-end code that runs in the browser without a backend.<br></p>

<p>Examples and uses:</p>
<ul>
  <li>Flashcard test apps, interactive quizzes.</li>
  <li>Mermaid conceptual diagrams or simple visualizers.<br>
</li>
</ul>

<p>Characteristics: artifacts are typically <strong>stateless browser applications</strong> with embedded content authored by the model. They’re useful for learning tools, prototyping, and visualization, but should be treated as <strong>convenience prototypes</strong> and validated before relying on them.<br></p>

<hr>

<h1 id="code-centric-development-tools-and-vibe-coding">Code-centric development tools and ‘vibe coding’</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-15-30-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-15-30-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-15-30-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-15-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Specialized developer tools (VS Code extensions, Cursor, GitHub Copilot-like apps) integrate LLMs with local project files so the model can reason over an entire codebase and perform multi-file edits autonomously.<br></p>

<p>Capabilities:</p>
<ul>
  <li>
<strong>Composer</strong> or agent-driven workflows let an LLM execute higher-level commands (create an app, add components, implement features) and commit changes.</li>
  <li>The presenter calls this <strong>“vibe coding”</strong> — delegating flow-level development tasks to the model.<br>
</li>
</ul>

<p>Tradeoffs: these environments accelerate prototyping but require validation, testing, and occasional rollbacks. The workflow blends human oversight with heavy automation to speed development while preserving final control.<br></p>

<hr>

<h1 id="multimodality-speech-inputoutput-and-advanced-voice">Multimodality: speech input/output and advanced voice</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-25-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-25-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-25-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-25-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>LLMs increasingly handle <strong>multiple modalities</strong>: speech, images, and video alongside text.<br></p>

<p>Speech modes:</p>
<ul>
  <li>
<strong>Speech-to-text</strong>: converts audio to text for faster prompting.</li>
  <li>
<strong>True audio</strong>: models that handle audio tokens natively without intermediate transcription, enabling text-to-speech, character or style voices, and fully native conversations.<br>
</li>
</ul>

<p>Benefits and caveats: native audio supports expressive, hands-free workflows and theatrical prompts, but <strong>availability varies by provider and tier</strong>, and safety/content filters may limit behavior. For many users, speech significantly increases convenience and accessibility.<br></p>

<hr>

<h1 id="notebooklm-and-on-demand-podcast-generation">NotebookLM and on-demand podcast generation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-35-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-35-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-35-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-35-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Notebook-style LLM interfaces can ingest curated sources (papers, PDFs, web pages) and generate <strong>long-form audio or podcast-style digests</strong> summarizing the material on demand.<br></p>

<p>Demo features:</p>
<ul>
  <li>Automatic generation of multi-minute narrated episodes.</li>
  <li>Interactive playback modes that allow user interruptions and contextual follow-ups.<br>
</li>
</ul>

<p>Use cases: passive learning during walks or commutes. These generated podcasts provide convenient digests but should be treated as <strong>synthesized summaries</strong>, not definitive analyses.<br></p>

<hr>

<h1 id="image-modalities-ocr-image-understanding-and-generation">Image modalities: OCR, image understanding, and generation</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-43-30-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-43-30-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-43-30-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-43-30.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Images can be tokenized into patches and processed by <strong>multimodal LLMs</strong> to enable OCR, visual question answering (VQA), and image generation from text prompts.<br></p>

<p>Practical workflows:</p>
<ul>
  <li>Upload images to transcribe labels, explain diagrams, extract nutritional or ingredient data.</li>
  <li>Use image-generation models (DALL·E, Ideogram, etc.) for stylized visuals and thumbnails.<br>
</li>
</ul>

<p>As always, extracted or generated content must be <strong>checked for fidelity</strong>, but image-in/image-out opens many practical uses in documentation, design, and data extraction.<br></p>

<hr>

<h1 id="video-understanding-via-mobile-camera-and-demo-use-cases">Video understanding via mobile camera and demo use cases</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-50-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-50-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-50-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-50-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Mobile apps increasingly allow <strong>live camera input</strong> or periodic image sampling so models can analyze physical scenes, recognize objects, read instruments, and answer contextual questions in real time.<br></p>

<p>User experience: point the phone camera, ask follow-ups, and receive grounded observations — useful for on-site assistance and accessibility scenarios. Under the hood providers may sample still frames or run image models over streamed frames; <strong>availability and implementation details vary by app and tier</strong>.<br></p>

<hr>

<h1 id="ai-video-generation-landscape">AI video generation landscape</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-53-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-53-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-53-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-53-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Multiple emerging video-generation models can synthesize short videos from text prompts with rapidly improving quality and stylistic differences across providers.<br></p>

<p>Practical notes:</p>
<ul>
  <li>Compare models for <strong>motion fidelity, rendering style, and artifact profiles</strong>.</li>
  <li>Use cases include marketing clips, concept visualizations, and quick motion prototyping.</li>
  <li>Consider <strong>licensing and style fit</strong> when selecting a provider, since outputs and terms differ across vendors.<br>
</li>
</ul>

<hr>

<h1 id="memory-features-for-personalized-experiences">Memory features for personalized experiences</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-55-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-55-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-55-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-55-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Persistent memory</strong> features let an LLM store and reuse user-specific information across conversations to produce more personalized, consistent responses.<br></p>

<p>How memory works:</p>
<ul>
  <li>Memories are represented as <strong>structured text entries</strong> prepended to future contexts.</li>
  <li>They can be edited or removed through memory-management controls.<br>
</li>
</ul>

<p>Benefits and cautions: memory improves recommendations and reduces repetitive setup, but <strong>privacy and correctness trade-offs</strong> require users to inspect stored memories. Memory implementations are vendor-specific and evolving.<br></p>

<hr>

<h1 id="custom-instructions-and-persistent-persona-tuning">Custom instructions and persistent persona tuning</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/01-57-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/01-57-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/01-57-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/01-57-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Custom instructions</strong> allow users to specify global preferences, tone, and identity details that the model should consider at the start of every conversation.<br></p>

<p>What they enable:</p>
<ul>
  <li>Consistent assistant behavior (verbosity, formality, role assumptions, learning goals) without repeating instructions per prompt.</li>
  <li>Account-level configuration that reduces friction for recurring tasks.<br>
</li>
</ul>

<p>Recommendation: craft instructions carefully to encode user preferences centrally and improve repeated interactions.<br></p>

<hr>

<h1 id="custom-gpts-and-saved-prompts-for-repeatable-tasks">Custom GPTs and saved prompts for repeatable tasks</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/02-02-00-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/02-02-00-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/02-02-00-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/02-02-00.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p><strong>Custom GPTs</strong> encapsulate reusable prompts, few-shot examples, and instruction templates into a named assistant that performs a specific task reliably.<br></p>

<p>Typical uses: language-learning extractors, OCR+translate pipelines, or specialized translators with example-driven formats.<br></p>

<p>Under the hood: mainly <strong>saved prompt engineering</strong> (few-shot exemplars and constraints) that standardizes output format and removes manual copy-paste. Custom GPTs speed up domain-specific automation where consistent output shape and granular control are required.<br></p>

<hr>

<h1 id="summary-practical-takeaways-and-system-level-considerations">Summary: practical takeaways and system-level considerations</h1>

<div class="row mt-3 text-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/karpathy/frames/lec09/02-08-34-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/karpathy/frames/lec09/02-08-34-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/karpathy/frames/lec09/02-08-34-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/karpathy/frames/lec09/02-08-34.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<p>Closing summary and operational principles: LLMs form a <strong>diverse, rapidly evolving ecosystem</strong> where model choice, tool availability, and modalities determine task suitability.<br></p>

<p>Key takeaways:</p>
<ul>
  <li>Treat base models as <strong>compressed knowledge artifacts</strong> (think “zip files”).</li>
  <li>Use <strong>search and tool integrations</strong> for fresh or executable needs.</li>
  <li>Prefer <strong>thinking models</strong> for deep reasoning tasks.</li>
  <li>Apply modality features (audio, image, video) to match natural user workflows.<br>
</li>
</ul>

<p>Safety and workflow rules:</p>
<ul>
  <li>Balance automation with <strong>human oversight</strong>.</li>
  <li>Validate critical outputs, inspect code and data manipulations, and carefully manage <strong>context, memory, and privacy</strong>.</li>
  <li>Experiment across providers to compose an effective personal <strong>LLM toolkit</strong>.</li>
</ul>

<hr>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
