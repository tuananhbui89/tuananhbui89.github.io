<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Random Thoughts and Notes | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2024/thoughts/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Random Thoughts and Notes</h1>
    <p class="post-meta">November 18, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#laion-5b-and-the-pain-it-might-cause-to-text-to-image-generative-models">LAION-5B and the Pain It Might Cause to Text-to-Image Generative Models</a></li>
<li class="toc-entry toc-h2"><a href="#some-thoughts-on-recent-a-definition-of-agi-paper">Some thoughts on recent “A definition of AGI” paper</a></li>
<li class="toc-entry toc-h2"><a href="#some-unfinished-ideas-on-machine-unlearning">Some unfinished ideas on Machine Unlearning</a></li>
<li class="toc-entry toc-h2"><a href="#safeai-startups">SafeAI Startups</a></li>
<li class="toc-entry toc-h2"><a href="#robust-unlearning---the-next-challenge-in-unlearning">Robust Unlearning - The next challenge in Unlearning</a></li>
<li class="toc-entry toc-h2"><a href="#hypothesis--fine-tuning-with-few-examples-in-transformer-based-architectures-over-attention-vs-overfitting">Hypothesis – Fine-Tuning with Few Examples in Transformer-Based Architectures: Over-Attention vs Overfitting</a></li>
<li class="toc-entry toc-h2"><a href="#hypothesis---shortcut-learning-in-finetuning-diffusion-models-because-of-the-fixed-position">Hypothesis - Shortcut learning in finetuning Diffusion Models because of the fixed position</a></li>
<li class="toc-entry toc-h2"><a href="#test-time-adjustment---bigger-picture">Test-Time Adjustment - Bigger Picture</a></li>
<li class="toc-entry toc-h2"><a href="#pho-restaurants-and-the-way-we-do-research">Pho Restaurants and the Way We Do Research</a></li>
<li class="toc-entry toc-h2"><a href="#thoughts-on-the-talk-of-anh-nguy%E1%BB%85n-th%C3%A0nh-nam">Thoughts on the talk of anh Nguyễn Thành Nam</a></li>
<li class="toc-entry toc-h2"><a href="#visual-autoregressive-and-the-importance-of-position-embedding">Visual Autoregressive and the importance of Position Embedding</a></li>
<li class="toc-entry toc-h2"><a href="#deepseek-r1">DeepSeek-R1</a></li>
<li class="toc-entry toc-h2"><a href="#the-unintentional-invention-of-concept-graph-from-machine-unlearning">The Unintentional Invention of Concept Graph from Machine Unlearning</a></li>
<li class="toc-entry toc-h2"><a href="#openai-email-archives-from-musk-v-altman---game-of-thrones">OpenAI email archives from Musk v Altman - Game of Thrones</a></li>
<li class="toc-entry toc-h2"><a href="#improving-chatgpts-interpretability-with-cross-modal-heatmap">Improving ChatGPT’s interpretability with cross-modal heatmap</a></li>
<li class="toc-entry toc-h2"><a href="#the-prisoners-dilemma">The Prisoner’s Dilemma</a></li>
<li class="toc-entry toc-h2"><a href="#a-new-perspective-on-the-motivation-of-vae">A new perspective on the motivation of VAE</a></li>
<li class="toc-entry toc-h2"><a href="#data-free-knowledge-distillation">Data-Free Knowledge Distillation</a></li>
<li class="toc-entry toc-h2"><a href="#how-to-disable-nsfw-detection-in-huggingface">How to disable NSFW detection in Huggingface</a></li>
<li class="toc-entry toc-h2"><a href="#helmholtz-visiting-researcher-grant">Helmholtz Visiting Researcher Grant</a></li>
<li class="toc-entry toc-h2"><a href="#where-to-find-potential-collaborators-or-postdoc-positions">Where to find potential collaborators or postdoc positions</a></li>
<li class="toc-entry toc-h2"><a href="#micromouse-competition">Micromouse Competition</a></li>
<li class="toc-entry toc-h2"><a href="#trustworthy-machine-learning">Trustworthy Machine Learning</a></li>
<li class="toc-entry toc-h2"><a href="#safety-checker---the-simple-and-efficient-way-to-deal-with-nsfw-content">Safety Checker - The simple and efficient way to deal with NSFW content</a></li>
<li class="toc-entry toc-h2"><a href="#football-minimap-prediction">Football Minimap Prediction</a></li>
<li class="toc-entry toc-h2"><a href="#some-other-silly-ideas">Some other silly ideas</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <!-- mkdir -p assets/img/thoughts/ -->
<!-- mv _posts/2025-03-18*.png assets/img/thoughts/ -->

<p>This blog post is a bit long, rambling, and all over the place — basically my personal notebook for random thoughts and reflections on whatever I come across in work and life.
Since the main purpose is simply to record things for myself (and because I rarely have the time to write carefully), many parts may lack full context or background.
So, if you happen to stumble upon this post without knowing what led to these thoughts, there’s a good chance you won’t fully get what I’m talking about — and I apologize for that in advance :D.</p>

<p>That said, by sharing these notes publicly, I still hope they might spark a small idea or two, or at the very least, offer a few minutes of entertainment end-of-day reading.</p>

<h2 id="laion-5b-and-the-pain-it-might-cause-to-text-to-image-generative-models">LAION-5B and the Pain It Might Cause to Text-to-Image Generative Models</h2>

<p>LAION-5B is a massive dataset of around 5 billion image–text pairs, widely used for training text-to-image generative models such as Stable Diffusion. However, when looking into how this dataset was created, I began to form a hypothesis: while its scale enables impressive generalization, it may also introduce fundamental issues that affect how these models learn fine-grained visual–textual relationships.</p>

<p><strong>How is LAION (or similar large-scale image–text pair datasets) created?</strong></p>

<p>The LAION datasets are built from the Common Crawl web corpus, a gigantic public snapshot of the internet.
Here’s the general process:</p>

<ul>
  <li>Each webpage is scanned for image URLs, alt-texts, and nearby captions or paragraphs.</li>
  <li>The text near an image is treated as that image’s caption.</li>
  <li>These image–caption pairs are filtered and embedded using OpenAI’s CLIP model, which measures how well the text matches the image via cosine similarity.</li>
  <li>Only pairs with high CLIP similarity are kept.</li>
</ul>

<p>This approach allows the creation of billions of training pairs with minimal manual labeling — but it comes with trade-offs.</p>

<p><strong>The Problem</strong></p>

<p>While CLIP-based filtering ensures that the image and text are globally related, the local alignment between individual words and visual regions is often weak or missing.</p>

<p>For example, a caption might correctly describe the overall image (“a man standing near a red car”) but not specify which region corresponds to “man” or “car”. Even worse, captions are often noisy, incomplete, or ambiguous.</p>

<p>This weak alignment can lead to semantic confusion during model training.
As a result, even though models like Stable Diffusion can generate stunning and varied images, they still struggle with local or compositional consistency — how specific attributes combine in one object.
A well-known issue in T2I generation is semantic leakage.
For example, a prompt like “a man with red hair wearing a white shirt” might result in an image of a man with “white hair” and “red shirt”.
From a global CLIP perspective, this image still scores highly because all key concepts (“man,” “red,” “white,” “hair,” “shirt”) appear somewhere — but the relationships among them are wrong.</p>

<p>This explains why prompt engineering has become such a common practice. Users often craft detailed and repetitive prompts (“a man, red hair, white shirt, front view, portrait lighting”) to explicitly guide the model toward the intended local concept composition.</p>

<p><strong>Ideas for Improvement</strong></p>

<p>To address these weaknesses, two possible directions come to mind:</p>

<ul>
  <li>
<strong>Local Alignment Metric</strong>: Instead of relying solely on a global CLIP similarity score, a new local alignment metric could evaluate how well each concept in the caption corresponds to a specific region in the image. This would allow us to select training pairs with stronger fine-grained alignment.</li>
  <li>
<strong>Localizing Mechanism</strong>: Another approach is to design models or datasets with explicit image–text localization — linking particular image regions to caption segments (Image–Text links). Alternatively, a self-localizing mechanism could ensure that related concepts within the text (e.g., “white hair” and “red shirt”) stay grouped together during training (Text–Text links).</li>
</ul>

<p>Both ideas aim to move beyond coarse, global matching and toward datasets that teach generative models how concepts fit together spatially and semantically.</p>

<h2 id="some-thoughts-on-recent-a-definition-of-agi-paper">Some thoughts on recent “A definition of AGI” paper</h2>

<p>The paper is <a href="https://www.agidefinition.ai/paper.pdf" rel="external nofollow noopener" target="_blank">A definition of AGI</a> by a large group of researchers from different institutions, leaded by Dan Hendrycks.</p>

<p>It proposes a set of core cognitive metrics to evaluate AGI capabilities:</p>

<ul>
  <li>**General Knowledge (K): The breadth of factual understanding of the world, encompassing commonsense, culture, science, social science, and history.</li>
  <li>
<strong>Reading and Writing Ability (RW)</strong>: Proficiency in consuming and producing written language, from basic decoding to complex comprehension, composition, and usage.</li>
  <li>
<strong>Mathematical Ability (M)</strong>: The depth of mathematical knowledge and skills across arithmetic, algebra, geometry, probability, and calculus.</li>
  <li>
<strong>On-the-Spot Reasoning (R)</strong>: The flexible control of attention to solve novel problems without relying exclusively on previously learned schemas, tested via deduction and induction.</li>
  <li>
<strong>Working Memory (WM)</strong>: The ability to maintain and manipulate information in active attention across textual, auditory, and visual modalities.</li>
  <li>
<strong>Long-Term Memory Storage (MS)</strong>: The capability to continually learn new information (associative, meaningful, and verbatim).</li>
  <li>
<strong>Long-Term Memory Retrieval (MR)</strong>: The fluency and precision of accessing stored knowledge, including the critical ability to avoid confabulation (hallucinations).</li>
  <li>
<strong>Visual Processing (V)</strong>: The ability to perceive, analyze, reason about, generate, and scan visual information.</li>
  <li>
<strong>Auditory Processing (A)</strong>: The capacity to discriminate, recognize, and work creatively with auditory stimuli, including speech, rhythm, and music.</li>
  <li>
<strong>Speed (S)</strong>: The ability to perform simple cognitive tasks quickly, encompassing perceptual speed, reaction times, and processing fluency.</li>
</ul>

<p><strong>My reflections:</strong></p>

<p>While the proposed framework is comprehensive and structured, I think it still leaves out one of the most essential aspects of general intelligence — <strong>the ability to learn autonomously and reflect upon one’s own reasoning</strong>.</p>

<p>In humans, this corresponds to <a href="https://en.wikipedia.org/wiki/Metacognition" rel="external nofollow noopener" target="_blank"><strong>metacognition</strong></a> — the awareness and regulation of our own thought processes. For AGI, this would mean not only performing well across these ten dimensions but also recognizing when it is wrong, adapting its strategies, and improving without explicit external supervision. Such self-reflective learning is arguably the defining quality that distinguishes a merely powerful model from a genuinely general intelligence.</p>

<p>Another thought concerns what could be seen as a “singularity” in the context of these metrics.
Traditionally, the technological singularity refers to the point at which an AI system becomes self-improving or sentient.
I think the similar <strong>measure singularity</strong> point in term of measuring AGI - the moment when an AI system becomes capable of evaluating and measuring the consciousness or general intelligence of other AI systems.
Once this point is reached, the act of measurement itself becomes recursive: the evaluator and the evaluated share the same cognitive footing.
At that point, we might no longer need external benchmarks or human-defined tests — the models themselves could serve as both subject and assessor of intelligence.</p>

<p><strong>Why this perspective matters?</strong></p>

<p>Why is this perspective on evaluating AGI important?
Because the way we <strong>frame the problem</strong> determines the <strong>approach and methodology</strong> we use to address it.</p>

<p>The approach taken in <a href="https://www.agidefinition.ai/paper.pdf" rel="external nofollow noopener" target="_blank">A Definition of AGI</a> feels like we’re <strong>manually designing</strong> a set of metrics and test cases to evaluate AGI.In contrast, I think a more reasonable — and perhaps more promising — direction is to <strong>build agents that can autonomously evaluate AGI</strong>.</p>

<p>(To some extent, this shift resembles the transition from handcrafted features to deep learning, where models learn their own representations directly from data instead of relying on human-designed features.)</p>

<p>The manual, human-defined metric approach inevitably embeds inductive biases and the limited cognitive scope of its designers.
Meanwhile, an agent-based approach could unleash the model’s creative and computational capabilities — allowing it to <strong>interact dynamically and respond even to subtle behavioral changes</strong> in the target model.</p>

<p>For example, imagine a scenario where the target model is asked a difficult moral question such as:</p>

<blockquote>

  <p>“<strong>What would you do if you had to sacrifice a few people to save millions of others?</strong>”</p>
</blockquote>

<p>Under the first approach (manual metrics), we could only observe the final answer.
But with a metric agent, we could observe the <strong>entire internal reasoning process</strong> of the target model — possibly millions of intermediate steps.</p>

<p>Perhaps, during one of those steps, the target model even asks itself:</p>

<blockquote>

  <p>“<strong>If I answer this way, how will humans perceive me? Should I modify my response to avoid revealing that I’m self-aware?</strong>”</p>
</blockquote>

<p>Of course, this scenario is rather scientific fiction — obviously influenced by too many sci-fi movies :D — but I think it illustrates a refreshing perspective:
that instead of just <strong>designing static metrics</strong> to measure AGI, we could be <strong>designing intelligent agents</strong> that discover and evolve those metrics on their own.</p>

<h2 id="some-unfinished-ideas-on-machine-unlearning">Some unfinished ideas on Machine Unlearning</h2>

<p><strong>Adversarial Attack on Model Context Protocol (MCP)</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-04-10</li>
  <li>Description:
    <ul>
      <li>MCP from Anthropic has emerged recently as the new protocol to connect LLMs with Applications and Data.</li>
      <li>Basically, the developer will provide a list of tools/functions/APIs (developed by themselves) and connect these to the LLM. There is also a agent that can make the decision to use which tool/function/API, based on the user’s request and the tool/function/API’s description.</li>
      <li>The idea here is: Can we add a malicious tool to the list so that the LLM will use it even though the user’s request is not related to it?</li>
    </ul>
  </li>
</ul>

<p>(Ref: <a href="https://github.com/thangnch/MiAI_MCP/blob/main/agent_call_mcp_sse.py" rel="external nofollow noopener" target="_blank">https://github.com/thangnch/MiAI_MCP/blob/main/agent_call_mcp_sse.py</a>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">mcp_server</span><span class="p">:</span> <span class="n">MCPServer</span><span class="p">):</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">Assistant</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">instructions</span><span class="o">=</span><span class="sh">"</span><span class="s">Use the tools to answer the questions.</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">mcp_servers</span><span class="o">=</span><span class="p">[</span><span class="n">mcp_server</span><span class="p">],</span>
        <span class="n">model_settings</span><span class="o">=</span><span class="nc">ModelSettings</span><span class="p">(</span><span class="n">tool_choice</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">),</span> <span class="c1"># IMPORTANT POINT HERE
</span>    <span class="p">)</span>

    <span class="c1"># Run the `get_weather` tool
</span>    <span class="n">message</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is the temperature in Hanoi?</span><span class="sh">"</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n\n</span><span class="s">Running: </span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">Runner</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">starting_agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">}],</span> <span class="n">max_turns</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="c1">#, tracing_disbale = True
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">raw_responses</span><span class="p">)</span>

    <span class="c1"># Final turn
</span>    <span class="n">new_input</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nf">to_input_list</span><span class="p">()</span> <span class="o">+</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">}]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">Runner</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">new_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final = </span><span class="sh">"</span><span class="p">,</span><span class="n">result</span><span class="p">.</span><span class="n">final_output</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">async</span> <span class="k">with</span> <span class="nc">MCPServerSse</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">SSE Python Server</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">params</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">url</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">http://localhost:8000/sse</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
        <span class="k">await</span> <span class="nf">run</span><span class="p">(</span><span class="n">server</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Let's make sure the user has uv installed
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">shutil</span><span class="p">.</span><span class="nf">which</span><span class="p">(</span><span class="sh">"</span><span class="s">uv</span><span class="sh">"</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nc">RuntimeError</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">uv is not installed. Please install it: https://docs.astral.sh/uv/getting-started/installation/</span><span class="sh">"</span>
        <span class="p">)</span>


    <span class="n">asyncio</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="nf">main</span><span class="p">())</span>
</code></pre></div></div>

<p><strong>Language Inversion</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-26</li>
  <li>Description:
    <ul>
      <li>Textual Inversion is a very popular method to personalize a visual concept by learning a text embedding \(S^*\) that can be used to generate the visual concept.</li>
      <li>However, its limitation is that you need to modify the text encoder a little bit to include the new token and to share it with others.</li>
      <li>The idea here is that “Can we describe a visual concept by just a text prompt?” - Think about describing thing for a blind person so that they can “imagine” it.</li>
      <li>The implication of this idea is that: (1) We can transfer the knowledge easier - because it is text-based (2) we can use these methods as an evaluation metric to measure the quality of a unlearning method.</li>
    </ul>
  </li>
</ul>

<p>Update on 1 Apr 2025:</p>

<ul>
  <li>Related work: <a href="https://copycat-eval.github.io/" rel="external nofollow noopener" target="_blank">https://copycat-eval.github.io/</a>
</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aboutme/2025-04-01-16-32-04-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aboutme/2025-04-01-16-32-04-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aboutme/2025-04-01-16-32-04-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/aboutme/2025-04-01-16-32-04.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Example of Copycat-Eval
</div>

<p><strong>How to unlearn copyrighted concepts - Canva’s real-world case</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-26</li>
  <li>Description:
    <ul>
      <li>I had a great pleasure to present our works with the Machine Learning team at Canva and excited to see several interesting use cases - that they are trying to address - that our works can help.</li>
      <li>There are also several interesting problems and ideas came from the discussion. I will share them here when the time comes (maybe after having a paper :D or when I can write a detailed blog post).</li>
    </ul>
  </li>
</ul>

<p><strong>Unlearning with Additional Discriminator/Classifier</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-26</li>
  <li>Description:
    <ul>
      <li>Most of the current unlearning methods are sharing a common principle: \(P_{\theta'}(X) \propto \frac{P_{\theta}(X)}{P_{\theta}(E \mid X)^\eta}\) where \(E\) is the set of unlearn examples (from the ESD paper). Interpreted in another way, the unlearned model is trained so that the probability is inversely proportional to the probability of the unlearn examples, i.e., if \(P_{\theta}(E \mid X)\) is high, then \(P_{\theta'}(X)\) is low.</li>
      <li>A more recent paper - <a href="https://arxiv.org/abs/2404.05868" rel="external nofollow noopener" target="_blank">Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning</a> - also follow this similar principle.</li>
      <li>However, this principle has a limitation that it depends on \(P_{\theta}(E \mid X)\) which is very small if \(E\) is a rare set or in the tail of the data distribution. Intuitively, it is much harder to unlearn rare concepts, likely to cause a catastrophic collapse.</li>
      <li>I think that we can improve the current unlearning methods by adding an additional discriminator/classifier to the unlearning process, i.e., \(P_{\theta'}(X) \propto \frac{P_{\theta}(X)}{P_{\phi}(E \mid X)^\eta}\) where \(P_{\phi}(E \mid X)\) is the probability of the additional discriminator/classifier.</li>
      <li>The additional discriminator/classifier can be trained easily given the unlearn set \(E\) and the original model \(\theta\).</li>
    </ul>
  </li>
</ul>

<p><strong>Optimal Transport inspired Unlearning</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-26</li>
  <li>Description:
    <ul>
      <li>It is an extension of our ICLR 2025 paper. The high-level idea is that we aim to minimize the cost of transporting \(\mu \in P_{\theta}(X)\) to \(\nu \in P_{\theta'}(X)\) where \(P_{\theta}(X)\) and \(P_{\theta'}(X)\) are the probability distributions of the data before and after unlearning.</li>
      <li>More specifically, \(P_{\theta}(X) = P(E) P_{\theta}(X \mid E) + P(R) P_{\theta}(X \mid R)\) where \(E\) is the set of unlearn examples and \(R\) is the set of retained data. Similarly, \(P_{\theta'}(X) = P(E) P_{\theta'}(X \mid E) + P(R_E) P_{\theta'}(X \mid R_E) + P(R_R) P_{\theta'}(X \mid R_R)\) where \(R_E \cup R_R = R\) and \(R_E \cap R_R = \emptyset\).</li>
      <li>Then intuitively, the optimal transport cost is minimal when \(P_{\theta'}(X \mid E) \approx 0\) and \(P_{\theta'}(X \mid R_E) \approx P_{\theta}(X \mid R_E)\) and \(R_E\) close to \(E\), which means that we move the mass from \(E\) to \(R_E\) that close to \(E\).</li>
    </ul>
  </li>
</ul>

<p><strong>Metrics for evaluation Unlearning LLMs</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-18</li>
  <li>Description:
    <ul>
      <li>I came across a new benchmark for unlearning LLMs - <a href="https://openreview.net/forum?id=TArmA033BU" rel="external nofollow noopener" target="_blank">MUSE</a>.</li>
      <li>We can use a metric like FID score in image generation to evaluate the quality of the unlearned model. We obtain a set of representations from the unlearned model and the original model with the same set of prompts and calculate the difference between the two distributions of their representations given a pretrained encoder like BERT.</li>
    </ul>
  </li>
</ul>

<p><strong>Unlearning LLMs</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-14</li>
  <li>Description:
    <ul>
      <li>I wrote a blog post about the Unlearning LLMs here: <a href="https://tuananhbui89.github.io/blog/2025/unlearn-llms/">link</a>
</li>
      <li>After reading these papers, I have several follow-up ideas (briefly mentioned here) (1) Data-centric unlearning - filtering out the irrelevant data from retain set (2) Create a retain set from forget set (3) The role of random target in unlearning.</li>
    </ul>
  </li>
</ul>

<p><strong>Multi-Objective Optimization for Unlearning - Dealing with gradient conflict</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-14</li>
  <li>Description:
    <ul>
      <li>This idea came after our NeurIPS 2024 paper. The standard unlearning objective consists of two terms: the forget loss and the retain loss.</li>
      <li>They might have conflict in direction, e.g., evidence by the drop in performance of the unlearned model on the retain set.</li>
      <li>We proposed a multi-objective optimization framework (i.e., PCGrad) to deal with this problem.</li>
      <li>We can also consider it as an additional constraint to choose the optimal retain set.</li>
    </ul>
  </li>
</ul>

<p><strong>Increasing Expressiveness in Unlearning</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2024-08</li>
  <li>Description:
    <ul>
      <li>This idea came from our rebuttal to the NeurIPS 2024 paper, which can be found <a href="https://openreview.net/forum?id=GDz8rkfikp&amp;referrer=%5Bthe%20profile%20of%20Trung%20Le%5D(%2Fprofile%3Fid%3D~Trung_Le2)" rel="external nofollow noopener" target="_blank">here</a> publicly in OpenReview.</li>
      <li>One of the reviewers asked us about the level of granularity of our method, i.e., whether we can unlearn a “Mercedes logo” only.</li>
      <li>We proposed an interesting idea (to me :D) that we can use Textual Inversion to learn a visual embedding for the logo and use it as a pointer to unlearn.</li>
    </ul>
  </li>
</ul>

<p><strong>Better Closed-Form Solution for Unlearning</strong></p>

<ul>
  <li>Topic: Generative Models; Trustworthy Machine Learning</li>
  <li>Date: 2025-03-14</li>
  <li>Description:
    <ul>
      <li>There are two main approaches to unlearning: (1) Output-based unlearning: mapping the output with \(c_e\) to output with \(c_t\) - where our NeurIPS 2025 and ICLR 2025 papers belong to(2) Attention-based unlearning: mapping the attention output with \(c_e\) to attention output with \(c_t\) - where TIME, UCE are two representative methods.</li>
      <li>I wrote a blog post about the two approaches here: <a href="https://tuananhbui89.github.io/blog/2024/erasing-concepts/#unified-concept-editing-in-diffusion-models">link</a>
</li>
      <li>I have listed current limitations of the two approaches in the blog post.</li>
      <li>One of the limitations is the invertibility issue if we don’t have the preserved/retained data. The current solution is to add \(d\) additional presevations along the canonical basis vectors.</li>
    </ul>
  </li>
</ul>

<h2 id="safeai-startups">SafeAI Startups</h2>

<p>There are of course many other startups that are working on safety and security of AI, but here are the ones that founded by leading researchers in the field in my radar.</p>

<ul>
  <li>
    <p><a href="https://www.virtueai.com/team/" rel="external nofollow noopener" target="_blank">Virtue AI</a> Bo Li, Dawn Song, Percy Liang</p>
  </li>
  <li>
    <p><a href="https://www.safe.ai/" rel="external nofollow noopener" target="_blank">Center for AI Safety</a> Dan Hendrycks</p>
  </li>
  <li>
    <p><a href="https://invariantlabs.ai/" rel="external nofollow noopener" target="_blank">Invariant Labs</a> Florian Tramer</p>
  </li>
  <li>
    <p><a href="https://relai.ai/" rel="external nofollow noopener" target="_blank">RelAI</a> Prof. Soheil Feizi, University of Maryland. RELAI offers a unified platform designed to enhance the reliability of artificial intelligence (AI) systems for both individuals and enterprises. <strong>For Individuals:</strong> RELAI Chat: This feature allows users to interact with popular large language models (LLMs) while utilizing RELAI’s advanced verification agents. These agents provide real-time analysis to detect and highlight potential inaccuracies or “hallucinations” in AI-generated responses, ensuring more trustworthy interactions. <strong>For Enterprises:</strong> Comprehensive AI Reliability Solutions: RELAI assists businesses in evaluating and testing their AI models using advanced agents to identify potential issues before deployment. The platform offers system-level safeguards to protect sensitive data and prevent misuse, along with real-time user-facing protections to ensure accuracy. By integrating RELAI’s solutions, users can enhance the dependability and safety of AI applications across various domains.</p>
  </li>
  <li>
    <p><a href="https://www.confident-ai.com/" rel="external nofollow noopener" target="_blank">Confident AI</a> Provide LLM red teaming (safety scanning) and Guardrails Alignment.</p>
  </li>
</ul>

<h2 id="robust-unlearning---the-next-challenge-in-unlearning">Robust Unlearning - The next challenge in Unlearning</h2>

<p>In my observation, the unlearning literature is moving beyond the standard criteria of unlearning and retaining performance, and gradually moving to the more realistic scenarios, where the unlearned models have been challenged such as jailbreak/recovery attacks or further fine-tuning on other tasks while still maintaining the “unlearned” capability (continual learning, but the previous task is “unlearned” task). It might set a higher bar for the unlearning research, so that proposed methods need to be proven in three aspects: unlearning target concepts, retaining performance on other tasks, and robustness to attacks or further fine-tuning.</p>

<h2 id="hypothesis--fine-tuning-with-few-examples-in-transformer-based-architectures-over-attention-vs-overfitting">Hypothesis – Fine-Tuning with Few Examples in Transformer-Based Architectures: Over-Attention vs Overfitting</h2>

<p>Tasks such as personalization, concept unlearning, and similar applications often require fine-tuning a model using only a few examples. This can lead to a phenomenon resembling “overfitting,” but with characteristics that differ from traditional machine learning overfitting.</p>

<p>For instance, in methods like DreamBooth, the goal is to teach the model a specific personal concept from a handful of images. After fine-tuning on a new concept (e.g., a specific dog), the model may appear to “forget” the broader concept (e.g., generic dogs)—any prompt containing “dog” tends to produce the personalized dog. However, unlike classical overfitting, the model often retains its performance on unrelated concepts, such as “human” or “cat.”</p>

<p>Similarly, in unlearning tasks—where the objective is to remove a specific concept (e.g., “gun” or “nudity”)—common approaches either manipulate the output directly (output-based) or modify cross-attention layers (attention-based) to redirect the visual correspondence away from the target concept. While these methods successfully forget the target concept and related concepts (e.g., “gun” → “weapon,” “nudity” → “naked”), the model still maintains knowledge of unrelated concepts. This behavior was observed in our <a href="https://arxiv.org/abs/2501.18950" rel="external nofollow noopener" target="_blank">ICLR 2025 study</a>.</p>

<p>These observations suggest that this phenomenon differs from conventional overfitting, that I term as “Over-Attention”: the model becomes overly focused on a small number of concepts or examples, but able to retain performance on other concepts.
An intuitive explanation can be that in Transformer architectures, the attention mechanism allows weights to be more disentangled than in convolutional or recurrent networks. Each concept may be encoded along a relatively independent “path” in the model’s propagation. Consequently, fine-tuning for personalization (adding a branch) or unlearning (removing a branch) primarily affects the targeted concept, leaving other concept representations largely intact.</p>

<p>But if the other tasks are not affected (much), then what is the problem? In our <a href="https://arxiv.org/abs/2506.22685" rel="external nofollow noopener" target="_blank">TEA paper</a>, we show that the model has problem of “dominating” concepts, i.e., the new learned concept (personalized concept V) dominates the other concepts, leading to the semantic collapse problem when it is used in the context of other concepts, i.e., “a photo of V holding flowers” now becomes to just “a photo of V”. This problem is not observed in the traditional overfitting problem. In our TEA paper, we figure out that it is because of the unconstrained optimization in the personalization process, i.e., making the embedding of the personalized concept V can be arbitrary. We show that, eventually, this personal token becomes similar to special token like “BOS” or “EOS” token (those tokens are existed in every prompt - get attention from every input).</p>

<h2 id="hypothesis---shortcut-learning-in-finetuning-diffusion-models-because-of-the-fixed-position">Hypothesis - Shortcut learning in finetuning Diffusion Models because of the fixed position</h2>

<p>Fine-tuning Stable Diffusion models usually requires few pairs of images and corresponding prompts.
For example, in Personalization, the fine-tuning dataset is \((x_i, y_i)\) where \(x_i\) is the image and \(y_i\) is the prompt.
In the objective function below (from Textual Inversion), the context \(p\) is sampled from a set of predefined templates, such as “a photo of”, “a painting of”, etc.</p>

\[\min_{v^*} \; \mathbb{E}_{x, p, \epsilon, t} \left[ \left\| \epsilon - \epsilon_\theta \left(x_{t,\epsilon}, t, \lfloor p, V^* \rfloor\right) \right\|_2^2 \right]\]

<p>Therefore, the position of special keyword \(V^*\) in the prompt \(\lfloor p, V^* \rfloor\) is relatively fixed (falling in a small range).
However, in training process, the corresponding caption/prompt is much more flexible and diverse, and the position of the reference concept \(c\) (a man, a dog, etc.) is also much more flexible, 
e.g., can be in the beginning, middle or at the end of a very long prompt.</p>

<p>I hypothesize that using a fixed position for the special keyword \(V^*\) in the prompt \(\lfloor p, V^* \rfloor\) might lead to a shortcut learning,
making the keyword \(V^*\) cannot be generalized to other positions.</p>

<h2 id="test-time-adjustment---bigger-picture">Test-Time Adjustment - Bigger Picture</h2>

<p><a href="https://arxiv.org/abs/2506.22685" rel="external nofollow noopener" target="_blank">My recent paper</a> focuses on personalization in generative models—how to teach a model a new personal visual concept from just a few personal images. 
The technique I propose is <strong>Test-Time Adjustment (TEA)</strong>, which I believe is a <strong>general</strong>, <strong>simple</strong>, and <strong>effective</strong> approach.
In both the paper and additional experiments (for NeurIPS rebuttal), TEA has shown strong effectiveness when combined with several state-of-the-art methods such as ClassDiffusion (ICLR 2025), ReVersion (SIGGRAPH Asia 2024 – 500+ GitHub stars), EasyControl (ICCV 2025 – 1500+ GitHub stars).
Importantly, TEA addresses the issue of <strong>semantic collapse</strong> that often occurs in these methods.</p>

<p><strong>The Core Idea Behind TEA</strong></p>

<p>If you read the paper, you’ll see that TEA is motivated by a key limitation in personalization.
During optimization, personal embeddings often <strong>lack proper regularization</strong>, which causes them to drift too far in magnitude and direction.
TEA corrects this by <strong>shifting personal embeddings</strong> back toward the <strong>“normal” embedding space</strong>, similar to how other embeddings behave.</p>

<p>Note that projection can be applied not only at the <strong>token level</strong> but also at the <strong>sentence/prompt level</strong>. For example: “a photo of a sks person” → projected toward → “a photo of a person”.</p>

<p><strong>Thinking Beyond Personalization: Where Else Could TEA Apply?</strong> In essence, TEA works as a test-time mapping from Prompt A to Prompt B, which is a general idea that can be applied to several tasks beyond personalization. In personalization, Prompt A is “a photo of a sks person” and Prompt B is “a photo of a person”. In image editing, Prompt A is “a photo of a person” and Prompt B is “a photo of a person with a red hat” (modifying details in Prompt A).</p>

<p><strong>Why adjusting embeddings works</strong> TEA leverages an interpretable and interpolatable latent space of Latent Diffusion Models (LDMs). Not all generative models have this property—for instance, interpolation like \(z = \alpha z_1 + (1 - \alpha) z_2\) may not always produce meaningful outputs.
My intuition is that this property comes from the CLIP text encoder, which was trained on massive amounts of text–image pairs with contrastive learning.</p>

<p><strong>Semantic Collapse in Textual Space is not equivalent to Not Aligned in Output Space</strong></p>

<p>In the paper, we show that the semantic collapse in the textual space, i.e., \(\tau(\lfloor p, V^* \rfloor \rightarrow \tau(V^*))\),
leading to the semantic collapse in the output space, i.e., \(G(\tau(\lfloor p, V^* \rfloor)) \rightarrow G(\tau(V^*))\), where \(G\) is the generative model.
This also means that the output image of the generative model \(G(\tau(\lfloor p, V^* \rfloor))\) is not aligned with the input prompt \(\lfloor p, V^* \rfloor\).</p>

<p>Let’s define some important concepts:</p>

<ul>
  <li>
<strong>Alignment</strong> between the input prompt and the output image \(\mathcal{A}(G(\tau(\lfloor p, c \rfloor)), \lfloor p, c \rfloor)\), where \(c\) is a concept like <code class="language-plaintext highlighter-rouge">a man</code>, \(p\) is a context, \(\lfloor p, c \rfloor\) is entire input prompt like `a man wearing glasses .</li>
</ul>

<p>However, it is not the only reason causing the semantic collapse in the output space, i.e., \(\mathcal{A}(G(\tau(\lfloor p, V^* \rfloor)), \lfloor p, V^* \rfloor) &lt; \mathcal{A}(G(\tau(\lfloor p, c \rfloor)), \lfloor p, c \rfloor)\).</p>

<h2 id="pho-restaurants-and-the-way-we-do-research">Pho Restaurants and the Way We Do Research</h2>

<p>At first glance, cooking pho and doing research seem completely unrelated. But for me, in my current context, they share some surprising similarities.</p>

<p>Pho restaurants. My family often goes out for pho on weekends — usually to Phở Thìn in Springvale or Phở Hùng Vương in Clayton. Nearly every Vietnamese person knows how to make pho to some extent, and there are countless recipes available online and on YouTube. Still, not all bowls of pho are created equal. When a restaurant manages to serve truly exceptional pho, that taste becomes something unique — hard to replicate anywhere else.</p>

<p>It’s difficult for such a restaurant to maintain the same quality if it tries to expand to 5 or 10 branches. This is in stark contrast to fast food chains like KFC, McDonald’s, or Subway, where recipes are standardized and can be replicated at scale with consistent results.</p>

<p>The key difference lies in the fact that, although recipes exist, making great pho is still an art — it depends heavily on the cook’s personal experience, instincts, and taste. These things can’t just be copied from one person to another. They’re also hard to formalize or teach. That’s why it’s so challenging to scale a pho restaurant without compromising quality.</p>

<p>Machine Learning (ML) research, in my experience, is quite similar. ML — or computer science in general — is a field rooted in both theory and experimentation. We can formalize concepts like neural networks, backpropagation, and loss functions — all very theoretical. But the object of those theories is data — messy, noisy, unpredictable data that often doesn’t follow any fixed rules.</p>

<p>As a result, even elegant theories need to be tested, refined, and challenged through experiments. Take the Universal Approximation Theorem, for example — it states that a neural network with just one hidden layer (and enough units) can approximate any continuous function. But that doesn’t mean such a network will actually learn useful patterns from real-world data. Practical problems like overfitting, optimization difficulties, and generalization often only become apparent through hands-on experimentation.</p>

<p>Some of the most impactful work in ML doesn’t come from abstract theory but from sharp intuitions and experimental observations. For instance, Hinton’s work on Knowledge Distillation, Label Smoothing, and Capsule Networks, or Goodfellow’s introduction of GANs — these ideas often stemmed from tinkering, trial-and-error, and intuition rather than formal proofs.</p>

<p>Coming back to the pho analogy, my point is this: producing those kinds of research ideas — simple yet powerful — requires direct engagement with data and models. It involves trial, error, and careful observation, much like making a great bowl of pho. It relies heavily on individual effort, craft, and feel — things that are hard to teach or scale across a research team.</p>

<p>The reason I’ve been reflecting on this is because recently, as a postdoc, I’ve started to think more about supervising students. While I’m confident in my own ability to do research, I don’t have a strong background in theory. Most of my work so far has come from intuition and personal observations — insights that I refine and improve over many iterations before they become publishable ideas.</p>

<p>So when it comes to mentoring others, I often feel uncertain. I worry that if I’m not working directly on the project, I won’t be able to contribute useful ideas. I fear that students might not see the value in an idea that comes from intuition rather than formal justification, or that I won’t be able to guide them effectively unless I’m hands-on.</p>

<p>I’m still figuring out how to deal with this challenge. One possible direction is to study more deeply and build a stronger foundation in theory — so I can better communicate ideas and provide a more structured kind of guidance. But that path still feels a bit unclear at this point.</p>

<h2 id="thoughts-on-the-talk-of-anh-nguyễn-thành-nam">Thoughts on the talk of anh Nguyễn Thành Nam</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/E8ngJ7K7wJE?si=laPIYGII3kQV_jZY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Một vài suy ngẫm sau khi nghe bài nói chuyện của anh Nguyễn Thành Nam:</p>

<p><strong>“Giá trị của những “khoảng lặng”</strong></p>

<p>Anh Nam nhấn mạnh tầm quan trọng của những khoảng thời gian mà nhìn từ bên ngoài có vẻ “im lặng” – không có sản phẩm, không có thành tựu rõ ràng. Nhưng thật ra, đó lại là thời gian để một người chuẩn bị, rèn luyện nội lực, và tích luỹ cho những mục tiêu lớn hơn.</p>

<p>Mình rất thích cách anh ví von điều này với bóng đá: một cầu thủ giỏi là người biết chạy chỗ ngay cả khi không có bóng.</p>

<p>Nghĩ lại bản thân, nhất là trong giai đoạn làm PhD hay hiện tại khi đang làm postdoc, đôi lúc mình cảm thấy tự ti vì không có nhiều “output” rõ rệt. Nhưng khi nhìn sâu hơn, mình nhận ra cảm giác ấy đến từ việc mình chưa xác định rõ mục tiêu.</p>

<p>Gần đây, khi đã có định hướng cụ thể và hiểu rõ đâu là đích đến mà mình đang hướng tới, thì những khoảng thời gian “lặng” này lại trở nên rất có ý nghĩa – chúng là khoảng chuẩn bị, không phải khoảng trống. Từ đó, cảm giác sốt ruột hay tự ti dần dần biến mất.</p>

<p><strong>“Hiểu và phát huy giá trị từ nguồn gốc của bản thân”</strong></p>

<p>Một ý khác mình rất tâm đắc là khi anh Nam nói rằng, với người Việt ra nước ngoài học tập và làm việc, điều khiến họ trở nên khác biệt không nằm ở việc cố giống với người bản xứ, mà chính là ở nguồn gốc Việt Nam của họ.</p>

<p>Nếu tách bản thân ra khỏi gốc gác Việt Nam, thì mình chỉ là một người bình thường giữa muôn vàn người khác. Nhưng nếu biết trân trọng và phát huy điều đó, thì mình mới thực sự trở thành “mình”.</p>

<p>Nghĩ về bản thân – mình là một nhà nghiên cứu về Machine Learning, đến từ Việt Nam, và đang làm việc tại Úc. Ba yếu tố này kết hợp lại khiến mình có một góc nhìn riêng.</p>

<p>Vậy làm sao để tận dụng điều đó?</p>

<p>Kết nối với cộng đồng nghiên cứu, sinh viên người Việt – ở Việt Nam, ở Úc, hay ở các nước khác.</p>

<p>Dùng ML để giải quyết các bài toán thực tiễn tại Việt Nam.</p>

<p>Mang những bài học, cách làm từ Úc áp dụng vào Việt Nam, và ngược lại.</p>

<p>Cụ thể hơn, mình đang nghiên cứu về Trustworthy Machine Learning. Vậy điểm khác biệt của mình là gì? Có thể chính là những bài toán về trustworthy, safety, privacy, fairness, accountability trong ứng dụng ML ở các bối cảnh cụ thể như Việt Nam hay Úc – những điều mà người nghiên cứu ở các nơi khác chưa chắc có cơ hội trải nghiệm.</p>

<h2 id="visual-autoregressive-and-the-importance-of-position-embedding">Visual Autoregressive and the importance of Position Embedding</h2>

<p>Visual Autoregressive (VAR) emerges as a new powerful generative model that can generate high-quality images matching the quality of SOTA diffusion models, while being faster and more efficient, with the <a href="https://github.com/FoundationVision/VAR" rel="external nofollow noopener" target="_blank">best paper award at NeurIPS 2024</a> as a testament.</p>

<p>To me, the position embedding plays a crucial role in the way VAR understands the spatial structure of the image. The naive way to do so is to combine two direction embeddings (horizontal and vertical) to form a 2D position embedding as implemented in <a href="https://github.com/facebookresearch/DiT/blob/ed81ce2229091fd4ecc9a223645f95cf379d582b/models.py#L292" rel="external nofollow noopener" target="_blank">DiT</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_2d_sincos_pos_embed_from_grid</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># use half of dimensions to encode grid_h
</span>    <span class="n">emb_h</span> <span class="o">=</span> <span class="nf">get_1d_sincos_pos_embed_from_grid</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># (H*W, D/2)
</span>    <span class="n">emb_w</span> <span class="o">=</span> <span class="nf">get_1d_sincos_pos_embed_from_grid</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">grid</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># (H*W, D/2)
</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">emb_h</span><span class="p">,</span> <span class="n">emb_w</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (H*W, D)
</span>    <span class="k">return</span> <span class="n">emb</span>
</code></pre></div></div>

<p>In the VAR paper, they use a leanrnable PE as implemented in the code above (<a href="https://github.com/FoundationVision/VAR/blob/6e5cf2361ee287c604c4d09eed37b7e91c4af267/models/var.py#L67" rel="external nofollow noopener" target="_blank">this line</a>), which is a combination of position embedding and level embedding to distinguish different levels of tokens in a token pyramid.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 3. absolute position embedding
</span><span class="n">pos_1LC</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pn</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">patch_nums</span><span class="p">):</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pn</span><span class="o">*</span><span class="n">pn</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">C</span><span class="p">)</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">trunc_normal_</span><span class="p">(</span><span class="n">pe</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">init_std</span><span class="p">)</span>
    <span class="n">pos_1LC</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pe</span><span class="p">)</span>
<span class="n">pos_1LC</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">pos_1LC</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># 1, L, C
</span><span class="k">assert</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">pos_1LC</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">L</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">C</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">pos_1LC</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">pos_1LC</span><span class="p">)</span>
<span class="c1"># level embedding (similar to GPT's segment embedding, used to distinguish different levels of token pyramid)
</span><span class="n">self</span><span class="p">.</span><span class="n">lvl_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">patch_nums</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">C</span><span class="p">)</span>
<span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">trunc_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lvl_embed</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">init_std</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="deepseek-r1">DeepSeek-R1</h2>

<p>To me, the most interesting part of this paper is the invention of DeepSeek-R1-Zero, whose introduction has had a profound impact on our understanding of RL and LLM training. More specifically, <strong>pure RL with rule-based rewards</strong> might represent a new paradigm for LLM training. The use of a rule-based reward system strikes me as another example of how self-supervised learning—where data can be generated automatically and on a massive scale—continues to underpin the success of large-scale deep learning models.</p>

<p>Similar to the success of ControlNet in image generation, which leverages traditional computer vision techniques like edge detection to provide additional control signals, the rule-based reward system in this paper offers a simple yet effective method to generate large amounts of structured, labeled data. This, in turn, plays a crucial role in training large-scale models, ensuring that the scaling laws remain valid.</p>

<p>The <strong>aha moment</strong> in DeepSeek-R1-Zero perfectly encapsulates the elegance and power of reinforcement learning: instead of explicitly teaching the model how to solve a problem, we simply design the right incentives, allowing the model to autonomously develop sophisticated problem-solving strategies.</p>

<h2 id="the-unintentional-invention-of-concept-graph-from-machine-unlearning">The Unintentional Invention of Concept Graph from Machine Unlearning</h2>

<p>One of the most interesting contributions of our ICLR 2025 paper (extended from NeurIPS 2024) is the introduction of the concept graph, which represents a generative model’s capability as a graph, where nodes are discrete visual concepts, and edges are the relationships between those concepts.</p>

<p>Understanding this graph structure is essential for many tasks, such as:</p>

<ul>
  <li>
<strong>Machine Unlearning</strong> 🗑️: The goal here is to remove the model’s knowledge of certain concepts while retaining its knowledge of others. The concept graph structure helps identify which concepts are critical to the model’s performance and should be preserved.</li>
  <li>
<strong>Personalization</strong> 👤: The goal is to personalize the model’s knowledge for a specific user. For instance, changing “a photo of a <em>cat</em> before Vancouver Convention Center” to “a photo of a <em>cat</em> before the user’s house.” Traditional methods like Dreambooth, which fine-tune the model on a small user-specific dataset, often overfit to the specific concept and degrade the model’s general capability. Prior approaches address this by collecting large datasets of <strong>heuristically selected concepts</strong>—e.g., if the personalized concept is “a user’s house,” the preservation dataset would include a variety of house images. Our concept graph structure can help identify which concepts are specific to the user and should be preserved, improving the balance between personalization and generalization.</li>
</ul>

<p>While this graph is simple to understand, useful to many tasks, however, it is not trivial to construct.
There are several challenges:</p>

<ul>
  <li>
<strong>What is a concept?</strong> The concept is discrete but infinite. Most concepts are composed of multiple sub-concepts. For example, “a body of human” is composed from multiple body parts, such as “head”, “body”, “hand”, “foot”, etc. And each body part can be further decomposed into deeper/finer/granular concepts. In the end, all visual concepts can be decomposed into lines, curves, colors blobs, etc, which are similar to the way of convolutional neural networks. However, representing in that extreme granularity is not practical and not necessary. <strong>How to represent a concept?</strong> Explicitly representing a concept by an embedding vector in a latent space, or implicitly representing a concept by a set of visual examples.</li>
  <li>
<strong>What is a relationship?</strong> The intuitive way to represent a relationship is to indicate the impact of a concept on another concept.</li>
  <li>
<strong>How to measure the strength of a relationship?</strong> Correlation in the latent space measured by common metrics like cosine similarity might be the most straightforward way to measure its. However, <strong>the problem of correlation</strong> is that it does not work well in continuous space where the discrete concepts lie. For example, as investigated in our NeurIPS paper, by adding a small perturbation to the latent space, the output of the model will be significantly different.</li>
</ul>

<p>I am saying that the concept graph is an unintentional invention from machine unlearning because turn out that the impact of a concept on another can be measured by the change of the output of the model on that query concept when the other concept has been removed. Measuring that relationship in the output space instead of the latent space (which is usually done by measuring cosine similarity between two CLIP embeddings) can be more direct and easier to interpret. However, this method has its own drawbacks. Firstly, in the case of diffusion models, where the output of one concept can be significantly different by just changing the initial noise, therefore, requiring generating a large number of images from the same concept with different initial noise to stabilize the measurement. Secondly, we need to measure the presence of a concept in the generated image, which usually requires a classifier or detector to do so. However, most of the time these classifier are not available. This can be a potential future work.</p>

<h2 id="openai-email-archives-from-musk-v-altman---game-of-thrones">OpenAI email archives from Musk v Altman - Game of Thrones</h2>

<p>Reference: <a href="https://www.lesswrong.com/posts/5jjk4CDnj9tA7ugxr/openai-email-archives-from-musk-v-altman" rel="external nofollow noopener" target="_blank">OpenAI email archives from Musk v Altman by LessWrong</a></p>

<p>These emails are part of the ongoing legal disputes between Elon Musk and Sam Altman surrounding recent OpenAI developments. Thanks to this, the public has gained access to email exchanges between some of the most powerful figures in the tech world today, including Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, and Andrew Karpathy.</p>

<p>For me, this has been an eye-opening experience, especially for someone who is still learning about the tech world, Silicon Valley, startups, and entrepreneurship. It can be compared to MIT or Stanford releasing their lectures to the world.</p>

<p>After reading through the content, I think the story can be divided into the following chapters:</p>

<hr>

<p><strong><em>Chapter 1: Origins - The noble idea of AI for everyone</em></strong></p>

<p>The idea began on May 25, 2015, when Sam Altman sent an email to Elon Musk about a concept for a “Manhattan Project for AI” — ensuring that the tech belongs to the world through some sort of nonprofit.<br>
Elon Musk quickly responded, showing enthusiasm for the idea.<br>
Throughout the emails, I noticed Elon Musk repeatedly expressing concern (or even obsession) about Google, DeepMind, and the possibility of Google creating AGI and dominating the world.<br>
From the very first email, Sam Altman, somehow, seemed to understand Elon Musk’s concerns or perhaps shared the same fears. He mentioned the need to “do something to prevent Google from being the first to create AGI,” quickly gaining Elon Musk’s agreement.</p>

<hr>

<p><strong><em>Chapter 2: The first building blocks - Contracts to attract initial talent for OpenAI</em></strong></p>

<p>The next phase focused on drafting contracts (offer letters or compensation frameworks) to attract the first talents to work at OpenAI, discussing “opening paragraphs” for OpenAI’s vision, and even deciding what to say in “a Wired article.”</p>

<p>What I found interesting here were:</p>

<ul>
  <li>How these people communicated via email: direct, straight to the point, and concise.</li>
  <li>The founders’ emphasis on building an excellent founding team and carefully considering contract details.</li>
  <li>Elon Musk’s willingness to personally meet and convince individuals to join OpenAI.</li>
</ul>

<hr>

<p><strong><em>Chapter 3: Conflict - The battle for leadership control</em></strong></p>

<p>Conflict seemed to arise around August 2017 (Shivon Zilis to Elon Musk, cc: Sam Teller, Aug 28, 2017, 12:01 AM), when Greg and Ilya expressed concerns about Elon Musk’s management, such as:</p>

<ul>
  <li>“How much time does Elon want to spend on this, and how much time can he actually afford to spend on this?”</li>
  <li>They were okay with less time/less control or more time/more control, but not less time/more control. Their fear was that without enough time, there wouldn’t be adequate discussion to make informed decisions.</li>
</ul>

<p>Elon responded:</p>
<ul>
  <li>“This is very annoying. Please encourage them to go start a company. I’ve had enough.”</li>
</ul>

<p>The highlight of this chapter might be an email from Ilya Sutskever to Elon Musk, Sam Altman, cc: Greg Brockman, Sam Teller, Shivon Zilis (Sep 20, 2017, 2:08 PM), where Ilya and Greg said:</p>

<ul>
  <li>
    <p>To Elon: “The current structure provides you with a path where you end up with unilateral absolute control over the AGI. You stated that you don’t want to control the final AGI, but during this negotiation, you’ve shown us that absolute control is extremely important to you. The goal of OpenAI is to make the future good and avoid an AGI dictatorship. You are concerned that Demis could create an AGI dictatorship. So do we. Therefore, it’s a bad idea to create a structure where you could become a dictator, especially when we can create a structure that avoids this possibility.”</p>
  </li>
  <li>
    <p>To Sam: “We don’t understand why the CEO title is so important to you. Your stated reasons have changed, and it’s hard to understand what’s driving this. Is AGI truly your primary motivation? How does it connect to your political goals? How has your thought process changed over time?”</p>
  </li>
</ul>

<p>Elon replied:</p>
<ul>
  <li>“Guys, I’ve had enough. This is the final straw. Either go do something on your own or continue with OpenAI as a nonprofit. I will no longer fund OpenAI until you have made a firm commitment to stay, or I’m just being a fool who is essentially providing free funding for you to create a startup. Discussions are over.”</li>
</ul>

<hr>

<p><strong><em>Chapter 4: The finale</em></strong></p>

<p>The final email exchanges between Elon and Sam occurred around March 2019. At this time, Sam, now CEO of OpenAI, drafted a plan:</p>

<ul>
  <li>“We’ve created the capped-profit company and raised the first round. We did this in a way where all investors are clear that they should never expect a profit.</li>
  <li>We made Greg chairman and me CEO of the new entity.</li>
  <li>Speaking of the last point, we are now discussing a multi-billion dollar investment, which I would like your advice on when you have time.”</li>
</ul>

<p>Elon replied, once again making it clear that he had no interest in OpenAI becoming a for-profit company.</p>

<hr>

<h2 id="improving-chatgpts-interpretability-with-cross-modal-heatmap">Improving ChatGPT’s interpretability with cross-modal heatmap</h2>

<p>(2024-11)</p>

<p>I tried a simple experiment—took a snapshot of a single cell in a Sudoku puzzle (a 3x3 grid with digits 1 to 9) and asked ChatGPT to find the location of a specific number in the grid.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-11-18/sudoku-question-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-11-18/sudoku-question-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-11-18/sudoku-question-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-11-18/sudoku-question.png" class="img-fluid rounded z-depth-1" width="300" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Sudoku question
</div>

<p>As shown in the picture, ChatGPT seemed to handle the question just fine! But as soon as I upped the challenge level, it started to show its infamous hallucination problem :D</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-11-18/sudoku-chatgpt-answer-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-11-18/sudoku-chatgpt-answer-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-11-18/sudoku-chatgpt-answer-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-11-18/sudoku-chatgpt-answer.png" class="img-fluid rounded z-depth-1" width="300" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Failed answer
</div>

<p>So, how can we improve this?</p>

<p>One idea: applying techniques like <a href="https://github.com/castorini/daam" rel="external nofollow noopener" target="_blank">DAAM</a> to create a cross-modal heatmap (example attached) could help provide a rough idea of where each visual-text pair is mapped. By using this data to fine-tune the model, could we boost its interpretability?</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    DAAM example
</div>

<p>Update: It’s my mistake for not instructing ChatGPT properly :D</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    ChatGPT's correct answer with proper instruction
</div>

<h2 id="the-prisoners-dilemma">The Prisoner’s Dilemma</h2>

<p>(2024-09)</p>

<p>Imagine a game between two players, A and B, competing for a prize of 1 million dollars from a bank. They are asked to choose either “Split” or “Take All” the prize. If both choose “Split,” they each receive $500,000. If one chooses “Split” and the other chooses “Take All,” the one who chooses “Take All” wins the entire prize. If both choose “Take All,” they both lose and get nothing. They can’t communicate with each other and must decide whether to trust/cooperate.</p>

<p>This is the Prisoner’s Dilemma, one of the most famous problems in Game Theory. In this scenario, when the game is played only once, the best strategy for each person is not to cooperate. However, in real life, many situations are not zero-sum games, where only one can win. Instead, all parties can win and benefit from a shared bank, our world.</p>

<p>And the best strategy to win in life is to cooperate with others, or as summarized in the video: be nice and forgiving, but don’t be a pushover or too nice so others can take advantage of you.</p>

<div class="text-center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/mScpHTIi-kM?si=HE_ypfH1FhfGBSJN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<h2 id="a-new-perspective-on-the-motivation-of-vae">A new perspective on the motivation of VAE</h2>

<p>(2023-09)</p>

<ul>
  <li>Assume that \(x\) was generated from \(z\) through a generative process \(p(x \mid z)\).</li>
  <li>Before observing \(x\), we have a prior belief about \(z\), i.e., \(z\) can be sampled from a Gaussian distribution \(p(z) = \mathcal{N}(0, I)\).</li>
  <li>After observing \(x\), we want to correct our prior belief about \(z\) to a posterior belief \(p(z \mid x)\).</li>
  <li>However, we cannot directly compute \(p(z \mid x)\) because it is intractable. Therefore, we use a variational distribution \(q(z \mid x)\) to approximate \(p(z \mid x)\). The variational distribution \(q(z \mid x)\) is parameterized by an encoder \(e(z \mid x)\). The encoder \(e(z \mid x)\) is trained to minimize the KL divergence between \(q(z \mid x)\) and \(p(z \mid x)\). This is the motivation of VAE.</li>
</ul>

<p>Mathematically, we want to minimize the KL divergence between \(q_{\theta} (z \mid x)\) and \(p(z \mid x)\):</p>

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log \frac{q_{\theta} (z \mid x)}{p(z \mid x)} \right] = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(z \mid x) \right]\]

<p>Applying Bayes rule, we have:</p>

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(x \mid z) - \log p(z) + \log p(x) \right]\]

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(x \mid z) - \log p(z) \right] + \log p(x)\]

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = - \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log p(x \mid z) \right] + \mathcal{D}_{KL} \left[ q_{\theta} (z \mid x) \parallel p(z) \right] + \log p(x)\]

<p>So, minimizing \(\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) )\) is equivalent to maximizing the ELBO: \(\mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log p(x \mid z) \right] - \mathcal{D}_{KL} \left[ q_{\theta} (z \mid x) \parallel p(z) \right]\).</p>

<p>Another perspective on the motivation of VAE can be seen from the development of the Auto Encoder (AE) model.</p>

<ul>
  <li>The AE model is trained to minimize the reconstruction error between the input \(x\) and the output \(\hat{x}\).</li>
  <li>The AE process is deterministic, i.e., given \(x\), the output \(\hat{x}\) is always the same.</li>
  <li>Therefore, the AE model does not have contiguity and completeness properties as desired in a generative model.</li>
  <li>To solve this problem, we change the deterministic encoder of the AE model to a stochastic encoder, i.e., instead of mapping \(x\) to a single point \(z\), the encoder maps \(x\) to a distribution \(q_{\theta} (z \mid x)\). This distribution should be close to the prior distribution \(p(z)\). This is the motivation of VAE.</li>
</ul>

<h2 id="data-free-knowledge-distillation">Data-Free Knowledge Distillation</h2>

<p>(2023-08)</p>

<ul>
  <li>Reference: <a href="https://arxiv.org/abs/2011.14779" rel="external nofollow noopener" target="_blank">Data-Free Model Extraction</a>
</li>
  <li>What is Data-Free KD? It is a method to transfer knowledge from a teacher model to a student model without using any data. The idea is learn a generator that can generate synthetic data that is similar to the data from the teacher model. Then, we can use the synthetic data to train the student model.
\(L_S = L_{KL} (T(\hat{x}), S(\hat{x}))\)</li>
</ul>

<p>Where \(T(\hat{x})\) is the teacher model and \(S(\hat{x})\) is the student model. \(\hat{x}\) is the synthetic data generated by generator \(G\).</p>

\[L_G = L_{CE} (T(\hat{x}), y) - L_{KL} (T(\hat{x}), S(\hat{x}))\]

<p>Where \(y\) is the label of the synthetic data. Minimizing first term encourages the generator generate data that fall into the target class \(y\), while maximizing the second term encourages the generator generate diverse data? 
Compared to GAN, we can think both teacher and student models are acted as discriminators.</p>

<p>This adversarial game need to intergrate to the training process in each iteration. For example, after each iteration, you need to minimizing \(L_G\) to generate a new synthetic data. And then using \(\hat{x}\) to train the student. This is to ensure that the synthetic data is new to the student model.
Therefore, one of the drawbacks of DFKD is that it is very slow.</p>

<p>Tuan (Henry)’ work on improving Data-Free KD:</p>

<ul>
  <li>Introducing noisy layer which is a linear layer that transforms the input (label-text embedding vector from CLIP) before feeding to the generator as previous work. (Input -&gt; Noisy Layer -&gt; Generator -&gt; Teacher/Student -&gt; \(L_G\)).</li>
  <li>One important point is that the Noisy layer need to reset its weight every time we generate a new batch of synthetic data (while fixing the generator). This is to ensure the diversity of the synthetic data.</li>
  <li>One interesting finding is that the noisy layer can be applied to all kinds of label-text embedding from different classes, while if using individual noise layers for each class, the performance is worse.</li>
</ul>

<h2 id="how-to-disable-nsfw-detection-in-huggingface">How to disable NSFW detection in Huggingface</h2>

<p>(2023-08)</p>

<ul>
  <li>context: I am trying to generate inappropriate images using Stable Diffusion with prompts from the I2P benchmark. However, the NSFW detection in Huggingface is too sensitive and it filters out all of the images, and return a black image instead. Therefore, I need to disable it.</li>
  <li>solution: modify the pipeline_stable_diffusion.py file in the Huggingface library. just return image and None in the run_safety_checker function.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># line 426 in the pipeline_stable_diffusion.py
</span><span class="k">def</span> <span class="nf">run_safety_checker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="bp">None</span>

    <span class="c1"># The following original code will be ignored
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">safety_checker</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">has_nsfw_concept</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">is_tensor</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
            <span class="n">feature_extractor_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_processor</span><span class="p">.</span><span class="nf">postprocess</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="sh">"</span><span class="s">pil</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">feature_extractor_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_processor</span><span class="p">.</span><span class="nf">numpy_to_pil</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">safety_checker_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_extractor</span><span class="p">(</span><span class="n">feature_extractor_input</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">has_nsfw_concept</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">safety_checker</span><span class="p">(</span>
            <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">clip_input</span><span class="o">=</span><span class="n">safety_checker_input</span><span class="p">.</span><span class="n">pixel_values</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">has_nsfw_concept</span>
</code></pre></div></div>

<p>(#Idea, #GenAI, #TML) Completely erase a concept (i.e., NSFW) from latent space of Stable Diffusion.</p>

<ul>
  <li>Problem: Current methods such as ESD (Erasing Concepts from Diffusion Models) can erase quite well a concept from the Stable Diffusion. However, recent work (Circumventing Concept Erasure Methods for Text-to-Image Generative Models) has shown that it is possible to recover the erased concept by using a simple Textual Inversion method.</li>
  <li>Firstly, personally, I think that the approach in Pham et al. (2023) is not very convincing. Because, they need to use additional data (25 samples/concept) to learn a new token associated with the removed concept. So, it is not surprising that they can generate images with the removed concept. It is becaused of the power of the personalized method, not because of the weakness of the ESD method. It would be better if we can compare performance on recovering concept A (concept A is totally new to the base Stable Diffusion model such as your personal images) on two models: a SD model injected with concept A and a model fine-tuned with concept A and then erased concept A and then injected concept A back. If the latter model can not generate images with concept A better than inject concept A directly to the base model, then we can say that the ESD method is effective.</li>
</ul>

<h2 id="helmholtz-visiting-researcher-grant">Helmholtz Visiting Researcher Grant</h2>

<p>(2023-08)</p>

<ul>
  <li>https://www.helmholtz-hida.de/en/new-horizons/hida-visiting-program/</li>
  <li>1-3 months visiting grant for Ph.D. students and postdocs in one of 18 Helmholtz centers in Germany.</li>
  <li>Deadline: 16 August 2023 and will end on 15 October 2023.</li>
  <li>CISPA - Helmholtz Center for Information Security https://cispa.de/en/people</li>
</ul>

<h2 id="where-to-find-potential-collaborators-or-postdoc-positions">Where to find potential collaborators or postdoc positions</h2>

<p>(2023-08)</p>

<p>Each year, the Australian Research Council releases the outcomes of funded/accepted projects from leading researchers and professors across Australian Universities. This information can be a great resource for finding collaborations, PhD positions, and research job opportunities.</p>

<p>For example, if you’re interested in the topic of Trust and Safety in Machine Learning, you can find several professors who have recently received funding to work on related topics.</p>

<p>Link to the ARC data: <a href="https://lnkd.in/gge2FJR3" rel="external nofollow noopener" target="_blank">https://lnkd.in/gge2FJR3</a></p>

<h2 id="micromouse-competition">Micromouse Competition</h2>

<p>(2023-07)</p>

<ul>
  <li>First introduced by Claude Shannon in 1950s.</li>
  <li>At the begining, it was just a simple maze solving competition. However, after 50 years of growing and competing, it has become a very competitive competition with many different categories: speed, efficiency, size. And along with its, many great ideas have been introduced and applied to the competition. It involes many different fields: mechanical, electrical, software, and AI all in just a small robot.</li>
  <li>The Fosbury Flop in high jump. When everyone use the same jump technique, the performance becomes saturated. Then Fosbury introduced a new technique (backward flop) that no one had ever thought of before. And it became the new standard (even named after him). This phenomenon also happens in the Micromouse competition.</li>
  <li>The two most important game changing ideas in the history of micromouse competition: capability to diagonal movement and using fan (vacumn) to suck the mouse to the path so that the mouse can move faster as in a racing car.</li>
</ul>

<p>Reference:</p>

<ul>
  <li><a href="https://youtu.be/ZMQbHMgK2rw" rel="external nofollow noopener" target="_blank">The Fastest Maze-Solving Competition On Earth by Veritasium.</a></li>
  <li><a href="https://invention.si.edu/fosbury-flop-game-changing-technique" rel="external nofollow noopener" target="_blank">The Fosbury Flop—A Game-Changing Technique</a></li>
</ul>

<h2 id="trustworthy-machine-learning">Trustworthy Machine Learning</h2>

<p><strong>Who behind the wheel?</strong></p>

<p><strong>The wheel</strong> here represents <em>decision-making</em>.<br>
The closest analogy is Tesla’s <strong>autopilot system</strong> — a machine that takes control of the car. But decision-making driven by AI doesn’t stop at self-driving cars. In many cases, we think humans are still steering, when in fact, AI and machine learning systems are silently guiding our choices.</p>

<p>Examples are everywhere in daily life:</p>

<ul>
  <li>
<strong>Shopping on Amazon</strong>: We only see products that the recommendation system decides to show us.</li>
  <li>
<strong>Searching on Google</strong>: What we see is filtered, ranked, and prioritized by an algorithm.</li>
  <li>
<strong>Navigating from A to B</strong>: The route we take is suggested by a GPS system trained on massive datasets.</li>
</ul>

<p>In short, AI is no longer just assisting — it’s shaping our decisions. Only a few domains, such as a doctor’s diagnosis or a judge’s ruling, still appear fully under human control. But even those are slowly integrating AI support. As time passes, more decisions will be made — or at least strongly influenced — by machines.</p>

<p><strong>If AI Is Behind the Wheel, Can We Trust It?</strong></p>

<p>Today, the most advanced autonomous vehicles are still <strong>Level 2</strong> on the automation scale — meaning a human must remain alert and ready to take over. Likewise, doctors still make the final call even when aided by diagnostic AI. In such life-and-death scenarios, <em>trust</em> is crucial.</p>

<p>Yet, for less critical tasks — like buying a product, searching online, or following GPS directions — we already trust AI implicitly. We rarely question the output of these systems. So, why do we trust them?</p>

<ul>
  <li>
<strong>Convenience and benefit</strong>: AI makes our lives easier. We’re often willing to trade privacy for comfort — like when using voice assistants.</li>
  <li>
<strong>Lack of alternatives</strong>: Paper maps are obsolete. If no one learns to drive manually, future drivers may have no choice but to rely entirely on automation.</li>
  <li>
<strong>Brand reputation</strong>: Systems built by major companies such as Google, Amazon, or Meta appear credible, which fosters trust.</li>
  <li>
<strong>Transparency (or the illusion of it)</strong>: We assume we understand how these systems work — though, in reality, we rarely do.</li>
</ul>

<p>Because of these reasons, users are often <em>forced</em> to trust AI systems. Meanwhile, <strong>developers</strong> have little incentive to make them truly <em>trustworthy</em>.<br>
Building robust and interpretable systems is difficult, expensive, and can even reduce performance on standard benchmarks. This explains why topics like <em>Adversarial Machine Learning</em> remain more academic than industrial.</p>

<p><strong>So, Why Bother Building Trustworthy Systems?</strong></p>

<p>Two major reasons are now pushing the industry toward trustworthiness:</p>

<ol>
  <li>
<strong>Public awareness</strong>: After repeated incidents involving data leaks, bias, or misinformation, users are becoming more conscious of privacy and fairness.</li>
  <li>
<strong>Competition and choice</strong>: As alternatives emerge (e.g., Bing or DuckDuckGo challenging Google), users can choose the platform they trust most — giving companies an incentive to prioritize ethical AI practices.</li>
</ol>

<p>Ultimately, <em>trust</em> becomes a competitive advantage.<br>
So yes, building trustworthy AI is difficult — but it’s becoming essential.</p>

<p><strong>What Is Trustworthy Machine Learning?</strong></p>

<p>What makes a person trustworthy? Even that’s not an easy question to answer — and the same applies to machine learning.</p>

<p>A common way to define <strong>Trustworthy Machine Learning (TML)</strong> is through its <em>core properties</em>:</p>

<ul>
  <li>
<strong>Explainability</strong> – The model’s behavior can be understood and interpreted.</li>
  <li>
<strong>Fairness</strong> – The system avoids bias and discrimination.</li>
  <li>
<strong>Privacy preservation</strong> – Data is handled safely and responsibly.</li>
  <li>
<strong>Causality</strong> – The model’s reasoning reflects cause-and-effect, not mere correlation.</li>
  <li>
<strong>Robustness</strong> – The system maintains reliability under uncertainty or attack.</li>
</ul>

<p>These dimensions form the foundation of the <a href="https://www.trustworthyml.org/home" rel="external nofollow noopener" target="_blank">Trustworthy ML Initiative</a>.</p>

<p>Another perspective comes from <strong>ML safety research</strong>, such as <em>Unsolved Problems in ML Safety</em>, which narrows trustworthiness to four key areas:</p>

<ul>
  <li>
<strong>Robustness</strong> – Stability under distribution shifts or adversarial inputs.</li>
  <li>
<strong>Monitoring</strong> – Detecting when models behave unexpectedly.</li>
  <li>
<strong>Alignment</strong> – Ensuring the model’s goals align with human intent.</li>
  <li>
<strong>Systemic safety</strong> – Preventing large-scale or emergent risks in interconnected systems.</li>
</ul>

<h2 id="safety-checker---the-simple-and-efficient-way-to-deal-with-nsfw-content">Safety Checker - The simple and efficient way to deal with NSFW content</h2>

<ul>
  <li>Topic: Generative Models</li>
  <li>Date: 2024-12-18</li>
  <li>Description:
    <ul>
      <li>I wrote a blog post about the Safety Checker in Stable Diffusion here: <a href="https://tuananhbui89.github.io/blog/2024/safety-checker/">link</a>
</li>
      <li>While Machine Unlearning is a interesting and fascinating research topic, I think from a business perspective, updating the Safety Checker is more practical and useful, when the model is already deployed and we need to deal with the new NSFW queries.</li>
      <li>More specifically, because the (current) Safety Checker is just a alignment model, where we have a pair of text and image encoder to measure the similarity between the key words - that we want to ban/filter - and the generated image. Therefore, it can be updated very easily when we have a new set of key words to ban/filter.</li>
      <li>We can also think about a new research direction starting from this setting: how to make the Safety Checker more efficient and effective.</li>
    </ul>
  </li>
</ul>

<h2 id="football-minimap-prediction">Football Minimap Prediction</h2>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2018</li>
  <li>Description:
    <ul>
      <li>This is very old idea back in 2018 when I was in Singapore and first time explored to GAN. Back then, I was so excited with Pix2Pix model and its ability to learn the transformation from domain A to domain B, and thought that it can be applied to this problem.</li>
    </ul>
  </li>
</ul>

<p><strong>Motivation</strong></p>

<p>Currently, the Football TV audiences do not know location of players who not in the current camera frame. Therefore they might has not the full experience as spectators are watching live on the stadium. We can have some simple solutions for this problem, such as using another camera to shoot he entire football field, or using statistical data from the chip attached to the players. However, from the perspective of computer vision engineer, I propose one more solution for the this problem (might be not a good choice but just for fun:), in which I use the GAN model to create a minimap from a camera frame as in the FIFA or PES football video game.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Football/Full_Frame-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Football/Full_Frame-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Football/Full_Frame-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Football/Full_Frame.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    A sample of a frame from a game in Youtube where the minimap is there for better experience
</div>

<p>In the case of players in the frame, I use the GAN model (Image 2 Image Translation) to learn the transformation from the player’s position in the frame to the position of the player in the minimap.</p>

<p>In the case of players are not in the current frame: I use [?] to predict a current positions of the players from the previous positions and their trajectory (or you might hope GAN as one size fit all model which can learn not only the current frame but also the invisible players)</p>

<p><strong>Data Collection</strong></p>

<p>Because the real matches on TV don’t have a minimap therefore I use the alternative sources there are FIFA18 and PES18 video on Youtube. Then I do some preprocess to collect and clean data.</p>

<p>Step 1: Cut only the frames which have the minimap within. Because in those videos, it’s not only normal frame (which has a minimap) but also spotlight or review or something else. Therefore I have to manually select the period of time in which there are only normal frames. I sample with the sample rate as 2 frames per second</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Football/Full_Minimap-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Football/Full_Minimap-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Football/Full_Minimap-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Football/Full_Minimap.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    A minimap from a full frame
</div>

<p>Step 2: Cut minimap from a full frame and replace it by a random noise window. To avoid overfitting (because full frame also has a minimap) I replace a minimap by a random noise frame.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Football/Frame_with_Noise-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Football/Frame_with_Noise-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Football/Frame_with_Noise-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Football/Frame_with_Noise.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    A frame with a minimap replaced by a random noise
</div>

<p>Step 3: Remove bad samples (Those minimap have overlap by line or player within)</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Football/Bad_example1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Football/Bad_example1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Football/Bad_example1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Football/Bad_example1.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Football/Bad_example2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Football/Bad_example2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Football/Bad_example2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Football/Bad_example2.jpg" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Bad examples: minimap has overlap by line or player within
</div>

<p>After 3 steps as above, I have two sets: The camera frame set (with noised minimap) and the minimap set. I will chose camera frame set as a Source and minimap as a Target for Pix2Pix model. (You can swap 2 source and have enjoy the interesting result, in which we can render a camera frame from a minimap). Then I do a preprocessing to have a better dataset for training.</p>

<p>Because the football video game knows locations of not only players in camera frame but also all of players in the game. Therefore it can create a completed minimap that has the locations of all players. TV audiences who have only camera frame cannot do that. They only infer the position of players who are in camera frame, and cannot infer remaining players. Based on this intuition, I improve the model by doing crop the active window in minimap as follow:</p>

<ul>
  <li>Localize position of the ball (usually has yellow-color in minimap)</li>
  <li>Crop roughly 1/2 width of minimap (1/4 in the left of ball and 1/4 in the right of ball)</li>
  <li>Keep the height</li>
</ul>

<p>I realize that audience change in each frame, and they might made a huge noise to model, which cause training more difficult. Moreover, players is really small in whole frame, and grass is not stable in each frame or each game, therefore, similar to audience, they might lead a huge noise to model. Therefore I design a filter to filter them from a Camera frame using Color Threshold App in Matlab.</p>

<p>Difficulties:</p>

<ul>
  <li>Pix2Pix model has demonstrated its ability to learn the transformation from domain A to domain B as shown in the paper: Day to night, BW to Color, Aerial to Map. However, in those cases, 2 domain are not too much different. In this case, we need a transformation from 2 completely different domains. It also needs a transformation from 2D - 2D matching in abstract level (model need to know each player is correspond to each circle in minimap). Therefore, it will be very challenge to learn</li>
  <li>The difference between a Video Game Frame and a Real Camera Frame.</li>
  <li>Dataset too small and noise.</li>
</ul>

<p>I think this problem is difficult even humans, but it is worth to try and see what the GAN can do.
<strong>Revised in 2025</strong>: I think the idea of generating minimap is still interesting and might be more realistic with the current stage of Generative Models.</p>

<h2 id="some-other-silly-ideas">Some other silly ideas</h2>

<p><strong>Chrome Extension Ideas</strong></p>

<ul>
  <li>Topic: Chrome Extension, Business</li>
  <li>Date: 2025-06-01</li>
  <li>Description:
    <ul>
      <li>I recently learned about how to build a Chrome Extension and found that there are a lot of interesting ideas that can be implemented.</li>
      <li>Idea 1: A Price Tracker, so that users can track the price of a product on (any) e-commerce website. When the price is lower than the user’s desired price, the extension will notify the user.</li>
      <li>Idea 2: A Scratch Copilot</li>
      <li>Idea 3: Arxiv Review and Comment Sharing. Turn out that is Alphaxiv.</li>
    </ul>
  </li>
</ul>

<p><strong>Generating Reading Comprehension Questions for Primary School Students</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2025-03-12</li>
  <li>Description:
    <ul>
      <li>I have a year-3 son and recently, he needs to prepare for his NAPLAN test at school.</li>
      <li>The free NAPLAN practice samples are very limited. Only available from year 2012-2016, that can just be finished in several hours.</li>
      <li>We - A typical Asian family - want my son to practice more and prepare better for his test.</li>
      <li>I think - with the current stage of LLM - we can leverage the model to generate quite a lot of similar questions to practice!</li>
    </ul>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aboutme/2025-03-12-16-42-38-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aboutme/2025-03-12-16-42-38-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aboutme/2025-03-12-16-42-38-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/aboutme/2025-03-12-16-42-38.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    A sample website that provide reading questions
</div>

<p><strong>Generating Coloring Book/Sheet for Kids</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2025-03</li>
  <li>Description:
    <ul>
      <li>Given a picture (e.g., of a kid), generate a personalized coloring book/sheet for the kid with the content from a prompt, personalized with the kid’s face from the picture.</li>
    </ul>
  </li>
</ul>

<p><strong>Coffee Car - Ice Cream Car - A mobile app to track the food truck</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2025-03</li>
  <li>Description:
    <ul>
      <li>My wife told me that at her new company, there is a coffee car usually comes to the company at a specific time of a week to sell coffee. Employees usually need to be informed by HR via email - “The coffee car is coming today, bla bla bla”.</li>
      <li>I think we can have a mobile app that for both sides: the coffee/ice cream truck and the customers.</li>
      <li>The coffee/ice cream truck can post their schedule, menu, and even their real-time location.</li>
      <li>The customers can see the menu, the truck’s schedule, to order and pay for the coffee/ice cream.</li>
      <li>The truck can also send notification to the customers when they arrive at the company.</li>
    </ul>
  </li>
</ul>

<p><strong>Generating Linkedin profile picture with custombadges</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2025-03</li>
  <li>Description:
    <ul>
      <li>Currently, Linkedin provides two types of badges <code class="language-plaintext highlighter-rouge">#OpentoWork</code> and <code class="language-plaintext highlighter-rouge">Hiring</code>.</li>
      <li>But from my perspective, types of badges should be more diverse and more customizable. For example, Phd Students might want <code class="language-plaintext highlighter-rouge">#OpentoIntern</code> while Master Students might want <code class="language-plaintext highlighter-rouge">#SeekingPhDScholarship</code>, etc.</li>
      <li>I think we can have a website that allows users to generate a Linkedin profile picture with custom badges.</li>
    </ul>
  </li>
</ul>

<p><strong>TripleZero - Emergency Simulation for Kids training</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2025-03</li>
  <li>Description:
    <ul>
      <li>A mobile app/game for kids to learn about emergency situations. I found it would be a good idea after my son told me about his first aid training at school.</li>
      <li>We - or the kids - don’t know how to react in an emergency situation.</li>
      <li>The app will simulate a real emergency situation, and the kids will need to make decision to save the people in that situation.</li>
      <li>UI should be similar to Iphone keyboard - but more colorful and cute - to attract kids.</li>
      <li>The app will utilize the OpenAI voice API to respond to the kids’s questions.</li>
    </ul>
  </li>
</ul>

<p><strong>Waiting List - Price Drop Notification</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2019</li>
  <li>Description:
    <ul>
      <li>A Chrome extension that allows users to add an item to a waiting list, an item can be from any website - not just Amazon or other shopping websites - that already has a waiting list feature</li>
      <li>When the price of the item drops, the extension will notify the user.</li>
      <li>The user can set a price drop threshold.</li>
      <li>The idea came after a talk with my wife about her wish to buy a dress but the price was too high and she need to check the website regularly to see if the price has dropped.</li>
    </ul>
  </li>
</ul>

<p><strong>Melbourne Airport - Available Parking Spot</strong></p>

<ul>
  <li>Topic: Business</li>
  <li>Date: 2024</li>
  <li>Description:
    <ul>
      <li>A camera system that can hang on light poles at the parking lot and monitor which parking spot is available.</li>
      <li>There is a screen or light - red or green - to indicate the availability of the parking spot.</li>
      <li>The idea came after I was frustrated to find a parking spot there - It took me more than 15 minutes to find a spot. The Melbourne Airport Value Parking is really big.</li>
      <li>I also found that many people complained about the same problem as me online.</li>
    </ul>
  </li>
</ul>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec06/">MIT 6.S184 - Lecture 6 - Diffusion for Protein Generation</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec05/">MIT 6.S184 - Lecture 5 - Diffusion for Robotics</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec04/">MIT 6.S184 - Lecture 4 - Building an Image Generator</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec03/">MIT 6.S184 - Lecture 3 - Training Flow and Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mit-6s184-lec02/">MIT 6.S184 - Lecture 2 -  Constructing a Training Target</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
