<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Random Thoughts and Notes | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2024/thoughts/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Random Thoughts and Notes</h1>
    <p class="post-meta">November 18, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#openai-email-archives-from-musk-v-altman---game-of-thrones">OpenAI email archives from Musk v Altman - Game of Thrones</a></li>
<li class="toc-entry toc-h2"><a href="#improving-chatgpts-interpretability-with-cross-modal-heatmap">Improving ChatGPT’s interpretability with cross-modal heatmap</a></li>
<li class="toc-entry toc-h2"><a href="#the-prisoners-dilemma">The Prisoner’s Dilemma</a></li>
<li class="toc-entry toc-h2"><a href="#a-new-perspective-on-the-motivation-of-vae">A new perspective on the motivation of VAE</a></li>
<li class="toc-entry toc-h2"><a href="#data-free-knowledge-distillation">Data-Free Knowledge Distillation</a></li>
<li class="toc-entry toc-h2"><a href="#how-to-disable-nsfw-detection-in-huggingface">How to disable NSFW detection in Huggingface</a></li>
<li class="toc-entry toc-h2"><a href="#helmholtz-visiting-researcher-grant">Helmholtz Visiting Researcher Grant</a></li>
<li class="toc-entry toc-h2"><a href="#where-to-find-potential-collaborators-or-postdoc-positions">Where to find potential collaborators or postdoc positions</a></li>
<li class="toc-entry toc-h2"><a href="#micromouse-competition">Micromouse Competition</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <!-- 
## The Unintentional Invention of Concept Graph from Machine Unlearning

One of the most interesting contributions of our NeurIPS paper and its extension (under-review) is the introduction of the concept graph, which represents a generative model's capability as a graph, where nodes are discrete visual concepts, and edges are the relationships between those concepts.

Understanding this graph structure is essential for many tasks, such as:  

- **Machine Unlearning** 🗑️: The goal here is to remove the model's knowledge of certain concepts while retaining its knowledge of others. The concept graph structure helps identify which concepts are critical to the model's performance and should be preserved.  
- **Personalization** 👤: The goal is to personalize the model's knowledge for a specific user. For instance, changing "a photo of a *cat* before Vancouver Convention Center" to "a photo of a *cat* before the user's house." Traditional methods like Dreambooth, which fine-tune the model on a small user-specific dataset, often overfit to the specific concept and degrade the model's general capability. Prior approaches address this by collecting large datasets of **heuristically selected concepts**—e.g., if the personalized concept is "a user's house," the preservation dataset would include a variety of house images. Our concept graph structure can help identify which concepts are specific to the user and should be preserved, improving the balance between personalization and generalization.

While this graph is simple to understand, useful to many tasks, however, it is not trivial to construct.
There are several challenges:

- **What is a concept?** The concept is discrete but infinite. Most concepts are composed of multiple sub-concepts. For example, "a body of human" is composed from multiple body parts, such as "head", "body", "hand", "foot", etc. And each body part can be further decomposed into deeper/finer/granular concepts. In the end, all visual concepts can be decomposed into lines, curves, colors blobs, etc, which are similar to the way of convolutional neural networks. However, representing in that extreme granularity is not practical and not necessary. **How to represent a concept?** Explicitly representing a concept by an embedding vector in a latent space, or implicitly representing a concept by a set of visual examples.
- **What is a relationship?** The intuitive way to represent a relationship is to indicate the impact of a concept on another concept.
- **How to measure the strength of a relationship?** Correlation in the latent space measured by common metrics like cosine similarity might be the most straightforward way to measure its. However, **the problem of correlation** is that it does not work well in continuous space where the discrete concepts lie. For example, as investigated in our NeurIPS paper, by adding a small perturbation to the latent space, the output of the model will be significantly different. Another issue is that

I am saying that the concept graph is an unintentional invention from machine unlearning because turn out that the impact of a concept on another can be measured by the change of the output of the model on that query concept when the other concept has been removed. Measuring in the output space instead of the latent space can be more direct and easier to interpret. However, this method has its own drawbacks. Firstly, in the case of diffusion models, where the output of one concept can be significantly different by just changing the initial noise, therefore, requiring generating a large number of images from the same concept with different initial noise to stabilize the measurement. Secondly, we need to measure the presence of a concept in the generated image, which usually requires a classifier or detector to do so. However, most of the time these classifier are not available. -->

<h2 id="openai-email-archives-from-musk-v-altman---game-of-thrones">OpenAI email archives from Musk v Altman - Game of Thrones</h2>

<p>Reference: <a href="https://www.lesswrong.com/posts/5jjk4CDnj9tA7ugxr/openai-email-archives-from-musk-v-altman" rel="external nofollow noopener" target="_blank">OpenAI email archives from Musk v Altman by LessWrong</a></p>

<p>These emails are part of the ongoing legal disputes between Elon Musk and Sam Altman surrounding recent OpenAI developments. Thanks to this, the public has gained access to email exchanges between some of the most powerful figures in the tech world today, including Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, and Andrew Karpathy.</p>

<p>For me, this has been an eye-opening experience, especially for someone who is still learning about the tech world, Silicon Valley, startups, and entrepreneurship. It can be compared to MIT or Stanford releasing their lectures to the world.</p>

<p>After reading through the content, I think the story can be divided into the following chapters:</p>

<hr>

<p><strong><em>Chapter 1: Origins - The noble idea of AI for everyone</em></strong></p>

<p>The idea began on May 25, 2015, when Sam Altman sent an email to Elon Musk about a concept for a “Manhattan Project for AI” — ensuring that the tech belongs to the world through some sort of nonprofit.<br>
Elon Musk quickly responded, showing enthusiasm for the idea.<br>
Throughout the emails, I noticed Elon Musk repeatedly expressing concern (or even obsession) about Google, DeepMind, and the possibility of Google creating AGI and dominating the world.<br>
From the very first email, Sam Altman, somehow, seemed to understand Elon Musk’s concerns or perhaps shared the same fears. He mentioned the need to “do something to prevent Google from being the first to create AGI,” quickly gaining Elon Musk’s agreement.</p>

<hr>

<p><strong><em>Chapter 2: The first building blocks - Contracts to attract initial talent for OpenAI</em></strong></p>

<p>The next phase focused on drafting contracts (offer letters or compensation frameworks) to attract the first talents to work at OpenAI, discussing “opening paragraphs” for OpenAI’s vision, and even deciding what to say in “a Wired article.”</p>

<p>What I found interesting here were:</p>

<ul>
  <li>How these people communicated via email: direct, straight to the point, and concise.</li>
  <li>The founders’ emphasis on building an excellent founding team and carefully considering contract details.</li>
  <li>Elon Musk’s willingness to personally meet and convince individuals to join OpenAI.</li>
</ul>

<hr>

<p><strong><em>Chapter 3: Conflict - The battle for leadership control</em></strong></p>

<p>Conflict seemed to arise around August 2017 (Shivon Zilis to Elon Musk, cc: Sam Teller, Aug 28, 2017, 12:01 AM), when Greg and Ilya expressed concerns about Elon Musk’s management, such as:</p>

<ul>
  <li>“How much time does Elon want to spend on this, and how much time can he actually afford to spend on this?”</li>
  <li>They were okay with less time/less control or more time/more control, but not less time/more control. Their fear was that without enough time, there wouldn’t be adequate discussion to make informed decisions.</li>
</ul>

<p>Elon responded:</p>
<ul>
  <li>“This is very annoying. Please encourage them to go start a company. I’ve had enough.”</li>
</ul>

<p>The highlight of this chapter might be an email from Ilya Sutskever to Elon Musk, Sam Altman, cc: Greg Brockman, Sam Teller, Shivon Zilis (Sep 20, 2017, 2:08 PM), where Ilya and Greg said:</p>

<ul>
  <li>
    <p>To Elon: “The current structure provides you with a path where you end up with unilateral absolute control over the AGI. You stated that you don’t want to control the final AGI, but during this negotiation, you’ve shown us that absolute control is extremely important to you. The goal of OpenAI is to make the future good and avoid an AGI dictatorship. You are concerned that Demis could create an AGI dictatorship. So do we. Therefore, it’s a bad idea to create a structure where you could become a dictator, especially when we can create a structure that avoids this possibility.”</p>
  </li>
  <li>
    <p>To Sam: “We don’t understand why the CEO title is so important to you. Your stated reasons have changed, and it’s hard to understand what’s driving this. Is AGI truly your primary motivation? How does it connect to your political goals? How has your thought process changed over time?”</p>
  </li>
</ul>

<p>Elon replied:</p>
<ul>
  <li>“Guys, I’ve had enough. This is the final straw. Either go do something on your own or continue with OpenAI as a nonprofit. I will no longer fund OpenAI until you have made a firm commitment to stay, or I’m just being a fool who is essentially providing free funding for you to create a startup. Discussions are over.”</li>
</ul>

<hr>

<p><strong><em>Chapter 4: The finale</em></strong></p>

<p>The final email exchanges between Elon and Sam occurred around March 2019. At this time, Sam, now CEO of OpenAI, drafted a plan:</p>

<ul>
  <li>“We’ve created the capped-profit company and raised the first round. We did this in a way where all investors are clear that they should never expect a profit.</li>
  <li>We made Greg chairman and me CEO of the new entity.</li>
  <li>Speaking of the last point, we are now discussing a multi-billion dollar investment, which I would like your advice on when you have time.”</li>
</ul>

<p>Elon replied, once again making it clear that he had no interest in OpenAI becoming a for-profit company.</p>

<hr>

<h2 id="improving-chatgpts-interpretability-with-cross-modal-heatmap">Improving ChatGPT’s interpretability with cross-modal heatmap</h2>

<p>(2024-11)</p>

<p>I tried a simple experiment—took a snapshot of a single cell in a Sudoku puzzle (a 3x3 grid with digits 1 to 9) and asked ChatGPT to find the location of a specific number in the grid.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-11-18/sudoku-question-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-11-18/sudoku-question-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-11-18/sudoku-question-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-11-18/sudoku-question.png" class="img-fluid rounded z-depth-1" width="300" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Sudoku question
</div>

<p>As shown in the picture, ChatGPT seemed to handle the question just fine! But as soon as I upped the challenge level, it started to show its infamous hallucination problem :D</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-11-18/sudoku-chatgpt-answer-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-11-18/sudoku-chatgpt-answer-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-11-18/sudoku-chatgpt-answer-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-11-18/sudoku-chatgpt-answer.png" class="img-fluid rounded z-depth-1" width="300" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    Failed answer
</div>

<p>So, how can we improve this?</p>

<p>One idea: applying techniques like <a href="https://github.com/castorini/daam" rel="external nofollow noopener" target="_blank">DAAM</a> to create a cross-modal heatmap (example attached) could help provide a rough idea of where each visual-text pair is mapped. By using this data to fine-tune the model, could we boost its interpretability?</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://media.licdn.com/dms/image/v2/D5622AQGFHx5WfAVXVg/feedshare-shrink_1280/feedshare-shrink_1280/0/1730434983661?e=1734566400&amp;v=beta&amp;t=em_hOf6h30DtmilPv3_LzrrlGMA90l4NXLB1Kwul1Qk" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    DAAM example
</div>

<p>Update: It’s my mistake for not instructing ChatGPT properly :D</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://media.licdn.com/dms/image/v2/D562CAQGE7lw9jW2oqg/comment-image-shrink_8192_1280/comment-image-shrink_8192_1280/0/1730446203816?e=1732489200&amp;v=beta&amp;t=ARa8dqrE5VvConuxvT_g9XjYB6rOJbg8jak5AQW1Mq8" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption text-center">
    ChatGPT's correct answer with proper instruction
</div>

<h2 id="the-prisoners-dilemma">The Prisoner’s Dilemma</h2>

<p>(2024-09)</p>

<p>Imagine a game between two players, A and B, competing for a prize of 1 million dollars from a bank. They are asked to choose either “Split” or “Take All” the prize. If both choose “Split,” they each receive $500,000. If one chooses “Split” and the other chooses “Take All,” the one who chooses “Take All” wins the entire prize. If both choose “Take All,” they both lose and get nothing. They can’t communicate with each other and must decide whether to trust/cooperate.</p>

<p>This is the Prisoner’s Dilemma, one of the most famous problems in Game Theory. In this scenario, when the game is played only once, the best strategy for each person is not to cooperate. However, in real life, many situations are not zero-sum games, where only one can win. Instead, all parties can win and benefit from a shared bank, our world.</p>

<p>And the best strategy to win in life is to cooperate with others, or as summarized in the video: be nice and forgiving, but don’t be a pushover or too nice so others can take advantage of you.</p>

<div class="text-center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/mScpHTIi-kM?si=HE_ypfH1FhfGBSJN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<h2 id="a-new-perspective-on-the-motivation-of-vae">A new perspective on the motivation of VAE</h2>

<p>(2023-09)</p>

<ul>
  <li>Assume that \(x\) was generated from \(z\) through a generative process \(p(x \mid z)\).</li>
  <li>Before observing \(x\), we have a prior belief about \(z\), i.e., \(z\) can be sampled from a Gaussian distribution \(p(z) = \mathcal{N}(0, I)\).</li>
  <li>After observing \(x\), we want to correct our prior belief about \(z\) to a posterior belief \(p(z \mid x)\).</li>
  <li>However, we cannot directly compute \(p(z \mid x)\) because it is intractable. Therefore, we use a variational distribution \(q(z \mid x)\) to approximate \(p(z \mid x)\). The variational distribution \(q(z \mid x)\) is parameterized by an encoder \(e(z \mid x)\). The encoder \(e(z \mid x)\) is trained to minimize the KL divergence between \(q(z \mid x)\) and \(p(z \mid x)\). This is the motivation of VAE.</li>
</ul>

<p>Mathematically, we want to minimize the KL divergence between \(q_{\theta} (z \mid x)\) and \(p(z \mid x)\):</p>

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log \frac{q_{\theta} (z \mid x)}{p(z \mid x)} \right] = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(z \mid x) \right]\]

<p>Applying Bayes rule, we have:</p>

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(x \mid z) - \log p(z) + \log p(x) \right]\]

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log q_{\theta} (z \mid x) - \log p(x \mid z) - \log p(z) \right] + \log p(x)\]

\[\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) ) = - \mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log p(x \mid z) \right] + \mathcal{D}_{KL} \left[ q_{\theta} (z \mid x) \parallel p(z) \right] + \log p(x)\]

<p>So, minimizing \(\mathcal{D}_{KL} (q_{\theta} (z \mid x) \parallel p(z \mid x) )\) is equivalent to maximizing the ELBO: \(\mathbb{E}_{q_{\theta} (z \mid x)} \left[ \log p(x \mid z) \right] - \mathcal{D}_{KL} \left[ q_{\theta} (z \mid x) \parallel p(z) \right]\).</p>

<p>Another perspective on the motivation of VAE can be seen from the development of the Auto Encoder (AE) model.</p>

<ul>
  <li>The AE model is trained to minimize the reconstruction error between the input \(x\) and the output \(\hat{x}\).</li>
  <li>The AE process is deterministic, i.e., given \(x\), the output \(\hat{x}\) is always the same.</li>
  <li>Therefore, the AE model does not have contiguity and completeness properties as desired in a generative model.</li>
  <li>To solve this problem, we change the deterministic encoder of the AE model to a stochastic encoder, i.e., instead of mapping \(x\) to a single point \(z\), the encoder maps \(x\) to a distribution \(q_{\theta} (z \mid x)\). This distribution should be close to the prior distribution \(p(z)\). This is the motivation of VAE.</li>
</ul>

<h2 id="data-free-knowledge-distillation">Data-Free Knowledge Distillation</h2>

<p>(2023-08)</p>

<ul>
  <li>Reference: <a href="https://arxiv.org/abs/2011.14779" rel="external nofollow noopener" target="_blank">Data-Free Model Extraction</a>
</li>
  <li>What is Data-Free KD? It is a method to transfer knowledge from a teacher model to a student model without using any data. The idea is learn a generator that can generate synthetic data that is similar to the data from the teacher model. Then, we can use the synthetic data to train the student model.
\(L_S = L_{KL} (T(\hat{x}), S(\hat{x}))\)</li>
</ul>

<p>Where \(T(\hat{x})\) is the teacher model and \(S(\hat{x})\) is the student model. \(\hat{x}\) is the synthetic data generated by generator \(G\).</p>

\[L_G = L_{CE} (T(\hat{x}), y) - L_{KL} (T(\hat{x}), S(\hat{x}))\]

<p>Where \(y\) is the label of the synthetic data. Minimizing first term encourages the generator generate data that fall into the target class \(y\), while maximizing the second term encourages the generator generate diverse data? 
Compared to GAN, we can think both teacher and student models are acted as discriminators.</p>

<p>This adversarial game need to intergrate to the training process in each iteration. For example, after each iteration, you need to minimizing \(L_G\) to generate a new synthetic data. And then using \(\hat{x}\) to train the student. This is to ensure that the synthetic data is new to the student model.
Therefore, one of the drawbacks of DFKD is that it is very slow.</p>

<p>Tuan (Henry)’ work on improving Data-Free KD:</p>

<ul>
  <li>Introducing noisy layer which is a linear layer that transforms the input (label-text embedding vector from CLIP) before feeding to the generator as previous work. (Input -&gt; Noisy Layer -&gt; Generator -&gt; Teacher/Student -&gt; \(L_G\)).</li>
  <li>One important point is that the Noisy layer need to reset its weight every time we generate a new batch of synthetic data (while fixing the generator). This is to ensure the diversity of the synthetic data.</li>
  <li>One interesting finding is that the noisy layer can be applied to all kinds of label-text embedding from different classes, while if using individual noise layers for each class, the performance is worse.</li>
</ul>

<h2 id="how-to-disable-nsfw-detection-in-huggingface">How to disable NSFW detection in Huggingface</h2>

<p>(2023-08)</p>

<ul>
  <li>context: I am trying to generate inappropriate images using Stable Diffusion with prompts from the I2P benchmark. However, the NSFW detection in Huggingface is too sensitive and it filters out all of the images, and return a black image instead. Therefore, I need to disable it.</li>
  <li>solution: modify the pipeline_stable_diffusion.py file in the Huggingface library. just return image and None in the run_safety_checker function.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># line 426 in the pipeline_stable_diffusion.py
</span><span class="k">def</span> <span class="nf">run_safety_checker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="bp">None</span>

    <span class="c1"># The following original code will be ignored
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">safety_checker</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">has_nsfw_concept</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">is_tensor</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
            <span class="n">feature_extractor_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_processor</span><span class="p">.</span><span class="nf">postprocess</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="sh">"</span><span class="s">pil</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">feature_extractor_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_processor</span><span class="p">.</span><span class="nf">numpy_to_pil</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">safety_checker_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_extractor</span><span class="p">(</span><span class="n">feature_extractor_input</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">has_nsfw_concept</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">safety_checker</span><span class="p">(</span>
            <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">clip_input</span><span class="o">=</span><span class="n">safety_checker_input</span><span class="p">.</span><span class="n">pixel_values</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">has_nsfw_concept</span>
</code></pre></div></div>

<p>(#Idea, #GenAI, #TML) Completely erase a concept (i.e., NSFW) from latent space of Stable Diffusion.</p>

<ul>
  <li>Problem: Current methods such as ESD (Erasing Concepts from Diffusion Models) can erase quite well a concept from the Stable Diffusion. However, recent work (Circumventing Concept Erasure Methods for Text-to-Image Generative Models) has shown that it is possible to recover the erased concept by using a simple Textual Inversion method.</li>
  <li>Firstly, personally, I think that the approach in Pham et al. (2023) is not very convincing. Because, they need to use additional data (25 samples/concept) to learn a new token associated with the removed concept. So, it is not surprising that they can generate images with the removed concept. It is becaused of the power of the personalized method, not because of the weakness of the ESD method. It would be better if we can compare performance on recovering concept A (concept A is totally new to the base Stable Diffusion model such as your personal images) on two models: a SD model injected with concept A and a model fine-tuned with concept A and then erased concept A and then injected concept A back. If the latter model can not generate images with concept A better than inject concept A directly to the base model, then we can say that the ESD method is effective.</li>
</ul>

<h2 id="helmholtz-visiting-researcher-grant">Helmholtz Visiting Researcher Grant</h2>

<p>(2023-08)</p>

<ul>
  <li>https://www.helmholtz-hida.de/en/new-horizons/hida-visiting-program/</li>
  <li>1-3 months visiting grant for Ph.D. students and postdocs in one of 18 Helmholtz centers in Germany.</li>
  <li>Deadline: 16 August 2023 and will end on 15 October 2023.</li>
  <li>CISPA - Helmholtz Center for Information Security https://cispa.de/en/people</li>
</ul>

<h2 id="where-to-find-potential-collaborators-or-postdoc-positions">Where to find potential collaborators or postdoc positions</h2>

<p>(2023-08)</p>

<p>Each year, the Australian Research Council releases the outcomes of funded/accepted projects from leading researchers and professors across Australian Universities. This information can be a great resource for finding collaborations, PhD positions, and research job opportunities.</p>

<p>For example, if you’re interested in the topic of Trust and Safety in Machine Learning, you can find several professors who have recently received funding to work on related topics.</p>

<p>Link to the ARC data: <a href="https://lnkd.in/gge2FJR3" rel="external nofollow noopener" target="_blank">https://lnkd.in/gge2FJR3</a></p>

<h2 id="micromouse-competition">Micromouse Competition</h2>

<p>(2023-07)</p>

<ul>
  <li>First introduced by Claude Shannon in 1950s.</li>
  <li>At the begining, it was just a simple maze solving competition. However, after 50 years of growing and competing, it has become a very competitive competition with many different categories: speed, efficiency, size. And along with its, many great ideas have been introduced and applied to the competition. It involes many different fields: mechanical, electrical, software, and AI all in just a small robot.</li>
  <li>The Fosbury Flop in high jump. When everyone use the same jump technique, the performance becomes saturated. Then Fosbury introduced a new technique (backward flop) that no one had ever thought of before. And it became the new standard (even named after him). This phenomenon also happens in the Micromouse competition.</li>
  <li>The two most important game changing ideas in the history of micromouse competition: capability to diagonal movement and using fan (vacumn) to suck the mouse to the path so that the mouse can move faster as in a racing car.</li>
</ul>

<p>Reference:</p>

<ul>
  <li><a href="https://youtu.be/ZMQbHMgK2rw" rel="external nofollow noopener" target="_blank">The Fastest Maze-Solving Competition On Earth by Veritasium.</a></li>
  <li><a href="https://invention.si.edu/fosbury-flop-game-changing-technique" rel="external nofollow noopener" target="_blank">The Fosbury Flop—A Game-Changing Technique</a></li>
</ul>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/watermark-diffusion/">Tree-Ring Watermarks - Fingerprints for Diffusion Images that are Invisible and Robust</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/sharpness/">Connection between Flatness and Generalization</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/fairness-irt/">Comprehensive Algorithm Portfolio Evaluation using Item Response Theory</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
