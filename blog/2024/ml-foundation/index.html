<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Foundations of Machine Learning | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="http://localhost:4000/blog/2024/ml-foundation/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Foundations of Machine Learning</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <h2 id="most-common-machine-learning-interview-questions">Most common machine learning interview questions</h2>

<p><strong>What is logistic regression?</strong></p>

<p>Logistic regression is a statistical model that uses a logistic function to model the probability of a binary response based on one or more predictor variables. It is used for binary classification problems.</p>

<p>Mathematically, the logistic function is defined as:</p>

\[P(Y=1|X) = \text{sigmoid} (W X + b)\]

<p>Where:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$P(Y=1</td>
          <td>X)$$ is the probability of the response variable Y being 1 given the input features X.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>\(\text{sigmoid} (z) = \frac{1}{1 + e^{-z}}\) is the sigmoid function, which maps any real-valued number to the range [0, 1].</li>
  <li>\(W\) is the weight matrix and \(b\) is the bias vector. \(X\) has dimension \(n \times m\), where \(n\) is the number of features and \(m\) is the number of instances. \(W\) has dimension \(1 \times n\) and \(b\) has dimension \(1 \times m\).</li>
</ul>

<p><strong>What is the difference between supervised and unsupervised learning?</strong></p>

<p>Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning that the input data is paired with the correct output. The model learns to make predictions based on this labeled data. Unsupervised learning, on the other hand, is a type of machine learning where the model is trained on an unlabeled dataset, meaning that the input data is not paired with the correct output. The model learns to find patterns and structure in the data without explicit guidance.</p>

<p><strong>What is overfitting and how can it be prevented?</strong></p>

<p>Overfitting occurs when a model learns the training data too well, to the point that it performs poorly on new, unseen data. Overfitting can be prevented by using techniques such as cross-validation, regularization, and early stopping.</p>

<p><strong>What is the bias-variance tradeoff?</strong></p>

<p>The bias-variance tradeoff is a fundamental concept in machine learning that refers to the balance between bias and variance in a model. Bias refers to the error introduced by approximating a real-world problem with a simplified model, while variance refers to the error introduced by the model’s sensitivity to small fluctuations in the training data. The tradeoff arises because reducing bias typically increases variance, and vice versa.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Bias-variance tradeoff
</div>

<p>It is worth noting that in the image above, the variance is the spread of the model’s predictions around the mean, not the error on the validation set as we usually see in DL tutorials. The generalization error is the sum of the bias, variance, and irreducible error.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-4-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-4-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-4-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-4.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Validation error
</div>

<p>Mathematically, the generalization error can be decomposed into three components:</p>

\[\text{Generalization error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible error}\]

\[\text{Bias} = \mathbb{E}[\hat{f}(x)] - f(x)\]

\[\text{Variance} = \mathbb{E}[\hat{f}(x) - \mathbb{E}[\hat{f}(x)]]^2\]

<p>Reference: https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54</p>

<p><strong>What is the difference between classification and regression?</strong></p>

<p>Classification is a type of supervised learning where the goal is to predict a discrete class label, such as “spam” or “not spam.” Regression, on the other hand, is a type of supervised learning where the goal is to predict a continuous value, such as a person’s age or the price of a house.</p>

<p><strong>What is the difference between precision and recall?</strong></p>

<p>Precision is the ratio of true positive predictions to the total number of positive predictions, while recall is the ratio of true positive predictions to the total number of actual positive instances. Precision measures the accuracy of the positive predictions, while recall measures the ability of the model to find all the positive instances.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-1.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Precision and recall
</div>

<p><strong>What is ROC curve?</strong></p>

<p>The ROC curve is a graphical representation of the tradeoff between the true positive rate (TPR) and the false positive rate (FPR) for a binary classification model. It is used to evaluate the performance of the model and to choose the optimal threshold for making predictions. Each point of the ROC curve represents a different threshold, and the area under the curve (AUC) is a measure of the model’s performance. The optimal threshold is the one that maximizes the TPR and minimizes the FPR. The best model is the one that has the highest AUC.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-2.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    ROC curve
</div>

<p><strong>What is the difference between bagging and boosting?</strong></p>

<p>Bagging and boosting are two ensemble learning techniques that combine multiple models to improve the overall performance. Bagging, or bootstrap aggregating, involves training multiple models on different subsets of the training data and then combining their predictions. Boosting, on the other hand, involves training multiple models sequentially, with each model focusing on the instances that were misclassified by the previous models. The main difference between bagging and boosting is the way the models are combined and the focus of each model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-3.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Bagging and Boosting
</div>

<p>Reference: https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422</p>

<p><strong>What is the difference between a generative model and a discriminative model?</strong></p>

<p>A generative model is a type of model that learns the joint probability distribution of the input features and the output labels, while a discriminative model is a type of model that learns the conditional probability distribution of the output labels given the input features. In other words, a generative model learns to generate new data, while a discriminative model learns to discriminate between different classes.</p>

<p>Mathematical explanation:</p>

<ul>
  <li>Generative model: \(P(X, Y)\) where \(X\) is the input features and \(Y\) is the output labels. \(P(X, Y)\) is the joint probability distribution of X and Y.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Discriminative model: $$P(Y</td>
          <td>X)\(where\)X\(is the input features and\)Y\(is the output labels.\)P(Y</td>
          <td>X)$$ is the conditional probability distribution of Y given X.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Example: Naive Bayes is a generative model, while logistic regression is a discriminative model.</li>
</ul>

<p><strong>What is the difference between L1 and L2 regularization?</strong></p>

<p>L1 and L2 regularization are two common techniques used to prevent overfitting in machine learning models. L1 regularization adds a penalty to the model’s loss function based on the absolute value of the model’s weights, while L2 regularization adds a penalty based on the squared value of the model’s weights. The main difference between L1 and L2 regularization is the type of penalty they impose on the model’s weights and the effect on the model’s sparsity.</p>

<p>L1 regularization:</p>

<ul>
  <li>Adds a penalty based on the absolute value of the model’s weights.</li>
  <li>Encourages sparsity in the model’s weights, meaning that many of the weights will be set to zero.</li>
  <li>Useful for feature selection and reducing the number of features in the model.</li>
  <li>Example: Lasso regression.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Loss function: $$\mathcal{L}(\theta) + \lambda \sum_{i=1}^{n}</td>
          <td>\theta_i</td>
          <td>$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Gradient: \(\nabla_{\theta} \mathcal{L}(\theta) + \lambda \text{sign}(\theta)\)</li>
  <li>Update rule: \(\theta_{t+1} = \theta_t - \alpha \nabla_{\theta} \mathcal{L}(\theta) - \alpha \lambda \text{sign}(\theta)\)</li>
</ul>

<p>L2 regularization:</p>

<ul>
  <li>Adds a penalty based on the squared value of the model’s weights.</li>
  <li>Encourages small weights in the model, but does not force them to be exactly zero.</li>
  <li>Useful for preventing overfitting and improving the generalization of the model.</li>
  <li>Example: Ridge regression.</li>
  <li>Loss function: \(\mathcal{L}(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2\)</li>
  <li>Gradient: \(\nabla_{\theta} \mathcal{L}(\theta) + 2 \lambda \theta\)</li>
  <li>Update rule: \(\theta_{t+1} = \theta_t - \alpha \nabla_{\theta} \mathcal{L}(\theta) - \alpha 2 \lambda \theta\)</li>
  <li>Note: The update rule for L2 regularization is also known as weight decay.</li>
</ul>

<p><strong>What is the difference between a hyperparameter and a parameter?</strong></p>

<p>A hyperparameter is a configuration setting for a machine learning model that is set before the model is trained, while a parameter is a variable that is learned by the model during training. Hyperparameters are used to control the learning process and the structure of the model, while parameters are used to make predictions based on the input data.</p>

<p>Examples of hyperparameters:</p>

<ul>
  <li>Learning rate</li>
  <li>Number of hidden layers</li>
  <li>Number of neurons in each layer</li>
  <li>Regularization strength</li>
  <li>Batch size</li>
</ul>

<p><strong>What is non-parametric machine learning?</strong></p>

<p>Non-parametric machine learning refers to a class of machine learning models that do not make strong assumptions about the functional form of the underlying data distribution. Instead of estimating a fixed number of parameters, non-parametric models use the data to determine the number of parameters needed to represent the data. Non-parametric models are often more flexible and can capture complex patterns in the data, but they may require more data and computational resources.</p>

<p>On the other hand, parametric machine learning refers to a class of machine learning models that make strong assumptions about the functional form of the underlying data distribution. “A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.” (Artificial Intelligence: A Modern Approach, page 737).</p>

<p>Examples of non-parametric machine learning models:</p>

<ul>
  <li>k-nearest neighbors (KNN)</li>
  <li>Decision trees</li>
  <li>Support vector machines (SVM)</li>
</ul>

<p>Examples of parametric machine learning models:</p>

<ul>
  <li>Linear regression</li>
  <li>Neural networks</li>
</ul>

<p><strong>What is k-nearest neighbors (KNN)?</strong></p>

<p>K-nearest neighbors (KNN) is a simple and intuitive machine learning algorithm that is used for both classification and regression tasks. In KNN, the output of a new instance is predicted based on the majority class of its k-nearest neighbors. The distance between instances is typically measured using Euclidean distance, but other distance metrics can also be used.</p>

<p>Pseudocode for KNN:</p>

<ol>
  <li>For each instance in the training data, calculate the distance between the new instance and the training instance.</li>
  <li>Select the k-nearest neighbors based on the distance metric.</li>
  <li>For classification tasks, predict the majority class of the k-nearest neighbors. For regression tasks, predict the average value of the k-nearest neighbors.</li>
  <li>Return the predicted output.</li>
</ol>

<p>The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning.</p>

<p><strong>What is k-means clustering?</strong></p>

<p>K-means clustering is a popular unsupervised machine learning algorithm that is used to partition a dataset into k clusters. The algorithm works by iteratively assigning instances to the nearest cluster center and updating the cluster centers based on the mean of the instances in each cluster. The goal of k-means clustering is to minimize the sum of squared distances between instances and their respective cluster centers.</p>

<p>Pseudocode for k-means clustering:</p>

<ol>
  <li>Initialize k cluster centers randomly or using a heuristic.</li>
  <li>Assign each instance to the nearest cluster center.</li>
  <li>Update the cluster centers based on the mean of the instances in each cluster.</li>
  <li>Repeat steps 2 and 3 until convergence.</li>
  <li>Return the final cluster assignments and cluster centers.</li>
</ol>

<p>K-means clustering is sensitive to the initial cluster centers and can converge to a local minimum. To mitigate this issue, the algorithm is often run multiple times with different initializations, and the best clustering is selected based on a predefined criterion.</p>

<p>Applications of k-means clustering include customer segmentation, image compression, and anomaly detection.</p>

<p><strong>What is Bayes’ theorem?</strong></p>

<p>Bayes’ theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It is named after the Reverend Thomas Bayes, who first formulated the theorem. Bayes’ theorem is used to update the probability of an event based on new evidence or information.</p>

<p>Mathematically, Bayes’ theorem is expressed as:</p>

\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]

<p>Where:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$P(A</td>
          <td>B)$$ is the probability of event A given event B, also known as the posterior probability. This is the probability of event A after considering new evidence.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$P(B</td>
          <td>A)$$ is the probability of event B given event A, also known as the likelihood. This is the probability of event B given that event A has occurred.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>\(P(A)\) is the prior probability of event A, which is the probability of event A before considering any new evidence. This is the initial belief about the probability of event A.</li>
  <li>\(P(B)\) is the prior probability of event B, also known as the marginal likelihood.</li>
</ul>

<p><strong>What is the Naive Bayes classifier?</strong></p>

<p>The Naive Bayes classifier is a simple and efficient machine learning algorithm that is based on Bayes’ theorem and the assumption of conditional independence between features. Despite its simplicity, the Naive Bayes classifier is often used as a baseline model for text classification and other tasks.</p>

<p>The Naive Bayes classifier is particularly well-suited for text classification tasks, such as spam detection and sentiment analysis, where the input features are typically word frequencies or presence/absence of words.</p>

<p>Mathematically, the Naive Bayes classifier predicts the class label of an instance based on the maximum a posteriori (MAP) estimation:</p>

\[\hat{y} = \underset{y \in \mathcal{Y}}{\text{argmax}} P(y|X)\]

<p>Where:</p>

<ul>
  <li>\(\hat{y}\) is the predicted class label.</li>
  <li>\(\mathcal{Y}\) is the set of possible class labels.</li>
  <li>\(X\) is the input features.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$P(y</td>
          <td>X)\(is the posterior probability of class label y given the input features X. The posterior probability\)P(y</td>
          <td>X)\(is calculated using Bayes' theorem and the assumption of conditional independence, i.e.,\)P(y</td>
          <td>X) = \frac{P(X</td>
          <td>y)P(y)}{P(X)}\(. The denominator\)P(X)$$ is constant for all class labels and can be ignored for the purpose of classification.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>What is Type I and Type II error?</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-foundation/image-5-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-foundation/image-5-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-foundation/image-5-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2024-foundation/image-5.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Type I and Type II error
</div>

<p>Null hypothesis (H0) is a statement that there is no relationship between two measured phenomena, or no association among groups. It is the default assumption that there is no effect or no difference. The alternative hypothesis (H1) is the statement that there is a relationship between two measured phenomena, or an association among groups. It is the opposite of the null hypothesis.</p>

<p>In the example of Innocent and Guilty, the null hypothesis is “Innocent” and the alternative hypothesis is “Guilty”. Type I error is the incorrect rejection of the null hypothesis (false positive), while Type II error is the failure to reject the null hypothesis when it is false (false negative).</p>

          </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
