<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Connection between Flatness and Generalization | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2024/sharpness/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Connection between Flatness and Generalization</h1>
    <p class="post-meta">July 26, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#does-dnns-generalize-or-memorize">Does DNNs generalize or memorize?</a></li>
<li class="toc-entry toc-h2"><a href="#connection-between-flatness-and-generalization">Connection between Flatness and Generalization</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <p>In this post, I will try to answer the question: “Why does flatness correlate with generalization?” Specifically, we understand that a flat minimum is a solution with a low gradient norm around it, indicating that the loss function is flat (i.e., has a small gradient) with respect to the parameters around this solution. However, generalization is measured concerning the data distribution, not the parameters. So, why does a flat minimum correlate with generalization?</p>

<p>First, let’s clarify some concepts:</p>

<ul>
  <li>
<strong>Flatness</strong> or <strong>Sharpness</strong>: A flat minimum is a solution with a low gradient norm around it, meaning the loss function is flat (small gradient) with respect to the parameters around the solution. The flatness of a minimum can be defined as the ratio of the largest to the smallest eigenvalue of the Hessian matrix at the minimum.</li>
  <li>
<strong>Generalization</strong>: Generalization is the ability of a model to perform well on unseen data. It is important to note that generalization is usually mentioned concerning the data distribution, not the parameters. There are many types of unseen data, the most common being held-out test data, which is drawn from the same distribution as the training data. Other types of unseen data include out-of-distribution (OOD) data and adversarial examples. OOD data is drawn from a different distribution than the training data, for example, a model trained on pictures of cats and dogs might be tested on drawings of animals. Adversarial examples are intentionally or unintentionally perturbed inputs that cause a model to make incorrect predictions. According to [1], there are two types of adversarial examples: off-manifold adversarial examples, generated by adding small perturbations or noise to the input data (e.g., standard gradient-based attacks like PGD), and on-manifold adversarial examples, which are generated by more complex transformations so that they remain within the data distribution.</li>
</ul>

<p>While flatness can be defined mathematically, the definition of generalization is still ambiguous to me.</p>

<h2 id="does-dnns-generalize-or-memorize">Does DNNs generalize or memorize?</h2>

<p>It is well known tha Deep Neural Networks (DNNs) are powerful models that can fit complex functions and perform well on unseen data on wide range of tasks. However, do DNNs really generalize or just memorize the training data? Surprisingly, there are many empirical evidences that show the latter.</p>

<p><strong>DNNs can memorize perfectly</strong></p>

<p>In this seminal paper [2], the authors argue that DNNs so powerful that they just memorize the training data but not generalize.
They did a very interesting experiment to show that DNNs can easily fit the training data perfectly even under extreme scenarios, such as:</p>

<ul>
  <li>Random labels: all the labels are replaced with random ones.</li>
  <li>Random pixels: a different random permutation is applied to each image independently.</li>
</ul>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/memorization-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/memorization-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/memorization-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/memorization.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Evaluation of memorization on CIFAR-10 dataset from [2]. Fig 1a shows the training loss under different scenarios. The red curve shows when the labels are replaced with random ones, which shows that (1) it takes longer time to start converging, (2) once the fitting starts, it converges quickly, and (3) it converges to overfit the training set perfectly. Fig 1c shows the test error w.r.t. the label corruption rate, which shows that the test error increases linearly with the label corruption rate, indicating that the model is not generalizing at all.
</div>

<p><strong>Shortcuts learning</strong></p>

<p>We acknowledge that DNNs can overfit but it is still surprising that they can fit the training data perfectly even under extreme scenarios.
Continuing the intriguing memorization property of DNNs, in [4] the authors claim that yes, DNNs can learn to memorize the training data, but they first tend to exploit the simple patterns in the data first before memorizing the data. They also show that regularization techniques make the model harder to memorize noisy data.
[5] also shows that DNNs tend to learn shortcuts (e.g., easy features or patterns) to solve the task rather than learning robust features (human interpretable features) that generalize well to unseen data.</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/shortcut-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/shortcut-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/shortcut-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/shortcut.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Example of shortcut learning from [5].
</div>

<p>In [3], the authors also claim that DNNs tend to learn features that useful for the task but not necessarily the features that are human-interpretable. To prove this, they generated adversarial examples that are imperceptible to humans but can fool the DNNs, e.g., the image of a dog that is classified as a cat. Then they relabel these adversarial examples to the incorrect labels and retrain the model. Surprisingly, the model can still classify the test data correctly.</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/adv-examples-not-bugs-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/adv-examples-not-bugs-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/adv-examples-not-bugs-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/adv-examples-not-bugs.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Experiment to show that adversarial examples are not bugs but features from [3].
</div>

<p><strong>Information Theory Perspective</strong></p>

<p>[7] brings a beautiful perspective to understand the generalization of DNNs from the information theory perspective. They argue that the generalization of DNNs can be understood by the information bottleneck principle, which states that the representation \(T\) should retain as much information about the input \(X\) as possible while being as informative as possible about the output \(Y\).</p>

<p>The process of information going through the layers of DNNs can be viewed as a Markov chain of information \(X \rightarrow T_1 \rightarrow T_2 \rightarrow \ldots \rightarrow T_k \rightarrow \hat{Y}\), where \(X\) is the input data, \(T_i\) is the representation at layer \(i\), and \(\hat{Y}\) is the output. By the chain rule of mutual information, we have</p>

\[I(X;Y) \geq I(T_1;Y) \geq I(T_2;Y) \geq \ldots \geq I(T_k;Y) \geq I(\hat{Y};Y)\]

<p>which means that the information about the ground truth \(Y\) is decreasing as we go deeper into the network.</p>

\[H(X) \geq I(X;T_1) \geq I(X;T_2) \geq \ldots \geq I(X;T_k) \geq I(X;\hat{Y})\]

<p>The information bottleneck principle [7] states that the representation \(T_i\) should retain as much information about the input \(X\) as possible while being as informative as possible about the output \(Y\).</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/markov-chain-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/markov-chain-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/markov-chain-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/markov-chain.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Illustration of the information chain in DNNs from [7].
</div>

<p>As in [7], the training process of DNNs can be divided into two phases: the fitting (or learning) phase and the forgetting phase. During the fitting phase, the model strives to fit the training data by capturing all available information. This is evidenced by the mutual information \(I(X, T)\) and \(I(T, Y)\) both increasing, indicating that the intermediate representations \(T\) are becoming more informative about the input data \(X\) or the output \(Y\).</p>

<p>In contrast, the forgetting phase involves the model discarding or ignoring irrelevant information that is not useful for the task, while retaining relevant information.
This phase is characterized by a decrease in the mutual information \(I(X, T)\), while \(I(T, Y)\) is maintained.
The model is effectively filtering out irrelevant information to focus on the task at hand.
Again, as discussed above, the useful information is not necessarily the human-interpretable features but the features that are useful for the task.</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/learning-phase-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/learning-phase-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/learning-phase-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/learning-phase.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Illustration of the learning and forgetting phase in DNNs from [7]. Each point represents a layer in the network, the green one is close to the input, and the orange one is close to the output. The left figure shows the Information Plane before training, where the mutual information $I(X, T)$ and $I(T, Y)$ still high at lower layers but very low at higher layers. The middle figure shows the end of fitting phase, where the mutual information $I(X, T)$ and $I(T, Y)$ both increase. The right figure shows the end of the forgetting phase, where the mutual information $I(X, T)$ decreases.
</div>

<p><strong>Connection to Overfitting</strong></p>

<p>As discussed in [7], the fitting phase is much faster than the forgetting phase, which means that the model can fit the training data quickly but it takes longer to forget the irrelevant information.
The forgetting phase is also called as the representation compression phase or encoding phase, where the model compresses the input data into a more compact representation that is relevant to the task.
While the increasing of \(I(T, Y)\) is expected from the cross-entropy loss minimization, the decreasing of \(I(X, T)\) is not trivial. And this is the result of standard SGD training, not a special regularization technique.</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/overfitting-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/overfitting-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/overfitting-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/overfitting.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Illustration of the connection between overfitting and generalization from [7].
</div>

<p>The left figure is the Information Plane of a model trained with a small dataset (5\%), which shows that the information about label \(I(Y,T)\) is significantly reduced during the forgetting phase, indicating the overfitting problem. This problem is not observed in the case of a large dataset (85\%), where the model can still retain the information about the label \(I(Y,T)\) during the forgetting phase.
Note that the information about the input \(I(X,T)\) is still decreasing in both cases, which means that the model is still filtering out irrelevant information,
and the overfitting problem mainly comes from the loss of information about the label \(I(Y,T)\) during the forgetting phase.</p>

<p>Side note: It is a worth-mentioning that the work in [7] is based on an assumption about the Markov chain of information in DNNs, which means that the information at layer \(i\) is only dependent on the information at layer \(i-1\). This assumption may not hold in modern DNNs, where skip connections, residual connections, and other complex architectures are used.</p>

<h2 id="connection-between-flatness-and-generalization">Connection between Flatness and Generalization</h2>

<p>The question about “why does flatness correlate with generalization?” is actually non-trivial than it seems.
Most the examplanation are based on the empirical observations or intuitions [8], rather than a rigorous theoretical proof.</p>

<p>The concept of sharp and flat minimizers have been discussed in the statistics and machine learning literature.
[9] was one of the first to introduce the concept of flat minimizers, which the function varies slowly in a relatively large neighborhood.
A flat minimum corresponds to weights many of which can be given with low precision, e.g., \(w_i = 0.1\) or \(w_i = 0.1001\) are almost equivalent, whereas a sharp minimum requires high precision.
The connection between flat minimal and overfitting can be explained through the lens of the minimum description length (MDL) theory, which suggests that lower complexity models correspond to high generalization performance. Since flat minimizers can be specified with lower precision than to sharp minimizers, they tend to have better generalization performance.
[8] show that large-batch training tends to converge to sharp minimizers, which are associated with poor generalization performance.</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/illustrate-flatness-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/illustrate-flatness-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/illustrate-flatness-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/illustrate-flatness.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Illustration of the connection between flatness and generalization from [8]. A flat minimum is more likely to generalize well to test data because it is less sensitive to small perturbations in the parameters, which can be caused by noise in the training data or the optimization process. Therefore, a small change in the parameters around a flat minimum is less likely to result in a significant change in the loss function.
</div>

<p>[10] proposed a new optimization algorithm called Sharpness-Aware Minimization (SAM) that aims to find flat minimizers by seeking out parameter values whose entire neighborhoods have uniformly low training loss value (equivalently, neighborhoods having both low loss and low curvature). The authors provided a generalization bound based on sharpness:</p>

<p>For any \(\rho&gt;0\) and any distribution \(\mathscr{D}\), with probability \(1-\delta\) over the choice of the training set \(\mathcal{S}\sim \mathscr{D}\),
\(\begin{equation}
    L_\mathscr{D}(\boldsymbol{w}) \leq \max_{\|\boldsymbol{\epsilon}\|_2 \leq \rho} L_\mathcal{S}(\boldsymbol{w} + \boldsymbol{\epsilon}) +\sqrt{\frac{k\log\left(1+\frac{\|\boldsymbol{w}\|_2^2}{\rho^2}\left(1+\sqrt{\frac{\log(n)}{k}}\right)^2\right) + 4\log\frac{n}{\delta} + \tilde{O}(1)}{n-1}}
\end{equation}\)
where \(n=|\mathcal{S}|\), \(k\) is the number of parameters and we assumed \(L_\mathscr{D}(\boldsymbol{w}) \leq \mathbb{E}_{\epsilon_i \sim \mathcal{N}(0,\rho)}[L_\mathscr{D}(\boldsymbol{w}+\boldsymbol{\epsilon})]\).</p>

<p>The bound shows that the generalization error \(L_\mathscr{D}(\boldsymbol{w})\) is upper bounded by the maximum training loss \(L_\mathcal{S}(\boldsymbol{w} + \boldsymbol{\epsilon})\) in a neighborhood of the parameters \(\boldsymbol{w}\).
Therefore, when minimizing the right-hand side of the bound, the algorithm is encouraged to find flat minimas that has lower generalization error \(L_\mathscr{D}(\boldsymbol{w})\).</p>

<p><strong>Controversy</strong></p>

<p>While in some extent, the flatness of the loss function around the minimum can be a good indicator of generalization as shown in series of SAM papers [10], there are also some controversies pointed out the opposite. For example, [11] showed that flatness is sensitive to reparameterization and cannot be used as a reliable indicator of generalization. More specifically, reparameterization is a transformation of the parameters that does not change the function represented by the model, e.g., changing the scale of the weights or changing the way the latent variables are sampled in VAEs. In [11], the authors pointed out that we can reparameterize the model without chaining its outputs while making the sharp minima arbitrarily flat and vice versa.</p>

<p>[12] provided a more intuitive explanation of the disconnection between flatness and generalization. More specifically, if defining the sharpness of the loss function \(L\) as in the SAM paper [10]:</p>

\[\begin{equation}\label{eq:s1}
    \max_{\Vert \boldsymbol{\epsilon} \Vert_{2} \leq \rho}L_S(\boldsymbol{w}+\boldsymbol{\epsilon}) - L_S(\boldsymbol{w}).
\end{equation}\]

<p>As illustrated in the figure below, if we consider the loss function \(L_S(\boldsymbol{w})\) is a convex function of \(\boldsymbol{w}\) with only two parameters \(w_1\) and \(w_2\) so its loss surface can be represented in a 2D space.
Then, if we assume that \(A\) is a scaling operator on the weight space that does not change the loss function, i.e., \(L_S(A\boldsymbol{w}) = L_S(\boldsymbol{w})\), so by varying the scaling factor of the weights, we can have a countour of the loss function that has the same value.
Within this setting, we can see that while having the same loss value, the two model \(\boldsymbol{w}\) and \(A\boldsymbol{w}\) can have arbitrarily different sharpness values as defined in Eq. \eqref{eq:s1}, i.e.,</p>

\[\max_{\Vert \boldsymbol{\epsilon} \Vert_{2} \leq \rho} L_S(\boldsymbol{w}+\boldsymbol{\epsilon}) \neq \max_{\Vert \boldsymbol{\epsilon} \Vert_{2} \leq \rho} L_S( A\boldsymbol{w}+\boldsymbol{\epsilon})\]

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/sphere-eps-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/sphere-eps-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/sphere-eps-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/sphere-eps.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Illustration of the scaling dependency problem of sharpness from [12].
</div>

<p>This means that the flatness of the loss function around the minimum is not necessarily correlated with generalization. And to mitigate this scaling dependency problem, the authors [12] proposed a new concept of adaptive sharpness-aware minimization (ASAM) that adaptively adjusts the sharpness of the loss function to make it invariant to scaling, i.e., instead of considering the sphere neighborhood of the parameters \(\Vert \boldsymbol{\epsilon} \Vert_{2} \leq \rho\) which takes every direction equally, the ASAM considers the ellipsoid neighborhood \(\Vert T^{-1}_\boldsymbol{w} \boldsymbol{\epsilon}\Vert _{p} \leq \rho\) where \(T^{-1}_\boldsymbol{w}\) is a normalization/weighted operator that makes the loss function invariant to scaling.</p>

<div class="text-center mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sharpness/ellipsoid-eps-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sharpness/ellipsoid-eps-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sharpness/ellipsoid-eps-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sharpness/ellipsoid-eps.png" class="img-fluid rounded z-depth-1" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

</div>
<div class="caption text-center">
    Illustration of the ellipsoid neighborhood of the parameters from [12].
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we have discussed the connection between flatness and generalization in DNNs. While flat minimizers are often associated with better generalization performance, there are also some controversies about the reliability of flatness as an indicator of generalization. The flatness of the loss function around the minimum can be a good indicator of generalization, but it is sensitive to reparameterization. More research is needed to better understand the relationship between flatness and generalization in DNNs.</p>

<h2 id="references">References</h2>

<p>[1] Stutz, David, Matthias Hein, and Bernt Schiele. “Disentangling adversarial robustness and generalization.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p>

<p>[2] Zhang, Chiyuan, et al. “Understanding deep learning (still) requires rethinking generalization.” Communications of the ACM 64.3 (2021): 107-115.</p>

<p>[3] Ilyas, Andrew, et al. “Adversarial examples are not bugs, they are features.” Advances in neural information processing systems 32 (2019).</p>

<p>[4] Arpit, Devansh, et al. “A closer look at memorization in deep networks.” International conference on machine learning. PMLR, 2017.</p>

<p>[5] Geirhos, Robert, et al. “Shortcut learning in deep neural networks.” Nature Machine Intelligence 2.11 (2020): 665-673.</p>

<p>[6] <a href="https://youtu.be/pFWiauHOFpY?si=4yyVv6Vu3tAqPIke" rel="external nofollow noopener" target="_blank">‘How neural networks learn’ - Part III: Generalization and Overfitting by Arxiv Insights</a></p>

<p>[7] Shwartz-Ziv, Ravid, and Naftali Tishby. “Opening the black box of deep neural networks via information.” arXiv preprint arXiv:1703.00810 (2017).</p>

<p>[8] Keskar, Nitish Shirish, et al. “On large-batch training for deep learning: Generalization gap and sharp minima.” arXiv preprint arXiv:1609.04836 (2016).</p>

<p>[9] Hochreiter, Sepp, and Jürgen Schmidhuber. “Flat minima.” Neural computation 9.1 (1997): 1-42.</p>

<p>[10] Foret, Pierre, et al. “Sharpness-aware Minimization for Efficiently Improving Generalization.” International Conference on Learning Representations. 2021.</p>

<p>[11] Dinh, Laurent, et al. “Sharp minima can generalize for deep nets.” International Conference on Machine Learning. PMLR, 2017.</p>

<p>[12] Kwon, Jungmin, et al. “Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks.” International Conference on Machine Learning. PMLR, 2021.</p>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">Foundation of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deepseek/">DeepSeek-R1</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
