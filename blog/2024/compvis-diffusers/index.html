<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Comparing Implementations of Diffusion Models - HuggingFace Diffusers vs. CompVis Stable Diffusion | Tuan-Anh  Bui</title>
    <meta name="author" content="Tuan-Anh  Bui">
    <meta name="description" content="Researcher in Generative AI and Trustworthy AI
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://tuananhbui89.github.io/blog/2024/compvis-diffusers/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tuan-Anh </span>Bui</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/"></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->
<!-- Page/Post style -->
<style type="text/css">
  
</style>


<div class="post">

  <header class="post-header">
    <h1 class="post-title">Comparing Implementations of Diffusion Models - HuggingFace Diffusers vs. CompVis Stable Diffusion</h1>
    <p class="post-meta">July 1, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/reading">
          <i class="fas fa-hashtag fa-sm"></i> reading</a>  
          <a href="/blog/tag/tml">
          <i class="fas fa-hashtag fa-sm"></i> tml</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          <a href="/blog/tag/genai">
          <i class="fas fa-hashtag fa-sm"></i> genai</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="table-of-contents">
      <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2">
<a href="#basic-functions">Basic Functions</a>
<ul>
<li class="toc-entry toc-h3"><a href="#diffusers">Diffusers</a></li>
<li class="toc-entry toc-h3"><a href="#compviss-stable-diffusion">CompVis’s Stable Diffusion</a></li>
<li class="toc-entry toc-h3"><a href="#example">Example</a></li>
</ul>
</li>
</ul>
    </div>
    <hr>
    
    <div id="markdown-content">
      <!-- path=assets/img/2024-07-diffusers -->

<p>This post is a note for myself to compare the implementations of diffusion models in HuggingFace’s Diffusers and CompVis’s Stable Diffusion.
I quite often need to switch between these two implementations, so I want to keep track of the differences between them.</p>

<p>The source code of two libraries can be found at:</p>

<ul>
  <li>HuggingFace Diffusers: <a href="https://github.com/huggingface/diffusers" rel="external nofollow noopener" target="_blank">https://github.com/huggingface/diffusers</a>
</li>
  <li>CompVis’s Stable Diffusion: <a href="https://github.com/CompVis/stable-diffusion" rel="external nofollow noopener" target="_blank">https://github.com/CompVis/stable-diffusion</a> and CompVis’s LDM: <a href="https://github.com/CompVis/latent-diffusion" rel="external nofollow noopener" target="_blank">https://github.com/CompVis/latent-diffusion</a>
</li>
</ul>

<h2 id="basic-functions">Basic Functions</h2>

<p>Below are the basic functions of a standard diffusion model pipeline, including:</p>

<ul>
  <li>Loading components such as tokenizer, scheduler, vae, unet.</li>
  <li>Converting images to latent space.</li>
  <li>Forward and backward diffusion process.</li>
  <li>Calculating loss.</li>
</ul>

<p>Note that the code snippets below just refer to specific functions and not meant to be a complete script. Read comments in the code to understand the context.</p>

<h3 id="diffusers">Diffusers</h3>

<p>taken from <code class="language-plaintext highlighter-rouge">train_text_to_image.py</code> in <a href="https://github.com/huggingface/diffusers/blob/7bfc1ee1b2cb0961ff111f50a9d096816e4dd921/examples/text_to_image/train_text_to_image.py" rel="external nofollow noopener" target="_blank">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import the necessary modules
</span><span class="kn">from</span> <span class="n">diffusers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoencoderKL</span><span class="p">,</span>
    <span class="n">DDPMScheduler</span><span class="p">,</span>
    <span class="n">DiffusionPipeline</span><span class="p">,</span>
    <span class="n">DPMSolverMultistepScheduler</span><span class="p">,</span>
    <span class="n">StableDiffusionPipeline</span><span class="p">,</span>
    <span class="n">UNet2DConditionModel</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">diffusers.optimization</span> <span class="kn">import</span> <span class="n">get_scheduler</span>

<span class="c1"># load components of the model
# Load tokenizer
</span><span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">tokenizer_name</span><span class="p">:</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CLIPTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">tokenizer_name</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">args</span><span class="p">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">:</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CLIPTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">tokenizer</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Load scheduler and models
</span><span class="n">noise_scheduler</span> <span class="o">=</span> <span class="n">DDPMScheduler</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">scheduler</span><span class="sh">"</span><span class="p">)</span>
<span class="n">text_encoder</span> <span class="o">=</span> <span class="n">CLIPTextModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">args</span><span class="p">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">text_encoder</span><span class="sh">"</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">revision</span>
<span class="p">)</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">AutoencoderKL</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">vae</span><span class="sh">"</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">revision</span><span class="p">)</span>
<span class="n">unet</span> <span class="o">=</span> <span class="n">UNet2DConditionModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">args</span><span class="p">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">unet</span><span class="sh">"</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">revision</span>
<span class="p">)</span>

<span class="c1"># Inside the training loop
# Convert images to latent space
</span><span class="n">latents</span> <span class="o">=</span> <span class="n">vae</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">pixel_values</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">)).</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">latents</span> <span class="o">=</span> <span class="n">latents</span> <span class="o">*</span> <span class="n">vae</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">scaling_factor</span>

<span class="c1"># Sample noise that we'll add to the latents
</span><span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">latents</span><span class="p">)</span>
<span class="n">bsz</span> <span class="o">=</span> <span class="n">latents</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Sample a random timestep for each image
</span><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">num_train_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">latents</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>

<span class="c1"># Add noise to the latents according to the noise magnitude at each timestep
# (this is the forward diffusion process)
</span><span class="n">noisy_latents</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="nf">add_noise</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>

<span class="c1"># Get the text embedding for conditioning
</span><span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">])[</span><span class="mi">0</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">)</span>

<span class="c1"># Predict the noise residual
</span><span class="n">model_pred</span> <span class="o">=</span> <span class="nf">unet</span><span class="p">(</span><span class="n">noisy_latents</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">).</span><span class="n">sample</span>
</code></pre></div></div>

<p><strong>Conditioning</strong></p>

<p>It is a worth noting that the <code class="language-plaintext highlighter-rouge">encoder_hidden_states</code> size is <code class="language-plaintext highlighter-rouge">(batch_size, sequence_length, hidden_size)</code>, e.g., <code class="language-plaintext highlighter-rouge">(4, 77, 768)</code> for a batch of 4 prompts with max length 77.</p>

<p>It means that the cross-attention layer in the UNet takes the entire sequence as the conditioning rather than just the last token. Confirmed by the code in the <code class="language-plaintext highlighter-rouge">attention.py</code> file in the <code class="language-plaintext highlighter-rouge">ldm</code> library. I let the code below for reading pleasure:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">heads</span>

    <span class="sh">"""</span><span class="s">
    x: [b, hw, dx] i.e., [2, 4096, 320] where hw is the input size, dx is input dimension/depth
    context: [b, m, dc] i.e., [2, 77, 768] where m is the sequence length, dc is the context dimension
    prompt: None
    </span><span class="sh">"""</span>

    <span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># [2, 4096, 320]
</span>    <span class="n">new_context</span> <span class="o">=</span> <span class="nf">default</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_k</span><span class="p">(</span><span class="n">new_context</span><span class="p">)</span> <span class="c1"># [2, 77, 320]
</span>    <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_v</span><span class="p">(</span><span class="n">new_context</span><span class="p">)</span> <span class="c1"># [2, 77, 320]
</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="sh">'</span><span class="s">b n (h d) -&gt; (b h) n d</span><span class="sh">'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">),</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span> 
    <span class="c1"># after rearrange: query: [16, 4096, 40], key: [16, 77, 40], value: [16, 77, 40], where h=8 is number of heads
</span>
    <span class="n">sim</span> <span class="o">=</span> <span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">b i d, b j d -&gt; b i j</span><span class="sh">'</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="c1"># [16, 4096, 77]
</span>
    <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="sh">'</span><span class="s">b ... -&gt; b (...)</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">max_neg_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">finfo</span><span class="p">(</span><span class="n">sim</span><span class="p">.</span><span class="n">dtype</span><span class="p">).</span><span class="nb">max</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="sh">'</span><span class="s">b j -&gt; (b h) () j</span><span class="sh">'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">)</span>
        <span class="n">sim</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="n">max_neg_value</span><span class="p">)</span>

    <span class="c1"># attention, what we cannot get enough of
</span>    <span class="n">attn</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [16, 4096, 77]
</span>    <span class="n">out</span> <span class="o">=</span> <span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">b i j, b j d -&gt; b i d</span><span class="sh">'</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="c1"># [16, 4096, 40]
</span>    <span class="n">out</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="sh">'</span><span class="s">(b h) n d -&gt; b n (h d)</span><span class="sh">'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">)</span> <span class="c1"># [2, 4096, 320]
</span>    
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="compviss-stable-diffusion">CompVis’s Stable Diffusion</h3>

<p>In CompVis library, the training parameters are packed in config <code class="language-plaintext highlighter-rouge">yaml</code> files in the <code class="language-plaintext highlighter-rouge">configs</code> folder, and the training script is in <code class="language-plaintext highlighter-rouge">train.py</code>.
The training method uses a <code class="language-plaintext highlighter-rouge">Trainer</code> class which is a wrapper of PyTorch Lightning’s <code class="language-plaintext highlighter-rouge">Trainer</code> class (refer to <a href="https://lightning.ai/docs/pytorch/stable/common/trainer.html" rel="external nofollow noopener" target="_blank">here</a>).</p>

<blockquote class="block-tip">
  <p><strong>Lightning Trainer</strong></p>

  <p>The Lightning Trainer does much more than just “training”. Under the hood, it handles all loop details for you, some examples include:</p>
  <ol>
    <li>Automatically enabling/disabling grads</li>
    <li>Running the training, validation and test dataloaders</li>
    <li>Calling the Callbacks at the appropriate times</li>
    <li>Putting batches and computations on the correct devices</li>
  </ol>
</blockquote>

<p>Here’s the pseudocode for what the trainer does under the hood (showing the train loop only)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># enable grads
</span><span class="n">torch</span><span class="p">.</span><span class="nf">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="c1"># calls hooks like this one
</span>    <span class="nf">on_train_batch_start</span><span class="p">()</span>

    <span class="c1"># train step
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="c1"># clear gradients
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1"># backward
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># update parameters
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div></div>

<p>In the config file, we can find the paths to the components of the model, such as the VAE, UNet, and scheduler. For example, in <code class="language-plaintext highlighter-rouge">configs/latent-diffusion/celebahq-ldm-vq-4.yaml</code>, these models are defined in the <code class="language-plaintext highlighter-rouge">target</code> field with the corresponding paths and training parameters.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">model</span><span class="pi">:</span>
  <span class="na">base_learning_rate</span><span class="pi">:</span> <span class="s">2.0e-06</span>
  <span class="na">target</span><span class="pi">:</span> <span class="s">ldm.models.diffusion.ddpm.LatentDiffusion</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">linear_start</span><span class="pi">:</span> <span class="m">0.0015</span>
    <span class="na">linear_end</span><span class="pi">:</span> <span class="m">0.0195</span>
    <span class="na">num_timesteps_cond</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">log_every_t</span><span class="pi">:</span> <span class="m">200</span>
    <span class="na">timesteps</span><span class="pi">:</span> <span class="m">1000</span>
    <span class="na">first_stage_key</span><span class="pi">:</span> <span class="s">image</span>
    <span class="na">image_size</span><span class="pi">:</span> <span class="m">64</span>
    <span class="na">channels</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">monitor</span><span class="pi">:</span> <span class="s">val/loss_simple_ema</span>

    <span class="na">unet_config</span><span class="pi">:</span>
      <span class="na">target</span><span class="pi">:</span> <span class="s">ldm.modules.diffusionmodules.openaimodel.UNetModel</span>
      <span class="na">params</span><span class="pi">:</span>
        <span class="na">image_size</span><span class="pi">:</span> <span class="m">64</span>
        <span class="na">in_channels</span><span class="pi">:</span> <span class="m">3</span>
        <span class="na">out_channels</span><span class="pi">:</span> <span class="m">3</span>
        <span class="na">model_channels</span><span class="pi">:</span> <span class="m">224</span>
        <span class="na">attention_resolutions</span><span class="pi">:</span>
        <span class="c1"># note: this isn\t actually the resolution but</span>
        <span class="c1"># the downsampling factor, i.e. this corresnponds to</span>
        <span class="c1"># attention on spatial resolution 8,16,32, as the</span>
        <span class="c1"># spatial reolution of the latents is 64 for f4</span>
        <span class="pi">-</span> <span class="m">8</span>
        <span class="pi">-</span> <span class="m">4</span>
        <span class="pi">-</span> <span class="m">2</span>
        <span class="na">num_res_blocks</span><span class="pi">:</span> <span class="m">2</span>
        <span class="na">channel_mult</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="m">1</span>
        <span class="pi">-</span> <span class="m">2</span>
        <span class="pi">-</span> <span class="m">3</span>
        <span class="pi">-</span> <span class="m">4</span>
        <span class="na">num_head_channels</span><span class="pi">:</span> <span class="m">32</span>
    <span class="na">first_stage_config</span><span class="pi">:</span>
      <span class="na">target</span><span class="pi">:</span> <span class="s">ldm.models.autoencoder.VQModelInterface</span>
      <span class="na">params</span><span class="pi">:</span>
        <span class="na">embed_dim</span><span class="pi">:</span> <span class="m">3</span>
        <span class="na">n_embed</span><span class="pi">:</span> <span class="m">8192</span>
        <span class="na">ckpt_path</span><span class="pi">:</span> <span class="s">models/first_stage_models/vq-f4/model.ckpt</span>
        <span class="na">ddconfig</span><span class="pi">:</span>
          <span class="na">double_z</span><span class="pi">:</span> <span class="kc">false</span>
          <span class="na">z_channels</span><span class="pi">:</span> <span class="m">3</span>
          <span class="na">resolution</span><span class="pi">:</span> <span class="m">256</span>
          <span class="na">in_channels</span><span class="pi">:</span> <span class="m">3</span>
          <span class="na">out_ch</span><span class="pi">:</span> <span class="m">3</span>
          <span class="na">ch</span><span class="pi">:</span> <span class="m">128</span>
          <span class="na">ch_mult</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="m">1</span>
          <span class="pi">-</span> <span class="m">2</span>
          <span class="pi">-</span> <span class="m">4</span>
          <span class="na">num_res_blocks</span><span class="pi">:</span> <span class="m">2</span>
          <span class="na">attn_resolutions</span><span class="pi">:</span> <span class="pi">[]</span>
          <span class="na">dropout</span><span class="pi">:</span> <span class="m">0.0</span>
        <span class="na">lossconfig</span><span class="pi">:</span>
          <span class="na">target</span><span class="pi">:</span> <span class="s">torch.nn.Identity</span>
    <span class="na">cond_stage_config</span><span class="pi">:</span> <span class="s">__is_unconditional__</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">target</span><span class="pi">:</span> <span class="s">main.DataModuleFromConfig</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">batch_size</span><span class="pi">:</span> <span class="m">48</span>
    <span class="na">num_workers</span><span class="pi">:</span> <span class="m">5</span>
    <span class="na">wrap</span><span class="pi">:</span> <span class="kc">false</span>
    <span class="na">train</span><span class="pi">:</span>
      <span class="na">target</span><span class="pi">:</span> <span class="s">taming.data.faceshq.CelebAHQTrain</span>
      <span class="na">params</span><span class="pi">:</span>
        <span class="na">size</span><span class="pi">:</span> <span class="m">256</span>
    <span class="na">validation</span><span class="pi">:</span>
      <span class="na">target</span><span class="pi">:</span> <span class="s">taming.data.faceshq.CelebAHQValidation</span>
      <span class="na">params</span><span class="pi">:</span>
        <span class="na">size</span><span class="pi">:</span> <span class="m">256</span>


<span class="na">lightning</span><span class="pi">:</span>
  <span class="na">callbacks</span><span class="pi">:</span>
    <span class="na">image_logger</span><span class="pi">:</span>
      <span class="na">target</span><span class="pi">:</span> <span class="s">main.ImageLogger</span>
      <span class="na">params</span><span class="pi">:</span>
        <span class="na">batch_frequency</span><span class="pi">:</span> <span class="m">5000</span>
        <span class="na">max_images</span><span class="pi">:</span> <span class="m">8</span>
        <span class="na">increase_log_steps</span><span class="pi">:</span> <span class="s">False</span>

  <span class="na">trainer</span><span class="pi">:</span>
    <span class="na">benchmark</span><span class="pi">:</span> <span class="s">True</span>
</code></pre></div></div>

<p><strong>How to train the model?</strong></p>

<p>IMO, Lightning is difficult to read and understand. I found this <a href="https://www.reddit.com/r/MachineLearning/comments/vovp8q/p_an_elegant_and_strong_pytorch_trainer/" rel="external nofollow noopener" target="_blank">post</a> in Reddit, saying that the path of just simple <code class="language-plaintext highlighter-rouge">training loop</code> function (it’s suck)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Trainer.fit() -&gt; Trainer._fit_impl() -&gt; Trainer._run() -&gt; Trainer._run_stage() -&gt; Trainer._run_train() -&gt; FitLoop.run() -&gt; FitLoop.advance() -&gt; TrainingEpochLoop.run() -&gt; TrainingEpochLoop.advance() -&gt; TrainingBatchLoop.run() -&gt; TrainingBatchLoop.advance() -&gt; OptimizerLoop.run() -&gt; OptimizerLoop.advance() -&gt; OptimizerLoop._run_optimization() -&gt; OptimizerLoop._make_closure() -&gt; OptimizerLoop._make_step_fn()
</code></pre></div></div>

<p>The training procedure is hidden in the class <code class="language-plaintext highlighter-rouge">LatentDiffusion</code> in <code class="language-plaintext highlighter-rouge">ldm/models/diffusion/ddpm.py</code>, function <code class="language-plaintext highlighter-rouge">training_step</code> (refer to this <a href="https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/models/diffusion/ddpm.py#L342" rel="external nofollow noopener" target="_blank">line</a>).
More specifically, the forward pass as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># convert images to latent space
</span><span class="n">encoder_posterior</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode_first_stage</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_first_stage_encoding</span><span class="p">(</span><span class="n">encoder_posterior</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

<span class="c1"># get conditioning
</span><span class="n">c</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_learned_conditioning</span><span class="p">(</span><span class="n">cond_key</span><span class="p">)</span>

<span class="c1"># random timestep
</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

<span class="c1"># add noise
</span><span class="n">noise</span> <span class="o">=</span> <span class="nf">default</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">))</span>

<span class="c1"># forward diffusion
# x_start is the input clean image
</span><span class="n">x_noisy</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="o">=</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>

<span class="c1"># apply model, backward diffusion
</span><span class="n">model_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">apply_model</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">cond</span><span class="p">)</span>

<span class="c1"># choose type of target, there are two types of output of the model, image or noise
# in the default setting of Latent Diffusion Models, the output is the epsilon rather than the image
</span><span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">parameterization</span> <span class="o">==</span> <span class="sh">"</span><span class="s">x0</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">x_start</span>
<span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">parameterization</span> <span class="o">==</span> <span class="sh">"</span><span class="s">eps</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">noise</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">()</span>

<span class="c1"># calculate loss
</span><span class="n">loss_simple</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_loss</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">mean</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">loss_dict</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">/loss_simple</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss_simple</span><span class="p">.</span><span class="nf">mean</span><span class="p">()})</span>

<span class="n">logvar_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logvar</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_simple</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logvar_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">logvar_t</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">l_simple_weight</span> <span class="o">*</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

<span class="n">loss_vlb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_loss</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">loss_vlb</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lvlb_weights</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_vlb</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">original_elbo_weight</span> <span class="o">*</span> <span class="n">loss_vlb</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="example">Example</h3>

<p>In the following, I will provide a simple code using CompVis’s Stable Diffusion for Textual Inversion, which is already implemented in HuggingFace’s Diffusers <a href="https://github.com/huggingface/diffusers/blob/7bfc1ee1b2cb0961ff111f50a9d096816e4dd921/examples/textual_inversion/textual_inversion.py" rel="external nofollow noopener" target="_blank">here</a>.</p>

<p>The full script including data can be found here <a href="https://github.com/tuananhbui89/diffusion_demo/tree/main/textual_inversion" rel="external nofollow noopener" target="_blank">https://github.com/tuananhbui89/diffusion_demo/tree/main/textual_inversion</a></p>

<p>It is a worth noting that in the original Textual Inversion, the final goal is to obtain a special token (e.g., <code class="language-plaintext highlighter-rouge">sks dog</code>) that serves two purposes: (1) it is associated to the visual representation of personal data, and (2) it is in text form so that users can easily use it to generate new images. To do that, in the original implementation, the original embedding matrix is replaced by a new one, however, in my implementation, I skip this step and directly optimize the embedding vector.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_inverse</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">train_data_dir</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Given a model and a set of reference images, learn an embedding vector that will generate an image similar to the reference images.

    Args:
        model: the model to be trained
        sampler: the sampler to be used for sampling
        train_data_dir: the reference images to be used for training
        args: the arguments for training

    Returns:
        emb: the learned embedding vector
    </span><span class="sh">"""</span>

    <span class="c1"># create a textual embedding variable to optimize
</span>    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">a photo of </span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">concept</span><span class="si">}</span><span class="sh">'</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_learned_conditioning</span><span class="p">([</span><span class="n">prompt</span><span class="p">])</span>
    <span class="n">org_emb</span> <span class="o">=</span> <span class="n">emb</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># create an optimizer to optimize the prompt
</span>    <span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">emb</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Dataset and DataLoaders creation:
</span>    <span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">PreprocessImage</span><span class="p">(</span>
        <span class="n">data_root</span><span class="o">=</span><span class="n">train_data_dir</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">resolution</span><span class="p">,</span>
        <span class="n">repeats</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">repeats</span><span class="p">,</span>
        <span class="n">center_crop</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">center_crop</span><span class="p">,</span>
        <span class="nb">set</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">dataloader_num_workers</span>
    <span class="p">)</span>    
    
    <span class="n">fixed_start_code</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># create a lambda function for cleaner use of sampling code (only denoising till time step t)
</span>    <span class="n">quick_sample_till_t</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">cond</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="nf">sample_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span>
                                                                <span class="n">cond</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">ddim_steps</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">ddim_eta</span><span class="p">,</span>
                                                                <span class="n">start_code</span><span class="o">=</span><span class="n">code</span><span class="p">,</span> <span class="n">till_T</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="c1"># create a function to decode and save the image
</span>    <span class="k">def</span> <span class="nf">decode_and_save_image</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">decode_first_stage</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">((</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">/</span><span class="mf">2.0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">'</span><span class="s">b c h w -&gt; b h w c</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">fromarray</span><span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span><span class="o">*</span><span class="mi">255</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
    
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">evaluation_folder</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">evaluation_folder/textual_inversion</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">evaluation_folder/textual_inversion/</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">concept</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">models_path</span><span class="si">}</span><span class="s">/embedding_textual_inversion</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># train the embedding
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">opt</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

            <span class="c1"># Convert images to latent space
</span>            <span class="n">batch_images</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">encoder_posterior</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode_first_stage</span><span class="p">(</span><span class="n">batch_images</span><span class="p">)</span>
            <span class="n">batch_z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_first_stage_encoding</span><span class="p">(</span><span class="n">encoder_posterior</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

            <span class="c1"># get conditioning - SKIP because in this case, it is the trainable embedding vector
</span>            <span class="n">cond</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">repeat_interleave</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">batch_z</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># random timestep
</span>            <span class="n">t_enc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">ddim_steps</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">long</span><span class="p">()</span>

            <span class="c1"># time step from 1000 to 0 (0 being good)
</span>            <span class="n">og_num</span> <span class="o">=</span> <span class="nf">round</span><span class="p">((</span><span class="nf">int</span><span class="p">(</span><span class="n">t_enc</span><span class="p">)</span><span class="o">/</span><span class="n">args</span><span class="p">.</span><span class="n">ddim_steps</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>
            <span class="n">og_num_lim</span> <span class="o">=</span> <span class="nf">round</span><span class="p">((</span><span class="nf">int</span><span class="p">(</span><span class="n">t_enc</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">args</span><span class="p">.</span><span class="n">ddim_steps</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>

            <span class="n">t_enc_ddpm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">og_num</span><span class="p">,</span> <span class="n">og_num_lim</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_z</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">device</span><span class="o">=</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># add noise
</span>            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">batch_z</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="p">.</span><span class="n">noise_scale</span>

            <span class="c1"># forward diffusion
</span>            <span class="n">x_noisy</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="o">=</span><span class="n">batch_z</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_enc_ddpm</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>

            <span class="c1"># backward diffusion
</span>            <span class="n">model_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">apply_model</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">t_enc_ddpm</span><span class="p">,</span> <span class="n">cond</span><span class="p">)</span>

            <span class="c1"># calculate loss
</span>            <span class="c1"># in the default setting of Latent Diffusion Models, the output is the epsilon rather than the image
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

            <span class="c1"># optimize
</span>            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Batch: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="c1"># inference with the learned embedding
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

            <span class="n">z_r_till_T</span> <span class="o">=</span> <span class="nf">quick_sample_till_t</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">args</span><span class="p">.</span><span class="n">start_guidance</span><span class="p">,</span> <span class="n">fixed_start_code</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">ddim_steps</span><span class="p">))</span>
            <span class="nf">decode_and_save_image</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_r_till_T</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">evaluation_folder/textual_inversion/</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">concept</span><span class="si">}</span><span class="s">/gen_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">.png</span><span class="sh">'</span><span class="p">)</span>

            <span class="n">z_r_till_T</span> <span class="o">=</span> <span class="nf">quick_sample_till_t</span><span class="p">(</span><span class="n">org_emb</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">args</span><span class="p">.</span><span class="n">start_guidance</span><span class="p">,</span> <span class="n">fixed_start_code</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">ddim_steps</span><span class="p">))</span>
            <span class="nf">decode_and_save_image</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_r_till_T</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">evaluation_folder/textual_inversion/</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">concept</span><span class="si">}</span><span class="s">/gen_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">_original.png</span><span class="sh">'</span><span class="p">)</span>

            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">models_path</span><span class="si">}</span><span class="s">/embedding_textual_inversion/emb_</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">concept</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">.pt</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">emb</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
</code></pre></div></div>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/diffusion-foundation/">Foundation of Diffusion Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nlp-foundation/">LLM Series - Part 1 - Important Concepts in NLP</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/thoughts/">Random Thoughts and Notes</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/erasing-concepts/">Fake Taylor Swift and the Adversarial Game of Concept Erasure and Injection</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/unlearn-llms/">Unlearning LLMs</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tuan-Anh  Bui. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7KGSMMS9MS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-7KGSMMS9MS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
